Received 8 September 2018; revised 27 February 2019; accepted 27 February 2019. Date of publication 5 March 2019; date of current version 16 September 2021. Digital Object Identiﬁer 10.1109/TETC.2019.2902661
Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach
JIADAI WANG , (Student Member, IEEE), LEI ZHAO , (Student Member, IEEE), JIAJIA LIU , (Senior Member, IEEE), AND NEI KATO , (Fellow, IEEE)
J. Wang is with the State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an 710071, China L. Zhao is with the Department of Electrical and Computer Engineering, University of Victoria, Victoria, BC V8W 3P6, Canada J. Liu is with the School of Cybersecurity, Northwestern Polytechnical University, Xi’an 710072, China N. Kato is with the Graduate School of Information Sciences, Tohoku University, Aobayama 6-3-09, Sendai 980-8579, Japan CORRESPONDING AUTHOR: J. LIU (liujiajia@nwpu.edu.cn)
ABSTRACT The development of mobile devices with improving communication and perceptual capabilities has brought about a proliferation of numerous complex and computation-intensive mobile applications. Mobile devices with limited resources face more severe capacity constraints than ever before. As a new concept of network architecture and an extension of cloud computing, Mobile Edge Computing (MEC) seems to be a promising solution to meet this emerging challenge. However, MEC also has some limitations, such as the high cost of infrastructure deployment and maintenance, as well as the severe pressure that the complex and mutative edge computing environment brings to MEC servers. At this point, how to allocate computing resources and network resources rationally to satisfy the requirements of mobile devices under the changeable MEC conditions has become a great aporia. To combat this issue, we propose a smart, Deep Reinforcement Learning based Resource Allocation (DRLRA) scheme, which can allocate computing and network resources adaptively, reduce the average service time and balance the use of resources under varying MEC environment. Experimental results show that the proposed DRLRA performs better than the traditional OSPF algorithm in the mutative MEC conditions.
INDEX TERMS Mobile edge computing, resource allocation, deep reinforcement learning

I. INTRODUCTION In recent years, with the development of intelligent mobile devices in smart home, connected vehicles, health monitoring and other scenarios, smart phone, smart bracelet, intelligent camera, virtual/augmented reality glasses and so on have been widely used in our daily life [1]–[3]. The communication and perception capabilities of these mobile devices are gradually improving, resulting in a proliferation of numerous complex and computation-intensive mobile applications, such as voice control, gesture recognition, 3D modeling, natural language processing and interactive games [4], [5], some of which can also be characterized by timecritical [6]. Mobile devices with limited resources face more severe capacity constraints than ever before, while running all applications on central cloud can easily lead to serious network congestion and performance degradation. At this

time, as a new concept of network architecture and an extension of cloud computing to edge networks, Mobile Edge Computing (MEC) pushes computing and storage resources to the proximity of mobile devices, as well as deploys applications in distributed mobile edge servers to provide a variety of computation-intensive and time-critical application services to mobile devices [7]. In this way, MEC architecture can not only alleviate the congestion of the core network, but also greatly improve the quality of computing experience by meeting the strict requirement of response delay. It seems to be a promising solution to the emerging challenge posed by numerous mobile devices.
MEC architecture has great beneﬁts in addressing the proliferation of mobile devices and mobile applications. However, there are also lots of limitations [8]. First, because of the high cost of infrastructure deployment and maintenance,

VOLUME 9, NO. 3, JULY-SEPT. 2021

2168-6750 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tps://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1529

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

it is impractical to deploy powerful MEC servers with base stations in edge network quite densely. Second, although the computing and storage capacities of MEC servers might be substantially higher than those of mobile devices, only a limited number of applications can be deployed on one MEC server to provide services for multitudinous mobile devices [9]. Last but not least, human activity is changeable in daily life, causing the diverse requirements of mobile devices, the varying situation of the request generation districts and the changing quantity of the overall requests, leading to the instability of the MEC environment. The burst request volume and mutative MEC environment can also bring the sudden increased computation load to MEC servers as well as the serious load imbalance among them, and the edge network links in certain regions will also become congested [10].
Due to the limitation of MEC architecture, how to allocate computing and network resources adaptively under the changeable MEC conditions has become a great challenge. Most of the existing studies focus on the task and computation ofﬂoading [9], the optimization of the application placement [11], the MEC computing resource allocation[12] and the energy consumption reducing [13], [14]. Although these works pave the way for the research of MEC and study MEC in different aspects, they have a common limitation: they do not consider the adaptability of resource allocation methods in the mutative MEC environment. When the request generation districts as well as the requirements of mobile devices change dramatically, without considering the varying MEC conditions will lead to a signiﬁcant impact on routing and computing load distribution [15].
Therefore, we propose a Deep Reinforcement Learning based Resource Allocation (DRLRA) algorithm, which can allocate computing and network resources adaptively under varying MEC conditions, greatly reduce the average service time and balance the use of resources. Our main contributions are summarized as follows:
 Our proposed DRLRA algorithm includes routing selection (network resource allocation) and computing resource allocation to adapt to the mutative MEC environment. We adopt the Deep Reinforcement Learning (DRL) technology [16], which can acquire knowledge in the environment, improve policy to adjust to the changeable environment and make sequences of decisions, to realize the adaptive resource allocation.
 We apply the Software Deﬁned Network (SDN) [17] technology to our MEC architecture. The advantage of SDN is the logical centralized control of distributed network infrastructures. In our considered MEC architecture, a DRL unit is deployed on the SDN controller [18], which can provide an overview of the MEC environment state to control the routing of requests.
 We consider the computing delay on MEC servers and the routing delay in edge network as the service time. The minimization of the service time can be realized through our proposed DRLRA to better support all

kinds of advanced applications and provide mobile users with commendable experience.  We take into account the resource allocation balancing, including the computing resource balancing and the network resource balancing. Our goal is to maintain the stability of the MEC architecture under varying conditions with multiple requests from mobile devices, so as to improve its adaptability to the changeable MEC environment. The rest of this paper is organized as follows: We review the related work in Section II. Then, we depict the system model in Section III, followed by the problem deﬁnition and formulation in Section IV. In Section V, we present the DRL based algorithm framework. Furthermore, in Section VI, we describe the proposed DRLRA algorithm detailedly. Performance evaluation is shown in Section VII compared with the classical OSPF algorithm. Finally, in Section VIII, we conclude this paper.
II. RELATED WORK With the increasing number of computation-intensive and time-critical mobile applications, mobile devices that possess limited resources face more capacity constraints than ever before, leading to the requirement for deploying various applications on relatively powerful MEC servers. However, only a limited number of applications can be deployed on MEC servers to serve mobile devices due to the high cost of deploying and maintaining dense infrastructures. Thus, resource allocation in MEC environment has become a hot research topic.
A. COMPUTATION OFFLOADING SCHEMES FOR MEC Considering the computation ofﬂoading aspect, in [19], the authors described the computation ofﬂoading system model and proposed an innovative ofﬂoading architecture for MEC. Chen et al. [20] studied the optimal task ofﬂoading problem in MEC with the aim of saving battery life of mobile devices and minimizing delay by transforming this problem into two sub-problems, i.e., task placement and resource allocation. Wang et al. [21] formulated computation ofﬂoading decision, physical resource block allocation and MEC computation resource allocation as an optimization problem. Guo et al. considered not only the resource allocation between mobile devices and MEC servers, but also the enormous computation resources in centralized cloud computing center. They studied the cloud-MEC collaborative computation ofﬂoading and proposed schemes based on approximation and game theory [22]. Moreover, some energy-efﬁcient resource allocation methods to minimize the mobile energy consumption under the constraint on computation latency were also emerged [12], [13].
B. MOBILE EDGE APPLICATION PLACEMENT Although MEC overcomes lots of obstacles of traditional cloud computing and can reduce response time signiﬁcantly, it is an intractable task to deploy applications effectively on

1530

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

MEC servers in edge networks to serve numerous mobile devices. Toward this end, Zhao et al. [11] investigated the optimal placement of virtual machine replica copies, which encapsulated the applications, to minimize the response time in edge networks and meet the various request demands among multiple applications. Similarly, Wang et al. [23] formulated a mathematical model to solve the problem of virtual machine placement and workload assignment for mobile cloud applications in a multi-application MEC scenario. What is more, to minimize the request completion time, the image placement in fog computing supported softwaredeﬁned embedded system was studied in [24], in which task images were laid in storage servers and computing process can be conducted on embedded devices or computing servers.
C. ADVANCED MACHINE LEARNING SOLUTIONS FOR RESOURCE ALLOCATION IN MEC
Traditional resource allocation methods are facing great challenges in meeting the increasing Quality of Service (QoS) [25] requirements of mobile devices. Therefore, some methods of applying machine learning to solve the resource allocation problem have emerged [26]. With cloud computing, Wang et al. [27] collected historical data to extract similarities among various scenarios using machine learning, thus the optimal or near optimal solutions of historical scenarios can be stored in advance. Moreover, Mao et al. [28] and Tang et al. [29] adopted deep learning technique to manage network resources, i.e., routing paths, because traditional routing protocols can not draw lessons from previous network anomalies. However, the network environment changes are innumerable, there will be a lot of unpredictable situations. Thus, simply using historical data as the training set might not be applicable to the varying network situation.
D. DISCUSSIONS In summary, most of the existing work does not take into account the inﬂuence of mutative MEC environment and also not have full investigation of the adaptability of resource allocation methods. Thus, when the MEC environment change dramatically, the burst request volume can bring sudden increased computation load to MEC servers, and the edge network links in certain regions will also become congested, leading to a signiﬁcant impact on routing and computing resource allocation. Accordingly, we take full use of the characteristics of DRL to propose a smart resource allocation algorithm, which can allocate computing and network resources adaptively under varying MEC conditions.
III. SYSTEM MODEL A. SDN-ENABLED MEC ARCHITECTURE In this section, we present the SDN-enabled MEC architecture as illustrated in Figure 1. Mobile devices with limited resources in smart home, connected vehicles, health monitoring and other scenarios have various computation-intensive

FIGURE 1. The SDN-enabled MEC architecture.
and time-critical applications with advanced functions, e.g., virtual reality, 3D modelling and feature recognition, leading to the urgent demand for further processing on the MEC Server (MECS). Thus, these mobile devices generate requests with diverse requirements to MECSs that are responsible for virtualizing resources to deploy various requisite applications, e.g., m1, m2, m3. Considering that the resources in each MECS are restricted, only a limited number of applications can be deployed on one MECS.
Requests generated by mobile devices will ﬁrst be assigned to their nearby MECS. If there is no required application, the requests will be routed to MECS hosting corresponding application. As shown in Figure 1, assume that MECS2, MECS3 and MECS5 hosting the application m1, the mobile device u1 within the service range of MECS1 requesting for m1 will be routed to one of the three MECSs for further processing. Therefore, in MEC architecture, the response time of the requests consists of three parts, i.e., the access delay of the requests to their nearby MECS, the routing delay among MECSs, and the computing delay in corresponding MECS. For the example above, the response time contains the u1’s access delay to MECS1, the routing delay for application m1 hosted in MECSi; i 2 2; 3; 5, and the processing delay in corresponding MECS. In this paper, we mainly take the routing delay and the processing delay into consideration as the service time.
The deployment and management of MECSs are costintensive. It is impractical to deploy applications massively on these MECSs to meet diverse requirements from mobile devices. Thus, requests should be well routed to MECS hosting corresponding applications in varying MEC environment. What is more, SDN technology is needed to

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1531

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

maintain infrastructures and realize more efﬁcient and intelligent edge network management. Using SDN, the logical centralized control of distributed network infrastructures can be achieved. The control plane of SDN makes decisions on how to transfer data among communication infrastructures, while the data plane carries on the data transmission.
In our considered SDN-enabled MEC architecture, a DRL unit running the proposed DRLRA algorithm is deployed on the SDN controller which provides centralized control and management functions to edge networks. First, the DRL unit receives a huge amount of data that represents the dynamic network states from the SDN protocol-enabled data plan via an application programming interface (API), e.g., OpenFlow [30] known as the dominant source southbound API available for SDN. Setting the data as the input of the DRL unit and updating the neural network training parameters after given rounds, the SDN controller can make intelligent routing decisions based on the output of the DRL unit for the underlying SDN protocol-enabled edge network.
B. SERVICE MODEL The considered edge network is modeled by a graph GðV; EÞ, where V ¼ fvi; i ¼ 1; 2; . . . ; jVjg is the set of MECSs, and E ¼ fei; i ¼ 1; 2; . . . ; jEjg is the set of data links between each MECS pair. The application requested by numerous mobile devices is in set M ¼ fmi; i ¼ 1; 2; . . . jMjg. When requests aggregated by MECS w arrive at MECS v hosting corresponding application m, Fwm;v is used to denote the computing resource requirement of requests. Moreover, the alteration of human activity in daily life, the changing requirement of mobile devices and the varying situation of the request generation districts may lead to the burst request volume and mutative MEC environment. Thus, various mobile devices accessed in their nearby MECS will generate requests for applications deployed on MECS at different rates, leading to the difference of computing load on jVj MECSs.
IV. PROBLEM DEFINITION AND FORMULATION We have described in Section III that the service time contains the request routing delay in edge network and the data processing delay in corresponding MECS hosting required applications. In this paper, we focus on designing a smart resource allocation algorithm for MEC by taking network resource and computing resource into consideration. We aim to minimize the average service time of all requests generated by mobile devices distributed in different districts as well as balance the computing load on each MECS and network load on data links distinctly. Therefore, the QoS and the adaptability of MEC architecture will all be increased.
A. PROBLEM DEFINITION Edge Network Routing Delay. When requests generated by mobile devices access the nearby MECS without their required application, they will be routed through edge network to other MECSs hosting corresponding applications.

Let Bmw;v represents the data size of aggregated requests in MECS w that demand the service of application m on MECS v. The edge network routing delay for requests within MECS w’s service region needing application m deployed on MECS v can be denoted as

twm;v

¼

P
ei 2E

Bmw;v m

Iwei;v;

8w; v 2 V ; 8m 2 M; 8Iwei;v 2 f0; 1g;

(1)

where Iwei;v indicates whether link ei 2 E in the routing path of requests that are aggregated by MECS w and are routed to MECS v. m is the edge network routing capacity.
Data Processing Delay. After routing to the destination
MECS, requests can be served on MECS with relatively
powerful processing capacity than mobile devices. Note that Fwm;v denotes the computing requirement of requests aggregated by MECS w that need the service of application m and
further processing on MECS v. The computing requirement
can be estimated as the required CPU cycles. Then, the data processing delay on MECS v can be deﬁned as

Twm;v

¼

Fwm;v c

;

8w; v 2 V; 8m 2 M;

(2)

where c is the allocated CPU capacity.

Variance of Network Resource Allocation. Various requ-

ests are routed to their destination MECSs. Thus, to mitigate

the network congestion and ensure successful transmission

for supporting multiple advanced functions of mobile devi-

ces, network resource allocation on data links should be bal-

anced. Let lneiet represents the network load on edge ei. Therefore, the variance of network resource allocation in

overall edge network can be represented by the disparity of

network load and denoted as

P
ei2E

 lneiet

À

P
ei 2E jEj

lneiet

2

varðlnetÞ ¼

jEj

:

(3)

Variance of Computing Resource Allocation. MECSs

hosting applications are responsible for data processing and

request serving. Since the requests may choose different rout-

ing paths to their destination MECS, the computing load on

each MECS will also be different. Let lcvp denotes the computing load on MECS v. Then, we use the variance of com-

puting load to express the load differences among MECSs

and the performance of computing resource allocation. It can

be calculated as

P
v2V

 lcvp

À

P
v2V jV j

lcvp

2

varðlcpÞ ¼

jV j

:

(4)

B. PROBLEM FORMULATION Average Service Time Minimization. As we illustrated above, the service time contains the request routing delay in edge network and the data processing delay in corresponding

1532

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

MECS hosting required applications. To improve QoS and
better support for advanced functions of mobile devices, we
aim to minimize the average service time of all requests gen-
erated by mobile devices distributed in different districts in an adaptive way. As we illustrated above, twm;v and Twm;v denote the edge network routing delay and the data processing delay
respectively. Then, the service time for aggregated requests in MECS w that need application m’s service on MECS v can be deﬁned as twm;v þ Twm;v. Thus, the average service time minimization problem can be formulated as

min

P
m2M

P
w2V

P
v2V

ðtwm;v

þ

Twm;vÞ

;

(5)

Ntotal

where Ntotal denotes the total number of requests generated by mobile devices in edge network.
Resource Allocation Balancing. Requests generated by numerous mobile devices will be routed to corresponding MECS for service, and the request generation rate varies in different districts, leading to the intensive pressure on network links and MECSs. Excessive load on selected network links and MECSs may lead to long routing delay and processing delay, resulting in an increase in the service time. Thus, balancing the network load among data links and the computing load among MECSs to achieve balanced allocation of resources will be distinctly important to reduce the average service time and alleviate the MECS’s service pressure. Subsequently, the adaptability of the MEC architecture will also be improved. The resource allocation balancing problem is to minimize the variance of network load on each link, i.e., minvarðlnetÞ, and the variance of computing load on each MECS, i.e., minvarðlcpÞ.

V. DEEP REINFORCEMENT LEARNING FRAMEWORK In this section, we ﬁrst introduce the reinforcement learning method and several strategies adopted in our proposed resource allocation algorithm DRLRA. Then, we present the classical Q-learning and the emerging deep Q network. The latter, which is a representative framework for DRL and our adopted algorithm framework, integrates the neural network into Q-learning to combine the perceptual ability of deep learning with the decision ability of reinforcement learning.

A. REINFORCEMENT LEARNING Reinforcement learning (RL) is a branch of machine learning, which focuses on acquiring knowledge in the environment, improving action policy to adapt to the environment, and making sequences of decisions [31]. RL system assumes that there is an agent implements actions in environment. By exploring environment and receiving feedback, without massive labeled data, RL system can form an adaptive model. Intuitively, RL is a process in which the agent constantly interacts with the environment, makes decision sequence and strengthens its decisionmaking ability. In our considered scenario, through the continuous interaction with the MEC environment, the

FIGURE 2. The illustration of reinforcement learning.

agent can take actions and obtain corresponding rewards.

Besides, its goal is to maximize the cumulative rewards.

As shown in Figure 2, for each episode, ﬁrst, at each

step t, the agent acquires the observation of the MEC envi-

ronment, that is, the state st. Then, the agent makes an action at based on a certain policy p, obtains the corre-

sponding reward rt, and enters a new state stþ1. Subse-

quently, the policy is updated based on the reward given

by the environment. The following are explanations of the

above reinforcement learning procedure.

 Episode is the training process of the agent. In this

paper, the episode is the resource allocation process

from MECSs that aggregate requests to the destination

MECSs with corresponding applications. It can be rep-

resented as a sequence of states, actions and rewards,

i.e., ½s1; a1; r1; s2; . . . ; snÀ1; anÀ1; rnÀ1; sn, where s1 is

the initial state in each episode and sn is the terminal

state.

 Policy p, which maps states to a probability distribution

over the actions, represents the agent’s behavior and

directs the agent how to choose action.

 Reward rt is used to feedback the agent’s behavior at step t. A favorable action will be given a positive

reward and vice versa.



RCtu¼muPlatTii¼vtegiÀrtreiw; gar2d

Rt ½0; 1,

can be where g

formulated as is the discount

rate that represents the impact of future reward on cur-

rent cumulative reward, and T is the terminal step of

the episode. Note that the agent can obtain one cumula-

tive reward after each episode.

 Action value function (Q function) is used in lots of

reinforcement learning algorithms such as Q-learning

that will be mentioned in the following. Action value

Qðst; atÞ denotes the expectation of the cumulative

reward after taking action at in state st according to the policy p.

B. Q-LEARNING Q-learning is one of the classical reinforcement learning
algorithms and a model-free learning method [32], which uses the action value Qðst; atÞ stored in Q-table to choose the action according to current state st and policy p. The target action value can be formulated as

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1533

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

FIGURE 3. A detailed framework of deep Q network.

Qðst; atÞ ¼ rt þ g max Qðstþ1; atþ1Þ:

(6)

atþ1

At the beginning of each episode in Q-learning, the envi-
ronment state s will be initialized. For each step t in the episode, the action at should be ﬁrst selected according to current state st and policy p. Then, the corresponding reward rt and the next state stþ1 can be obtained. Subsequently, the action value Qðst; atÞ should be updated as

Qðst; atÞ Qðst; atÞ

þ bðrt þ g max Qðstþ1; atþ1Þ À Qðst; atÞÞ;

(7)

atþ1

where b 2 ð0; 1Þ is the learning rate. This procedure will be repeated until the terminal state is reached.

C. DEEP Q NETWORK Traditional Q-learning uses Q-table to store action values, while in our considered scenario, the state of the MEC environment is complex and varied. Thus, it is impractical to use one table to store all action values and also time-consuming to search for the corresponding state frequently in a large table. Recent advances that combine deep learning with reinforcement learning have caused great excitement in the ﬁeld of machine learning, leading to the ﬂourish of DRL. As shown in Figure 3, Deep Q Network (DQN) is one of the representative and effective algorithm framework of DRL, which directly uses neural network with parameter v to approximate Q function and generate action values. The input of the neural network is the state s and the output is the

action value Qðs; a; vÞ. Without loss of accuracy, we use Q-value to represent action value Qðs; a; vÞ in the following.
After the Q-values are outputted by the neural network, in this paper, we use the "-greedy strategy to select the action. "-greedy strategy randomly extracts one action with a probability of " 2 ð0; 1Þ and choose action a ¼ arg maxat Qðst; at; vÞ with a probability of 1 À ". In this way, DRL agent can not only maximize rewards in known information, but also explore environment beyond what is currently known. Consequently, state-action pairs that are not presented in the sample can also be learned.
Neural network training requires the loss function optimization process, that is, to minimize the deviation between labels and output results, and the parameters of the neural network will be updated by backpropagation and gradient descent [33]. Neural network in DQN is no exception. The goal of DQN is to make the Q-value close to the target Q-value, and the Q-learning algorithm is used to provide the so-called label. In this paper, we adopt Mean Square Error (MSE) [34] as the loss function of neural network in DQN, which can be deﬁned as

LðvÞ ¼ E½ðr þ gmaxatþ1 Q0ðstþ1; atþ1; vÀÞ À Qðst; at; vÞÞ2;

(8)

where r þ gmaxatþ1 Q0ðstþ1; atþ1; vÀÞ is the target Q-value calculated by the result of target network with parameters vÀ, and Qðst; at; vÞ is the evaluation Q-value outputted by evaluation network with parameters v. As shown in Figure 3,
the target network is used to provide immovable labels and
improve the stability and convergence of training. The initial
parameters of target network are the same as evaluation network. However, v is updated every step, but vÀ is updated
every stationary C steps. Thus, the parameters update rate of
target network is slower than evaluation network.

VI. DEEP REINFORCEMENT LEARNING BASED RESOURCE ALLOCATION ALGORITHM
Due to the variation of the request generation rate and the change in the distribution of requests, smart resource allocation algorithm should consider the state of the MEC environment to make favorable decisions. Therefore, in this section, we propose a resource allocation algorithm DRLRA based on DQN framework to minimize the average service time and balance the resource allocation in an adaptive way.

A. TRAINING RL system can form an adaptive model by exploring environment and receiving feedback without massive labeled data. It just adapt to the changeable MEC environment that we considered. As described in Section V, traditional Qlearning uses Q-table to store Q values. When the state of the environment is complex and varied, it will be impractical to store all Q values in one table and also time-consuming to search the state in a large table frequently. Therefore, we adopt the above mentioned DQN framework to realize our

1534

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

DRLRA, which directly uses neural network to generate Q
value, that is, uses the MEC environment state as input and each possible action’s Q value as output. The training stage of our proposed algorithm is described in the following.
Three key elements of RL system that our DRLRA used, i.e., state, action and reward, should be ﬁrst deﬁned:
 State: The state of our proposed DRLRA consists of locations of requests generated for application m and aggre-
gated by MECS v within its service region. The state vector can be represented as s ¼ fpmv ; 8v 2 V; 8m 2 Mg.  Action: In our DRL system, the action consists of jVj Á jMj elements. The action vector can be given as a ¼ famv ; 8v 2 V; 8m 2 Mg, while amv denotes the action taken by the requests for application m aggre-
gated by MECS v. Reasonable action can be easily
selected from adjacent MECSs of each MECS.  Reward: In each step t, after taking a possible action at,
the DRL agent can acquire a certain reward, which needs to reﬂect the objective of our proposed smart resource allocation algorithm DRLRA, that is minimizing the
average service time and balancing the resource alloca-
tion of both network resource and computing resource.
Thus, these two objectives should be considered in the
reward that represents the environment feedback. We
formulate the reward that the DRL agent receives from the environment as r ¼ w1 Á T þ w2 Á bnet þ w3 Á bcp, where T denotes the service time of requests, bnet repre-
sents the network resource allocation balance degree in overall edge network, and bcp indicates the computing
resource allocation balance degree on all MECSs. Note that bnet and bcp are calculated by the variance of the transmission load on each network link, i.e., varðlnetÞ, and the variance of the computing load on each MECS, i.e., varðlcpÞ, respectively. In addition, wi; i 2 1; 2; 3 represents the weight of elements, i.e., T, bnet and bcp, in the
expression of the reward r.
As shown in Figure 3 and Algorithm 1, given the three key elements, i.e., state, action and reward, we ﬁrst initialize the experience replay memory D with certain capacity P as
well as the evaluation and target network with random parameters v and vÀ. The episode is the resource allocation
process from MECSs that aggregate requests to destination
MECSs with corresponding applications. For each episode k, we ﬁrst initialize state s. Then, for each step t, state st should be taken as the input of the evaluation network, and action at can be chosen according to the evaluation network’s output based on "-greedy strategy. After choosing action at, the certain reward rt and the next state stþ1 can be obtained by the predeﬁned deﬁnitions. We store transition ðst; at; rt; stþ1Þ in replay memory D to update the evaluation network.
In evaluation network updating Procedure 1, we ﬁrst randomly sample a minibatch of transition items ðsj; aj; rj; sjþ1Þ from replay memory D. Note that randomly select transition
items from the replay memory to train the neural network
can break the correlation between samples and improve

data utilization. Then, the evaluation Q value Qðsj; aj; vÞ and the target Q value gmaxajþ1 Q0ðsjþ1; ajþ1; vÀÞ will be calculated through evaluation network and target network respectively. Subsequently, the parameters v of evaluation
network can be updated by the MSE loss function as described in Eq. (8), while the parameters vÀ of target net-
work should be updated every C steps. C is a certain factor
to control the target network update rate. Finally, the trained
evaluation network can be obtained through the iteration of
the step t in episode k, where t denotes the tth resource allo-
cation approach and k represents the kth resource allocation
procedure.

Algorithm 1. DRL Based Resource Allocation (DRLRA)

Algorithm

Require: Discount rate g, exploration rate ", replay mem-

ory capacity P

1: Initialize replay memory D to capacity P

2: Initialize evaluation network with parameters v 3: Initialize target network with parameters vÀ ¼ v

4: for each episode k do

5: Initialize state s1 6: for each step t do

7:

Generate random number h 2 ½0; 1

8:

if h < " then

9:

Randomly select an action at

10:

else

11:

Select at ¼ arg maxat Qðst; at; vÞ, where Q is

estimated by evaluation network

12:

end if

13:

Execute action at in emulator

14:

Observe reward rt and new state stþ1

15:

Store transition ðst; at; rt; stþ1Þ in D

16:

Execute Procedure 1 for evaluation network

updating

17:

Each C steps reset vÀ ¼ v

18: end for

19: end for 20: Execute Procedure 2 to ﬁnd the routing path.

Procedure 1. Evaluation Network Updating Procedure

1: Sample random minibatch of transitions ðsj; aj; rj; sjþ1Þ

from D

2: if episode terminates at stepjþ1 then

3: Set yj ¼ rj

4: else

5:

Set yj ¼ rj þ gmaxajþ1 Q0ðsjþ1; ajþ1; vÀÞ, where Q0 is

estimated by target network

6: end if

7: Execute gradient descent using MSE function ðyj À Q ðsj; aj; vÞÞ2 with respect to the evaluation network

parameters v

It should be noticed that we aim to minimize the average service time and balance the resource allocation. These two objectives are all reﬂected by the reward we predeﬁned. That is, if DRL agent takes a good action in its current environment state,

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1535

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

such as all requests on MECSs are closer to their destinations, or overall usage of network resource and computing resource becomes more balanced, the DRL agent will gain a positive feedback and vice versa. What is more, when requests get access to their destination MECS with corresponding application m, the service time is also considered in the reward calculation.

B. RUNNING Our proposed DRLRA is running on DRL unit deployed on SDN controller. As shown in Procedure 2, in the ﬁrst place, the SDN controller collects the information of the entire edge network and updates the environment state s in time for DRL unit. For each resource allocation step t, the overall action at can be selected as at ¼ arg maxat Qðst; at; vÞ and will be taken for all requests aggregated on each MECS, followed by the observation of the reward rt and the new state stþ1. After that, the SDN controller will send control messages by the control plane function, e.g., OpenDayLight (ODL) and Open Network Operating System (ONOS) [35], to each MECS.
Procedure 2. Running Procedure Require: The trained evaluation network with parameters v 1: Obtain initial state s1 2: for each step t do 3: Select at ¼ arg maxat Qðst; at; vÞ 4: Execute action at in emulator 5: Observe reward rt and new state stþ1 6: end for
VII. PERFORMANCE EVALUATION A. EXPERIMENTAL SETTINGS Without loss of generality, we adopted a real-world online network topology from the Topology Zoo [36], i.e., NSFCNET in Beijing, China, as the edge network topology. A simulator in Python was developed to realize the DRLRA algorithm for smart resource allocation in varying MEC environment. Based on the characteristics of several in-lab testing applications described in [37], we set the average request data size within 20 KB and the request requirement of CPU cycles between 50 Megacycles and 1 Gigacycles. The computing ability of the MECS was set between 1 and 4 GHz. The average data transmission rate was distributed between 250 and 1,000 Mbps according to the work by Rimal et al. [38]. The deep neural network structure in DRL was set as three fully connected layers, and each hidden layer was set to 64 units. A classical routing strategy, i.e., Open Shortest Path First (OSPF) protocol [39], was also presented to be our benchmark. Both algorithms ran on a workstation with double Intel Xeon E5-2630 V4 2.2 GHz CPUs, 128 GB Random Access Memory (RAM), Nvidia Titan 12G GPU, and Ubuntu 14.04 64-bit operating system.
B. AVERAGE SERVICE TIME MINIMIZATION ANALYSIS In the following, we will analyze the impact of application deployed number, request aggregation districts number,

FIGURE 4. Performance comparison of the conventional OSPF and the proposed DRLRA under different application deployed number.
processing capacity of MECS, computing requirement of applications and data routing capacity on average service time, which comprises the request routing delay in edge network and the data processing delay in corresponding MECS hosting required applications.
From Figure 4 we can see that with the growing application deployed number (the number of MECSs where applications are placed), the average service time will decrease. On the one hand, the increase of the application deployed number will raise the number of MECSs that provide resources for services. Thus, the distance from different request generation districts to MECSs hosting corresponding applications will be reduced, leading to the signiﬁcantly alleviation of the MECSs’ computing pressure and the reduction of the computing delay. On the other hand, when the number of deployed applications increases to a certain extent, the routing delay and computing delay will all be greatly reduced, resulting in a continuous narrowing of the disparity between the two algorithms’ performance. However, in reality, applications are deployed with restricted edge resources due to that the deployment and maintenance of MECSs are cost-intensive. Obviously, our DRLRA algorithm is better than OSPF when application deployed number is not too much with limited edge resources.
As shown in Figure 5, the average service time keeps an upward tendency as the number of request aggregation districts increases. This is because that the rise of request aggregation districts will lead to the congestion of routing paths in edge network and the increase of computing load on MECSs. We can see that our algorithm is much better than OSPF when there are not too much request aggregation districts. It is mainly because at this time the computing load on MECS is not too heavy, the routing delay dominates the service time, and our scheme has a more intelligent routing strategy. With the gradual increase of request aggregation districts number, all routing paths become gradually congested and the computing load has not dominated the service time yet,

1536

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

FIGURE 5. Comparison of the average service time for OSPF and the proposed DRLRA with different number of request aggregation districts.
thus the disparity between the performance of our DRLRA and that of OSPF is narrowed. However, with the further increase of request aggregation districts, the computing delay gradually dominates the service time due to that our algorithm can allocate computing resources intelligently and reduce the data processing delay. Thus, in this time, the DRLRA’s performance can be more outstanding than that of OSPF.
We can clearly observe from Figure 6 that as the computing power of the MECS increases, the computing delay will be gradually decreased. Thus, the average service delay of requests will also be reduced. When the computing power is moderate, the performance of our algorithm can be greatly improved. The performance disparity between the two algorithms will be narrowed when the computing capacity becomes most powerful. Therefore, our algorithm is very

FIGURE 7. Impact of the computing requirement of applications on average service time of the OSPF and the proposed DRLRA.
practical in real situation where most MECSs have moderate processing capacity.
It can be easily obtained from Figure 7 that as the computing requirement of applications continues to go up, the pressure on MECSs becomes more and more severe, and the computing latency increases signiﬁcantly. When computing resource requirement is not too much, the average processing time of requests on MECS is relatively small. Nonetheless, with the increasing requirement of computing resources, the need for requests allocation balancing on MECSs will become more urgent. Thus, our smart resource allocation scheme will continue to highlight its superiority.
As shown in Figure 8, we also have conducted experiment aiming to illustrate the performance of the two algorithms in terms of the average service time under different network routing capabilities. It can be seen from Figure 8 that the

FIGURE 6. Impact of the MECS’s processing capacity on average service time of the conventional OSPF and the proposed DRLRA.

FIGURE 8. Performance comparison of conventional OSPF and the proposed DRLRA under different data routing capacities.

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1537

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

FIGURE 9. Comparison of the variance of network load for OSPF and the proposed DRLRA with different application deployed number.
improvement of data routing capacity can signiﬁcantly reduce the average service time. However, when confronting the increasing requests, routing capacity can not growth unlimitedly. Therefore, with the same routing capability, the proposed DRLRA algorithm can better reduce the average service time.
C. RESOURCE ALLOCATION BALANCING ANALYSIS In the following, we focus on the impact of application deployed number, request aggregation districts number, processing capacity of MECS to analyze the balance of resource allocation, which contains the network load balancing in overall edge network and the computing load balancing among all MECSs.
We conduct experiments to verify the problem of network load balancing in the routing process. We can see from Figure 9 that as the application deployed number increases, our smart resource allocation algorithm DRLRA can always obtain a stable variance of network load. This means that DRLRA will take full account of the load situation in the current network and make adaptive routing decisions in the process of routing path selection, so that the network resources can be used evenly, and the network congestion caused by the burst request volume in the routing process can also be effectively prevented. On the contrary, the performance of OSPF in balancing network load get into worse compared with our DRLRA algorithm when the MEC environment becoming more and more complicated with the increasing number of MECSs deploying various applications.
We have also conducted further experiments to verify the performance of our smart resource allocation algorithm in balancing computing resources with the increasing number of request aggregation districts. We can brieﬂy obtain that the growth of request aggregation districts will result in an increase of the number of requests received on each MECS hosting corresponding applications to provide the service,

FIGURE 10. Comparison of the variance of computing load for OSPF and the proposed DRLRA under different number of request aggregation districts.
leading to the decrease of the variance of computing load on all MECSs. As the number of request aggregation districts increases, the request’s requirement for computing resources becomes more and more complex. Our smart resource allocation algorithm is highly efﬁcient in balancing the allocation of computing resources for requests aggregated by MECSs. From Figure 10, we can see that this advantage become more distinct as the number of request aggregation districts arises.
Figure 11 illustrates that with the increasing processing capacity of MECS, the overall computing load becomes more balanced. That is mainly because we use the average service time of requests on MECSs as a measure of load balancing. Increased computing capacity will lead to a decrease in service time, resulting in a more balanced calculated computing
FIGURE 11. Comparison of the variance of computing load for OSPF and the proposed DRLRA with various processing capacity of MECS.

1538

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach
carefully considered. Based on our experimental results, it is better to set the minibatch size to 32 or 64 in the proposed DRLRA.

FIGURE 12. Convergence performance of the proposed DRLRA under different minibatch sizes.
load. We can obtain from Figure 11 that as the computing power increases, the proposed DRLRA can always get better performance than OSPF. This is mainly because that our approach is not only to select the nearest MECS as the destination for processing, but also to make a resource allocation decision based on the MEC environment and the average service time.
D. CONVERGENCE PERFORMANCE ANALYSIS We have carried out experiments to demonstrate the convergence performance of our DRL-based DRLRA algorithm. As we can see from Figure 12, at the beginning of the training process, the average service time with different minibatch sizes is high, because DRL agent has not learned enough information to make good decisions for resource allocation. A lot of unreasonable actions will be chosen, resulting in routing loops and network congestion. With the increase in the number of episode, the average service time decreases gradually until a relatively stable value is reached.
Figure 12 also shows the effect of minibatch size on the convergence performance of DRLRA. We set the minibatch size to 16, 32, 64 and 128 respectively in our experiments. As the minibatch size increases, the convergence speed of the DRLRA algorithm becomes faster. This is because we use the minibatch gradient descent method to complete the training process, where minibatch is used to determine the number of experience samples per training. Samples of very small size, such as minibatch size ¼ 16 in our simulation, will result in greater randomness and slow down the convergence speed of the DRL-based algorithm. When the minibatch size is too large, such as minibatch size ¼ 128, it is easy to make the gradient decreases to a single direction all the time, so that the algorithm might converge to the poor local optimal solution, leading to the ﬂuctuation of average service time. Therefore, the size of the minibatch needs to be

VIII. CONCLUSION In this paper, we have investigated the resource allocation issue including computing resource allocation and network resource allocation in mutative MEC environment. We have considered this issue in two aspects, i.e., average service time minimization and resource allocation balancing, and proposed a smart resource allocation algorithm DRLRA based on the emerging DRL technology. On the basis of the DRL’s capacity of learning adaptively in the environment and characteristic of making sequences of decisions, our algorithm can be more adaptable to the varying edge network conditions and more suitable for MEC environment with burst request volume. Extensive simulations have been conducted to evaluate the performance of DRLRA. The experimental results showed that compared with the classical OSPF algorithm under multiple conditions, our proposed DRLRA achieved much better performance. Therefore, it is obviously that DRL technology has great potential in smart resource allocation and deserves further study. Future work is in progress to try other DRL framework such as Double DQN (DDQN) and Actor-Critic Algorithm (A3C), as well as incorporate more environment details into the state representation.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foundation of China (61771374, 61771373, 61801360, and 61601357), in part by China 111 Project (B16037), in part by the Fundamental Research Fund for the Central Universities (JB171501, JB181506, JB181507, and JB181508), and in part by the Science and Technology Innovation Program (201809168CX9JC10).
REFERENCES
[1] I. Bisio, A. Delﬁno, F. Lavagetto, and A. Sciarrone, “Enabling IoT for inhome rehabilitation: Accelerometer signals classiﬁcation methods for activity and movement recognition,” IEEE Internet Things J., vol. 4, no. 1, pp. 135–146, Feb. 2017.
[2] W. Sun, J. Liu, and H. Zhang, “When smart wearables meet intelligent vehicles: Challenges and future directions,” IEEE Wireless Commun., vol. 24, no. 3, pp. 58–65, Jun. 2017.
[3] H. Guo, J. Liu, and L. Zhao, “Big data acquisition under failures in FiWi enhanced smart grid,” IEEE Trans. Emerging Topics Comput., 2017, doi: 10.1109/TETC.2017.2675911.
[4] Y. Mao, J. Zhang, S. Song, and K. B. Letaief, “Stochastic joint radio and computational resource management for multi-user mobile-edge computing systems,” IEEE Trans. Wireless Commun., vol. 16, no. 9, pp. 5994–6009, Sep. 2017.
[5] G. Araniti, I. Bisio, M. De Sanctis, A. Orsino, and J. Cosmas, “Multimedia content delivery for emerging 5G-satellite networks,” IEEE Trans. Broadcast., vol. 62, no. 1, pp. 10–23, Mar. 2016.
[6] H. Guo, J. Liu, J. Zhang, W. Sun, and N. Kato, “Mobile-edge computation ofﬂoading for ultradense IoT networks,” IEEE Internet Things J., vol. 5, no. 6, pp. 4977–4988, Dec. 2018, doi: 10.1109/JIOT.2018.2838584.
[7] Y. Mao, J. Zhang, and K. B. Letaief, “Dynamic computation ofﬂoading for mobile-edge computing with energy harvesting devices,” IEEE J. Sel. Areas Commun., vol. 34, no. 12, pp. 3590–3605, Dec. 2016.

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1539

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

[8] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey on mobile edge computing: The communication perspective,” IEEE Commun. Surveys Tuts., vol. 19, no. 4, pp. 2322–2358, Oct.–Dec. 2017.
[9] X. Sun and N. Ansari, “EdgeIoT: Mobile edge computing for the internet of things,” IEEE Commun. Mag., vol. 54, no. 12, pp. 22–29, Dec.
2016. [10] F. Tang, Z. M. Fadlullah, B. Mao, and N. Kato, “An intelligent trafﬁc load
prediction based adaptive channel assignment algorithm in SDN-IoT: A deep learning approach,” IEEE Internet Things J., vol. 5, no. 6, pp. 5141–5154, Dec. 2018, doi: 10.1109/JIOT.2018.2838574. [11] L. Zhao and J. Liu, “Optimal placement of virtual machines for supporting multiple applications in mobile edge networks,” IEEE Trans. Veh. Technol., vol. 67, no. 7, pp. 6533–6545, Jul. 2018. [12] C. You, K. Huang, H. Chae, and B.-H. Kim, “Energy-efﬁcient resource allocation for mobile-edge computation ofﬂoading,” IEEE Trans. Wireless Commun., vol. 16, no. 3, pp. 1397–1411, Mar. 2017. [13] A. Al-Shuwaili and O. Simeone, “Energy-efﬁcient resource allocation for mobile edge computing-based augmented reality applications,” IEEE Wireless Commun. Lett., vol. 6, no. 3, pp. 398–401, Jun. 2017. [14] H. Guo, J. Liu, Z. Fadlullah, and N. Kato, “On minimizing energy consumption in FiWi enhanced LTE-A HetNets,” IEEE Trans. Emerging Topics Comput., vol. 6, no. 4, pp. 579–591, Oct.–Dec. 2018, doi: 10.1109/ TETC.2016.2598478. [15] L. Zhao, J. Liu, Y. Shi, W. Sun, and H. Guo, “Optimal placement of virtual machines in mobile edge computing,” in Proc. IEEE Global Commun. Conf., 2017, pp. 1–6. [16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, 2015, Art. no. 529.
[17] K. Kaur, S. Garg, G. S. Aujla, N. Kumar, J. J. Rodrigues, and M. Guizani, “Edge computing in the industrial internet of things environment: Software-deﬁned-networks-based edge-cloud interplay,” IEEE Commun. Mag., vol. 56, no. 2, pp. 44–51, Feb. 2018.
[18] Z. A. Qazi, J. Lee, T. Jin, G. Bellala, M. Arndt, and G. Noubir, “Application-awareness in SDN,” ACM SIGCOMM Comput. Commun. Rev., vol. 43, no. 4, pp. 487–488, 2013.
[19] X. Wei, S. Wang, A. Zhou, J. Xu, S. Su, S. Kumar, and F. Yang, “MVR: An architecture for computation ofﬂoading in mobile edge computing,” in Proc. IEEE Int. Conf. Edge Comput., 2017, pp. 232–235.
[20] M. Chen and Y. Hao, “Task ofﬂoading for mobile edge computing in software deﬁned ultra-dense network,” IEEE J. Sel. Areas Commun., vol. 36, no. 3, pp. 587–597, Mar. 2018.
[21] C. Wang, F. R. Yu, C. Liang, Q. Chen, and L. Tang, “Joint computation ofﬂoading and interference management in wireless cellular networks with mobile edge computing,” IEEE Trans. Veh. Technol., vol. 66, no. 8, pp. 7432–7445, Aug. 2017.
[22] H. Guo and J. Liu, “Collaborative computation ofﬂoading for multi-access edge computing over ﬁber-wireless networks,” IEEE Trans. Veh. Technol., vol. 67, no. 5, pp. 4514–4526, May 2018.
[23] W. Wang, Y. Zhao, M. Tornatore, A. Gupta, J. Zhang, and B. Mukherjee, “Virtual machine placement and workload assignment for mobile edge computing,” in Proc. IEEE Int. Conf. Cloud Netw., 2017, pp. 1–6.
[24] D. Zeng, L. Gu, S. Guo, Z. Cheng, and S. Yu, “Joint optimization of task scheduling and image placement in fog computing supported software-deﬁned embedded system,” IEEE Trans. Comput., vol. 65, no. 12, pp. 3702–3712, Dec. 2016.
[25] Z. Fadlullah, H. Nishiyama, Y. Kawamoto, H. Ujikawa, K. Suzuki and N. Yoshimoto, “Cooperative QoS control scheme based on scheduling information in FiWi access network,” IEEE Trans. Emerging Topics Comput., vol. 1, no. 2, pp. 375–383, Dec. 2013.
[26] Z. Fadlullah, F. Tang, B. Mao, N. Kato, O. Akashi, T. Inoue, and K. Mizutani, “State-of-the-art deep learning: Evolving machine intelligence toward tomorrow’s intelligent network trafﬁc control systems,” IEEE Commun. Surveys Tuts., vol. 19, no. 4, pp. 2432–2455, Oct.–Dec. 2017.
[27] J.-B. Wang, J. Wang, Y. Wu, J.-Y. Wang, H. Zhu, M. Lin, and J. Wang, “A machine learning framework for resource allocation assisted by cloud computing,” IEEE Netw., vol. 32, no. 2, pp. 144–151, Mar./Apr. 2018.
[28] B. Mao, Z. M. Fadlullah, F. Tang, N. Kato, O. Akashi, T. Inoue, and K. Mizutani, “Routing or computing? The paradigm shift towards intelligent computer network packet transmission based on deep learning,” IEEE Trans. Comput., vol. 66, no. 11, pp. 1946–1960, Nov. 2017.

[29] F. Tang, B. Mao, Z. M. Fadlullah, N. Kato, O. Akashi, T. Inoue, and K. Mizutani, “On removing routing protocol from future wireless networks: A real-time deep learning approach for intelligent trafﬁc control,” IEEE Wireless Commun., vol. 25, no. 1, pp. 154–160, Feb. 2018.
[30] A. L. Aliyu, P. Bull, and A. Abdallah, “Performance implication and analysis of the openﬂow SDN protocol,” in Proc. IEEE Int. Conf. Adv. Inf. Netw. Appl. Workshops, 2017, pp. 391–396.
[31] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “OpenAI Gym,” Accessed: Mar. 2019. [Online]. Available: https://arxiv.org/abs/1606.01540., 2016.
[32] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep Q-learning with model-based acceleration,” in Proc. Int. Conf. Mach. Learn., 2016, pp. 2829–2838.
[33] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, 2015, Art. no. 436.
[34] L. Chen, H. Qu, J. Zhao, B. Chen, and J. C. Principe, “Efﬁcient and robust deep learning with correntropy-induced loss function,” Neural Comput. Appl., vol. 27, no. 4, pp. 1019–1031, 2016.
[35] S. Bhowmik, M. A. Tariq, B. Koldehofe, F. Durr, T. Kohler, and K. Rothermel, “High performance publish/subscribe middleware in software-deﬁned networks,” IEEE/ACM Trans. Netw., vol. 25, no. 3, pp. 1501–1516, Jun. 2017.
[36] S. Knight, H. X. Nguyen, N. Falkner, R. Bowden, and M. Roughan, “The internet topology zoo,” IEEE J. Sel. Areas Commun., vol. 29, no. 9, pp. 1765–1775, Oct. 2011.
[37] K. Ha, P. Pillai, G. Lewis, S. Simanta, S. Clinch, N. Davies, and M. Satyanarayanan, “The impact of mobile multimedia applications on data center consolidation,” in Proc. IEEE Int. Conf. Cloud Eng., 2013, pp. 166–176.
[38] B. P. Rimal, D. P. Van, and M. Maier, “Cloudlet enhanced ﬁber-wireless access networks for mobile-edge computing,” IEEE Trans. Wireless Commun., vol. 16, no. 6, pp. 3601–3618, Jun. 2017.
[39] M. Caria, T. Das, and A. Jukan, “Divide and conquer: Partitioning OSPF networks with SDN,” in Proc. IFIP/IEEE Int. Symp. Integr. Netw. Manage., 2015, pp. 467–474.
JIADAI WANG (S’18) received the BS degree in information security from Qingdao University, in 2017. She is working toward the PhD degree in the School of Cyber Engineering, Xidian University. Her research interests cover smart city, mobile edge computing, and deep reinforcement learning. She is a student member of the IEEE.
LEI ZHAO (S’17) received the BS degree in computer science and technology from Xidian University, in 2015, and the MS degree in computer system architecture from Xidian University, in 2018. He is working toward the PhD degree in the Department of Electrical and Computer Engineering, University of Victoria. His research interests cover smart city, mobile edge computing, software deﬁned network, and deep reinforcement learning. He is a student member of the IEEE.

1540

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

Wang et al.: Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach

JIAJIA LIU (S’11-M’12-SM’15) received the BS degree in computer science from the Harbin Institute of Technology, in 2004, the MS degree in computer science from Xidian University, in 2009, and the PhD degree in information sciences from Tohoku University, in 2012. He was a full professor with the School of Cyber Engineering, Xidian University, from 2013 to 2018, and was the director of Internet of Things Security Research Center, Xidian University from 2016 to 2018. Since Jan. 2019, he has been a full professor with the School of Cybersecurity, Northwestern Polytechnical University. He was selected into the prestigious “Huashan Scholars” program by Xidian University in 2015. He has published around 130 peer-reviewed papers in many high quality publications, including prestigious IEEE journals and conferences. He received IEEE ComSoc Asia-Paciﬁc Outstanding Young Researcher Award in 2017, IEEE TVT Top Editor Award in 2017, the Best Paper Awards from many international conferences including IEEE ﬂagship events, such as IEEE GLOBECOM in 2016, IEEE WCNC in 2012 and 2014, IEEE ICNIDC in 2018. He was the recipient of the prestigious 2012 Niwa Yasujiro Outstanding Paper Award due to his exceptional contribution to the analytics modeling of two-hop ad hoc mobile networks, which has been regarded by the award committees as the theoretical foundation for analytical evaluation techniques of future ad hoc mobile networks. His research interests cover a wide range of areas including load balancing, wireless and mobile ad hoc networks, ﬁber-wireless networks, Internet of Things, cloud computing and storage, network security, LTE-A and 5G, SDN and NFV. He is a distinguished lecturer of the IEEE Communications Society. He is a senior member of the IEEE.

NEI KATO (F 13) is a full professor and the director of the Research Organization of Electrical Communication (ROEC), Tohoku University, Japan. He has been engaged in research on computer networking, wireless mobile communications, satellite communications, ad hoc & sensor & mesh networks, smart grid, IoT, Big Data, and pattern recognition. He has published more than 400 papers in prestigious peer-reviewed journals and conferences. He is the vice-president (Member & Global Activities) of IEEE Communications Society (2018-2019), the editor-in-chief of the IEEE Network Magazine (20152017), the editor-in-chief of the IEEE Transactions on Vehicular Technology (2017-), the associate editor-in-chief of the IEEE Internet of Things Journal (2013-), and the chair of the IEEE Communications Society Sendai Chapter. He served as a member-at-large on the Board of Governors, IEEE Communications Society (2014-2016), a vice chair of Fellow Committee of IEEE Computer Society (2016), a member of IEEE Computer Society Award Committee (2015-2016) and IEEE Communications Society Award Committee (2015-2017). He has also served as the chair of Satellite and Space Communications Technical Committee (2010-2012) and Ad Hoc & Sensor Networks Technical Committee (2014-2015) of IEEE Communications Society. His awards include Minoru Ishida Foundation Research Encouragement Prize (2003), Distinguished Contributions to Satellite Communications Award from the IEEE Communications Society, Satellite and Space Communications Technical Committee (2005), the FUNAI information Science Award (2007), the TELCOM System Technology Award from Foundation for Electrical Communications Diffusion (2008), the IEICE Network System Research Award (2009), the IEICE Satellite Communications Research Award (2011), the KDDI Foundation Excellent Research Award (2012), IEICE Communications Society Distinguished Service Award (2012), IEICE Communications Society Best Paper Award (2012), Distinguished Contributions to Disaster-resilient Networks R&D Award from Ministry of Internal Affairs and Communications, Japan (2014), Outstanding Service and Leadership Recognition Award 2016 from IEEE Communications Society Ad Hoc & Sensor Networks Technical Committee, Radio Achievements Award from Ministry of Internal Affairs and Communications, Japan (2016) and Best Paper Awards from IEEE ICC/GLOBECOM/WCNC/VTC. He is a distinguished lecturer of IEEE Communications Society and Vehicular Technology Society. He is a fellow of the IEEE and IEICE.

VOLUME 9, NO. 3, JULY-SEPT. 2021
Authorized licensed use limited to: KAUST. Downloaded on May 24,2022 at 11:55:17 UTC from IEEE Xplore. Restrictions apply.

1541

