IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

1133

Resource Allocation Based on Deep Reinforcement Learning in IoT Edge Computing

Xiong Xiong, Kan Zheng , Senior Member, IEEE, Lei Lei , Senior Member, IEEE, and Lu Hou , Student Member, IEEE

Abstract— By leveraging mobile edge computing (MEC), a huge amount of data generated by Internet of Things (IoT) devices can be processed and analyzed at the network edge. However, the MEC system usually only has the limited virtual resources, which are shared and competed by IoT edge applications. Thus, we propose a resource allocation policy for the IoT edge computing system to improve the efﬁciency of resource utilization. The objective of the proposed policy is to minimize the long-term weighted sum of average completion time of jobs and average number of requested resources. The resource allocation problem in the MEC system is formulated as a Markov decision process (MDP). A deep reinforcement learning approach is applied to solve the problem. We also propose an improved deep Q-network (DQN) algorithm to learn the policy, where multiple replay memories are applied to separately store the experiences with small mutual inﬂuence. Simulation results show that the proposed algorithm has a better convergence performance than the original DQN algorithm, and the corresponding policy outperforms the other reference policies by lower completion time with fewer requested resources.
Index Terms— Internet of Things (IoT); Mobile edge computing (MEC); Markov decision process (MDP); Reinforcement learning.
I. INTRODUCTION
M OBILE edge computing (MEC) is an emerging technology that provides cloud-computing capabilities within the radio access network (RAN) in close proximity to terminal devices. It enables a wide variety of applications and services to run at the edge of the mobile networks, which not only considerably reduces service latency, but also alleviates congestion in other parts of mobile core networks. The presence of MEC system at the edge of the RAN also allows applications to exploit local content and real-time information about localaccess network, which can be used to offer content-related
Manuscript received October 13, 2019; revised December 3, 2019; accepted January 29, 2020. Date of publication April 8, 2020; date of current version May 21, 2020. This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61671089 and in part by the BUPT Excellent Ph.D. Students Foundation under Grant CX2016314. (Corresponding author: Kan Zheng.)
Xiong Xiong, Kan Zheng, and Lu Hou are with Intelligent Computing and Communications (IC2) Laboratory, Wireless Signal Processing and Networks Laboratory (WSPN), Key Lab of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100088, China (e-mail: zkan@bupt.edu.cn).
Lei Lei is with the School of Engineering, University of Guelph, Guelph, ON N1G 2W1, Canada.
Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/JSAC.2020.2986615

and location-awareness services. As such, the applications and services deployed on the MEC system can offer the good quality of user experience [1]–[3].
Internet of Things (IoT) applications can beneﬁt greatly from the MEC technology. The IoT devices are normally constrained in terms of computation and storage capabilities. They have to upload sensor data to the remote server for further processing or storage [4], [5]. The huge amount of trafﬁc generated by IoT devices may congest mobile networks, and processing the sensor data in the remote IoT cloud may not ensure the latency requirement of some IoT applications [6]. With the aid of MEC system, the sensor data can be processed and analyzed immediately at the edge of the mobile network instead of forwarded directly to the remote IoT cloud, which not only facilitates low latency, but also eases security and backhaul impacts [7]–[11].
The MEC system is built based on the virtualization technology that has been widely used in network functions virtualization (NFV) [12]. A MEC framework consists of several general entities, especially involving mobile edge host, mobile edge host level management, etc [13]. The mobile edge host level management offers functionality for managing the mobile edge host and the applications running on it. The mobile edge host contains a mobile edge platform and a virtualization infrastructure which provides virtual compute, storage, and network resources. All the idle virtual resources are shared and competed by the mobile edge applications. Once an application requests some idle resources, the requested resources are associated to the application and become isolated to others. Beneﬁting from the ﬂexibility of virtualization, the requested resources can also be adjusted on demand [14].
In this paper, we focus on the resource allocation problem in the MEC system for IoT applications. Compared with the IoT cloud, the MEC system is normally equipped with limited resources. Different IoT edge application may require different amounts of resources to ensure the quality of service [15]. One important challenge is how to coordinate the limited resources for each application to achieve high resource utilization. That is, each application has to decide whether to request more idle resources or release some of the requested resources to avoid the shortage or wasting. Moreover, there is also a challenge for each application of how to allocate the requested resources to ensure that more sensor data can be processed in the MEC system as soon as possible. Therefore, it is necessary to ﬁnd an optimal policy to utilize the limited resources in a high efﬁcient and reasonable way.

0733-8716 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1134

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

As a classic formulation of sequential decision making, Markov decision process (MDP) is a suitable theory for formulating this problem. To solve MDP problems, reinforcement learning provides a bunch of solution methods, such as dynamic programming, Monte Carlo methods, temporal-difference learning, etc [16]. More modern reinforcement learning methods apply nonlinear function approximation to solve the problem with large state and action spaces, e.g., deep Q-network (DQN) [17], trust region policy optimization (TRPO) [18], and so on [19]. Recently, many researches have applied reinforcement learning algorithms to solve resource allocation problems in either MEC or IoT systems [20]–[26]. For example, the dynamic programming methods are applied to solve the resource allocation problem in fog nodes for IoT applications with the objective of maximizing the average total serve utility and minimizing the idle time [27]. In [28], a reinforcement learning based method is used to jointly optimize the ofﬂoading decision and computational resource allocation with the objective of minimizing the sum of delay and energy consumptions. The same problem is also studied in [29], where a Q-learning based resource allocation is proposed for the ofﬂoading decision problem in the IoT edge network. In [30], the authors propose a DQN-based strategic computation ofﬂoading algorithm for MEC environment to minimize the long-term weighted sum of execution delay, the handover delay and the computation task dropping cost. Also, a DQN-based task ofﬂoading and resource allocation is proposed to minimize the ofﬂoading cost including the energy, computation, and delay cost [31]. In [32], the authors propose a resource allocation strategy for orchestrating the networking, caching and computing resources in vehicular network, which is obtained by the double dueling DQN algorithm. In [33], the joint computation ofﬂoading and multi-user scheduling problem is formulated as an inﬁnite-horizon average-reward continuous-time MDP model, and solved by a deep reinforcement learning method to minimize the long-term average weighted sum of delay and power consumption. An actor-critic reinforcement learning method is applied to solve the joint caching, computing, and radio resource control problem in fog-enabled IoT with the objective of minimizing the average end-to-end delay [34].
In this paper, we propose a resource allocation policy for the IoT edge computing system to minimize the long-term weighted sum of average completion time of jobs and average number of requested resources, which has been seldom investigated in the related works. The optimization problem is formulated as a MDP model, which is solved by an improved DQN algorithm. The main contributions of this paper are summarized as follows:
• We improve the DQN algorithm by applying multiple replay memories to separately store the experiences with small mutual inﬂuence, which can further improve the training process of Q-networks.
• A new mechanism is designed in the MDP formulation, where the decision epoch is decoupled from the real timestep. By this mechanism, the actions for scheduling

Fig. 1. Illustration of the system model.
jobs and adjusting resources can be separated into two subspaces, which can reduce the size of action space. • We also design a new architecture of the Q-network for our proposed algorithm, where a ﬁlter layer is added at the end of network to ﬁlter out the state-action values of invalid actions.
The rest of the paper is organized as follows. Section II introduces the system model and the MDP formulation. In Section III, the improved DQN algorithm is proposed as the solution of the MDP problem. In Section IV, the performances of the proposed algorithm and the corresponding policy are evaluated by simulation, which are compared with the original DQN algorithm and the other reference policies. The conclusions are drawn in Section V.
II. SYSTEM MODEL AND PROBLEM FORMULATION
As illustrated in Fig. 1, we consider a MEC system deployed at the base station in a single-cell cellular network. In its serving area, a mass of IoT devices are randomly distributed, and upload sensor data to the network over machine-tomachine (M2M) connectivity. After arriving at the MEC system, the sensor data are queued and waiting to be scheduled.
The mobile edge host level management monitors the status information of IoT edge applications, including their resource allocation and the jobs waiting in the queue. In the MEC context, a job is considered as the encapsulation of the sensor data sent from an IoT device. Based on the monitored information, the mobile edge host level management can decide how to schedule each job, e.g., allocating virtual resources to the job for local processing, or forwarding the job to the remote IoT cloud for further processing. Moreover, it also makes decision on whether to adjust the requested virtual resources (i.e., request more resources, or release some of the requested resources) for each application to minimize the on-demand risk of shortage or plethora of virtual resources.
For simplicity, we consider only one mobile edge application deployed on the MEC system, and only focus on the allocation of computing resources. Due to massive data generated by the IoT application, one objective is to complete more jobs in the MEC system as soon as possible, not only to reduce the data transfer in the backhaul, but also to ensure low latency performance. Meanwhile, processing jobs with fewer

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

TABLE I SUMMARY OF MAIN NOTATIONS

1135

¡ Fig. 2. Illustration of the state s = C, Q, iψ, n˜c, n˜b . C: allocation of computing resources to jobs; Q: observation part of job queue; iψ: adjusting indicator; n˜c: number of computing resources requested in the next timestep; n˜b: number of jobs in the backlog part of job queue.
requested computing resources is another objective, which can improve the utilization rate of limited resources in the MEC system. To achieve these goals, we formulate the problem as a MDP. The mobile edge host is considered as the environment, and the mobile edge host level management plays the role of the agent which performs decisions continually to interact with the environment. The environment of a MDP model contains several key elements, including state, action and reward. For our problem, the status information of the application is adopted as the state. The action refers to scheduling the jobs or adjusting the computing resources, which is taken by the agent based on current state. The environment responds to the action and steps into a new state along with a reward signal, which is designed to coincide with the optimization objective. The details of the environment are introduced in the following subsections respectively.

A. State
The state s ∈ S describes the status information of the mobile edge application deployed on the MEC system, which can be given as

S = {s | s = (C, Q, iψ, n˜c, n˜b)} ,

(1)

where C denotes the allocation of computing resources to jobs, Q signiﬁes the state of the observation part of job queue, iψ is the value of adjusting indicator, n˜c is the number of computing resources that is requested by the mobile edge application in the next timestep, and n˜b denotes the number of jobs in the backlog part of job queue. As shown in Fig. 2, each part of s
is elaborated as below. The allocation of computing resources C is formulated as
a nc × nτ matrix, where nc is the number of total computing

resources on the mobile edge host and nτ denotes the number of time slices in the sliding window. That is, each row of C represents a computing resource cic , ic ∈ {1, 2, . . . , nc},

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1136

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

which is scheduled for allocation starting from current time slice and looking ahead nτ time slices into the future. The duration of one time slice is called a time unit, which is the same as the duration of one timestep. The sliding
window moves on by one time slice as one timestep goes on. The speciﬁc value of a time unit depends on the practical application, e.g., tens of milliseconds. Each element of C deﬁned as the computing unit indicates a computing resource during one time slice, which is the smallest unit of resource allocation. For a certain computing unit cic,iτ , it indicates the allocation state of the ic-th computing resource at the iτ -th time slice, where iτ ∈ {1, 2, . . . , nτ }. Moreover, the value of computing unit cic,iτ is one of the possibilities in the set of {−1, 0, 1}, which represents three different states as follows:
• unavailable (cic,iτ = −1): the computing unit can not be allocated to any jobs, since it has not been requested;
• available (cic,iτ = 0): the computing unit is requested by the mobile edge application but has not been allocated;
• allocated (cic,iτ = 1): the computing unit is allocated to a job.
Initially, some of the total nc computing resources are requested by the application, and all of them are in the avail-
able state. Then, the requested computing units are allocated on demand to the jobs waiting in the queue. After being
allocated, the state of the computing units is changed to “allocated", i.e., [cic,iτ = 0] → [cic,iτ = 1].
The number of requested computing resources should be
adjusted according to the shift of demand by jobs. Considering the startup overhead, the computing resources can be only
adjusted periodically in speciﬁed timesteps with the ﬁx interval nψ instead of every timestep, i.e., the adjusting action has a larger period than the scheduling action. Therefore, an additional value n˜c ∈ {1, 2, . . . , nc} is required to record how many available computing resources will be requested at the nτ -th time slice in the next timestep. In other words, when the system goes into the next timestep, the sliding window of computing resources moves on by one time slice, and the new number of available computing units at the last column of C is indicated by n˜c. For simplicity, we assume that the n˜c computing units with the lowest indices at the newly coming time slice are available, while the others are unavailable, i.e., ci,nτ = 0, ∀i ∈ {1, . . . , n˜c}, and ci,nτ = −1, ∀i ∈ {n˜c + 1, . . . , nc}, where the notation with the superscript “ " denotes the value in the next timestep. Moreover, due to the value of n˜c possibly adjusted, the number of computing resources requested at
different time slice in the sliding window varies as time goes on. Thus, the notation n˜icτ is also introduced here to denote the number of computing units requested at iτ -th time slice, where iτ ∈ {1, 2, . . . , nτ }.
For the job queue, there are n˜q ∈ {1, 2, . . . } jobs held and waiting for allocation at the start of a timestep, and all the n˜q jobs are scheduled until the next timestep (the details will be explained in Section II-B). That is, n˜q also indicates the arrival rate of jobs. To keep the state space small, we logically divide the job queue into two parts, i.e., the observation part that contains ﬁrst no jobs, and the backlog part that contains the rest with the max length nb. It is assumed that only the

information of jobs in the observation part, denoted as Q, can be fully monitored, regardless of how many jobs are actually in the queue. Intuitively, it is reasonable to only pay attention to the earlier-arriving jobs in order to avoid long waiting time [35]. Moreover, this assumption also facilitates the state representation to be applied as input to a neural network. Thus, the state of the observation part Q can be represented by a no-column matrix, i.e.,

Q = q1 q2 . . . qno ,

(2)

where qiq is a two dimension column vector that indicates the information of the iq-th job, for ∀iq ∈ {1, 2, . . . , no}. Furthermore, qiq can be denoted as q1,iq , q2,iq , where
denotes matrix transposition, q1,iq signiﬁes the job size, i.e., the number of computing units required to process the
job, and q2,iq indicates the transmission delay which is the amount of time of the job transmitted from device to MEC
system. For a certain mobile edge application, we assume
that the sizes of all jobs are discrete random variables with
the same distribution and the maximum value of nl, and the transmission delays also obey another coincident distribution.
Due to the diversity of mobile edge applications, the distrib-
utions of job size, transmission delay and arrival rate n˜q can be regarded as conﬁguration to different scenarios.
It is worth noting that there are two particular cases for the state of the observation part Q according to different number of jobs in the queue n˜q. When the actual number of jobs is less than the length of observation part (i.e., n˜q < no), the last no − n˜q columns of Q are padded with dummy jobs that have zero size and delay, i.e., qi = [0, 0] , ∀i ∈ {n˜q + 1, . . . , no}. While n˜q > no, the jobs out of backlog part are discarded, and the other jobs beyond ﬁrst no are recorded in the backlog part, which can be retrieved latter after some of the ﬁrst no jobs scheduled. The number of jobs in the backlog part is indicated
by n˜b as a part of the system state, which is subjected to n˜b = min (max (n˜q − no, 0) , nb).
As the rest part of s, the value of adjusting indicator iψ ∈ {1, 2, . . . , nψ} counts the number of timesteps elapsing after the last time when adjusting n˜c. The value of iψ increases by one per timestep, and iψ is reset to one if larger than nψ. When iψ < nψ, the decisions of how to schedule the jobs are made in these timesteps, e.g., how many available computing
units are allocated to them. When iψ = nψ, the agent has a chance to adjust n˜c in this timestep. The details of iψ and n˜c are to be further explained in Section II-C.

B. Action
As introduced above, the action a ∈ A serves two functions for different purposes, i.e., one for specifying how to schedule the jobs in the observation part of queue; and the other for appointing how many computing resources should be requested in the future. However, combining these two functions in one action and scheduling all the jobs in queue may lead to a large action space, which may make the problem too complicated.
Therefore, a new mechanism is designed to reduce the size of action space. We ﬁrst decouple the decision epoch

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

1137

Fig. 3. Illustration of the mechanism where the decision epoch and timestep are decoupled, and the action space A are also divided into two subspace,
i.e., Ac for scheduling jobs and Aψ for adjusting requested computing resources. The timestep is frozen except when the last job in the queue is scheduled at the epoch t + 1, and the number of requested computing resources is adjusted at the epoch t + k.

t from the real timestep τ . There are more than one decision epochs to execute actions in each timestep. At each timestep, time is frozen to schedule each job in the queue (including both observation part and backlog part) from head to tail by sequence, until the last job in the queue is scheduled, or the number of requested computing resources is adjusted. For the sake of illustration, Fig. 3 gives an example of this mechanism, where the decision epoch and timestep are out of sync, and the timestep goes on only at the epoch t + 2 and t + k + 1. This mechanism will be further elucidated in the following Section II-C.
By this mechanism, multiple jobs can be scheduled at the same timestep, and the two functions are also decoupled into two corresponding action subspaces. Thus, the action space A are divided and given as

A = Ac ∪ Aψ, Ac ∩ Aψ = ∅,

(3)

Fig. 4. Illustration of the action a = (δτ , δc) with an example where q1,1 = 6, nc = 8 and nτ = 6 (i.e., C is a 8 × 6 matrix).

where Ac consists of the actions of scheduling the ﬁrst job q1 in queue, Aψ includes the actions of adjusting the number of computing resources requested in the next timestep, and
the notation ∅ stands for the empty set. As a consequence,
each action is responsible for a unique task, either scheduling the ﬁrst job a ∈ Ac (when iψ < nψ), or adjusting the number of requested computing resources a ∈ Aψ (when iψ = nψ), which can keep the action space small. The details of scheduling action subspace Ac and adjusting action subspace Aψ are described respectively as follows.
The scheduling action subspace Ac can be further denoted as

Ac =

(δτ , δc)

δτ ∈ {1, 2, . . . , nτ } , δc ∈ {1, 2, . . . , nl}

∪ {(−1, ∅)} , (4)

where each action a ∈ Ac is described by a tuple, (−1, ∅) means not allocating any computing unit to the ﬁrst job
which is forwarded directly to the remote IoT cloud for further processing, and (δτ , δc) denotes allocating δc computing resources to the ﬁrst job q1 continuously from the δτ -th time slice in the sliding window until the job completion. In other words, the action a = (δτ , δc) is to ﬁnd available computing units in C to place q1.

However, in a speciﬁc state s, not all of actions in Ac are valid due to some limitations. For example, for the job q1 = [q1,1, q2,1] , the action (δτ , nl) is invalid when q1,1 < nl. Thus, only a part of actions are valid in a state s, which make

up a subset of Ac denoted by Ac(s) ⊆ Ac. For simplicity, the action a = (δτ , δc) are assumed as follows, i.e.,

(a) The size of the ﬁrst job q1,1 must be divisible by δc, i.e., q1,1 (mod δc) = 0. Thus, δc is subjected to δc ∈ {i | q1,1 (mod i) = 0, i ∈ {1, . . . , q1,1}}. Also,

the number of time slices required to process the job

can be obtained by dτ = q1,1/δc. For example, as shown in Fig. 4, when q1,1 = 6, δc ∈ {1, 2, 3, 6}. (b) Only the continuous δc available computing resources with the same starting time slice δτ can be allocated to the job q1. Here “continuous"

means that the indices of computing resources are

consecutive. For a speciﬁc iτ ∈ {1, . . . , nτ }, all

computing units from {ci,iτ | i ∈ {j, . . . , j + δc − 1}},

∀j ∈

1, . . . , n˜icτ − δc + 1 are considered

continuous, e.g., {c2,5, c3,5, c4,5} indicated by

orange in Fig. 4. In addition, the requested

computing resources with the lowest and highest

indices are also considered continuous, i.e.,

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1138

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

ci,iτ | i ∈ 1, . . . , j + δc − 1 − n˜icτ ∪ j, . . . , n˜icτ , ∀j ∈ n˜icτ − δc + 2, . . . , n˜icτ . For example, in Fig. 4, {c1,1, c6,1} in blue are continuous. (c) Despite of many potential continuous δc available computing resources as described in the assumption b, only
those of them with the lowest starting index j are adopted
for allocation.

According to the above assumptions a and b, the available
computing units allocated by action a = (δτ , δc) to the job q1 must fully satisfy its resource requirement q1,1, which should be a δc × dτ submatrix of C (denoted as Cδc×dτ ), or two decomposed submatrixes Cδc(1)×dτ and C , δc(2)×dτ where δc(1) + δc(2) = δc. Moreover, the action a = (δτ , δc) can uniquely identify the available computing units for the job
q1 according to the assumption c, as shown in Fig. 4, when a = (5, 3), {ci,j | ∀j ∈ {5, 6}, ∀i ∈ {2, 3, 4}} are allocated to the job instead of {ci,j | ∀j ∈ {5, 6}, ∀i ∈ {3, 4, 5}} or others. With all the above assumptions, the set of valid scheduling
actions Ac(s) can be derived as

Ac(s) = {(δτ , δc)} ∪ {(−1, ∅)} , s.t. ∀δτ ∈ {1, . . . , nτ } , ∀δc ∈ {i | q1,1 (mod i) = 0, i ∈ {1, . . . , q1,1}} ,

∃Cδc×dτ = 0 ∨ ∃Cδc(1)×dτ = 0 ∧ ∃Cδc(2)×dτ = 0 ,

δc(1) = j + δc − 1 − n˜δcτ ,

δc(2) = n˜δcτ − j + 1,

∀k ∈ 1, . . . , n˜δcτ − δc + 1 (used by Cδc×dτ ),

∀j ∈ n˜δcτ − δc + 2, . . . , n˜δcτ ,

(5)

where 0 denotes the zero matrix with the same shape as that

on the left side of the equation, and the submatrix Cδc×dτ can

be further denoted by

⎡

⎤

ck,δτ . . . ck,δτ +dτ −1

Cδc×dτ = ⎢⎣

...

...

...

⎥⎦ . (6)

ck+δc−1,δτ . . . ck+δc−1,δτ +dτ −1

Similarly, the submatrixes Cδc(1)×dτ and Cδc(2)×dτ are given as

⎡

⎤

c1,δτ . . . c1,δτ +dτ −1

Cδc(1)×dτ = ⎢⎣ ...

...

...

⎥⎦ ,

⎡cδc(1),δτ . . . c ⎤ δc(1),δτ +dτ −1

cj,δτ . . . cj,δτ +dτ −1

Cδc(2)×dτ = ⎢⎣ ...

...

...

⎥⎦ .

cn˜δcτ ,δτ . . . cn˜δcτ ,δτ +dτ −1

(7a) (7b)

For example, the set of valid scheduling actions Ac(s) in Fig. 4 is equal to {(1, 1), (1, 2)} ∪ {(2, 2), (2, 3), (2, 6)} ∪ {(3, 2), (3, 3), (3, 6)} ∪ {(4, 2), (4, 3), (5, 3), (6, 6)} according

to (5).

On the other hand, each action a ∈ Aψ is also described as a tuple for uniformity, and the adjusting action subspace Aψ is given as

Aψ = {(−2, δψ) | δψ ∈ {1, 2, . . . , nc}} ,

(8)

TABLE II STATE TRANSITION CASES

where δψ denotes the new value of n˜c after adjusting.

C. State Transition

The state transition is a function of a pair of state and action

(s, a). For a certain state s = (C, Q, iψ, n˜c, n˜b) at the decision epoch t, an action is taken either from Ac(s) or Aψ to make

the state move to a successor state s = C , Q , iψ, n˜c, n˜b , i.e., s −→a s . Different from “ ", the notation with the

superscript “ " denotes the value at the next decision epoch.

There are several different cases of state transition according

to the state s as shown in Table II. Each case of the state

transition is explained in detail as follows.

(a) When iψ ≤ nψ and there are more than one jobs waiting

in the queue (i.e., q1 = 0 and q2 = 0), or when

iψ = nψ and q1 = 0 ∧ q2 = 0, an action is selected

from Ac(s) to schedule the ﬁrst job q1, which should

be either a = (δτ , δc) or a = (−1, ∅). If a = (δτ , δc),

the available computing units described by Cδc×dτ ,

or Cδc(1)×dτ and After allocation,

C , δc(2)×dτ are allocated these computing units

to the in C

job q1. should

change to the allocated state in the next state s , thereby

C = C. If a = (−1, ∅), it just forwards the job to

the remote IoT cloud without allocating any computing

unit, which has nothing to do with C (i.e., C = C).

Meanwhile, after scheduling the job, the ﬁrst job q1 is dequeued. Thus, the new state of Q is derived by

rolling one column of Q along the row, i.e., qi = qi+1 for ∀i ∈ {1, . . . , no − 1}, and the tail of observation
part qno is assigned by the ﬁrst job loaded from the backlog part if n˜b > 0. Otherwise, the job queue is
padded with a dummy job [0, 0] (also denoted as 0).

Moreover, n˜b = max (n˜b − 1, 0), and the rest parts of s are the same as before iψ = iψ, n˜c = n˜c. Till now,
all parts of the state are updated, and then the system

moves to the next decision epoch (i.e., t = t + 1) with

the new state s . In this case, the decision epoch passes

away after state transition, while the timestep is frozen

(i.e., τ = τ ). For example, as shown in Fig. 3, the state

transitions at the decision epoch t and t + 2 belongs to

this case.

(b) When iψ < nψ and there is only one job waiting in the queue (i.e., q1 = 0 ∧ qi = 0 for ∀i ∈ {2, . . . , n˜o}), the state transition from s to s ﬁrstly follows case a.

Besides, there are some additional operations. After the

only job scheduled, the job queue is empty, and the

system steps into not only the next decision epoch (i.e.,

t = t + 1) but also a new timestep (i.e., τ = τ + 1).

Thus, the adjusting indicator increases by one timestep,

i.e., iψ = iψ + 1. In the new timestep, n˜q new jobs arrive at the MEC system, which ﬁll up the job queue and

decide the value of Q and n˜b. For example, the state

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

1139

transition at the epoch t + 1 as shown in Fig. 3 is in
this case. (c) When iψ = nψ and there is no job waiting in the queue
(i.e., q1 = 0), it is the decision epoch to adjust n˜c. In this case, the action a = (−2, δψ) is chosen from Aψ, and the new value of n˜c is set to n˜c = δψ. After adjusting n˜c, the system also steps into a new timestep similar to case b, but the adjusting indicator is reset to one (i.e., iψ = 1). Also, the newly arriving jobs are revealed in the queue state Q and n˜b in the next timestep. In Fig. 3, the state transition at the epoch t + k is an example of this case.
The state transition repeats iteratively according to the
cases mentioned above. Therefore, the state transition in the successor state can also be derived. When iψ = nψ ∧ q1 = 0 ∧ q2 = 0 in the state s, the state transition follows case a. After then, the state transition in the successor state follows case c, which is donated as s −−−−−−c−as−e −a−−−−→ s −c−as−e→c s .
q1 =0∧q2 =0∧iψ =nψ
Similarly, the other state transition cases in the successor state
are summarized as follows:
• s −−−−−−−−c−a−se−a−−−−−−−→ s −c−as−e →a s ,
q1=0∧q2=0∧q2=0∧iψ <nψ
• s −−−−−−−−c−a−se−a−−−−−−−→ s −c−a−se−→b s ,
q1=0∧q2=0∧q2=0∧iψ <nψ
• s −−−−−−c−as−e −a−−−−→ s −c−as−e →a s ,
q1 =0∧q2 =0∧iψ =nψ
• s −−−−−−ca−s−e −b−−−−→ s −c−as−e →a s ,
q1 =0∧q2 =0∧iψ <nψ
• s −−−−c−as−e −c−−→ s −c−as−e →a s .
q1=0∧iψ =nψ
Therefore, combining these state transition cases and formulas (5) and (8), the valid action space of a certain state s
can be given as

A(s) = Ac(s), q1 = 0 .

(9)

Aψ, q1 = 0

Due to the complexity of our problem, it is hard to directly derive the state-transition probability for each state p(s |s, a). However, without p(s |s, a), the problem can still be solved by model-free reinforcement learning algorithms, which is further explained in Section III.

D. Reward
A reward signal is designed to coincide with the goal of an optimization problem. In this paper, the goal is to complete each job arriving at the MEC system as soon as possible with the least computing resources requested. In another word, the objective is deﬁned as minimizing the weighted sum of the average completion time of jobs and the average number of requested computing units. By decomposing the objective into small pieces at each decision epoch, the reward r(s, a) can be also constructed as a function of state s and action a, which is generated along with each state transition s −a−,−r(−s,−a→) s .
As described in Section II-C, for a given state s where q1 = 0, the state transition follows case a or b, and an action a ∈ Ac(s) is applied to schedule the job q1. Then,

the completion time of job rˆc(s, a) can be calculated by

rˆc(s, a) =

q2,1 + δτ + q1,1/δc, q2,1 + d0,

a = (δτ , δc) ∧ q1 = 0 a = (−1, ∅) ∧ q1 = 0,
(10)

where d0 denotes the total time spent on sending the job from the MEC system to the IoT cloud, and processing it after then.
For simplicity, it is assumed that d0 is a constant and d0 is much greater than the span of the sliding window nτ (i.e., d0 > nτ ). Thus, if a = (−1, ∅), the completion time of job should be the sum of its transmission delay q2,1 and d0. If a = (δτ , δc), the completion time of job is calculated by adding up its transmission delay q2,1 and its processing time δτ +q1,1/δc, which is derived by the number of timesteps elapsing till the
computing units allocated at last time slot have been executed. On the other hand, when q1 = 0, the state transition follows
case c. Based on the adopted action a = (−2, δψ) ∈ Aψ, it is easy to forecast the total number of requested computing units
before the next timestep for adjusting, which is given by

rˆψ(s, a) = δψ · nψ, if a = (−2, ψ) ∧ q1 = 0. (11)
Typically, in the short term, the higher reward the state transition generates, the better the decision is made in that state [16]. Therefore, the reward r(s, a) is further designed as

r(s, a) = − (1 − β) · rˆc(s, a), q1 = 0 ,

(12)

−β · rˆψ(s, a)

q1 = 0

where β is a weight coefﬁcient between the completion time of job rˆc(s, a) and the requested computing units rˆψ(s, a), 0 < β < 1, and the negative sign is also introduced to perform the mathematical conversion.
According to (12), we can further obtain the return, which is deﬁned as the cumulative sum of reward generated at each decision epoch over the long run [16]. The return G(t) from the decision epoch t is given as:

∞

∞

G(t) =

γk−tR(k+1) =

γk−tr S(k), A(k)

k=t

k=t

= −(1 − β)Gc(t) − βGψ(t),

∞

Gc(t) =

γk−trˆc S(k), A(k) ,

(13a) (13b)

k=t, q1(k) =0

∞

Gψ(t) =

γk−trˆψ S(k), A(k) ,

(13c)

k=t, q1(k) =0

where γ, 0 < γ ≤ 1, is the discount rate, and R(k), S(k), A(k) denote the reward, state and action at the k-th decision epoch
respectively. To differ from other subscripts, the decision epoch k is grouped by brackets. Accordingly, the symbol q1(k) indicates the information of the ﬁrst job in the state S(k). As shown in (13a), the return G(t) consists of two parts, i.e., Gc(t) standing for the sum of completion time of all jobs, and Gψ(t) which is the sum of total requested computing units and also indicates the resource utilization. Therefore, to maximize
the expected return Eπ G(t) is equivalent to the optimization

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1140

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

objective, i.e., minimizing the weighted sum of the completion time of jobs and the number of requested computing resources in the long term.

III. SOLUTION

The solution of the MDP problem is to ﬁnd the optimal policy π∗ that maximizes the expected return Eπ∗ G(t) . A policy π is a mapping from states to probabilities of selecting
each possible action, which can be formally represented by a probability distribution π(a|s) over a ∈ A(s) for each s ∈ S. For any MDP, there exists an optimal policy π∗ that is better
than or equal to all other policies. We improve the deep Q-network (DQN) algorithm by apply-
ing multiple replay memories to further improve the training process. The DQN algorithm evolves from Q-learning, which
is a value-based method and involves an action-value function (also called Q-function). For a certain policy π, the actionvalue function Qπ(s, a) is deﬁned as

Qπ(s, a) = Eπ G(t) | S(t) = s, A(t) = a

∞

Eπ

γk−tR(k+1) S(t) = s, A(t) = a . (14)

k=t

The optimal policy π∗ always achieves the optimal actionvalue function Q∗(s, a), which has the largest value for a

speciﬁc state-action pair, i.e.,

Q∗(s, a) = Qπ∗(s, a) = max Qπ(s, a).

(15)

π

Conversely, if the optimal action-value function Q∗(s, a) is

obtained, the optimal policy can be immediately found by maximizing over Q∗(s, a), which is given by

π∗(a|s) = arg max Q∗(s, a).

(16)

a∈A(s)

Therefore, ﬁnding the optimal policy π∗ is equivalent to obtaining the optimal action-value function Q∗(s, a). Due to the self consistency, the optimal action-value function can be written in a special form called Bellman optimality equation [16], which is given by

Q∗(s, a)

= Eπ

R(t+1)

+

γ

max
a

Q∗(s

,a

)

|

S(t)

=

s,

A(t)

=

a

,

(17)

where γ is the discount rate, 0 < γ < 1. Based on (17), the optimal action-value function can be estimated by temporal-difference updates in an iterative manner, which is shown as

Q(s, a) ← Q(s, a)+α r(s, a)+γ max Q(s , a )−Q(s, a) ,
a
(18)

where Q(s, a) denotes the learned action-value function, and α is called learning rate. After multiple iteration, Q(s, a) can converge to Q∗(s, a) independent of the policy being followed.
Different from Q-learning, the DQN uses an artiﬁcial neural network Q(s, a; θ), called prediction network, as a function approximator to estimate the action-value function,

Q(s, a; θ) ≈ Q∗(s, a), where θ is the weights of the neural network and the number of θ is much less than the number of states. The input of the prediction network is the state s, and the corresponding values of all possible actions are
generated as the output. Moreover, another neural network Q(s, a; θ−), called target network, is also used to estimate the target value in (17), i.e., r(s, a) + γ maxa Q(s , a ; θ−) ≈ r(s, a) + γ maxa Q∗(s , a ). The target network has the same structure as the prediction network. However, its weights θ− are copied from θ every ﬁxed number of iterations nθ instead of every training epoch. Both the prediction network and the
target network are called Q-network. To learn Q(s, a; θ), a mean-squared error loss function L(θ)
is used and given by

1 L(θ)=|D˜ |
×
e∈D˜

r(s, a)+γ max Q(s , a ; θ−)−Q(s, a; θ) 2 ,
a
(19)

where e = (s, a, r (s, a) , s ) is an experience (also called
sample) that represents a state transition with the reward, and D˜ is a batch of experiences. Minimizing L(θ) is equivalent
to reducing the mean-squared error in the Bellman equa-
tion, which can be achieved by updating weights θ using
different algorithms, such as Momentum, RMSProp, Adam,
etc [36]. Therefore, the DQN substitutes the temporal-different
update (18) with the updating weights θ based on (19).
Different from the original DQN algorithm, we apply multiple replay memories Dim, ∀im ∈ {0, 1, . . . , nc} in our modiﬁcation to store experiences of different situations, respectively. The replay memory D0 stores the samples where adjusting actions a ∈ Aψ are taken, while the sample of scheduling e = (s, a, r(s, a), s ), a ∈ Ac(s) is pushed into the memory Dn˜c , n˜c ∈ {1, . . . , nc} according to the number of requested computing resources n˜c of the current state s = (C, Q, iψ, n˜c, n˜b). This design stems from our formulation of the problem that scheduling and adjusting actions are taken for
two different tasks, and the experiences with different number of requested resources n˜c have little relation to each others. Thus, it can improve the training process of Q-networks.
The completed algorithm for training is presented in
Algorithm 1. On each interaction with the environment,
the agent selects the action according to a ε-greedy rule, i.e., selecting the greedy policy with probability 1 − ε and a random action with probability ε, where εmin ≤ ε ≤ 1. It is worth noting that multiple εim, ∀im ∈ {0, 1, . . . , nc} are also utilized to achieve a better exploration in different situations.
Each εim is associated with the situation, where the replay memory Dim is applied, and decays exponentially according to the factor εdecay by each iteration until the minimal value εmin. After the action taken, a state transition occurs, which is saved into the corresponding memory Dim as described above. Then, a batch of experiences D˜ used in (19) are randomly
selected from the memory Dim for the purpose of training the Q-network.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

1141

Algorithm 1 Deep Q-Network With Multiple Replay

Memories

1: Initialize prediction network Q(s, a; θ) with random

weights θ 2: Initialize target network Q(s, a; θ−) with weights θ− = θ

3: Initialize replay memories Dim , im ∈ {0, 1, . . . , nc} with the same capacity |D|

4: Initialize εim = 1, ∀im ∈ {0, 1, . . . , nc} 5: Initialize state s = (C, Q, iψ, n˜c, n˜b)
6: for t ← 1, . . . , ∞ do

7: if q1 = 0 then

8:

A(s) ← Aψ

9:

im ← 0

10: else

adjusting according to (9)
scheduling

11:

A(s) ← Ac(s)

12:

im ← n˜c

13: end if

according to (9) and (5)

14: Generate random number p from 0 to 1 ε−greedy

15: a ← randomly selected from A(s) 0 ≤ p < εim

arg maxa∈A(s) Q(s, a; θ)

εim ≤ p < 1

16: Take action a and perform state transition s −a−,−r(−s,−a→) s

17: e ← (s, a, r(s, a), s )

according to II-C and (12)

18: Save e into replay memory Dim 19: Random sample a batch of experiences D˜ from Dim

20: Calculate L(θ)

according to (19)

21: Update weights θ

22: if εim > εmin then

23:

εim ← εim · εdecay

24: end if

exponential decay of εim

25: if t mod nθ = 0 then

26:

θ− ← θ

every nθ steps

27: end if

28: s ← s

29: end for

In our implementation, a multi-layer neural network is used
as illustrated in Fig. 5. Before sent into the network, the state
s is preprocessed ﬁrstly by normalization, since each part of state s = (C, Q, iψ, n˜c, n˜b) has different ranges of value. For example, the computing resources C is treated as an onechannel image, where each computing unit cic,iτ of C has the maximal value of 1 and the minimal value of −1, while the size of jobs q1,iq ranges from 1 to nl. The normalization method for each part is given by

g(x)

=

x− xmax

xmean − xmin

,

(20)

where xmean = (xmax + xmin)/2. After normalization, all parts of the state are reshaped to 1-dimensional vectors by row, and concatenated together by the sequence (C, Q, iψ, n˜c, n˜b). Then, the preprocessed state is sent into the input layer of the neural network which has (nc · nτ + 2 · no + 3) neurous, and propagates through nh fully connected layers. The output of the last fully connected layer has the same size (nτ · nl + nc + 1) as the total action space |A|, and each element of the output is mapping to an action-value function

Fig. 5. Illustration of the proposed Q-network, where a ﬁlter layer is added at the end of the network to ﬁlter out the state-action values of invalid actions.
of action a ∈ A. However, considering not all actions are valid in the state s, we hence add a ﬁlter layer at the end of the network to ﬁlter out the state-action values of invalid actions. This ﬁlter layer takes another input called action mask, which is a vector derived from valid actions A(s). The length of action mask is also |A|, and i-th element of the action mask indicates whether the i-th action in A is valid. If the i-th action is invalid, the i-th element in the output of the Q-network is set to −∞ (i.e., the minimum value of ﬂoat type in the implementation). Otherwise, the ﬁlter layer has no effect on the output of valid actions. This ﬁlter layer does not affect the back propagation of neural network, since both the predict and target values of the invalid action are equal to −∞ and reduce each other to zero when calculating the loss function (19). With this ﬁlter layer, the operation for getting greedy action arg maxa∈A(s) Q(s, a; θ) can be easily realized.
IV. SIMULATION RESULTS
To evaluate the performance of proposed model, we develop a python simulator running on Google TensorFlow-1.14.1 The main conﬁguration of the simulation is listed in Table III.
In the experiments, a certain mobile edge application is assumed to be deployed on the MEC system, and the IoT devices of the application are scattered uniformly in the serving area of the network. Each packet of sensor data sent to the MEC system suffers from different delays due to various wireless transmissions. For simplify, the transmission delay of each packet is assumed to obey an uniform distribution between 1 and 4 time units. After arriving at the MEC system, each packet of sensor data is encapsulated as a job. The size of jobs follows a discrete uniform distribution with the range from 1 to nl = 4. The overall jobs sent from IoT devices follows a Poisson distribution with the mean rate of λ = 3 [37], i.e., there are average 3 jobs per timestep arriving at the MEC system and waiting in the queue to be scheduled.
In the MEC system, there are nc = 10 total computing resources on the mobile edge host, and the sliding window for allocating computing resources contains nτ = 6 time slices, i.e., the shape of C is 10 × 6. Both the observation part and the backlog part of the job queue have the same length of no = nb = 5. The maximal value of adjusting
1 https://www.tensorﬂow.org/versions/r1.14/api_docs

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1142

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

TABLE III EXPERIMENT CONFIGURATION
indicator is nψ = 12, which is conﬁgured as the twice as much as the length of sliding window nτ . For the reward function (12), the total time of transmission and process for the job forwarded to the remote IoT cloud d0 is set to 20, which is large enough when compared to the time of processing the job locally in the MEC system. As we know, minimizing the completion time of jobs has the same importance as minimizing the number of requested computing units. Thus, the weight coefﬁcient β is set to 0.5. Most of the environment parameters are summarized in Table III. It also gives the parameters used in Algorithm 1. There are nc + 1 = 11 replay memories to store experiences, and the capacity of each replay memory is 1200. The parameters of ε-greedy rule are set as εmin = 0.01 and εdecay = 0.99. The discount rate γ in (19) for estimating the target value is 0.99. The Q-network contains two fully connected layers. Each of them has 64 neurons, and applies the rectiﬁed linear units (ReLU) activation function. With the environment settings, the size of the input layer of the Q-network can be calculated by (nc · nτ + 2 · no + 3) = 73. We can also obtain the size of the adjusting action subspace Aψ according to (8), which is the same as the number of total computing resources nc = 10. Moreover, based on the maximal size of job nl, the size of the scheduling action subspace Ac is ﬁxed to (nτ · nl + 1) = 25 according to (5). By combining these two subspaces together, the action space A has the size of 35, which is also the size of the output layer of the Q-network. The prediction network is updated by using the Adam algorithm with a learning rate of 0.001, and each update applies a batch of |D˜ | = 64 samples. The weights of target network θ− are copied from θ every nθ = 16 epochs.

A. Training Performances We ﬁrst compare the training performance of the
improved DQN algorithm with multiple replay memories (i.e., Algorithm 1) against the original DQN with one replay memory. Fig. 6 shows the loss, reward, and average state-action value during the training processes of these two algorithms. Both of the training processes last 260 k training epochs. For comparison purpose, an exponential moving average (EMA) algorithm is applied to smooth each raw data curve of these metrics. Thus, the data tendency can be revealed by the smoothed curves.
As shown in Fig. 6a, the loss curves of these two algorithms decrease with different rates as the training process goes on. The smoothed loss curve of multiple replay memories decreases rapidly to the value of approximately 0.7 during the ﬁrst 50 k training epochs, and continues to decrease gradually to the value about 0.3 at the end of training. On the other hand, the smoothed loss curve of one replay memory also decreases within the ﬁrst 10 k training epochs, and then reaches the value of approximately 1. However, it does not constantly decrease anymore after around the 10 k-th training epoch. Instead, it ﬂuctuates around the value of 1. At the end of training, the DQN with multiple replay memories has much lower smoothed loss value than the DQN with one replay memory, i.e., a better convergence performance.
Fig. 6b shows the average state-action value at each training epoch, which is calculated by averaging all outputs of the prediction network, i.e., a∈A Q(s, a; θ)/|A|. The reason of applying the average value is that the training process with higher average state-action value is more likely to outperform the others. Conversely, it is not necessary to record the state-action value of the executed action in each training epoch, since different state-action pairs corresponding to different state-action values, and different training processes may follow different state transition trajectories.
We ﬁrstly focus on the performances of the DQN with multiple replay memories. As shown in Fig. 6b, the average state-action values are around 0 in the initial several training epochs. It is because the kernels of each fully connected layers of Q-networks are initialized by the Glorot normal initializer, and the biases of each layers are initialized by zeros, which leads to the outputs of Q-networks in a small scale. Moreover, the average state-action values vary between 0 and −350 during the ﬁrst 50 k training epochs. It is due to the fact that the parameters εim of ε-greedy rule do not decay to the minimal value of εmin, and the agent takes more exploration in these training epochs. Thus, the Q-networks are not well trained, which is coincident with the above analysis, i.e, the loss values are not convergent in the ﬁrst 50 k training epochs as shown in Fig. 6a. On the other hand, Fig. 6c also reﬂects this phenomenon. Initially, the agent always takes exploratory moves (i.e., random action), which leads to a low immediate reward. The reward curve increases with the increasing number of the training epochs, because the agent takes more greedy actions according to state-action values with

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

1143

The average state-action value and reward curves of the DQN with one replay memory are also shown in Fig. 6b and Fig. 6c, respectively. The reward curve has a similar tendency to that of the DQN with multiple replay memories. At the end of training, the smoothed reward converges to the value about −10. The smoothed average state-action value curve decreases with the increasing number of training epochs, and converges to the value about −240 at the end of training. It can be seen that both of the average state-action value and reward of the DQN with one replay memory are much lower than those of the DQN with multiple replay memories. In other words, the DQN with multiple replay memories can learn a better policy with the same training epochs.
Therefore, it can be found that the improved DQN with multiple replay memories has a better training performance than the original DQN algorithm.

Fig. 6. Comparison of training performance between the improved DQN with multiple replay memories and the DQN with one replay memory.
εim decaying. After around 50 k training epochs, the smoothed average state-action value curve is convergent to the value about −180, and the smoothed reward curve converges to the value about −6.

B. Performance Evaluation
We compare the policy learned by Algorithm 1 with the following reference policies, i.e.,
• Resource Precedence Policy: When the job queue is not empty q1 = 0, this policy ﬁrstly selects the actions that can allocate the most available resources to the job, i.e., A1(s) = (δτ , δc) = arg max(δτ ,δc)∈Ac(s) δc . Then, the policy takes the action a with the least time slice offset from A1(s), i.e., a = arg min(δτ ,δc)∈A1(s) δτ .
• Time Precedence Policy: Similar to the previous policy, this policy ﬁrstly selects the actions with the least time slice offset, i.e., A1(s) =
(δτ , δc) = arg min(δτ ,δc)∈Ac(s) δτ . Then, the policy takes the action a that allocates the most resources from A1(s), i.e., a = arg max(δτ ,δc)∈A1(s) δc. Both of the reference policies are based on the fact that allocating the most available resources is aiming to reduce duration of processing the job q1,1/δc, and the least time slice offset means the minimal waiting time δτ . To ensure the sufﬁcient computing resources for processing jobs locally, these two policies always request the most computing resources at each adjusting epoch, i.e, a = nc = arg max(−2,δψ)∈Aψ δψ, when q1 = 0. The experiment of each policy runs 200 k decision epochs. The policy of the improved DQN algorithm is obtained according to (16), which is based on the prediction network trained in the previous Section IV-A. Moreover, the weights of prediction network are not updated any more in the following experiments. Fig. 7 shows the average values of reward, completion time of jobs, and the number of requested computing units over the decision epochs. For each policy, the average reward is equal to the negative value of the sum of average completion time of jobs and average number of requested computing units, which conforms to our expectations according to the design of reward function in (12). As can be seen, the average completion time of the policy learned by Algorithm 1 is equal to 4.03, which is longer than those of the other reference policies, i.e., 3.39 and 3.38, respectively. It implies that scheduling jobs with the learned policy may cost more time on average.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1144

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

Fig. 7. The average values of reward, completion time of jobs, and the number of requested computing units over the decision epochs.

However, on the other hand, the learned policy requests less computing units from the MEC system during the long run. From the results of average reward, we can see that the policy obtained by Algorithm 1 learns the tradeoff between the completion time of jobs and the number of requested computing units. As a consequence, the average reward of the learned policy is −6.91, which is higher than those of the other reference policies, i.e., −8.19 and −8.18, respectively. The higher average reward means the smaller weighted sum of the average completion time of jobs and the average number of requested computing units. Therefore, the policy learned by Algorithm 1 outperforms the reference policies, and is much closer to optimization objective.
More experiments are carried out to show the tradeoff mentioned above. As shown in Fig. 8a, the policy learned by Algorithm 1 always requests 6 computing resources at each adjusting epoch, instead of the total number of computing resources nc = 10. It is because the learned policy takes adjusting actions based on the observed states, which contains the information of the queue state, such as the number of jobs waiting in the queue, the number of computing units required by each waiting job, etc. After being well trained, the policy can estimate the reasonable number of computing resources requested in adjusting epochs to avoid the resource waste, which can be demonstrated by the resource utilization as shown in Fig. 8b. The resource utilization is calculated by dividing the number of computing units allocated to jobs by the total number of requested computing units. The resource utilization of the learned policy is 0.83, while both of the reference policies have the same resource utilization of 0.5. That is, about a half of the requested computing units are wasted when applying the reference policies. By contrast, the policy learned by Algorithm 1 makes more efﬁcient use of the computing units through the more reasonable selection of adjusting actions.
Fig. 9a shows the percentage of jobs processed in the MEC system. It can be seen that all the policies allocate computing resources to jobs as far as possible, instead of forwarding

Fig. 8. Performance of adjusting actions.
Fig. 9. Performance of scheduling actions.
jobs to the remote IoT cloud for further processing. Our proposed Algorithm 1 can learn this behavior, because the action of forwarding the job generates a much lower reward than processing the job locally according to (10). Moreover,

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

ZHENG et al.: RESOURCE ALLOCATION BASED ON DEEP REINFORCEMENT LEARNING

1145

this behavior is not affected by the aforementioned adjusting action selection, i.e., requesting less computing units to keep a high resource utilization. It indicates that the learned policy still requests enough computing units for processing the jobs in the MEC system. However, the adjusting action selection has a side effect on the average completion time per job as shown in Fig. 9b. The average completion time per job of the learned policy is about 4.2, which is a little larger than those of the reference policies, i.e., 3.53 and 3.52, respectively. It is because the requested computing resources are just enough for allocating the jobs. Therefore, the jobs after scheduled may be queued at different time slice offsets of the computing resources, which leads to more processing time. Nevertheless, the slowdown caused by the side effect is not more than the duration of one timestep, which is acceptable to the most IoT edge applications.
V. CONCLUSION
In this paper, we have formulated the resource allocation problem in IoT edge computing system as a MDP model. To solve the problem, we propose an improved DQN algorithm where multiple replay memories are applied. Supported by simulations, the proposed algorithm can achieve the less loss value than the original DQN algorithm with the same number of training epochs. Moreover, the proposed algorithm can also achieve higher values of average state-action value and average reward. It demonstrates that our proposed algorithm has a better convergence performance. The simulation results also show that the corresponding policy learned by our proposed algorithm outperforms the other reference policies by a higher average reward over the long run. It means that the learned policy can achieve the smaller weighted sum of average completion time of jobs and average number of requested resources, which is coincident with our optimization objective.
REFERENCES
[1] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision and challenges,” IEEE Internet Things J., vol. 3, no. 5, pp. 637–646, Oct. 2016.
[2] P. Mach and Z. Becvar, “Mobile edge computing: A survey on architecture and computation ofﬂoading,” IEEE Commun. Surveys Tuts., vol. 19, no. 3, pp. 1628–1656, 3rd Quart., 2017.
[3] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge computing: A survey,” IEEE Internet Things J., vol. 5, no. 1, pp. 450–465, Feb. 2018.
[4] Y. Mehmood, N. Haider, M. Imran, A. Timm-Giel, and M. Guizani, “M2M communications in 5G: State-of-the-art architecture, recent advances, and research challenges,” IEEE Commun. Mag., vol. 55, no. 9, pp. 194–201, 2017.
[5] J. Kim, S. Kim, T. Taleb, and S. Choi, “RAPID: Contention resolution based random access using context ID for IoT,” IEEE Trans. Veh. Technol., vol. 68, no. 7, pp. 7121–7135, Jul. 2019.
[6] Mobile-Edge Computing (MEC); Service Scenarios, Standard ETSI GS MEC-IEG 004 V1.1.1 (2015-11), Nov. 2015.
[7] N. H. Motlagh, M. Bagaa, and T. Taleb, “Energy and delay aware task assignment mechanism for UAV-based IoT platform,” IEEE Internet Things J., vol. 6, no. 4, pp. 6523–6536, Aug. 2019.
[8] G. Premsankar, M. Di Francesco, and T. Taleb, “Edge computing for the Internet of Things: A case study,” IEEE Internet Things J., vol. 5, no. 2, pp. 1275–1284, Apr. 2018.
[9] J. Pan and J. McElhannon, “Future edge cloud and edge computing for Internet of Things applications,” IEEE Internet Things J., vol. 5, no. 1, pp. 439–449, Feb. 2018.

[10] P. Porambage, J. Okwuibe, M. Liyanage, M. Ylianttila, and T. Taleb, “Survey on multi-access edge computing for Internet of Things realization,” IEEE Commun. Surveys Tuts., vol. 20, no. 4, pp. 2961–2991, 4th Quart., 2018.
[11] W. Yu et al., “A survey on the edge computing for the Internet of Things,” IEEE Access, vol. 6, pp. 6900–6919, 2018.
[12] D. Sabella, A. Vaillant, P. Kuure, U. Rauschenbach, and F. Giust, “Mobile-edge computing architecture: The role of MEC in the Internet of Things,” IEEE Consum. Electron. Mag., vol. 5, no. 4, pp. 84–91, Oct. 2016.
[13] Multi-Access Edge Computing (MEC); Framework and Reference Architecture, Standard ETSI GS MEC 003 V2.1.1 (2019-01), Mar. 2016.
[14] J. An et al., “EiF: Toward an elastic IoT fog framework for AI services,” IEEE Commun. Mag., vol. 57, no. 5, pp. 28–33, May 2019.
[15] R. Morabito, I. Farris, A. Iera, and T. Taleb, “Evaluating performance of containerized IoT services for clustered devices at the network edge,” IEEE Internet Things J., vol. 4, no. 4, pp. 1019–1030, Aug. 2017.
[16] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). Cambridge, MA, USA: MIT Press, 1998.
[17] V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.
[18] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust region policy optimization,” 2015, arXiv:1502.05477. [Online]. Available: http://arxiv.org/abs/1502.05477
[19] M. Wiering and M. v. Otterlo, Eds., Reinforcement Learning: State-ofthe-Art (Adaptation, Learning, and Optimization). New York, NY, USA: Springer, 2012.
[20] K. Zheng, H. Meng, P. Chatzimisios, L. Lei, and X. Shen, “An SMDPbased resource allocation in vehicular cloud computing systems,” IEEE Trans. Ind. Electron., vol. 62, no. 12, pp. 7920–7928, Dec. 2015.
[21] Q. Zheng, K. Zheng, H. Zhang, and V. C. M. Leung, “Delay-optimal virtualized radio resource scheduling in software-deﬁned vehicular networks via stochastic learning,” IEEE Trans. Veh. Technol., vol. 65, no. 10, pp. 7857–7867, Oct. 2016.
[22] Q. Ye, J. Li, K. Qu, W. Zhuang, X. S. Shen, and X. Li, “End-to-end quality of service in 5G networks: Examining the effectiveness of a network slicing framework,” IEEE Veh. Technol. Mag., vol. 13, no. 2, pp. 65–74, Jun. 2018.
[23] Q. Ye, W. Zhuang, S. Zhang, A.-L. Jin, X. Shen, and X. Li, “Dynamic radio resource slicing for a two-tier heterogeneous wireless network,” IEEE Trans. Veh. Technol., vol. 67, no. 10, pp. 9896–9910, Oct. 2018.
[24] H. Peng, Q. Ye, and X. S. Shen, “SDN-based resource management for autonomous vehicular networks: A multi-access edge computing approach,” IEEE Wireless Commun., vol. 26, no. 4, pp. 156–162, Aug. 2019.
[25] L. Lei, Y. Tan, S. Liu, K. Zhuang, K. Zheng, and X. Shen, “Deep reinforcement learning for autonomous Internet of Things: Model, applications and challenges,” 2019, arXiv:1907.09059. [Online]. Available: http://arxiv.org/abs/1907.09059
[26] M. Bagaa, A. Ksentini, T. Taleb, R. Jantti, A. Chelli, and I. Balasingham, “An efﬁcient D2D-based strategies for machine type communications in 5G mobile systems,” in Proc. IEEE Wireless Commun. Netw. Conf., Apr. 2016, pp. 1–6.
[27] A. T. Nassar and Y. Yilmaz, “Reinforcement learning-based resource allocation in fog RAN for IoT with heterogeneous latency requirements,” Jan. 2019, arXiv:1806.04582. [Online]. Available: http://arxiv.org/abs/1806.04582
[28] J. Li, H. Gao, T. Lv, and Y. Lu, “Deep reinforcement learning based computation ofﬂoading and resource allocation for MEC,” in Proc. IEEE Wireless Commun. Netw. Conf. (WCNC), Apr. 2018, p. 6.
[29] X. Liu, Z. Qin, and Y. Gao, “Resource allocation for edge computing in IoT networks via reinforcement learning,” in Proc. IEEE Int. Conf. Commun. (ICC), May 2019, pp. 1–6.
[30] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Performance optimization in mobile-edge computing via deep reinforcement learning,” in Proc. IEEE 88th Veh. Technol. Conf. (VTC-Fall), Aug. 2018, pp. 1–6.
[31] L. Huang, X. Feng, C. Zhang, L. Qian, and Y. Wu, “Deep reinforcement learning-based joint task ofﬂoading and bandwidth allocation for multiuser mobile edge computing,” Digit. Commun. Netw., vol. 5, no. 1, pp. 10–17, Oct. 2018.
[32] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and computing for connected vehicles: A deep reinforcement learning approach,” IEEE Trans. Veh. Technol., vol. 67, no. 1, pp. 44–55, Jan. 2018.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

1146

IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 38, NO. 6, JUNE 2020

[33] L. Lei, H. Xu, X. Xiong, K. Zheng, W. Xiang, and X. Wang, “Multiuser resource control with deep reinforcement learning in IoT edge computing,” IEEE Internet Things J., vol. 6, no. 6, pp. 10119–10133, Dec. 2019.
[34] Y. Wei, F. R. Yu, M. Song, and Z. Han, “Joint optimization of caching, computing, and radio resources for fog-enabled IoT using natural actor– critic deep reinforcement learning,” IEEE Internet Things J., vol. 6, no. 2, pp. 2061–2073, Apr. 2019.
[35] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, Resource Management With Deep Reinforcement Learning. New York, NY, USA: ACM, 2016, pp. 50–56.
[36] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.
[37] Cellular System Support for Ultra-Low Complexity and Low Throughput Internet of Things (CIoT), document (TR) 45.820, 11 2015, Version 13.1.0, 3rd Generation Partnership Project (3GPP), .
Xiong Xiong received the B.S. degree from the Beijing University of Posts and Telecommunications (BUPT), China, in 2013, where he is currently pursuing the Ph.D. degree. His research interests include M2M networks, software deﬁned radio, and reinforcement learning.

Lei Lei (Senior Member, IEEE) received the B.S. and Ph.D. degree in telecommunications engineering from the Beijing University of Posts and Telecommunications, China, in 2001 and 2006, respectively. She is currently an Associate Professor with the School of Engineering, University of Guelph, Canada. Her current research interests include deep reinforcement learning, the Internet-of-Things, smart grid, and mobile cloud computing.

Kan Zheng (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees from the Beijing University of Posts and Telecommunications (BUPT), China, in 1996, 2000, and 2005, respectively. He is currently a Full Professor with BUPT. He has rich experience in research and standardization of new emerging technologies. He has authored over 200 journal articles and conference papers in the ﬁeld of wireless communications, vehicular networks, the IoT, security, and so on. He holds editorial board positions with several journals. He has organized several special issues in the journals including the IEEE COMMUNICATIONS SURVEYS AND TUTORIALS, the IEEE Communications Magazine, and the IEEE SYSTEMS JOURNAL. He has also served in the organizing/TPC committees for more than ten conferences.

Lu Hou (Student Member, IEEE) received the B.Eng. degree from the School of Information and Communication Engineering, BUPT, in 2014, and the Ph.D. degree in 2019. He is currently working on a Postdoctoral Research Program with the Intelligent Computing and Communications (IC2) Lab, Beijing University of Posts and Telecommunications. His research interests include the Internet of things, cyber security, and reinforcement learning.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:31:44 UTC from IEEE Xplore. Restrictions apply.

