Processing-in-memory: A workload-driven perspective
Many modern and emerging applications must process increasingly large volumes of data. Unfortunately, prevalent computing paradigms are not designed to efﬁciently handle such large-scale data: The energy and performance costs to move this data between the memory subsystem and the CPU now dominate the total costs of computation. This forces system architects and designers to fundamentally rethink how to design computers. Processing-inmemory (PIM) is a computing paradigm that avoids most data movement costs by bringing computation to the data. New opportunities in modern memory systems are enabling architectures that can perform varying degrees of processing inside the memory subsystem. However, many practical system-level issues must be tackled to construct PIM architectures, including enabling workloads and programmers to easily take advantage of PIM. This article examines three key domains of work toward the practical construction and widespread adoption of PIM architectures. First, we describe our work on systematically identifying opportunities for PIM in real applications and quantify potential gains for popular emerging applications (e.g., machine learning, data analytics, genome analysis). Second, we aim to solve several key issues in programming these applications for PIM architectures. Third, we describe challenges that remain for the widespread adoption of PIM.

S. Ghose A. Boroumand
J. S. Kim J. Gomez-Luna
O. Mutlu

1 Introduction
A wide range of application domains have emerged as computing platforms of all types have become more ubiquitous in society. Many of these modern and emerging applications must now process very large datasets [1–8]. For example, an object classiﬁcation algorithm in an augmented reality application typically trains on millions of example images and video clips and performs classiﬁcation on real-time high-deﬁnition video streams [7, 9]. In order to process meaningful information from the large amounts of data, applications turn to artiﬁcial intelligence (AI), or machine learning, and data analytics to methodically mine through the data and extract key properties about the dataset.
Due to the increasing reliance on manipulating and mining through large sets of data, these modern applications greatly overwhelm the data storage and movement resources of a modern computer. In a contemporary computer, the main memory [consisting of dynamic
Digital Object Identiﬁer: 10.1147/JRD.2019.2934048

random-access memory (DRAM)] is not capable of performing any operations on data. As a result, to perform any operation on data that is stored in memory, the data needs to be moved from the memory to the CPU via the memory channel, a pin-limited off-chip bus (e.g., conventional double data rate, or DDR, memories use a 64-bit memory channel [10–12]). To move the data, the CPU must issue a request to the memory controller, which then issues commands across the memory channel to the DRAM module containing the data. The DRAM module then reads and returns the data across the memory channel, and the data moves through the cache hierarchy before being stored in a CPU cache. The CPU can operate on the data only after the data is loaded from the cache into a CPU register.
Unfortunately, for modern and emerging applications, the large amounts of data that need to move across the memory channel create a large data movement bottleneck in the computing system [13, 14]. The data movement bottleneck incurs a heavy penalty in terms of both performance and energy consumption [13–20]. First, there is a long latency and signiﬁcant energy involved in bringing data from

ß Copyright 2019 by International Business Machines Corporation. Copying in printed form for private use is permitted without payment of royalty provided that (1) each reproduction is done without alteration and (2) the Journal reference and IBM copyright notice are included on the ﬁrst page. The title and abstract, but no other portions, of this paper may be copied by any means or distributed
royalty free without further permission by computer-based and other information-service systems. Permission to republish any other portion of this paper must be obtained from the Editor.
0018-8646/19 ß 2019 IBM

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 1

DRAM. Second, it is difﬁcult to send a large number of requests to memory in parallel, in part because of the narrow width of the memory channel. Third, despite the costs of bringing data into memory, much of this data is not reused by the CPU, rendering the caching either highly inefﬁcient or completely unnecessary [5, 21], especially for modern workloads with very large datasets and random access patterns. Today, the total cost of computation, in terms of performance and in terms of energy, is dominated by the cost of data movement for modern data-intensive workloads such as machine learning and data analytics [5, 15, 16, 21–25].
The high cost of data movement is forcing architects to rethink the fundamental design of computer systems. As data-intensive applications become more prevalent, there is a need to bring computation closer to the data, instead of moving data across the system to distant compute units. Recent advances in memory design enable the opportunity for architects to avoid unnecessary data movement by performing processing-in-memory (PIM), also known as near-data processing. The idea of performing PIM has been proposed for at least four decades [26–36], but earlier efforts were not widely adopted due to the difﬁculty of integrating processing elements for computation with DRAM. Innovations such as three-dimensional (3-D)stacked memory dies that combine a logic layer with DRAM layers [5, 37–40], the ability to perform logic operations using memory cells themselves inside a memory chip [18, 20, 41–49], and the emergence of potentially more computation-friendly resistive memory technologies [50–61] provide new opportunities to embed generalpurpose computation directly within the memory [5, 16–19, 21, 22, 24, 25, 41–43, 47–49, 62–100].
While PIM can allow many data-intensive applications to avoid moving data from memory to the CPU, it introduces new challenges for system architects and programmers. In this article, we examine two major areas of challenges and discuss solutions that we have developed for each challenge. First, programmers need to be able to identify opportunities in their applications where PIM can improve their target objectives (e.g., application performance, energy consumption). As we discuss in Section 3, whether to execute part or all of an application in memory depends on: 1) architectural constraints, such as area and energy limitations, and the type of logic implementable within memory; and 2) application properties, such as the intensities of computation and memory accesses, and the amount of data shared across different functions. To solve this ﬁrst challenge, we have developed toolﬂows that help the programmer to systematically determine how to partition work between PIM logic (i.e., processing elements on the memory side) and the CPU in order to meet all architectural design constraints and maximize targeted beneﬁts [16, 22–24, 75]. Second, system architects and

programmers must establish efﬁcient interfaces and mechanisms that allow programs to easily take advantage of the beneﬁts of PIM. In particular, the processing logic inside memory does not have quick access to important mechanisms required by modern programs and systems, such as cache coherence and address translation, which programmers rely on for software development productivity. To solve this second challenge, we develop a series of interfaces and mechanisms that are designed speciﬁcally to allow programmers to use PIM in a way that preserves conventional programming models [5, 16–24, 62, 75].
In providing a series of solutions to these two major challenges, we address many of the fundamental barriers that have prevented PIM from being widely adopted, in a programmer-friendly way. We ﬁnd that a number of future challenges remain against the adoption of PIM, and we discuss them brieﬂy in Section 6. We hope that our work inspires researchers to address these and other future challenges, and that both our work and future works help to enable the widespread commercialization and usage of PIM-based computing systems.
2 Overview of PIM The costs of data movement in an application continue to increase signiﬁcantly as applications process larger data sets. PIM provides a viable path to eliminate unnecessary data movement by bringing part or all of the computation into the memory. In this section, we brieﬂy examine key enabling technologies behind PIM and how new advances and opportunities in memory design have brought PIM signiﬁcantly closer to realization.
2.1 Initial push for PIM Proposals for PIM architectures extend back as far as the 1960s. Stone’s Logic-in-Memory computer is one of the earliest PIM architectures, in which a distributed array of memories combine small processing elements with small amounts of RAM to perform computation within the memory array [36]. Between the 1970s and the early 2000s, a number of subsequent works propose different ways to integrate computation and memory, which we broadly categorize into two families of work. In the ﬁrst family, which includes NON-VON [35], Computational RAM [27, 28], EXECUBE [31], Terasys [29], and IRAM [34], architects add logic within DRAM to perform data-parallel operations. In the second family of works, such as Active Pages [33], FlexRAM [30], Smart Memories [32], and DIVA [26], architects propose more versatile substrates that tightly integrate logic and reconﬁgurability within DRAM itself to increase ﬂexibility and the available compute power. Unfortunately, many of these works were hindered by the limitations of existing memory technologies, which

3 : 2 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

Figure 1
High-level overview of a 3D-stacked DRAM architecture. Reproduced from [14].
prevented the practical integration of logic in or near the memory.
2.2 New opportunities in modern memory systems
Due to the increasing need for large memory systems by modern applications, DRAM scaling is being pushed to its practical limits [101–104]. It is becoming more difﬁcult to increase the density [101, 105–107], reduce the latency [107–112], and decrease the energy consumption [101, 113, 114] of conventional DRAM architectures. In response, memory manufacturers are actively developing two new approaches for main memory system design, both of which can be exploited to overcome prior barriers to implementing PIM architectures.
The ﬁrst major innovation is 3D-stacked memory [5, 37–40]. In a 3D-stacked memory, multiple layers of memory (typically DRAM) are stacked on top of each other, as shown in Figure 1. These layers are connected together using vertical through-silicon vias (TSVs) [38, 39]. With current manufacturing process technologies, thousands of TSVs can be placed within a single 3D-stacked memory chip. The TSVs provide much greater internal memory bandwidth than the narrow memory channel. Examples of 3D-stacked DRAM available commercially include High-Bandwidth Memory (HBM) [37, 38], Wide I/O [115], Wide I/O 2 [116], and the Hybrid Memory Cube (HMC) [40].
In addition to the multiple layers of DRAM, a number of prominent 3D-stacked DRAM architectures, including HBM and HMC, incorporate a logic layer inside the chip [37, 38, 40]. The logic layer is typically the bottommost layer of the chip and is connected to the same TSVs as the memory layers. The logic layer provides a space inside the DRAM chip where architects can implement functionality that interacts with both the processor and the DRAM cells. Currently, manufacturers make limited use of the logic layer, presenting an opportunity for architects to

implement new PIM logic in the available area of the logic layer. We can potentially add a wide range of computational logic (e.g., general-purpose cores, accelerators, reconﬁgurable architectures) in the logic layer, as long as the added logic meets area, energy, and thermal dissipation constraints.
The second major innovation is the use of byteaddressable resistive nonvolatile memory (NVM) for the main memory subsystem. In order to avoid DRAM scaling limitations entirely, researchers and manufacturers are developing new memory devices that can store data at much higher densities than the typical density available in existing DRAM manufacturing process technologies. Manufacturers are exploring at least three types of emerging NVMs to augment or replace DRAM at the main memory layer:
1) phase-change memory (PCM) [50–56]; 2) magnetic RAM (MRAM) [57, 58]; 3) metal-oxide resistive RAM (RRAM) or memristors
[59–61].
All three of these NVM types are expected to provide memory access latencies and energy usage that are competitive with or close enough to DRAM, while enabling much larger capacities per chip and nonvolatility in main memory.
NVMs present architects with an opportunity to redesign how the memory subsystem operates. While it can be difﬁcult to modify the design of DRAM arrays due to the delicacy of DRAM manufacturing process technologies as we approach scaling limitations, NVMs have yet to approach such scaling limitations. As a result, architects can potentially design NVM memory arrays that integrate PIM functionality. A promising direction for this functionality is the ability to manipulate NVM cells at the circuit level in order to perform logic operations using the memory cells themselves. A number of recent works have demonstrated that NVM cells can be used to perform a complete family of Boolean logic operations [41–46], similar to such operations that can be performed in DRAM cells [18, 20, 47–49].
2.3 Two approaches: Processing-near-memory vs. processing-using-memory
Many recent works take advantage of the memory technology innovations that we discuss in Section 2.2 to enable PIM. We ﬁnd that these works generally take one of two approaches, which are summarized in Table 1: 1) processing-near-memory or 2) processing-usingmemory. Processing-near-memory involves adding or integrating PIM logic (e.g., accelerators, very small inorder cores, reconﬁgurable logic) close to or inside the

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 3

Table 1 Summary of enabling technologies for the two approaches to PIM used by recent works.
memory (e.g., [5, 6, 16, 21–25, 62, 64–73, 75, 77, 79, 81, 83–87, 90, 91, 117]). Many of these works place PIM logic inside the logic layer of 3D-stacked memories or at the memory controller, but recent advances in silicon interposers (in-package wires that connect directly to the through-silicon vias in a 3D-stacked chip) also allow for separate logic chips to be placed in the same die package as a 3D-stacked memory while still taking advantage of the TSV bandwidth. In contrast, processing-using-memory makes use of intrinsic properties and operational principles of the memory cells and cell arrays themselves, by inducing interactions between cells such that the cells and/or cell arrays can perform computation. Prior works show that processing-using-memory is possible using static RAM (SRAM) [63, 76, 100], DRAM [17–20, 47–49, 80, 87, 118], PCM [41], MRAM [44–46], or RRAM/memristive [42, 43, 88, 92–99] devices. Processing-using-memory architectures enable a range of different functions, such as bulk copy and data initialization [17, 19, 63], bulk bitwise operations (e.g., a complete set of Boolean logic operations) [18, 41, 44–49, 63, 80, 106–108, 118], and simple arithmetic operations (e.g., addition, multiplication, implication) [42, 43, 63, 76, 80, 88, 92–100].
2.4 Challenges to the adoption of PIM In order to build PIM architectures that are adopted and readily usable by most programmers, there are a number of challenges that need to be addressed. In this article, we discuss two of the most signiﬁcant challenges facing PIM. First, programmers need to be able to identify what portions of an application are suitable for PIM, and architects need to understand the constraints imposed by different substrates when designing PIM logic. We address this challenge in Section 3. Second, once opportunities for PIM have been identiﬁed and PIM architectures have been designed, programmers need a way to extract the beneﬁts of PIM without having to resort to complex programming models. We address this challenge in Section 4. While these two challenges represent some of the largest obstacles to

widespread adoption for PIM, a number of other important challenges remain, which we discuss brieﬂy in Section 6.
3 Identifying opportunities for PIM in applications
In order to decide when to use PIM, we must ﬁrst understand which types of computation can beneﬁt from being moved to memory. The opportunities for an application to beneﬁt from PIM depend on the constraints of the target architecture and the properties of the application.
3.1 Design constraints for PIM The target architecture places a number of fundamental constraints on the types of computation that can beneﬁt from PIM. As we discuss in Section 2.3, there are two approaches to implementing PIM (processing-near-memory and processing-using-memory). Each approach has its own constraints on what type of logic can be efﬁciently and effectively implemented in memory.
In the case of processing-near-memory, PIM logic must be added close to the memory, either in the logic layer of a 3D-stacked memory chip or in the same package. This places a limit on how much PIM logic can be added. For example, in an HMC-like 3D-stacked memory architecture implemented using a 22-nm manufacturing processing technology, we estimate that there is around 50–60 mm2 of area available for architects to add new logic into the DRAM logic layer [40]. The available area can be further limited by the architecture of the memory. For example, in HMC, the 3D-stacked memory is partitioned into multiple vaults [40], which are vertical slices of 3D-stacked DRAM. Logic placed in a vault has fast access to data stored in the memory layers of the same vault, as the logic is directly connected to the memory in the vault by the TSVs (see Section 2.2), but accessing data stored in a different vault takes signiﬁcantly longer latency. As a result, architects often replicate PIM logic in each vault to minimize the latency of PIM operations. The tradeoff of this is that the amount of area available per vault is signiﬁcantly lower: For a 32-vault 3D-stacked memory chip, there is approximately 3.5–4.4 mm2 of area available for PIM logic [119–121].
A number of target computing platforms have additional constraints beyond area. For example, consumer devices such as smartphones, tablets, and netbooks are extremely stringent in terms of both the area and energy budget they can accommodate for any new hardware enhancement. Any additional logic added to memory can potentially translate into a signiﬁcant cost in consumer devices. In fact, unlike PIM logic that is added to server or desktop environments, consumer devices may not be able to afford the addition of full-blown general-purpose PIM cores [22–24, 68, 120], GPU PIM cores [75, 85, 90], or complex PIM accelerators

3 : 4 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

[5, 62, 119] to 3D-stacked memory. As a result, a major challenge for enabling PIM in consumer devices is to identify what kind of in-memory logic can both maximize energy efﬁciency and be implemented at minimum possible cost. Another constraint is thermal dissipation in 3D-stacked memory, as adding PIM logic in the logic layer can potentially raise the DRAM temperature beyond acceptable levels [85, 90].
In the case of processing-using-memory, the cells and memory array themselves are used to implement PIM logic. Additional logic in the controller and/or in the array itself may be required to enable logic operations on the cells or in the memory array, or to provide more specialized functionality beyond what the cells and memory array themselves can perform easily (e.g., dedicated adders or shifters).
3.2 Choosing what to execute in memory After the constraints on what type of hardware can potentially be implemented in memory are determined, the properties of the application itself are a key indicator of whether portions of an application beneﬁt from PIM. A naive assumption may be to move highly memory-intensive applications completely to PIM logic. However, we ﬁnd that there are cases where portions of these applications still beneﬁt from remaining on the CPU. For example, many proposals for PIM architectures add small general-purpose cores near memory (which we call PIM cores). While PIM cores tend to be ISA-compatible with the CPU, and can execute any part of the application, they cannot afford to have large, multilevel cache hierarchies or execution logic that is as complex as the CPU, due to area, energy, and thermal constraints. PIM cores often have no or small caches, restricting the amount of temporal locality they can exploit, and no sophisticated aggressive out-of-order or superscalar execution logic, limiting the PIM cores’ abilities to extract instruction-level parallelism. As a result, portions of an application that are either compute-intensive or cache-friendly should remain on the larger, more sophisticated CPU cores [16, 21–24, 75].
We ﬁnd that in light of these constraints, it is important to identify which portions of an application are suitable for PIM. We call such portions PIM targets. While PIM targets can be identiﬁed manually by a programmer, the identiﬁcation would require signiﬁcant programmer effort along with a detailed understanding of the hardware tradeoffs between CPU cores and PIM cores. For architects who are adding custom PIM logic (e.g., ﬁxed-function accelerators, which we call PIM accelerators) to memory, the tradeoffs between CPU cores and PIM accelerators may not be known before determining which portions of the application are PIM targets, since the PIM accelerators are tailored for the PIM targets.
To alleviate the burden of manually identifying PIM targets, we develop a systematic toolﬂow for identifying

PIM targets in an application [16, 22–24]. This toolﬂow uses a system that executes the entire application on the CPU to evaluate whether each PIM target meets the constraints of the system under consideration. For example, when we evaluate workloads for consumer devices, we use hardware performance counters and our energy model to identify candidate functions that could be PIM targets. A function is a PIM target candidate in a consumer device if it meets the following conditions:
1) It consumes the most energy out of all functions in the workload since energy reduction is a primary objective in consumer workloads.
2) Its data movement consumes a signiﬁcant fraction (e.g., more than 20%) of the total workload energy to maximize the potential energy beneﬁts of offloading to PIM.
3) It is memory-intensive (i.e., its last-level cache misses per kilo instruction, or MPKI, is greater than 10 [122–125]), as the energy savings of PIM is higher when more data movement is eliminated.
4) Data movement is the single largest component of the function’s energy consumption.
We then check if each candidate function is amenable to PIM logic implementation using two criteria. First, we discard any PIM targets that incur any performance loss when run on simple PIM logic (i.e., PIM core, PIM accelerator). Second, we discard any PIM targets that require more area than is available in the logic layer of 3D-stacked memory. Note that for pre-built PIM architectures with ﬁxed PIM logic, we instead discard any PIM targets that cannot be executed on the existing PIM logic.
While our toolﬂow was initially designed to identify PIM targets for consumer devices [16], the toolﬂow can be modiﬁed to accommodate any other hardware constraints. For example, in our work on reducing the cost of cache coherence in PIM architectures [22–24], we consider the amount of data sharing (i.e., the total number of cache lines that are read concurrently by the CPU and by PIM logic). In that work, we eliminate any potential PIM target that would result in a high amount of data sharing if the target were ofﬂoaded to a PIM core, as this would induce a large amount of cache coherence trafﬁc between the CPU and PIM logic that would counteract the data movement savings (see Section 4.2).
3.3 Case Study: PIM opportunities in TensorFlow By performing our constraint analysis (see Section 3.1) and using our systematic PIM target toolﬂow (see Section 3.2), we ﬁnd that a number of key modern workloads are well suited for PIM. In particular, we ﬁnd that machine learning and data analytics workloads are particularly amenable for PIM, as they are often

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 5

Figure 2
Energy breakdown during TensorFlow Lite inference execution on four input networks. Reproduced from [16].

Figure 3
Execution time breakdown of inference. Reproduced from [16].

partitioned into compute-intensive and memory-intensive application phases. These workloads beneﬁt highly from PIM when only the memory-intensive PIM targets (that ﬁt our system constraints) are ofﬂoaded to PIM logic. Such workloads include neural network inference [126], graph analytics [127–132], and hybrid transactional/ analytical processing databases [133–135].
As a case study, we present a detailed analysis using our PIM target identiﬁcation approach for TensorFlow Lite [126], a version of Google’s TensorFlow machine learning library that is speciﬁcally tailored for mobile and embedded platforms. TensorFlow Lite enables a variety of tasks, such as image classiﬁcation, face recognition, and Google Translate’s instant visual translation [136], all of which perform inference on consumer devices using a convolutional neural network that was pretrained on cloud servers. We target a processing-near-memory platform in this case study, where we add small in-order PIM cores or ﬁxed-function PIM accelerators into the logic layer of a 3D-stacked DRAM. We model a 3D-stacked DRAM similar to the Hybrid Memory Cube [40], where the memory contains 16 vaults (i.e., vertical slices of DRAM). We add one PIM core or PIM accelerator per vault, ensuring that the area of the PIM core or the PIM accelerator does not exceed the total available area for logic inside each vault (3.5– 4.4 mm2) [119–121]. Each PIM core or PIM accelerator can execute one PIM target at a time. Details about our methodology, along with the speciﬁc parameters of the target platform, can be found in our prior work [16].
Inference begins by feeding input data (e.g., an image) to a neural network. A neural network is a directed acyclic graph consisting of multiple layers. Each layer performs a number of calculations and forwards the results to the next layer. The calculation can differ for each layer, depending on the type of the layer. A fully connected layer performs matrix multiplication (MatMul) on the input data, to extract high-level features. A 2-D convolution layer applies a convolution ﬁlter (Conv2D) across the input data to extract low-level features. The last layer of a neural network is the output layer, which performs classiﬁcation to generate a prediction based on the input data.

Energy analysis: Figure 2 shows the breakdown of the energy consumed by each function in TensorFlow Lite, for four different input networks: ResNet-v2-152 [137], VGG19 [138], Residual-GRU [139], and Inception-ResNet-v2 [140]. As convolutional neural networks consist mainly of 2-D convolution layers and fully connected layers [141], the majority of energy is spent on these two types of layers. However, we ﬁnd that there are two other functions that consume a signiﬁcant fraction of the system energy: packing/unpacking and quantization. Packing and unpacking reorder the elements of matrices to minimize cache misses during matrix multiplication. Quantization converts 32-bit ﬂoating point and integer values (used to represent both the weights and activations of a neural network) into 8-bit integers, which improves the execution time and energy consumption of inference by reducing the complexity of operations that the CPU needs to perform. These two functions together account for 39.3% of total system energy on average. The rest of the energy is spent on a variety of other functions such as random sampling, reductions, and simple arithmetic, each of which contributes to less than 1% of total energy consumption (labeled Other in Figure 2).
Even though the main goal of packing and quantization is to reduce energy consumption and inference latency, our analysis shows that they generate a large amount of data movement and, thus, lose part of the energy savings they aim to achieve. Figure 3 shows that a signiﬁcant portion (27.4% on average) of the execution time is spent on the packing and quantization process. We do not consider Conv2D and MatMul as being candidates for ofﬂoading to PIM logic because: 1) a majority (67.5%) of their energy is spent on computation; and 2) Conv2D and MatMul require a relatively large and sophisticated amount of PIM logic [77, 119], which may not be cost-effective for consumer devices. PIM effectiveness for packing: We highlight how PIM can be used to effectively improve the performance and energy consumption of packing. GEneralized Matrix Multiplication (GEMM) is the core building block of neural networks and is used by both 2-D convolution and fully connected layers. These two layers account for the majority

3 : 6 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

of TensorFlow Lite execution time. To implement fast and energy-efﬁcient GEMM, TensorFlow Lite employs a lowprecision, quantized GEMM library called gemmlowp [142]. The gemmlowp library performs GEMM by executing its innermost kernel, an architecture-speciﬁc GEMM code portion for small ﬁxed-size matrix chunks, multiple times. First, gemmlowp fetches matrix chunks that ﬁt into the LLC from DRAM. Then, it executes the GEMM kernel on the fetched matrix chunks in a block-wise manner.
Each GEMM operation (i.e., a single matrix multiply calculation using the gemmlowp library) involves three steps. First, to minimize cache misses, gemmlowp employs a process called packing, which reorders the matrix chunks based on the memory access pattern of the kernel to make the chunks cache-friendly. Second, the actual GEMM computation (i.e., the innermost GEMM kernel) is performed. Third, after performing the computation, gemmlowp performs unpacking, which converts the result matrix chunk back to its original order.
Packing and unpacking account for up to 40% of the total system energy and 31% of the inference execution time, as shown in Figures 2 and 3, respectively. Due to their unfriendly cache access pattern and the large matrix sizes, packing and unpacking generate a signiﬁcant amount of data movement. For instance, for VGG-19, 35.3% of the total energy goes to data movement incurred by packingrelated functions. On average, we ﬁnd that data movement is responsible for 82.1% of the total energy consumed during the packing/unpacking process, indicating that packing and unpacking are bottlenecked by data movement.
Packing and unpacking are simply preprocessing steps to prepare data in the right format for the innermost GEMM kernel. Ideally, the CPU should execute only the innermost GEMM kernel and assume that packing and unpacking are already taken care of. PIM can enable such a scenario by performing packing and unpacking without any CPU involvement. Our PIM logic packs matrix chunks and sends the packed chunks to the CPU, which executes the innermost GEMM kernel. Once the innermost GEMM kernel completes, the PIM logic receives the result matrix chunk from the CPU and unpacks the chunk, while the CPU executes the innermost GEMM kernel on a different matrix chunk. PIM effectiveness for quantization: TensorFlow Lite performs quantization twice for each Conv2D operation. First, quantization is performed on the 32-bit input matrix before Conv2D starts, which reduces the complexity of operations required to perform Conv2D on the CPU by reducing the width of each matrix element to 8 bits. Then, Conv2D runs, during which gemmlowp generates a 32-bit result matrix. Quantization

Figure 4
Quantization on (a) CPU versus (b) PIM. Reproduced from [16].
is performed for the second time on this result matrix (this step is referred to as re-quantization). Accordingly, invoking Conv2D more frequently (which occurs when there are more 2-D convolution layers in a network) leads to higher quantization overheads. For example, VGG-19 requires only 19 Conv2D operations, incurring small quantization overheads. On the other hand, ResNet-v2 requires 156 Conv2D operations, causing quantization to consume 16.1% of the total system energy and 16.8% of the execution time. The quantization overheads are expected to increase as neural networks get deeper, as a deeper network requires a larger matrix.
Figure 4(a) shows how TensorFlow quantizes the result matrix using the CPU. First, the entire matrix needs to be scanned to identify the minimum and maximum values of the matrix (❶ in the ﬁgure). Then, using the minimum and maximum values, the matrix is scanned a second time to convert each 32-bit element of the matrix into an 8-bit integer (❷). These steps are repeated for requantization of the result matrix (❸ and ❹). The majority of the quantization overhead comes from data movement. Because both the input matrix quantization and the result matrix requantization need to scan a large matrix twice, they exhibit poor cache locality and incur a large amount of data movement. For example, for the ResNet-v2 network, 73.5% of the energy consumed during quantization is spent on data movement, indicating that the computation is relatively cheap (in comparison, only 32.5% of Conv2D/ MatMul energy goes to data movement, while the majority goes to multiply–accumulate computation). It can be seen that 19.8% of the total data movement energy of inference execution comes from quantization and requantization. As Figure 4(b) shows, we can ofﬂoad both quantization (❺ in the ﬁgure) and requantization (❻) to PIM to eliminate data movement. This frees up the CPU to focus on GEMM

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 7

Figure 5
Energy (left) and performance (right) for TensorFlow Lite kernels, averaged across four neural network inputs: ResNet-v2 [137], VGG-19 [138], Residual-GRU [139], Inception-ResNet [140]. Adapted from [16].
calculation, and allows the next Conv2D operation to be performed in parallel with requantization (❼). Evaluation: We evaluate how TensorFlow Lite beneﬁts from PIM execution using: 1) custom 64-bit low-power single-issue cores similar in design to the ARM Cortex-R8 [143]; and 2) ﬁxed-function PIM accelerators designed for packing and quantization operations, with each accelerator consisting of four simple ALUs and consuming less than 0.25 mm2 of area [16]. Figure 5 (left) shows the energy consumption of PIM execution using PIM cores (PIMCore) or ﬁxed-function PIM accelerators (PIM-Acc) for the four most time- and energy-consuming GEMM operations for each input neural network in packing and quantization, normalized to a processor-only baseline (CPU-Only). We make three key observations. First, PIM-Core and PIM-Acc decrease the total energy consumption of a consumer device system by 50.9% and 54.9%, on average across all four input networks, compared to CPU-Only. Second, the majority of the energy savings comes from the large reduction in data movement, as the computation energy accounts for a negligible portion of the total energy consumption. For instance, 82.6% of the energy reduction for packing is due to the reduced data movement. Third, we ﬁnd that the data-intensive nature of these kernels and their low computational complexity limit the energy beneﬁts PIM-Acc provides over PIM-Core.
Figure 5 (right) shows the speedup of PIM-Core and PIM-Acc over CPU-Only as we vary the number of GEMM operations performed. For CPU-Only, we evaluate a scenario where the CPU performs packing, GEMM calculation, quantization, and unpacking. To evaluate PIM-Core and PIM-Acc, we assume that packing and quantization are handled by the PIM logic, and the CPU performs GEMM calculation. We ﬁnd that, as the number of GEMM operations increases, PIM-Core and PIM-Acc provide greater performance improvements over CPU-Only. For example, for one GEMM operation, PIM-Core and PIM-Acc achieve speedups of 13.1% and

17.2%, respectively. For 16 GEMM operations, the speedups of PIM-Core and PIM-Acc increase to 57.2% and 98.1%, respectively, over CPU-Only. These improvements are the result of PIM logic: 1) exploiting the higher bandwidth and lower latency of 3D-stacked memory, and 2) enabling the CPU to perform GEMM in parallel while the PIM logic handles packing and quantization.
We conclude that our approach to identifying PIM targets can be used to signiﬁcantly improve performance and reduce energy consumption for the TensorFlow Lite mobile machine learning framework.
4 Programming PIM architectures: Key issues While many applications have signiﬁcant potential to beneﬁt from PIM, a number of practical considerations need to be made with regards to how portions of an application are ofﬂoaded, and how this ofﬂoading can be accomplished without placing an undue burden on the programmer. When a portion of an application is ofﬂoaded to PIM logic, the PIM logic executes the ofﬂoaded piece of code, which we refer to as a PIM kernel. In this section, we study four key issues that affect the programmability of PIM architectures:
1) the different granularities of an ofﬂoaded PIM kernel;
2) how to handle data sharing between PIM kernels and CPU threads;
3) how to efﬁciently provide PIM kernels with access to essential virtual memory address translation mechanisms;
4) how to automate the identiﬁcation and ofﬂoading of PIM targets (i.e., portions of an application that are suitable for PIM; see Section 3.2).
4.1 Ofﬂoading Granularity In Section 3.3, our case study on identifying opportunities for PIM in TensorFlow Lite makes an important assumption: PIM kernels are ofﬂoaded at the granularity of an entire function. However, there are a number of different granularities at which PIM kernels can be ofﬂoaded. Each granularity requires a different interface and different design decisions. We evaluate four ofﬂoading granularities in this section: a single instruction, a bulk operation, an entire function, and an entire application.
At one extreme, a PIM kernel can consist of a single instruction from the view of the CPU. For example, a PIMenabled instruction (PEI) [21] can be added to an existing ISA, where each PIM operation is expressed and semantically operates as a single instruction. Figure 6 shows an example architecture that can be used to enable PEIs [21]. In this architecture, a PEI is executed on a PEI Computation Unit (PCU). To enable PEI execution in either the host CPU or in memory, a PCU is added to each host CPU and to each vault in an HMC-like 3D-stacked

3 : 8 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

Figure 6
Example architecture for PIM-enabled instructions. Adapted from [21].
memory. While the work done in a PCU for a PEI might have required multiple CPU instructions in the baseline CPU-only architecture, the CPU only needs to execute a single PEI instruction, which is sent to a central PEI Management Unit (PMU in Figure 6). The PMU launches the appropriate PIM operation on one of the PCUs. Implementing PEIs with low complexity and minimal changes to the system requires three key rules. First, for every PIM operation, there is a single PEI in the ISA that is used by the CPU to trigger the operation. This keeps the mapping between PEIs and PIM operations simple, and allows for the gradual introduction of new instructions. This also avoids the need for virtual memory address translation in memory, as the translation is done in the CPU before sending the PEI to memory. Second, a PIM operation is limited to operating on a single cache line. This eliminates the need for careful data mapping, by ensuring that a single PEI operates on data that is mapped to a single memory controller, and eases cache coherence, by needing no more than a single cache line to be kept coherent between a PEI performed in memory and the CPU cache. Third, a PEI is treated as atomic with respect to other PEIs and uses memory fences to enforce atomicity between a PEI and a normal CPU instruction. An architecture with support for PEIs increases the average performance across ten graphprocessing, machine-learning, and data-mining applications by 32% over a CPU-only baseline for small input sets and by 47% for large input sets [21]. While PEIs allow for simple coordination between CPU threads and PIM kernels, they limit both the complexity of computation performed and the amount of data processed by any one PIM kernel, which can incur high overheads when a large number of PIM operations need to be performed.
One approach to perform more work than a single PEI is to ofﬂoad bulk operations to memory, as is done by a number of processing-using-memory architectures. For a bulk operation, the same operation is performed across a large, often contiguous, region of memory (e.g., an 8-KB row of DRAM). Mechanisms for processing-using-memory

can perform a variety of bulk functions, such as bulk copy and data initialization [17, 19], bulk bitwise operations [18, 20, 47–49, 80], and simple arithmetic operations [42, 43, 63, 76, 88, 92–100]. As a representative example, the Ambit processing-using-memory architecture, which enables bulk bitwise operations using DRAM cells, accelerates the performance of database queries and operations on the set data structure by 3–12Â over a CPU-only baseline [18]. There are two tradeoffs to performing bulk operations in memory. First, there are limitations in the amount of data that a single bulk operation processes: For example, a bulk bitwise operation in Ambit cannot be performed on less than one row at a time. Second, the operations that a processing-using-memory architecture can perform are much simpler than those that a general-purpose core can perform, due to limits in the amount of logical functionality that can be implemented inside the memory array.
A second approach to perform more work than a single PEI is to ofﬂoad at the granularity of an application function or a block of instructions in the application [16, 22–24, 62, 75]. There are several ways to demarcate which portions of an application should be ofﬂoaded to PIM. One approach is to surround the portion with compiler directives. For example, if we want to ofﬂoad a function to PIM, we can surround it with #PIM_begin and #PIM_end directives, which a compiler can use to generate a thread for execution on PIM. This approach requires compiler and/or library support to dispatch a PIM kernel to memory, as the programmer needs some way to indicate which regions of a program should be ofﬂoaded to PIM and which regions should not be ofﬂoaded. Our TensorFlow case study in Section 3.3 shows that ofﬂoading at the granularity of functions provides speedups of up to 98.1% when the 16 most time- and energy-consuming GEMM operations make use of PIM accelerators for packing and quantization [16]. As we discuss in Sections 4.2 and 4.3, another issue with this approach is the need to coordinate between the CPU and PIM logic, as CPU threads and PIM kernels can potentially execute concurrently. Examples of this coordination include cache coherence [22–24] and address translation [62]. We note that using simple pragmas to indicate the beginning and end of a PIM kernel represents a ﬁrst step for identifying the blocks of instructions in a program that should be ofﬂoaded to PIM, and we encourage future works to develop more robust and expressive interfaces and mechanisms for PIM ofﬂoading that can allow for better coordination between the CPU and PIM logic (e.g., by building on expressive memory interfaces [144, 145]).
At the other extreme, a PIM kernel can consist of an entire application. Executing an entire application in memory can avoid the need to communicate at all with the CPU. For example, there is no need to perform cache coherence (see Section 4.2) between the CPU and PIM

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 9

logic, as they work on different programs entirely. While this is a simple solution to maintain programmability and avoid signiﬁcant modiﬁcation to hardware, it signiﬁcantly limits the types of applications that can be executed with PIM. As we discuss in Section 3.2, applications with signiﬁcant computational complexity or high temporal locality are best suited for the CPU, but signiﬁcant portions of these applications can beneﬁt from PIM. In order to obtain beneﬁts for such applications when only the entire application can be ofﬂoaded, changes must be made across the entire system. We brieﬂy examine two successful examples of entire application ofﬂoading: Tesseract [5] and GRIM-Filter [6].
Tesseract [5] is an accelerator for in-memory graph processing. Tesseract adds an in-order core to each vault in an HMC-like 3D-stacked memory and implements an efﬁcient communication protocol between these in-order cores. Tesseract combines this new architecture with a message-passing-based programming model, where message passing is used to perform operations on the graph nodes by moving the operations to the vaults where the corresponding graph nodes are stored. For ﬁve state-of-the-art graph processing workloads with large real-world graphs, Tesseract improves the average system performance by 13.8Â, and reduces the energy consumption by 87%, over a conventional CPU-only system [5]. Other recent works build on Tesseract by improving locality and communication for further beneﬁts [146, 147].
GRIM-Filter [6] is an in-memory accelerator for genome seed ﬁltering. In order to read the genome (i.e., DNA sequence) of an organism, geneticists often need to reconstruct the genome from small segments of DNA known as reads, as current DNA extraction techniques are unable to extract the entire DNA sequence. A genome read mapper can perform the reconstruction by matching the reads against a reference genome, and a core part of read mapping is a computationally expensive dynamic programming algorithm that aligns the reads to the reference genome. One technique to signiﬁcantly improve the performance and efﬁciency of read mapping is seed ﬁltering [148–151], which reduces the number of reference genome seeds (i.e., segments) that a read must be checked against for alignment by quickly eliminating seeds with no probability of matching. GRIM-Filter proposes a state-of-the-art ﬁltering algorithm and places the entire algorithm inside memory [6]. This requires adding simple accelerators in the logic layer of 3D-stacked memory and introducing a communication protocol between the read mapper and the ﬁlter. The communication protocol allows GRIM-Filter to be integrated into a full genome read mapper (e.g., FastHASH [148], mrFAST [152], BWA-MEM [153]) by allowing: 1) the read mapper to notify GRIM-Filter

about the DRAM addresses on which to execute customized in-memory ﬁltering operations; 2) GRIMFilter to notify the read mapper once the ﬁlter generates a list of seeds for alignment. Across ten real genome read sets, GRIM-Filter improves the performance of a full state-of-the-art read mapper by 3.65Â over a conventional CPU-only system [6].
4.2 Sharing data between PIM logic and CPUs In order to maximize resource utilization within a system capable of PIM, PIM logic should be able to execute at the same time as CPUs, akin to a multithreaded system. In a traditional multithreaded execution model that uses shared memory between threads, writes to memory must be coordinated between multiple cores to ensure that threads do not operate on stale data values. Due to the per-core caches used in CPUs, this requires that when one core writes data to a memory address, cached copies of the data held within the caches of other cores must be updated or invalidated, which is known as cache coherence. Cache coherence involves a protocol that is designed to handle write permissions for each core, invalidations and updates, and arbitration when multiple cores request exclusive access to the same memory address. Within a chip multiprocessor, the per-core caches can perform coherence actions over a shared interconnect.
Cache coherence is a major system challenge for enabling PIM architectures as general-purpose execution engines. If PIM processing logic is coherent with the processor, the PIM programming model is relatively simple, as it remains similar to conventional shared memory multithreaded programming, which makes PIM architectures easier to adopt in general-purpose systems. Thus, allowing PIM processing logic to maintain such a simple and traditional shared memory programming model can facilitate the widespread adoption of PIM. However, employing traditional ﬁne-grained cache coherence (e.g., a cache-block-based MESI protocol [154]) for PIM forces a large number of coherence messages to traverse the narrow memory channel, potentially undoing the beneﬁts of high-bandwidth and low-latency PIM execution. Unfortunately, solutions for coherence proposed by prior PIM works [5, 21, 75] either place some restrictions on the programming model (by eliminating coherence and requiring message-passingbased programming) or limit the performance and energy gains achievable by a PIM architecture.
To preserve traditional programming models and maximize performance and energy gains, we propose a coherence mechanism for PIM called CoNDA [22–24], which does not need to send a coherence request for every memory access. Instead, as shown in Figure 7, CoNDA enables efﬁcient coherence by having the PIM logic:

3 : 10 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

Figure 7
High-level operation of CoNDA. Adapted from [22].
1) speculatively acquire coherence permissions for multiple memory operations over a given period of time (which we call optimistic execution; ❶ in the ﬁgure);
2) batch the coherence requests from the multiple memory operations into a set of compressed coherence signatures (❷ and ❸);
3) send the signatures to the CPU to determine whether the speculation violated any coherence semantics.
Whenever the CPU receives compressed signatures from the PIM core (e.g., when the PIM kernel ﬁnishes), the CPU performs coherence resolution (❹), where it checks if any coherence conﬂicts occurred. If a conﬂict exists, any dirty cache line in the CPU that caused the conﬂict is ﬂushed, and the PIM core rolls back and re-executes the code that was optimistically executed. Our execution model shares similarities with Bulk-style mechanisms [155–159] (i.e., mechanisms that speculatively execute chunks of instructions and use speculative information on memory accesses to track potential data conﬂicts) and with works that use transactional memory semantics (e.g., [160–164]). However, unlike these past works, the CPU in CoNDA executes conventionally, does not bundle multiple memory accesses into an atomic transaction, and never rolls back, which can make it easier to enable PIM by avoiding the

need for complex checkpointing logic or memory access bundling in a sophisticated out-of-order superscalar CPU.
Figure 8 shows the performance, normalized to CPUonly, of CoNDA and several state-of-the-art cache coherence mechanisms for PIM [22–24]: FG (ﬁne-grained coherence per cache line), CG (coarse-grained locks on shared data regions), and NC (noncacheable data regions). We demonstrate that for applications such as graph workloads and HTAP databases, CoNDA improves average performance by 66.0% over the best prior coherence mechanism for performance (FG), and comes within 10.4% of an unrealistic ideal PIM mechanism where coherence takes place instantly with no cost (Ideal-PIM in Figure 8). For the same applications, CoNDA reduces memory system energy by 18.0% (not shown) over the best prior coherence mechanism for memory system energy (CG). CoNDA’s beneﬁts increase as application data sets become larger [22]: When we increase the dataset sizes by an order of magnitude (not shown), we ﬁnd that CoNDA improves performance by 8.4Â over CPU-only and by 38.3% over the best prior coherence mechanism for performance (FG), coming within 10.2% of Ideal-PIM.
In our prior work on CoNDA [22–24], we provide a detailed discussion of the following:
1) the need for a new coherence model for workloads such as graph frameworks and HTAP databases;
2) the hardware support needed to enable the CoNDA coherence model;
3) a comparison of CoNDA to multiple state-of-the-art coherence models.
4.3 Virtual memory A signiﬁcant hurdle to efﬁcient PIM execution is the need for virtual memory. An application operates in a virtual address space, and when the application needs to access its data inside main memory, the CPU core must ﬁrst perform an address translation, which converts the data’s virtual address into a physical address within main memory. The

Figure 8
Speedup of PIM with various cache coherence mechanisms, including CoNDA [22-24], normalized to CPU-only. Adapted from [22].

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 11

mapping between a virtual address and a physical address is stored in memory in a multilevel page table. Looking up a single virtual-to-physical address mapping requires one memory access per level, incurring a signiﬁcant performance penalty. In order to reduce this penalty, a CPU contains a translation lookaside buffer (TLB), which caches the most recently used mappings. The CPU also includes a page table walker, which traverses the multiple page table levels to retrieve a mapping on a TLB miss.
PIM kernels often need to perform address translation, such as when the code ofﬂoaded to memory needs to traverse a pointer. The pointer is stored as a virtual address and must be translated before PIM logic can access the physical location in memory. A simple solution to provide address translation support for PIM logic could be to issue any translation requests from the PIM logic to the CPU-side virtual memory structures. However, if the PIM logic needs to communicate with existing CPU-side address translation mechanisms, the beneﬁts of PIM could easily be nulliﬁed, as each address translation would need to perform a longlatency request across the memory channel. The translation might sometimes require a page table walk, where the CPU must issue multiple memory requests to read the page table, which would further increase trafﬁc on the memory channel.
A naive solution is to simply duplicate the TLB and page walker within memory (i.e., within the PIM logic). Unfortunately, this is prohibitively difﬁcult or expensive for the following three reasons:
1) Coherence would have to be maintained between the CPU and memory-side TLBs, introducing extra complexity and off-chip requests.
2) The duplicated hardware is very costly in terms of storage overhead and complexity.
3) A memory module can be used in conjunction with many different processor architectures, which use different page table implementations and formats, and ensuring compatibility between the in-memory TLB/page walker and all of these different processor architectures is difﬁcult.
We study how to solve the challenge of virtual memory translation in the context of IMPICA, our in-memory accelerator for efﬁcient pointer chasing [62]. In order to maintain the performance and efﬁciency of the PIM logic, we completely decouple the page table of the PIM logic from that of the CPU. This presents us with two advantages. First, the page table logic in PIM is no longer tied to a single architecture (unlike the CPU page table, which is part of the architectural speciﬁcation) and allows a memory chip with PIM logic to be paired with any CPU. Second, we now have an opportunity to develop a new page table design that is much more efﬁcient for our in-memory accelerator.

We make two key observations about the behavior of a pointer chasing accelerator. First, the accelerator operates only on certain data structures that can be mapped to contiguous regions in the virtual address space, which we refer to as PIM regions. As a result, it is possible to map contiguous PIM regions with a smaller, region-based page table without needing to duplicate the page table mappings for the entire address space. Second, we observe that if we need to map only PIM regions, we can collapse the hierarchy present in conventional page tables, which allows us to limit the hardware and storage overhead of the PIM page table, and cut the number of memory accesses per page table walk down from four (for a conventional fourlevel page table) to two. Based on these observations, we build an efﬁcient page table structure that can be used for a wide range of PIM accelerators, where the accelerators access data in only one region of the total data set. The region-based page table improves the performance of IMPICA by 13.5%, averaged across three linked data traversal benchmarks and a real database workload [165], over a conventional four-level page table. Note that these beneﬁts are part of the 34% performance improvement that IMPICA provides across all of the workloads. More detail on our page table design can be found in our prior work on IMPICA [62].
4.4 Enabling programmers and compilers to ﬁnd PIM targets
In Section 3.2, we discuss our toolﬂow for identifying PIM targets. While this toolﬂow is effective at enabling the co-design of PIM architectures and applications that can take advantage of PIM, it still requires a nontrivial amount of effort on the part of the programmer, as the programmer must ﬁrst run the toolﬂow and then annotate programs using directives such as the ones we discuss in Section 4.1. There is a need to develop even easier methodologies for ﬁnding PIM targets. One alternative is to automate the toolﬂow by developing a PIM compiler that can execute the toolﬂow and then automatically annotate the portions of an application that should be ofﬂoaded to PIM. For example, TOM [75] proposes a compiler-based technique to automatically: 1) identify basic blocks in GPU applications that should be ofﬂoaded to PIM; and 2) map the data needed by such blocks appropriately to memory modules, so as to minimize data movement. Another alternative is to provide libraries of common functions that incorporate PIM ofﬂoading. Programmers could simply call library functions, without worrying about how PIM ofﬂoading takes place, allowing them to easily take advantage of the beneﬁts of PIM. There has been little work in this area to date, and we strongly encourage future researchers and developers to explore these approaches to programming PIM architectures.

3 : 12 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

5 Related work We brieﬂy survey recent related works in processing-inmemory. We provide a brief discussion of early PIM proposals in Section 2.1. Processing-near-memory for 3D-stacked memories: With the advent of 3D-stacked memories, we have seen a resurgence of PIM proposals [13, 14, 20, 82]. Recent PIM proposals add compute units within the logic layer to exploit the high bandwidth available. These works primarily focus on the design of the underlying logic that is placed within memory, and in many cases propose special-purpose PIM architectures that cater only to a limited set of applications. These works include accelerators for matrix multiplication [91], data reorganization [117], graph processing [5, 22–24, 84], databases [22–24, 65], in-memory analytics [68], MapReduce [87], genome sequencing [6], data-intensive processing [70], consumer device workloads [16], machine learning workloads [16, 66, 77, 79], and concurrent data structures [81]. Some works propose more generic architectures by adding PIM-enabled instructions [21], GPGPUs [75, 85, 90], single-instruction multiple-data processing units [83], or reconﬁgurable hardware [67, 69, 71] to the logic layer in 3D-stacked memory. A recently developed framework [25, 166] allows for the rapid design space exploration of processing-near-memory architectures. Processing-using-memory: A number of recent works have examined how to perform memory operations directly within the memory array itself, which we refer to as processing-using-memory [13, 14, 20, 49]. These works take advantage of inherent architectural properties of memory devices to perform operations in bulk. While such works can signiﬁcantly improve computational efﬁciency within memory, they still suffer from many of the same programmability and adoption challenges that PIM architectures face, such as the address translation and cache coherence challenges that we focus on in this article. Mechanisms for processing-using-memory can perform a variety of functions, such as bulk copy and data initialization for DRAM [17, 19]; bulk bitwise operations for DRAM [18, 47–49, 80, 118], PCM [41], or MRAM [44–46]; and simple arithmetic operations for SRAM [63, 76, 100] and RRAM/memristors [42, 43, 88, 92–99]. Processing in the DRAM module or memory controller: Several works have examined how to embed processing functionality near memory, but not within the DRAM chip itself. Such an approach can reduce the cost of PIM manufacturing, as the DRAM chip does not need to be modiﬁed or specialized for any particular functionality. However, these works are often unable to take advantage of the high internal bandwidth of 3D-stacked DRAM, which reduces the efﬁciency of PIM execution, and may still

suffer from many of the same challenges faced by architectures that embed logic within the DRAM chip. Examples of this work include:
1) Gather-Scatter DRAM [87], which embeds logic within the memory controller to remap a single memory request across multiple rows and columns within DRAM;
2) work by Hashemi et al. [72, 73] to embed logic in the memory controller that accelerates dependent cache misses and performs runahead execution [167];
3) Chameleon [64] and the Memory Channel Network architecture [168], which propose methods to integrate logic within the DRAM module but outside of the chip to reduce manufacturing costs.
Addressing challenges to PIM adoption: Recent work has examined design challenges for systems with PIM support that can affect PIM adoption. A number of these works improve PIM programmability, such as CoNDA [22–24], which provides efﬁcient cache coherence support for PIM; the study by Sura et al. [89], which optimizes how programs access PIM data; PEI [21], which introduces an instruction-level interface for PIM that preserves the existing sequential programming models and abstractions for virtual memory and coherence; TOM [75], which automates the identiﬁcation of basic blocks that should be ofﬂoaded to PIM and the data mapping for such blocks; work by Pattnaik et al. [85], which automates whether portions of GPU applications should be scheduled to run on GPU cores or PIM cores; and work by Liu et al. [81], which designs PIM-speciﬁc concurrent data structures to improve PIM performance. Other works tackle hardware-level design challenges, including IMPICA [62], which introduces in-memory support for address translation and pointer chasing; work by Hassan et al. [74] to optimize the 3D-stacked DRAM architecture for PIM; and work by Kim et al. [78] that enables PIM logic to efﬁciently access data across multiple memory stacks. There is a recent work on modeling and understanding the interaction between programs and PIM hardware, such as NAPEL [25, 166], a framework that predicts the potential performance and energy beneﬁts of using PIM.
6 Future challenges In Sections 3 and 4, we demonstrate the need for several solutions to ease programming effort in order to take advantage of the beneﬁts of PIM. We believe that a number of other challenges remain for the widespread adoption of PIM:

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 13

1) PIM Programming Model: Programmers need a well-deﬁned interface to incorporate PIM functionality into their applications. While we brieﬂy discuss several interfaces and mechanisms for ofﬂoading different granularities of applications to PIM, deﬁning a complete programming model for how a programmer should invoke and interact with PIM logic remains an open problem.
2) Data and Logic Mapping: To maximize the beneﬁts of PIM, all of the data that needs to be read from or written to by a single PIM kernel or by a single PIM core should be mapped to the same memory stack or memory channel [21, 75]. This requires system architects and programmers to rethink how and where data is allocated. Likewise, for processingusing-memory architectures, programs often require more complex logic functions than the bitwise operations enabled by these architectures, and require some form of logic mapping or synthesis to allow programmers or compilers to efﬁciently implement these more complex logic functions on the processing-using-memory substrates [169–171]. There is a need to develop robust programmertransparent data mapping and logic mapping/synthesis support for PIM architectures.
3) PIM Runtime Scheduling: There needs to be coordination between PIM logic and the PIM kernels that are either being executed currently or waiting to be executed. Determining when to enable and disable PIM execution [21], what to execute in memory, how to share PIM cores and PIM accelerators across multiple CPU threads/cores, and how to coordinate between PIM logic accesses and CPU accesses to memory are all important runtime attributes that must be addressed.
New performance and energy prediction frameworks [25] and simulation tools [166] can help researchers with solving several of the remaining challenges. We refer the reader to our overview works [13, 14, 172, 173] on enabling the adoption of PIM for further discussion of these challenges.
7 Conclusion While many important classes of emerging AI, machine learning, and data analytics applications are operating on very large datasets, conventional computer systems are not designed to handle such large-scale data. As a result, the performance and energy costs associated with moving data between main memory and the CPU dominate the total costs of computation, which is a phenomenon known as the data movement bottleneck. To alleviate this bottleneck, a number of recent works propose PIM, where unnecessary data movement is reduced or eliminated by bringing some or all of the computation into memory. There are many practical system-level challenges that need to be solved to enable the

widespread adoption of PIM. In this work, we examine how these challenges relate to programmers and system architects, and describe several of our solutions to facilitate the systematic ofﬂoading of computation to PIM logic. In a case study, we demonstrate our ofﬂoading toolﬂow with Google’s TensorFlow Lite framework for neural network inference, demonstrating that we can achieve performance improvements of up to 98.1%, while reducing energy consumption by an average of 54.9%. We then discuss the need for mechanisms that preserve conventional programming models when ofﬂoading computation to PIM. We discuss several such mechanisms, which provide various methods of ofﬂoading portions of applications to PIM logic, sharing data between PIM logic and CPUs, enabling efﬁcient virtual memory access for PIM, and automating PIM target identiﬁcation and ofﬂoading. Finally, we describe a number of remaining challenges to the widespread adoption of PIM. We hope that our work and analysis inspire researchers to tackle these remaining challenges, which can enable the commercialization of PIM architectures.
Acknowledgment We thank all of the members of the SAFARI Research Group, and our collaborators at Carnegie Mellon, ETH Z€urich, and other universities, who have contributed to the various works we describe in this article. Thanks also goes to our research group’s industrial sponsors over the past ten years, especially Alibaba, Facebook, Google, Huawei, Intel, Microsoft, NVIDIA, Samsung, Seagate, and VMware. This work was supported in part by the Intel Science and Technology Center for Cloud Computing, in part by the Semiconductor Research Corporation, in part by the Data Storage Systems Center at Carnegie Mellon University, and in part by past NSF Grants 1212962, 1320531, and 1409723 and past NIH Grant HG006004.
References
1. J. Dean and L. A. Barroso, “The tail at scale,” Commun. ACM, vol. 55, pp. 74–80, 2013.
2. M. Ferdman, A. Adileh, O. Kocberber, et al., “Clearing the clouds: A study of emerging scale-out workloads on modern hardware,” in Proc. 17th Int. Conf. Archit. Support Programming Lang. Oper. Syst., 2012, pp. 37–48.
3. S. Kanev, J. P. Darago, K. Hazelwood, et al., “Proﬁling a warehouse-scale computer,” in Proc. 42nd Annu. Int. Symp. Comput. Archit., 2015, pp. 158–169.
4. L. Wang, J. Zhan, C. Luo, et al., “BigDataBench: A big data benchmark suite from internet services,” in Proc. IEEE 20th Int. Symp. High Perform. Comput. Archit., 2014.
5. J. Ahn, S. Hong, S. Yoo, et al., “A scalable processing-in-memory accelerator for parallel graph processing,” in Proc. 42nd Annu. Int. Symp. Comput. Archit., 2015, pp. 105–117.
6. J. S. Kim, D. Senol, H. Xin, et al., “GRIM-Filter: Fast seed location ﬁltering in DNA read mapping using processing-in-memory technologies,” BMC Genomics, vol. 19, 2018, Art. no. 89.
7. K. Hsieh, G. Ananthanarayanan, P. Bodik, et al., “Focus: Querying large video datasets with low latency and low cost,” in Proc. 12th USENIX Conf. Oper. Syst. Des. Implementation, 2018, pp. 269–286.

3 : 14 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

8. K. Hsieh, A. Harlap, N. Vijaykumar, et al., “Gaia: Geo-distributed machine learning approaching LAN speeds,” in Proc. 14th USENIX Conf. Netw. Syst. Des. Implementation, 2017, pp. 629–647.
9. G. Ananthanarayanan, P. Bahl, P. Bodik, et al., “Real-time video analytics: The killer app for edge computing,” Computer, vol. 50, no. 10, pp. 58–67, 2017.
10. JEDEC Solid State Technology Association, JESD79-3F: DDR3 SDRAM Standard, Jul. 2012.
11. JEDEC Solid State Technology Association, JESD79-4B: DDR4 SDRAM Standard, Jun. 2017.
12. D. Lee, L. Subramanian, R. Ausavarungnirun, et al., “Decoupled direct memory access: Isolating CPU and IO trafﬁc by leveraging a dual-data-port DRAM,” in Proc. Int. Conf. Parallel Archit. Compilation, 2015, pp. 174–187.
13. S. Ghose, K. Hsieh, A. Boroumand, et al., “The processing-inmemory paradigm: Mechanisms to enable adoption,” in BeyondCMOS Technologies for Next Generation Computer Design. New York, NY, USA: Springer, 2019.
14. S. Ghose, K. Hsieh, A. Boroumand, et al., “Enabling the adoption of processing-in-memory: Challenges, mechanisms, future research directions,” arXiv:1802.00320, 2018.
15. S. W. Keckler, W. J. Dally, B. Khailany, et al., “GPUs and the future of parallel computing,” IEEE Micro, vol. 31, no. 5, pp. 7–17, Sep./Oct. 2011.
16. A. Boroumand, S. Ghose, Y. Kim, et al., “Google workloads for consumer devices: Mitigating data movement bottlenecks,” in Proc. 23rd Int. Conf. Archit. Support Programming Lang. Oper. Syst., 2018, pp. 336–331.
17. K. K. Chang, P. J. Nair, S. Ghose, et al., “Low-cost inter-linked subarrays (LISA): Enabling fast inter-subarray data movement in DRAM,” in Proc. IEEE Int. Symp. High Perform. Comput. Archit., 2016, pp. 568–580.
18. V. Seshadri, D. Lee, T. Mullins, et al., “Ambit: In-memory accelerator for bulk bitwise operations using commodity DRAM technology,” in Proc. 50th Annu. IEEE/ACM Int. Symp. Microarchit., 2017, pp. 273–287.
19. V. Seshadri, Y. Kim, C. Fallin, et al., “RowClone: Fast and energy-efﬁcient in-DRAM bulk data copy and initialization,” in Proc. 46th Annu. IEEE/ACM Int. Symp. Microarchit., 2013, pp. 185–197.
20. V. Seshadri and O. Mutlu, “Simple operations in memory to reduce data movement,” Adv. Comput., vol. 106, pp. 107–166, 2017.
21. J. Ahn, S. Yoo, O. Mutlu, et al., “PIM-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture,” in Proc. 42nd Annu. Int. Symp. Comput. Archit., 2015, pp. 336–348.
22. A. Boroumand, S. Ghose, M. Patel, et al., “CoNDA: Efﬁcient cache coherence support for near-data accelerators,” in Proc. 46th Int. Symp. Comput. Archit., 2019, pp. 629–642.
23. A. Boroumand, S. Ghose, M. Patel, et al., “LazyPIM: An efﬁcient cache coherence mechanism for processing-in-memory,” IEEE Comput. Archit. Lett., vol. 16, no. 1, pp. 46–50, Jan.–Jun. 2017.
24. A. Boroumand, S. Ghose, M. Patel, et al., “LazyPIM: Efﬁcient support for cache coherence in processing-in-memory architectures,” arXiv:1706.03162, 2017.
25. G. Singh, J. Gomez-Luna, G. Mariani, et al., “NAPEL: Nearmemory computing application performance prediction via ensemble learning,” in Proc. 46th Annu. Des. Autom. Conf., 2019.
26. J. Draper, J. Chame, M. Hall, et al., “The architecture of the DIVA Processing-in-memory chip,” in Proc. 16th Int. Conf. Supercomput., 2002, pp. 14–25.
27. D. Elliott, M. Stumm, W. M. Snelgrove, et al., “Computational RAM: Implementing processors in memory,” IEEE Des. Test, vol. 16, no. 1, pp. 32–41, Jan. 1999.
28. D. G. Elliott, W. M. Snelgrove, and M. Stumm, “Computational RAM: A memory-SIMD hybrid and its application to DSP,” in Proc. IEEE Custom Integr. Circuits Conf., 1992, pp. 30.6.1–30.6.4.
29. M. Gokhale, B. Holmes, and K. Iobst, “Processing in memory: The Terasys massively parallel PIM array,” IEEE Comput., vol. 28, no. 4, pp. 23–31, Apr. 1995.

30. Y. Kang, W. Huang, S.-M. Yoo, et al., “FlexRAM: Toward an advanced intelligent memory system,” in Proc. IEEE Int. Conf. Comput. Des., 1999, pp. 192–201.
31. P. M. Kogge, “EXECUBE–A new architecture for scaleable MPPs,” in Proc. Int. Conf. Parallel Process., 1994, pp. 77–84.
32. K. Mai, T. Paaske, N. Jayasena, et al., “Smart memories: A modular reconﬁgurable architecture,” in Proc. 27th Annu. Int. Symp. Comput. Archit., 2000, pp. 161–171.
33. M. Oskin, F. T. Chong, and T. Sherwood, “Active pages: A computation model for intelligent memory,” in Proc. 25th Annu. Int. Symp. Comput. Archit., 1998, pp. 192–203.
34. D. Patterson, T. Anderson, N. Cardwell, et al., “A case for intelligent RAM,” IEEE Micro, vol. 17, no. 2, pp. 34–44, Mar. 1997.
35. D. E. Shaw, S. J. Stolfo, H. Ibrahim, et al., “The NON-VON database machine: A brief overview,” IEEE Database Eng. Bull., vol. 4, no. 2, pp. 41–52, Dec. 1981.
36. H. S. Stone, “A logic-in-memory computer,” IEEE Trans. Comput., vol. 19, no. 1, pp. 73–78, Jan. 1970.
37. JEDEC Solid State Technology Association, JESD235B: High Bandwidth Memory (HBM) DRAM Standard, Nov. 2018.
38. D. Lee, S. Ghose, G. Pekhimenko, et al., “Simultaneous multi-layer access: Improving 3D-stacked memory bandwidth at low cost,” ACM Trans. Archit. Code Optim., vol. 12, 2016, Art. no. 63.
39. G. H. Loh, “3D-Stacked memory architectures for multi-core processors,” in Proc. 35th Annu. Int. Symp. Comput. Archit., 2008, pp. 453–464.
40. Hybrid Memory Cube Consortium, “HMC Speciﬁcation 2.0,” 2014.
41. S. Li, C. Xu, Q. Zou, et al., “Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories,” in Proc. 53rd Annu. Des. Autom. Conf., 2016.
42. Y. Levy, J. Bruck, Y. Cassuto, et al., “Logic operations in memory using a memristive akers array,” Microelectronics J., vol. 45, pp. 1429–1437, 2014.
43. S. Kvatinsky, D. Belousov, S. Liman, et al., “MAGIC— Memristor-aided logic,” IEEE Trans. Circuits Syst. II, Exp. Briefs, vol. 61, no. 11, pp. 895–899, Nov. 2014.
44. S. Angizi, Z. He, and D. Fan, “PIMA-Logic: A novel processingin-memory architecture for highly ﬂexible and energy-efﬁcient logic computation,” in Proc. 55th Annu. Des. Autom. Conf., 2018.
45. S. Angizi, Z. He, A. S. Rakin, et al., “CMP-PIM: An energyefﬁcient comparator-based processing-in-memory neural network accelerator,” in Proc. 55th Annu. Des. Autom. Conf., 2018.
46. S. Angizi, J. Sun, W. Zhang, et al., “AlignS: A processing-inmemory accelerator for DNA short read alignment leveraging SOT-MRAM,” in Proc. 56th Annu. Des. Autom. Conf., 2019.
47. V. Seshadri, K. Hsieh, A. Boroumand, et al., “Fast bulk bitwise AND and OR in DRAM,” IEEE Comput. Archit. Lett., vol. 14, no. 2, pp. 127–131, Jul. 2015.
48. V. Seshadri, D. Lee, T. Mullins, et al., “Buddy-RAM: Improving the performance and efﬁciency of bulk bitwise operations using DRAM,” arXiv:1611.09988, 2016.
49. V. Seshadri and O. Mutlu, “In-DRAM bulk bitwise execution engine,” arXiv:1905.09822, 2019.
50. B. C. Lee, E. Ipek, O. Mutlu, et al., “Architecting phase change memory as a scalable DRAM alternative,” in Proc. 36th Annu. Int. Symp. Comput. Archit., 2009, pp. 2–13.
51. B. C. Lee, E. Ipek, O. Mutlu, et al., “Phase change memory architecture and the quest for scalability,” Commun. ACM, vol. 53, no. 7, pp. 99–106, Jul. 2010.
52. B. C. Lee, P. Zhou, J. Yang, et al., “Phase-change technology and the future of main memory,” IEEE Micro, vol. 30, no. 1, pp. 143–143, Jan./Feb. 2010.
53. M. K. Qureshi, V. Srinivasan, and J. A. Rivers, “Scalable high performance main memory system using phase-change memory technology,” in Proc. 36th Annu. Int. Symp. Comput. Archit., 2009, pp. 24–33.
54. H.-S. P. Wong, S. Raoux, S. Kim, et al., “Phase change memory,” Proc. IEEE, vol. 98, no. 12, pp. 2201–2227, Dec. 2010.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 15

55. H. Yoon, J. Meza, N. Muralimanohar, et al., “Efﬁcient data mapping and buffering techniques for multi-level cell phasechange memories,” ACM Trans. Archit. Code Optim., vol. 11, Dec. 2014, Art. no. 40.
56. P. Zhou, B. Zhao, J. Yang, et al., “A durable and energy efﬁcient main memory using phase change memory technology,” in Proc. 36th Annu. Int. Symp. Comput. Archit., 2009, pp. 14–23.
57. E. K€ult€ursay, M. Kandemir, A. Sivasubramaniam, et al., “Evaluating STT-RAM as an energy-efﬁcient main memory alternative,” in Proc. IEEE Int. Symp. Perform. Anal. Syst. Softw., 2013, pp. 256–267.
58. H. Naeimi, C. Augustine, A. Raychowdhury, et al., “STT-RAM scaling and retention failure,” Intel Technol. J., vol. 17, pp. 54–75, May 2013.
59. L. Chua, “Memristor—The missing circuit element,” IEEE Trans. Circuit Theory, vol. 18, no. 5, pp. 507–519, Sep. 1971.
60. D. B. Strukov, G. S. Snider, D. R. Stewart, et al., “The missing memristor found,” Nature, vol. 453, pp. 80–83, May 2008.
61. H.-S. P. Wong, H.-Y. Lee, S. Yu, et al., “Metal-oxide RRAM,” Proc. IEEE, vol. 100, no. 6, pp. 1951–1970, Jun. 2012.
62. K. Hsieh, S. Khan, N. Vijaykumar, et al., “Accelerating pointer chasing in 3D-stacked memory: Challenges, mechanisms, evaluation,” in Proc. 34th Int. Conf. Comput. Des., 2016.
63. S. Aga, S. Jeloka, A. Subramaniyan, et al., “Compute caches,” in Proc. Int. Symp. High Perform. Comput. Archit., 2017.
64. H. Asghari-Moghaddam, Y. H. Son, J. H. Ahn, et al., “Chameleon: Versatile and practical near-DRAM acceleration architecture for large memory systems,” in Proc. 49th Annu. IEEE/ACM Int. Symp. Microarchit., 2016.
65. O. O. Babarinsa and S. Idreos, “JAFAR: Near-Data processing for databases,” in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2015, pp. 2069–2070.
66. P. Chi, S. Li, C. Xu, et al., “PRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-Based main memory,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 27–39.
67. A. Farmahini-Farahani, J. H. Ahn, K. Morrow, et al., “NDA: NearDRAM acceleration architecture leveraging commodity DRAM devices and standard memory modules,” in Proc. IEEE 21st Int. Symp. High Perform. Comput. Archit., 2015, pp. 283–295.
68. M. Gao, G. Ayers, and C. Kozyrakis, “Practical near-data processing for In-Memory analytics frameworks,” in Proc. Int. Conf. Parallel Archit. Compilation, 2015, pp. 113–124.
69. M. Gao and C. Kozyrakis, “HRL: Efﬁcient and ﬂexible reconﬁgurable logic for near-data processing,” in Proc. Int. Symp. High Perform. Comput. Archit., 2016, pp. 126–137.
70. B. Gu, A. S. Yoon, D.-H. Bae, et al., “Biscuit: A framework for near-data processing of Big data workloads,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 153–165.
71. Q. Guo, N. Alachiotis, B. Akin, et al., “3D-stacked memory-side acceleration: Accelerator and system design,” in Proc. Workshop Near-Data Process., 2014.
72. M. Hashemi, O. Mutlu, and Y. N. Patt, “Continuous runahead: Transparent hardware acceleration for memory intensive workloads,” in Proc. 49th Annu. IEEE/ACM Int. Symp. Microarchit., 2016.
73. M. Hashemi, Khubaib, E. Ebrahimi, et al., “Accelerating dependent cache misses with an enhanced memory controller,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 444–455.
74. S. M. Hassan, S. Yalamanchili, and S. Mukhopadhyay, “Near data processing: Impact and optimization of 3D memory system architecture on the uncore,” in Proc. Int. Symp. Memory Syst., 2015, pp. 11–21.
75. K. Hsieh, E. Ebrahimi, G. Kim, et al., “Transparent ofﬂoading and mapping (TOM): Enabling programmer-transparent near-data processing in GPU systems,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 204–216.
76. M. Kang, M.-S. Keel, N. R. Shanbhag, et al., “An energy-efﬁcient VLSI architecture for pattern recognition via deep embedding of computation in SRAM,” in Proc. Int. Conf. Acoustics Speech Signal Process., 2014, pp. 8326–8330.

77. D. Kim, J. Kung, S. Chai, et al., “Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 380–392.
78. G. Kim, N. Chatterjee, M. O’Connor, et al., “Toward standardized near-data processing with unrestricted data placement for GPUs,” in Proc. Int. Conf. High Perform. Comput. Netw., Storage Anal., 2017.
79. J. H. Lee, J. Sim, and H. Kim, “BSSync: Processing near memory for machine learning workloads with bounded staleness consistency models,” in Proc. Int. Conf. Parallel Archit. Compilation, 2015, pp. 241–252.
80. S. Li, D. Niu, K. T. Malladi, et al., “DRISA: A DRAM-based reconﬁgurable in-situ accelerator,” in Proc. 50th Annu. IEEE/ ACM Int. Symp. Microarchit., 2017, pp. 288–301.
81. Z. Liu, I. Calciu, M. Herlihy, et al., “Concurrent data structures for near-memory computing,” in Proc. 29th ACM Symp. Parallelism Algorithms Archit., 2017, pp. 235–245.
82. G. H. Loh, N. Jayasena, M. Oskin, et al., “A processing in memory taxonomy and a case for studying ﬁxed-function PIM,” in Proc. 3rd Workshop Near-Data Process., 2013.
83. A. Morad, L. Yavits, and R. Ginosar, “GP-SIMD processing-inmemory,” ACM Trans. Archit. Code Optim., vol. 11, 2015, Art. no. 53.
84. L. Nai, R. Hadidi, J. Sim, et al., “GraphPIM: Enabling instruction-level PIM ofﬂoading in graph computing frameworks,” in Proc. IEEE Int. Symp. High Perform. Cmput. Archit., 2017, pp. 457–468.
85. A. Pattnaik, X. Tang, A. Jog, et al., “Scheduling techniques for GPU architectures with processing-in-memory capabilities,” in Proc. Int. Conf. Parallel Archit. Compilation, 2016, pp. 31–44.
86. S. H. Pugsley, J. Jestes, H. Zhang, et al., “NDC: Analyzing the impact of 3D-stacked memoryþLogic devices on mapreduce workloads,” in Proc. Int. Symp. Perform. Anal. Syst. Softw., 2014, pp. 190–200.
87. V. Seshadri, T. Mullins, A. Boroumand, et al., “Gather-scatter DRAM: In-DRAM address translation to improve the spatial locality of non-unit strided accesses,” in Proc. 48th Int. Symp. Microarchit., 2015, pp. 267–280.
88. A. Shaﬁee, A. Nag, N. Muralimanohar, et al., “ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars,” in Proc. 43rd Int. Symp. Comput. Archit., 2016, pp. 14–26.
89. Z. Sura, A. Jacob, T. Chen, et al., “Data access optimization in a processing-in-memory system,” in Proc. 12th ACM Int. Conf. Comput. Frontiers, 2015, Art. no. 6.
90. D. P. Zhang, N. Jayasena, A. Lyashevsky, et al., “TOP-PIM: Throughput-oriented programmable processing in memory,” in Proc. 23rd Int. Symp. High-Perform. Parallel Distrib. Comput., 2014, pp. 85–98.
91. Q. Zhu, T. Graf, H. E. Sumbul, et al., “Accelerating sparse matrixmatrix multiplication with 3D-stacked logic-in-memory hardware,” in Proc. High Perform. Extreme Comput. Conf., 2013.
92. S. Kvatinsky, A. Kolodny, U. C. Weiser, et al., “MemristorBased IMPLY logic design procedure,” in Proc. IEEE 29th Int. Conf. Comput. Des., 2011, pp. 142–147.
93. S. Kvatinsky, G. Satat, N. Wald, et al., “Memristor-based material implication (IMPLY) logic: Design principles and methodologies,” IEEE Trans. Very Large Scale Integr., vol. 22, no. 10, pp. 2054–2066, Oct. 2014.
94. P.-E. Gaillardon, L. Amaru, A. Siemon, et al., “The programmable logic-in-memory (PLiM) computer,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2016, pp. 427–432.
95. D. Bhattacharjee, R. Devadoss, and A. Chattopadhyay, “ReVAMP: ReRAM based VLIW architecture for in-memory computing,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2017, pp. 782–787.
96. S. Hamdioui, L. Xie, H. A. D. Nguyen, et al., “Memristor based computation-in-memory architecture for data-intensive applications,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2015, pp. 1718–1725.
97. L. Xie, H. A. D. Nguyen, M. Taouil, et al., “Fast boolean logic mapped on memristor crossbar,” in Proc. 33rd IEEE Int. Conf. Comput. Des., 2015, pp. 335–342.

3 : 16 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

98. S. Hamdioui, S. Kvatinsky, G. Cauwenberghs, et al., “Memristor for computing: Myth or reality?,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2017, pp. 722–731.
99. J. Yu, H. A. D. Nguyen, L. Xie, et al., “Memristive devices for computation-in-memory,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2018, pp. 1646–1651.
100. C. Eckert, X. Wang, J. Wang, et al., “Neural cache: Bit-serial incache acceleration of deep neural networks,” in Proc. ACM/IEEE 45th Annu. Int. Symp. Comput. Archit., 2018, pp. 383–396.
101. U. Kang, H.-S. Yu, C. Park, et al., “Co-architecting controllers and DRAM to enhance DRAM process scaling,” in The Memory Forum, 2014.
102. O. Mutlu, “Memory scaling: A systems architecture perspective,” in Proc. 5th IEEE Int. Memory Workshop, 2013.
103. O. Mutlu, “The RowHammer problem and other issues we may face as memory becomes denser,” in Proc. Des. Autom. Test Eur. Conf. Exhib., 2017, pp. 1116–1121.
104. O. Mutlu and L. Subramanian, “Research problems and opportunities in memory systems,” Supercomput. Frontiers Innov., Int. J., vol. 1, pp. 19–55, 2014.
105. Y. Kim, R. Daly, J. Kim, et al., “Flipping bits in memory without accessing them: An experimental study of DRAM disturbance errors,” in Proc. 41st Annu. Int. Symp. Comput. Archit., 2014, pp. 361–372.
106. O. Mutlu and J. S. Kim, “RowHammer: A retrospective,” IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., to be published, doi: 10.1109/TCAD.2019.2915318.
107. J. A. Mandelman, R. H. Dennard, G. B. Bronner, et al., “Challenges and future directions for the scaling of dynamic random-access memory (DRAM),” IBM J. Res. Develop., vol. 46, no. 2/3, pp. 187– 212, Mar./May, 2002.
108. K. K. Chang, A. Kashyap, H. Hassan, et al., “Understanding latency variation in modern DRAM chips: Experimental characterization, analysis, and optimization,” in Proc. ACM SIGMETRICS Int. Conf. Meas. Model. Comput. Sci., 2016, pp. 323–336.
109. D. Lee, Y. Kim, V. Seshadri, et al., “Tiered-latency DRAM: A low latency and low cost DRAM architecture,” in Proc. IEEE 19th Int. Symp. High Perform. Comput. Archit., 2013, pp. 615–626.
110. D. Lee, Y. Kim, G. Pekhimenko, et al., “Adaptive-latency DRAM: Optimizing DRAM timing for the common-case,” in Proc. IEEE 21st Int. Symp. High Perform. Comput. Archit., 2015, pp. 489–501.
111. D. Lee, S. Khan, L. Subramanian, et al., “Design-induced latency variation in modern DRAM Chips: Characterization, analysis, and latency reduction mechanisms,” in ACM SIGMETRICS Perform. Eval. Rev., vol. 44, pp. 54–54, 2017.
112. S. Ghose, T. Li, N. Hajinazar, et al., “Demystifying complex workload–DRAM interactions: An experimental study,” in Proc. SIGMETRICS/Perform. Joint Int. Conf. Manage. Model. Comput. Syst., 2019, p. 93.
113. S. Ghose, A. G. Yaglikci, R. Gupta, et al., “What your DRAM power models are not telling you: Lessons from a detailed experimental study,” Proc. ACM Meas. Anal. Comput. Syst., vol. 2, 2018, Art. no. 38.
114. K. K. Chang, A. G. Yaglıkc¸ı, S. Ghose, et al., “Understanding reduced-voltage operation in modern DRAM Devices: Experimental characterization, analysis, and mechanisms,” Proc. ACM Meas. Anal. Comput. Syst., vol. 1, 2017, Art. no. 10.
115. JEDEC Solid State Technology Association, JESD229: Wide I/O Single Data Rate (Wide I/O SDR) Standard, Dec. 2011.
116. JEDEC Solid State Technology Association, JESD229-2: Wide I/O 2 (WideIO2) Standard, Aug. 2014.
117. B. Akin, F. Franchetti, and J. C. Hoe, “Data reorganization in memory using 3D-Stacked DRAM,” in Proc. 42nd Annu. Int. Symp. Comput. Archit., 2015, pp. 131–143.
118. S. Angizi and D. Fan, “Accelerating bulk bit-wise X(N)OR operation in processing-in-DRAM platform,” arXiv:1904.05782, 2019.
119. M. Gao, J. Pu, X. Yang, et al., “TETRIS: Scalable and efﬁcient neural network acceleration with 3D memory,” in Proc. 22nd Int. Conf. Archit. Support Program. Lang. Oper. Syst., 2017, pp. 751–764.

120. M. Drumond, A. Daglis, N. Mirzadeh, et al., “The mondrian data engine,” in Proc. 44th Annu. Int. Symp. Comput. Archit., 2017, pp. 639–651.
121. J. Jeddeloh and B. Keeth, “Hybrid memory cube new DRAM architecture increases density and performance,” in Proc. Symp. VLSI Technol., 2012, pp. 87–88.
122. C. Chou, P. Nair, and M. K. Qureshi, “Reducing refresh power in mobile devices with morphable ECC,” in Proc. 45th Annu. IEEE/ IFIP Int. Conf. Dependable Syst. Netw., 2015, pp. 355–366.
123. Y. Kim, D. Han, O. Mutlu, et al., “ATLAS: A scalable and high-performance scheduling algorithm for multiple memory controllers,” in Proc. Int. Symp. High-Perform. Comput. Archit., 2010.
124. Y. Kim, M. Papamichael, O. Mutlu, et al., “Thread cluster memory scheduling: Exploiting differences in memory access behavior,” in Proc. 43rd Annu. IEEE/ACM Int. Symp. Microarchit., 2010, pp. 65–76.
125. S. P. Muralidhara, L. Subramanian, O. Mutlu, et al., “Reducing memory interference in multicore systems via application-aware memory channel partitioning,” in Proc. 44th Annu. IEEE/ACM Int. Symp. Microarchit., 2011, pp. 374–385.
126. Google LLC, “TensorFlow Lite: For mobile & IoT.” [Online]. Available: htt_ps://www_.tensorﬂow.org/lite/
127. J. Shun and G. E. Blelloch, “Ligra: A lightweight graph processing framework for shared memory,” in Proc. 18th ACM SIGPLAN Symp. Principles Practice Parallel Program., 2013, pp. 135–146.
128. S. Brin and L. Page, “The anatomy of a large-scale hypertextual websearch engine,” in Proc. 7th Int. Conf. World Wide Web, 1998, pp. 107–117.
129. S. Hong, H. Chafﬁ, E. Sedlar, et al., “Green-Marl: A DSL for easy and efﬁcient graph analysis,” in Proc. 17th Int. Conf. Architectural Support Programming Lang. Operating Syst., 2012, pp. 349–362.
130. S. Hong, S. Salihoglu, J. Widom, et al., “Simplifying scalable graph processing with a domain-speciﬁc language,” in Proc. Annu. IEEE/ACM Int. Symp. Code Gener. Optim., 2014, p. 208.
131. G. Malewicz, M. H. Austern, A. J. C. Bik, et al., “Pregel: A system for large-scale graph processing,” in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2010, pp. 135–146.
132. J. Xue, Z. Yang, Z. Qu, et al., “Seraph: An efﬁcient, low-cost system for concurrent graph processing,” in Proc. 23rd Int. Symp. High-Perform. Parallel Distrib. Comput., 2014, pp. 227–238.
133. MemSQL, Inc., “MemSQL.” [Online]. Available: ht_tp://www_. memsql.com/
134. SAP SE, “SAP HANA.” [Online]. Available: ht_tp://www_.hana. sap.com/
135. M. Stonebraker and A. Weisberg, “The VoltDB main memory DBMS,” IEEE Data Eng. Bull., vol. 36, no. 2, pp. 21–27, Jun. 2013.
136. Google LLC, “Google Translate app.” [Online]. Available: htt_ps://translate.google.com/intl/en/about/
137. K. He, X. Zhang, S. Ren, et al., “Identity mappings in deep residual networks,” in Proc. Eur. Conf. Comput. Vis., 2016, pp. 630–645.
138. K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Proc. 3rd IAPR Asian Conf. Pattern Recognit., 2015.
139. G. Toderici, D. Vincent, N. Johnston, et al., “Full resolution image compression with recurrent neural networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 5435– 5443.
140. C. Szegedy, S. Ioffe, V. Vanhoucke, et al., “Inception-v4, InceptionResNet and the impact of residual connections on learning,” in Proc. 31st AAAI Conf. Artif. Intell., 2017, pp. 4278–4284.
141. R. Adolf, S. Rama, B. Reagen, et al., “Fathom: Reference workloads for modern deep learning methods,” in Proc. IEEE Int. Symp. Workload Characterization, 2016, pp. 148–157.
142. Google LLC, “Gemmlowp: A small self-contained low-precision GEMM library.” [Online]. Availble: htt_ps://github.com/google/ gemmlowp
143. ARM Holdings PLC, “ARM Cortex-R8.” [Online]. Available: htt_ps://developer.arm.com/products/processors/cortex-r/cortex-r8

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 17

144. N. Vijaykumar, A. Jain, D. Majumdar, et al., “A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory,” in Proc. 45th Annu. Int. Symp. Comput. Archit., 2018, pp. 207–220.
145. N. Vijaykumar, E. Ebrahimi, K. Hsieh, et al., “The locality descriptor: A holistic cross-layer abstraction to express data locality in GPUs,” in Proc. 45th Annu. Int. Symp. Comput. Archit., 2018, pp. 829–842.
146. G. Dai, T. Huang, Y. Chi, et al., “GraphH: A processing-inmemory architecture for large-scale graph processing,” IEEE Trans. Comput-Aided Des. Integr. Circuits Syst., vol. 38, no. 4, pp. 640–653, Apr. 2019.
147. M. Zhang, Y. Zhuo, C. Wang, et al., “GraphP: Reducing communication for PIM-based graph processing with efﬁcient data partition,” in Proc. IEEE Int. Symp. High Perform. Comput. Archit., 2018, pp. 544–557.
148. H. Xin, D. Lee, F. Hormozdiari, et al., “Accelerating read mapping with FastHASH,” BMC Genomics, vol. 14, 2013, Art. no. S13.
149. H. Xin, J. Greth, J. Emmons, et al., “Shifted hamming distance: A fast and accurate SIMD-Friendly ﬁlter to accelerate alignment veriﬁcation in read mapping,” Bioinformatics, vol. 31, pp. 1553–1560, 2015.
150. M. Alser, H. Hassan, H. Xin, et al., “GateKeeper: A new hardware architecture for accelerating Pre-Alignment in DNA short read mapping,” Bioinformatics, vol. 33, pp. 3355–3363, 2017.
151. M. Alser, H. Hassan, A. Kumar, et al., “Shouji: A fast and efﬁcient pre-alignment ﬁlter for sequence alignment,” Bioinformatics, 2019, doi: 10.1093/bioinformatics/btz234.
152. C. Alkan, J. M. Kidd, T. Marques-Bonet, et al., “Personalized copy number and segmental duplication maps using next-generation sequencing,” Nature Genetics, vol. 41, pp. 1061–1067, 2009.
153. H. Li and R. Durbin, “Fast and accurate short read alignment with burrows-wheeler transform,” Bioinformatics, vol. 25, pp. 1754–1760, 2009.
154. M. S. Papamarcos and J. H. Patel, “A low-overhead coherence solution for multiprocessors with private cache memories,” in Proc. 11th Annu. Int. Symp. Comput. Archit., 1984, pp. 348–354.
155. L. Ceze, J. Tuck, P. Montesinos, et al., “BulkSC: Bulk enforcement of sequential consistency,” in Proc. 34th Annu. Int. Symp. Comput. Archit., 2007, pp. 278–289.
156. L. Ceze, J. Tuck, C. Cas¸caval, et al., “Bulk disambiguation of speculative threads in multiprocessors,” in Proc. 33rd Annu. Int. Symp. Comput. Archit., 2006, pp. 227–238.
157. E. Vallejo, M. Galluzzi, A. Cristal, et al., “Implementing kilo-instruction multiprocessors,” in Proc. Int. Conf. Pervasive Services, 2005, pp. 325–336.
158. T. F. Wenisch, A. Ailamaki, B. Falsaﬁ, et al., “Mechanisms for store-wait-free multiprocessors,” in Proc. 34th Annu, Int. Symp. Comput. Archit., 2007, pp. 266–277.
159. C. Zilles and G. Sohi, “Master/slave speculative parallelization,” in Proc. 35th Annu. IEEE/ACM Int. Symp. Microarchit., 2002, pp. 85–96.
160. C. S. Ananian, K. Asanovic, B. C. Kuszmaul, et al., “Unbounded transactional memory,” in Proc. 11th Int. Symp. High-Perform. Comput. Archit., 2005, pp. 316–327.
161. L. Hammond, V. Wong, M. Chen, et al., “Transactional memory coherence and consistency,” in Proc. 31st Annu. Int. Symp. Comput. Archit., 2004, p. 102.
162. M. Herlihy and J. E. B. Moss, “Transactional memory: Architectural support for lock-free data structures,” in Proc. 20th Annu. Int. Symp. Comput. Archit., 1993, pp. 289–300.
163. K. E. Moore, J. Bobba, M. J. Moravan, et al., “LogTM: Log-based transactional memory,” in Proc. 12th Int. Symp. High-Perform. Comput. Archit., 2006, pp. 254–265.
164. N. Shavit and D. Touitou, “Software transactional memory,” Distrib. Comput., vol. 10, pp. 99–116, 1997.
165. X. Yu, G. Bezerra, A. Pavlo, et al., “Staring into the abyss: An evaluation of concurrency control with one thousand cores,” in Proc. VLDB Endowment, 2014, pp. 209–220.

166. SAFARI Research Group, “Ramulator for processing-in-memory – GitHub repository.” [Online]. Available: htt_ps://github.com/ CMU-SAFARI/ramulator-pim/
167. O. Mutlu, J. Stark, C. Wilkerson, et al., “Runahead execution: An alternative to very large instruction windows for out-of-order processors,” in Proc. 9th Int. Symp. High-Perform. Comput. Archit., 2003, pp. 129–140.
168. M. Alian, S. W. Min, H. Asgharimoghaddam, et al., “Applicationtransparent near-memory processing architecture with memory channel network,” in Proc. 51st Annu. IEEE/ACM Int. Symp. Microarchit., 2018, pp. 802–814.
169. R. Ben Hur, N. Wald, N. Talati, et al., “SIMPLE MAGIC: Synthesis and in-memory mapping of logic execution for memristor-aided logic,” in Proc. IEEE/ACM Int. Conf. Comput.Aided Des., 2017, pp. 225–232.
170. D. Bhattacharjee and A. Chattopadhyay, “Delay-optimal technology mapping for in-memory computing using ReRAM devices,” in Proc. IEEE/ACM Int. Conf. Comput.-Aided Des., 2016.
171. S. Shirinzadeh, M. Soeken, P.-E. Gaillardon, et al., “Logic synthesis for RRAM-based in-memory computing,” IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 37, no. 7, pp. 1422–1435, Jul. 2018.
172. O. Mutlu, S. Ghose, J. Gomez-Luna, et al., “Processing data where it makes sense: Enabling in-memory computation,” Microprocessors Microsyst., vol. 67, pp. 28–41, 2019.
173. O. Mutlu, S. Ghose, J. Gomez-Luna, et al., “Enabling practical processing in and near memory for data-intensive computing,” in Proc. Des. Autom. Conf. Spec. Session In- Near-Memory Comput. Paradigms, 2019.
Received December 27, 2018; accepted for publication
July 3, 2019
Saugata Ghose Carnegie Mellon University, Pittsburgh, PA 15213 USA (ghose@cmu.edu). Dr. Ghose received dual B.S. degrees in computer science and in computer engineering from Binghamton University, State University of New York, Binghamton, NY, USA, in 2007, and M.S. and Ph.D. degrees in computer engineering from Cornell University, Ithaca, NY, USA, in 2014. Since 2014, he has been working with Carnegie Mellon University (CMU), Pittsburgh, PA, USA, where he is currently a Systems Scientist with the Department of Electrical and Computer Engineering. He was the recipient of the NDSEG Fellowship and the ECE Director’s Ph.D. Teaching Award while at Cornell, the Best Paper Award at the 2017 DFRWS Digital Forensics Research Conference Europe, and a Wimmer Faculty Fellowship at CMU. His current research interests include processing-in-memory, low-power memories, application-and systemaware memory and storage systems, and data-driven architectures. For more information, see his webpage at https://ece.cmu.edu/$saugatag/.
Amirali Boroumand Carnegie Mellon University, Pittsburgh, PA 15213 USA (amirali@cmu.edu). Mr. Boroumand received a B.S. degree in computer hardware engineering from the Sharif University of Technology, Tehran, Iran, in 2014. Since 2014, he has been working toward a Ph.D. degree at Carnegie Mellon University, Pittsburgh, PA, USA. His current research interests include programming support for processing-in-memory and in-memory architectures for consumer devices and for databases.
Jeremie S. Kim Carnegie Mellon University, Pittsburgh, PA 15213 USA; ETH Z€urich, Z€urich 8092, Switzerland (jeremiek@andrew.cmu. edu). Mr. Kim received B.S. and M.S. degrees in electrical and computer engineering from Carnegie Mellon University, Pittsburgh, PA, USA, in 2015. He is currently working toward a Ph.D. degree with Onur Mutlu at Carnegie Mellon University and ETH Z€urich, Z€urich, Switzerland. His current research interests include computer architecture, memory latency/power/reliability, hardware security, and bioinformatics, and he has several publications on these topics.

3 : 18 S. GHOSE ET AL.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

Juan Gomez-Luna ETH Z€urich, Z€urich 8092, Switzerland (juang@ethz.ch). Dr. Gomez-Luna received B.S. and M.S. degrees in
telecommunication engineering from the University of Seville, Seville,
Spain, in 2001, and a Ph.D. degree in computer science from the University of Cordoba, Cordoba, Spain, in 2012. Between 2005 and 2017, he was a Lecturer with the University of Cordoba. Since 2017, he has been a Postdoctoral Researcher with ETH Z€urich. His current research interests include software optimization for GPUs and
heterogeneous systems, GPU architectures, near-memory processing,
medical imaging, and bioinformatics.

Onur Mutlu ETH Z€urich, Z€urich 8092, Switzerland; Carnegie Mellon University, Pittsburgh, PA 8092 USA (omutlu@ethz.ch). Dr. Mutlu received dual B.S. degrees in computer engineering and in psychology from the University of Michigan, Ann Arbor, MI, USA, in 2000, and M.S. and Ph.D. degrees in electrical and computer engineering from the University of Texas at Austin, Austin, TX, USA, in 2002 and 2006, respectively. He is currently a Professor of computer science with ETH Z€urich, Z€urich, Switzerland. He is also a faculty member with Carnegie Mellon University, where he previously held the William D. and Nancy W. Strecker Early Career Professorship. His industrial experience includes starting the Computer Architecture Group at Microsoft Research, where he worked from 2006 to 2009, and various product and research positions at Intel Corporation, Advanced Micro Devices, VMware, and Google. He was the recipient of the ACM SIGARCH Maurice Wilkes Award, the inaugural IEEE Computer Society Young Computer Architect Award, the inaugural Intel Early Career Faculty Award, faculty partnership awards from various companies, and a healthy number of best paper and “Top Pick” paper recognitions at various computer systems and architecture venues. He is an ACM Fellow and an elected member of the Academy of Europe (Academia Europaea). His current broader research interests include computer architecture, systems, and bioinformatics. He is especially interested in interactions across domains and between applications, system software, compilers, and microarchitecture, with a major current focus on memory and storage systems. For more information, see his webpage at https://people.inf.ethz.ch/omutlu/.

IBM J. RES. & DEV. VOL. 63 NO. 6 PAPER 3 NOVEMBER/DECEMBER 2019

S. GHOSE ET AL. 3 : 19

