Title:          Deep Compressive Offloading: Speeding Up Neural Network Inference by Trading Edge Computation for Network Latency
Subject:        -  Human-centered computing  ->  Ubiquitous and mobile computing.-  Computing methodologies  ->  Machine learning.-  Computer systems organization  ->  Embedded and cyber-physical systems.
Keywords:       Deep Learning, Edge Computing, Offloading, Compressive Sensing, Compressive Offloading, Internet of Things
Author:         Shuochao Yao1, Jinyang Li, Dongxin Liu, Tianshi Wang, and Shengzhong Liu, Huajie Shao, Tarek Abdelzaher
Creator:        LaTeX with acmart 2020/04/30 v1.71 Typesetting articles for the Association for Computing Machinery and hyperref 2016/05/05 v6.83n Hypertext links for LaTeX
Producer:       pdfTeX-1.40.17; modified using iText® 7.1.11-SNAPSHOT ©2000-2020 iText Group NV (AGPL-version)
CreationDate:   Wed Oct 21 17:27:54 2020
ModDate:        Thu Nov  5 09:14:57 2020
Tagged:         no
Form:           none
Pages:          13
Encrypted:      no
Page size:      612 x 792 pts (letter) (rotated 0 degrees)
File size:      19912055 bytes
Optimized:      yes
PDF version:    1.7
