GLOBECOM 2021 - 2021 IEEE Global Communications Conference | 978-1-7281-8104-2/21/$31.00 ©2021 IEEE | DOI: 10.1109/GLOBECOM46510.2021.9685179

Packet-Loss-Tolerant Split Inference for Delay-Sensitive Deep Learning
in Lossy Wireless Networks

Sohei Itahara∗, Takayuki Nishio†, and Koji Yamamoto∗ ∗Graduate School of Informatics, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501 Japan †School of Engineering, Tokyo Institute of Technology, Ookayama, Meguro-ku, Tokyo, 152-8550, Japan
†nishio@ict.e.titech.ac.jp

Abstract—The distributed inference framework is an emerging technology for real-time applications empowered by cutting-edge deep machine learning (ML) on resource-constrained Internet of things (IoT) devices. In distributed inference, computational tasks are ofﬂoaded from the IoT device to other devices or the edge server via lossy IoT networks. However, narrow-band and lossy IoT networks cause non-negligible packet losses and retransmissions, resulting in non-negligible communication latency. This study solves the problem of the incremental retransmission latency caused by packet loss in a lossy IoT network. We propose a split inference with no retransmissions (SI-NR) method that achieves high accuracy without any retransmissions, even when packet loss occurs. In SI-NR, the key idea is to train the ML model by emulating the packet loss by a dropout method, which randomly drops the output of hidden units in a neural network layer. This enables the SI-NR system to obtain robustness against packet losses. Our ML experimental evaluation reveals that SINR obtains accurate predictions without packet retransmission at a packet loss rate of 60%.
I. INTRODUCTION
The Internet of things (IoT) is envisioned to provide many novel applications by combining the physical sensing and actuation of IoT devices with deep learning-based data analysis. Although deep learning technology is developing rapidly, it remains challenging to implement deep learning applications on resource-constrained IoT systems while satisfying the latency and privacy demands of IoT applications. For example, factory automation and smart grids require latency of less than 10 ms and 20 ms, respectively [1]. On the other hand, in smart home applications, IoT sensors such as visual and audio sensors obtain privacy-sensitive data that should not be exposed [2].
Distributed inference frameworks have been studied to address latency and privacy challenges of deep learning deployment in IoT. In the distributed inference framework for deep neural networks (DNNs), computationally expensive tasks are ofﬂoaded from the IoT devices to the locally located sink nodes or edge servers to reduce computation latency, communication latency, and the risk of data leakage compared with cloud computing. In distributed DNN inference, a DNN is divided horizontally or vertically. The IoT devices and the edge server collaboratively process the portion of DNN, the so-called sub-DNN, by exchanging messages (e.g., outputs of sub-DNN) via IoT networks. However, narrow-band and lossy IoT networks cause non-negligible packet losses, which can

Fig. 1. Overview of the proposed split inference with no retransmissions. Red arrows indicate the upload of intermediate representations. During the upload, intermediate representations are partially lost, but the edge server does not require any retransmissions and processes forwarding using only the successfully transmitted representations.
degrade the reliability of the inference and communication efﬁciency.
A straightforward approach to solve the packet loss problem is to employ a reliable retransmission protocol, such as the transmission control protocol (TCP). However, retransmission causes non-negligible end-to-end communication latency, especially in narrow-band and lossy wireless networks. This latency problem motivated us to design a packet-loss-tolerant distributed inference method. Speciﬁcally, we aim to build an inference method in which a device and an edge server collaboratively provide accurate predictions via lossy but realtime connections, such as user datagram protocol (UDP).
To this end, we propose a novel distributed inference framework called split inference with no retransmissions (SINR). Fig. 1 shows the overview of SI-NR. In SI-NR, even when the messages exchanged among the nodes are partially dropped owing to packet loss in the networks, one can obtain accurate inference results using only successfully received parts. To obtain such packet-loss tolerance, our key idea is to train a DNN with emulating the packet loss by a dropout method, which randomly drops the output of hidden units

978-1-7281-8104-2/21/$31.00 ©2021 IEEE Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

in a DNN layer. This training enables the model to predicts output accurately from dropped information. We evaluate our method using an object classiﬁcation task, CIFAR-10. The results showed that our method achieved satisfactory accurate results even with a high packet loss rate.

II. RELATED WORK: DISTRIBUTED INFERENCE FOR DEEP NEURAL NETWORKS
Distributed inference is a technique for performing an inference task collaboratively on multiple nodes. This study focuses on distributed inference with DNNs, called distributed DNN inference [3]–[8]. In distributed inference frameworks, the DNN is horizontally [3]–[6] or vertically [7], [8] partitioned, and the IoT devices and the edge server collaboratively process DNN inference. In the inference with a vertical model split, IoT devices and edge servers process sub-DNNs in parallel with shared input data to accelerate the computation by parallel processing. On the other hand, in the inference with a horizontal model split, the IoT device and the edge server process their sub-DNNs sequentially. First, an IoT device that obtains sensor data processes a sub-DNN. Subsequently, other devices process their sub-DNNs with the outputs of other subDNNs in order of the original DNN. Therefore, the inference with a horizontal model split may require longer computational time than the vertical model split. However, the horizontal split involves less risk of data leakage than the vertical split because the entire raw data are processed by the source IoT device and not shared, whereas portions of raw data are shared in the vertical split. Thus, this study focuses on the horizontal split to satisfy the data-privacy demands of IoT applications.
Details of the distributed DNN inference with a horizontal model split are explained as follows. A well-trained DNN is divided into sub-DNNs by layers. The input-side sub-DNN is stored by the IoT device, and the output-side sub-DNN is stored by the edge server. The device obtains the output of the sub-DNN (i.e., the intermediate representations of the original DNN) from the raw input. Next, the intermediate representation is transmitted to the edge server, and the server generates an inference result from the intermediate representation with its sub-DNN. Some techniques [5], [9] were proposed to reduce the communication cost of transmitting intermediate representations. These methods reduced the amount of trafﬁc in distributed inference, but the problem of packet loss in lossy IoT networks has not been addressed. Unlike these works, this study aims to provide a packet loss-tolerant distributed inference method, which is an orthogonal work.
The impact of packet loss on DNN inference was studied in [10]. In [10], it is assumed that all DNN inference tasks are ofﬂoaded to the server, and the input images are corrupted owing to UDP packet loss. This study revealed that, as reported for conventional centralized inference [11], the DNN can provide accurate predictions for partially corrupted images in environments where the packet loss rate is less than 1%. This work demonstrated the feasibility and effectiveness of employing UDP for AI-empowered time-critical applications in nonlossy networks and has motivated us to build a distributed

Fig. 2. Detailed procedure of the proposed split inference with no retransmissions. Red arrows indicate the upload of the intermediate representations from the device to the edge server via the UDP connection, which does not retransmit dropped packets. The edge server obtains prediction results using only the successfully transmitted intermediate representations.
inference method that can withstand harsh environments, i.e., lossy IoT networks where the packet loss rate could be more than several tens of percent.
III. PROPOSED METHOD: SPLIT INFERENCE WITH NO RETRANSMISSIONS
We assume an application scenario of automated surveillance in public places, roadsides, or factories, where IoT devices and edge servers cooperatively predict accidents such as collisions to avoid them. IoT devices equipped with cameras monitor the target area and send information to the edge server. Based on the information, the edge detects objects and their movement and then predicts the probability of the accident. In the application scenario, latency is a critical issue because the edge server requires to complete the prediction until the incident actually occurs.
Fig. 2 shows the system model consisting of a cloud server, an edge server, and an IoT device. The cloud server trains a DNN model to solve the inference tasks generated on the IoT device. The portions of the well-trained DNN (sub-DNNs) are distributed to the IoT device and the edge server. The IoT device and edge server solve inference tasks collaboratively with distributed sub-DNNs. The device has a much weaker computational capacity than the cloud server and the edge server. We assume that the cloud server holds a sufﬁcient amount of data and computational capacity to train the DNN model.
The proposed distributed inference method called split inference with no retransmissions (SI-NR) consists of two phases: a model training phase and a split inference (SI) phase. In the model training phase, the cloud server trains a DNN using a preobtained dataset. To achieve robustness to packet loss in the SI phase, the DNN is trained by emulating the packet loss using a dropout method. Details of the model

Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

training are described in the next section. The trained DNN is horizontally divided into an input-side and output-side of the original DNN, which are called input sub-DNN and output sub-DNN, as shown in Fig. 2. The input and output sub-DNNs are sent to the device and the edge server, respectively. The SI phase is conducted when an inference task is generated in the IoT device. In the SI phase, the device generates intermediate representations using the input sub-DNN and sends the intermediate representations to the edge server via the UDP connection, which does not retransmit dropped packets. The edge server obtains the prediction results with the output sub-DNN by inputting the successfully received intermediate representations from the IoT device.
This study assumes that the transmitted intermediate representations are probabilistically dropped in the communication channel owing to packet loss. More formally, considering that the device sends a vector x via a communication channel with a packet loss rate p, the edge server successfully receives a vector f c(x, p) denoted as follows:

f c(x, p) = x m(p),

(1)

where operator indicates the element-wise product and m(p) is a binary vector derived from the Bernoulli distribution with an expected value of 1 − p.
In a real-world communication system, the vector of the intermediate representations x is divided into multiple packets and then transmitted. Therefore, when a packet is dropped, the consecutive elements of x are lost. To avoid such burst loss, the device shufﬂes the elements of the vector and stores them in packets. The edge server constructs vector x from successfully received packets, which results in (1).

A. Training a packet-loss-tolerant model

We leverage a dropout technique [12] to make a DNN model tolerant to packet loss. The dropout technique was originally proposed as an efﬁcient regularization method for DNNs, and it randomly drops the output of hidden units in a DNN layer. The dropout can train the DNN for longer periods without overﬁtting, which improves the test accuracy [12]. Thus, the dropout technique has been used in various DNN architectures. In this study, we leverage the dropout to emulate packet loss in the model training phase and enable the model to perform accurate inferences from incomplete intermediate representations.
In each training iteration with the dropout technique, the outputs of the hidden units are set to zero using a dropout layer with a dropout rate r. In addition to omitting the hidden unit outputs, the surviving (not omitted) hidden units are multiplied by 1/(1 − r). More formally, the dropout behavior f d(·, r) is represented as follows:

xi+1

=

f d(yi, r)

=

1

1 −

r yi

m(r),

(2)

where yi is the hidden unit of the ith layer, and xi+1 is the input of the i + 1th layer. Comparing equations (1) and (2), we ﬁnd that the drops of intermediate representations due

to packet loss can be emulated by the dropout technique in the training phase. Therefore, the model trained with the dropout technique can provide accurate inferences even when the intermediate representations are dropped because of the packet loss. The detailed training procedure is described as follows.
The cloud server ﬁrst determines a division point, which separates a DNN into input and output sub-DNNs. The division point is determined by considering the computational capacity of the IoT device and the edge server. When the DNN is separated close to the input layer, the input sub-DNN becomes small, which requires less computational power on the IoT device because more computational tasks are ofﬂoaded to the edge server. A dropout layer is inserted into the division point, and then the model with the dropout layer is trained with the preobtained datasets. Finally, the well-trained DNN is horizontally divided at the division point into the input and output sub-DNNs by removing the dropout layer. More formally, the entire DNN model is denoted as f ent(·|went), where went are trainable parameters. The entire DNN consists of two DNNs connected by the dropout layer as follows:
f ent(·|went) = f out(·|wout) ◦ f dr(·, r) ◦ f in(·|win), (3)
where f in(·|win), f out(·|wout), and f dr(·, r) indicate the input sub-DNN, output sub-DNN, and dropout layer with a dropout rate of r, respectively. The input sub-DNN f in(·|win) and an output sub-DNN f out(·|wout) are distributed to the IoT device and the edge server, respectively.
Training with a larger dropout rate implies that the DNN is trained to adapt to a more lossy communication channel, which improves the packet loss tolerance. On the other hand, as mentioned in [12], a larger training dropout rate degrades the achievable model performance, i.e., performance without any packet loss. Therefore, the dropout rate is selected considering both the packet loss rate of the channel and the model performance requirements.

B. Details of the split inference phase

The SI phase is conducted when an inference task with input
x is generated in the IoT device. First, the device generates the intermediate representations yint as follows:

yint := {yiint | yiint ∈ f in(x | win)}.

(4)

The elements of vector yint are stored in packets to transmit them to the edge server. As mentioned in Section III, if the elements are stored in packets in order, a packet loss causes the loss of consecutive elements, which is not consistent with the assumption of packet loss in the model training. Instead, the device permutates the elements randomly and stores them in packets. A packet pi is represented as follows:

pi := {ykinjt | i ≤ j < i + s},

(5)

where kj and s are the permuted identiﬁcation of the element and the number of elements stored in a packet, respectively.

Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

The edge server reconstructs the vector of intermediate representations from a subset of transmitted packets P r, where

P r = {pi | pi is received successfully}.

(6)

Next, the edge server multiplies 1/(1 − p) to the vector, where p is the packet loss ratio, similarly to the dropout layer in the training phase. Thus, the reconstructed vector is calculated as

yint = 1 yint m(p).

(7)

1−p

Let f c (·, p) denote the communication channel with a packet
loss rate p and the multiplication of 1/(1 − p). The prediction result ypred can be written as

ypred = f out(·|wout) ◦ f c (·, p) ◦ f in(x|win). (8)

If f dr(·, r) in the training phase is close to f c (·, p) in the SI phase, the model is expected to accurately predict from the incomplete intermediate representations.

C. Mathematical analysis of latency and accuracy

This section provides a mathematical analysis of the transmission latency and prediction accuracy of two methods: the proposed method and the split inference with packet retransmission. Speciﬁcally, it is shown that the proposed split inference can guarantee latency, whereas the split inference with packet retransmission cannot guarantee latency but achieves a higher prediction accuracy than the proposed method.
In the proposed method, if nt packets are transmitted using a communication channel with a packet loss rate p, the probability density function (PDF) of the number of received packets is calculated as follows:

f rec(n) =

nt n

pnt−n(1 − p)n,

if 0 ≤ n ≤ nt;

(9)

0,

otherwise.

The expected number of received packets is denoted by (1 − p)nt. Let nint and acc(α) denote the number of packets that

contain intermediate representations and the accuracy using

α% of intermediate representations, respectively. Then, we can

obtain the PDF of the accuracy as

f acc(acc(α)) = f rec α nint .

(10)

100

Assuming throughput b and packet size l, the PDF of latency

is calculated as

f latency(t) = δ(t − ntT ),

(11)

where T = l/b. The variance of latency is 0, and the expected value of latency is denoted as ntT .
In contrast, all the transmitted packets are received when using packet retransmission; therefore, the PDF of the accuracy is

f acc(acc(α)) = δ(α − 100).

(12)

PDF of latency is

f latency(nT ) =

n−1 nt −1

pn−nt (1 − p)nt ,

if n ≥ nt;

(13)

0,

otherwise.

Input CNN Block 1 CNN Block 2 CNN Block 3 CNN Block 4 CNN Block 5
FC Block Output

Convolution + ReLU Convolution
Batch Normalization ReLU
Max-Pooling Dropout
FC + ReLU + Dropout FC + ReLU + Dropout
FC + Softmax

Fig. 3. Architecture of DNN. Each CNN block consists of two or three convolutional, batch normalization, max-pooling, and dropout layers. The FC block consists of three FC layers.

TABLE I DETAILED ARCHITECTURE OF DNN.

Block ID
1 2 3 4 5

Num. conv. layers
2 2 3 3 3

Num. output channels
64 128 256 512 512

Data size of intermediate representation
65.5 kB 32.8 kB 16.4 kB 8.2 kB 2.0 kB

Comparing the PDFs of communication latency in two methods, the proposed split inference method (11) and split inference with packet retransmission (13), the variance of the proposed method was 0, whereas that of the split inference with packet retransmission was larger than 0. In other words, the proposed method can guarantee latency, whereas split inference with packet retransmission cannot guarantee latency. On the other hand, comparing the PDFs of the accuracy, (10) and (12), the variance of accuracy of the proposed method was larger than 0, whereas that of the split inference with packet retransmission was 0. However, the following machine learning (ML) evaluation reveals that the range of the accuracy of the proposed method was less than 1% under a packet loss rate of several tens of percent.
IV. EVALUATION
A. Setup
We conducted ML experiments as follows. An IoT device and an edge server were assumed to be connected via a wireless channel, where packets were randomly dropped with probability p. Hence, the elements of the intermediate representation vector transmitted by the IoT device were randomly dropped. To calculate the communication latency, the packet size and throughput of the wireless channel (including MAC and network layer overheads) were set to 500 bytes and 9.0 Mbit/s.
We used an image recognition dataset, CIFAR-101, with 50,000 training and 10,000 testing images that represented 10 image classes, such as “dog” and “ship.” In the model training phase, the cloud server used all training dataset. The test dataset was used to evaluate the inference performance of the SI phase of the proposed method.
1https://www.cs.toronto.edu/∼kriz/cifar.html

Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

The DNN model used in the experiments is shown in Fig. 3. The model was designed with reference to VGG16 [13], which consisted of ﬁve convolutional blocks and a fully connected (FC) block. Each convolutional block included two or three 3 × 3 convolution layers activated by the rectiﬁed linear unit (ReLU), and the block was followed by 2×2 max-pooling and dropout layers. In each convolutional block, the convolutional layers had the same number of output channels. Additionally, one of the two convolutional layers was followed by the batch normalization layer. Table I describes each CNN block, such as its number of convolutional layers and the data size of intermediate representations. The FC block consists of three FC layers (256 and 128 units with ReLU activation and dropout, and 10 other units activated by softmax).
The training dataset was divided into updating and validation datasets in a ratio of 9:1. The DNN model was updated using only the updating dataset for multiple epochs. In each epoch, the model is evaluated using the validation dataset. The training is completed if there are 150 epochs performed, or if the validation loss is increased after 20 epochs consecutively, which indicates that the model is starting to overﬁt. The same dropout rate was used to train all dropout layers. The Adam optimizer, the training rate of 0.001, and the mini-batch size of 128 were selected as hyperparameters.
To evaluate the accuracy of the split inference, the CNN was divided at a speciﬁc CNN block. Considering that the CNN is divided at the CNN block i, the input sub-DNN consists of the CNN blocks 1, 2, . . . , i, and the output sub-DNN consists of the CNN blocks i + 1, i + 2, . . . , 5, and the FC block. The packet loss in the communication channel is emulated by the dropout. In particular, the dropout rate of the CNN block i is set to the packet loss rate p, which ranges from 0 to 0.9. The dropout rates of the other CNN blocks and the FC block were set to 0.
B. Results
1) Cumulative distribution function of the accuracy and latency: Fig. 4 illustrates the cumulative distribution function (CDF) of the accuracy and latency for the proposed SI and conventional SI with retransmission, respectively. The CDF is obtained from (10), (11), (12), and (13), with the parameters described in Section IV-A and the packet loss rate of 0.2. The model was divided at the CNN block 1. The number of transmitted packets was the same in both methods, but the proposed method received fewer packets owing to the nonretransmission policy.
In Fig. 4 (a), the accuracy of the SI-NR is approximately 0.9, whereas that of the conventional SI is stable. This is because the proposed method does not perform any retransmission, and thus a part of the intermediate representation is stochastically dropped owing to packet loss. In contrast, the conventional method always obtains complete intermediate representations owing to the retransmission. On the one hand, the packet loss-induced accuracy degradation in the proposed method is less than 1%. This result indicates that the proposed method

CDF
Test accuracy
CDF

1.0 0.8 0.6 0.4 0.2 0.0
0.89

SI with retransmission Proposed: SI-NR
0.90 Accuracy

1.0 SI with retransmission

0.8

Proposed: SI-NR

0.6

0.4

0.2

0.0

0.91

50

60

70

80

90

Communication latency (ms)

(a) Accuracy

(b) Communication latency

Fig. 4. Cumulative distribution function of the accuracy and communication latency for the conventional inference with packet retransmission and the proposed split inference with no retransmissions. The number of packets to be transmitted is the same for the two methods.

1.00 0.95 0.90 0.85 0.80
0.0 0.1 0.2 0.3 0.4 0.5 Training dropout rate

Fig. 5. Test accuracy without any packet loss as a function of training dropout rate. The training dropout rate of 0.0 indicates the model trained without dropout.

achieves packet loss tolerance, which is veriﬁed in more detail in the following sections.
On the other hand, Fig. 4 (b) shows that the communication latency of the proposed method is stable and smaller than that of the conventional method. This is because in the proposed method, the device sends only a predeﬁned number of packets, whereas the conventional method transmits a larger number of packets until all packets are successfully received. These results demonstrate that the proposed method achieves a guaranteed small communication latency with a slight degradation in model performance.
2) Impact of the training dropout rate on the achievable model accuracy: Fig. 5 shows the test accuracy without any packet loss. Consistently with a previous study [12], training with dropout improves testing accuracy, owing to its regularization capability. In the ML task we adopted in this evaluation, the model trained with a dropout rate of 0.2 achieved the highest test accuracy. As mentioned in [12], the dropout rate that achieves the highest performance depends on the ML task and DNN architecture. Therefore, it should be tuned on the validation dataset in the training phase while considering the packet loss rate of the lossy networks.
3) Impact of the training dropout rate on the prediction accuracy of split inference: Fig. 6 shows the decreases in test accuracy in the method without any packet loss as a function of packet loss rate, when the DNN is divided at block 1. As the packet loss rate increases, the model accuracy degrades. The proposed method with a larger training dropout rate

Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

Decrease in accuracy Decrease in accuracy

0.10
Trained wo. Dr Trained w. Dr 0.1 Trained w. Dr 0.2 Trained w. Dr 0.4
0.05

0.10
Block1 wo. Dr

Block3 wo. Dr

Block5 wo. Dr

Block1 w. Dr

Block3 w. Dr

0.05

Block5 w. Dr

0.00 0.0 0.2 0.4 0.6 0.8 1.0 Packet loss rate

0.00 0.0 0.2 0.4 0.6 0.8 1.0 Packet loss rate

Fig. 6. Effect of the training dropout rate on the model performance degradation. The dotted black line indicates the results obtained using the DNN trained without any dropout layer. The green, blue, and red lines indicate the results using the DNN trained with dropout rates of 0.1, 0.2, and 0.4, respectively.

Fig. 7. Effect of the division point on the model performance degradation. The dotted lines and solid lines indicate the results obtained using the model trained without any dropout layer and with a dropout rate of 0.2, respectively. Different colors indicate different division layers that split the DNN into input and output sub-DNNs.

better mitigated the performance degradation. In particular, the performance of the model trained without dropout was degraded by more than 0.1, when more than 60% of the packets were dropped. The model trained with a dropout rate of 0.4 showed only a slight performance degradation. This result demonstrates that model training with the dropout technique signiﬁcantly improves the packet loss tolerance of the split model.
4) Effect of the division point on the prediction accuracy: Fig. 7 shows the decrease in the test accuracy for the model divided at different points. Consistently with the results in the above section, the DNN trained with dropout achieved less accuracy degradation than that without dropout, regardless of the division points. On the one hand, the DNN divided at the division point closer to the output layer achieved less performance degradation for a high packer loss rate. In addition to the increase in the robustness against packet loss, as the division point approaches the output layer of the original DNN, the data size of the intermediate representation became generally smaller, as shown in Table I, which reduced communication latency. However, it also increased the size of the input sub-DNN, which should be processed by the resource-constrained IoT device, thereby increasing the computation latency. Therefore, the division point should be carefully determined to maximize the model accuracy under the constraints of the total latency.
V. CONCLUSION
We have presented a packet loss tolerant split inference method, SI-NR. In SI-NR, the key idea is to train the ML model by emulating the packet loss by a dropout method so that the model gains packet loss tolerance. Our experimental ML evaluation reveals that SI-NR obtains an accurate prediction without packet retransmission under a high packet loss rate. An interesting area for future work is an optimization framework that determines the division point and the dropout rate to maximize the model accuracy in lossy wireless networks under the constraints of the total latency of communication and computation.

ACKNOWLEDGEMENT
This work was supported in part by JST PRESTO Grant
Number JPMJPR2035.
REFERENCES
[1] P. Schulz et al., “Latency critical IoT applications in 5G: Perspective on the design of radio interface and network architecture,” IEEE Commun. Mag., vol. 55, no. 2, pp. 70–78, Feb. 2017.
[2] H. Lin and N. W. Bergmann, “IoT privacy and security challenges for smart home environments,” Information, vol. 7, no. 3, pp. 1–15, Jul. 2016.
[3] N. D. Lane and P. Georgiev, “Can deep learning revolutionize mobile sensing?” in Proc. ACM HotMobile, Santa Fe, NM, USA, Feb. 2015, pp. 117–122.
[4] J. Wang, J. Zhang, W. Bao, X. Zhu, B. Cao, and P. S. Yu, “Not just privacy: Improving performance of private deep learning in mobile cloud,” in Proc. ACM SIGKDD, London, UK, Jul. 2018, pp. 2407–2416.
[5] Y. Koda et al., “Communication-efﬁcient multimodal split learning for mmwave received power prediction,” IEEE Commun. Lett., vol. 24, no. 6, pp. 1284–1288, Mar. 2020.
[6] W. Liu, J. Cao, L. Yang, L. Xu, X. Qiu, and J. Li, “Appbooster: Boosting the performance of interactive mobile applications with computation ofﬂoading and parameter tuning,” IEEE Trans. Parallel Distrib. Syst., vol. 28, no. 6, pp. 1593–1606, Jun. 2017.
[7] J. Kim, Y. Park, G. Kim, and S. J. Hwang, “SplitNet: Learning to semantically split deep networks for parameter reduction and model parallelization,” in Proc. ICML, vol. 70, Sydney, Australia, Aug. 2017, pp. 1866–1874.
[8] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “Deepthings: Distributed adaptive deep learning inference on resource-constrained IoT edge clusters,” IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst, vol. 37, no. 11, pp. 2348–2359, Oct. 2018.
[9] K. Bhardwaj, C.-Y. Lin, A. Sartor, and R. Marculescu, “Memory-and communication-aware model compression for distributed deep learning inference on IoT,” ACM Trans. Embed. Comput. Syst., vol. 18, no. 5s, pp. 1–22, Oct. 2019.
[10] J. Liu and Q. Zhang, “To improve service reliability for AI-powered time-critical services using imperfect transmission in mec: An experimental study,” IEEE Internet Things J., vol. 7, no. 10, pp. 9357–9371, Oct. 2020.
[11] S. Dodge and L. Karam, “Understanding how image quality affects deep neural networks,” in Proc. IEEE QoMEX, Lisbon, Portugal, Jun. 2016, pp. 1–6.
[12] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving neural networks by preventing co-adaptation of feature detectors,” arXiv:1207.0580, pp. 1–18, Jul. 2012.
[13] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Proc. ICLR, San Diego, CA, USA, May 2015, pp. 1–14.

Authorized licensed use limited to: KAUST. Downloaded on August 29,2022 at 13:57:47 UTC from IEEE Xplore. Restrictions apply.

