BigNAS: Scaling up Neural Architecture Search with Big Single-Stage Models
Jiahui Yu1,2(B), Pengchong Jin1, Hanxiao Liu1, Gabriel Bender1, Pieter-Jan Kindermans1, Mingxing Tan1, Thomas Huang2, Xiaodan Song1,
Ruoming Pang1, and Quoc Le1
1 Google Brain, New York, USA jiahuiyu@google.com
2 University of Illinois at Urbana-Champaign, Champaign, USA
Abstract. Neural architecture search (NAS) has shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has become a popular strategy to rank the relative quality of diﬀerent architectures (child models) using a single set of shared weights. However, while one-shot model weights can eﬀectively rank diﬀerent network architectures, the absolute accuracies from these shared weights are typically far below those obtained from stand-alone training. To compensate, existing methods assume that the weights must be retrained, ﬁnetuned, or otherwise post-processed after the search is completed. These steps signiﬁcantly increase the compute requirements and complexity of the architecture search and model deployment. In this work, we propose BigNAS, an approach that challenges the conventional wisdom that post-processing of the weights is necessary to get good prediction accuracies. Without extra retraining or post-processing steps, we are able to train a single set of shared weights on ImageNet and use these weights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our discovered model family, BigNASModels, achieve top1 accuracies ranging from 76.5% to 80.9%, surpassing state-of-the-art models in this range including EﬃcientNets and Once-for-All networks without extra retraining or post-processing. We present ablative study and analysis to further understand the proposed BigNASModels.
Keywords: Eﬃcient neural architecture search · AutoML
1 Introduction
Designing network architectures that are both accurate and eﬃcient is crucial for deep learning on edge devices. A single neural network architecture can require more than an order of magnitude more inference time if it is deployed on a slower
Electronic supplementary material The online version of this chapter (https:// doi.org/10.1007/978-3-030-58571-6 41) contains supplementary material, which is available to authorized users.
c Springer Nature Switzerland AG 2020 A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12352, pp. 702–717, 2020. https://doi.org/10.1007/978-3-030-58571-6_41

BigNAS: Neural Architecture Search with Big Single-Stage Models 703
device [37]. Furthermore, even two devices which have similar overall speeds (e.g., phone CPUs made by diﬀerent manufacturers) can favor very diﬀerent network architectures due to hardware and device driver diﬀerences [33]. This makes it appealing to not only search for architectures of varying sizes that are optimized for speciﬁc devices, but also ensure that these models can be deployed eﬀectively.
In the past, to optimize network architectures for a single device and latency target [31], Neural Architecture Search (NAS) methods [27,38,39] have shown to be eﬀective. While early NAS methods were prohibitively expensive for most practitioners, recent eﬃcient NAS methods based on weight sharing reduce search costs by orders of magnitude [1,22,25,35]. These methods work by training a super-network and then identifying a path through the network – a subset of its operations – which gives the best possible accuracy while satisfying a userspeciﬁed latency constraint for a speciﬁc hardware device. The advantage of this approach is that we can train the super-network and then use it to rank many diﬀerent candidate architectures from a user-deﬁned search space.
However, the absolute accuracies of predictions obtained from this supernetwork are typically much lower than those of models trained from scratch in stand-alone fashion [1]. For this reason, it is commonly assumed that signiﬁcant post-processing of the super-network’s weights is necessary to obtain highquality accuracies for model deployment. For example, one proposed solution is to retrain a separate model for each device of interest and each latency budget of interest [5,33]. However, this incurs signiﬁcant overhead, especially if the number of deployment scenarios is large. A second solution would be to post-process the weights after training is ﬁnished; for example, using the progressive shrinking heuristic proposed for Once-for-All networks [4]. However, this post-processing step complicates the model training pipeline. Moreover, the child models from Once-for-All networks [4] still requires ﬁne-tuning with additional epochs (e.g., 75 epochs on ImageNet) to achieve the best accuracies.
In this work, we reassess the popular belief that the retraining or postprocessing of the shared weights is necessary in order to obtain competitive accuracies. We propose several techniques to bridge the gap between the distinct initialization and learning dynamics across small and big child models with shared parameters. With these techniques, we are able to train a singlestage model : a single model from which we can directly slice high-quality child models without any extra post-processing.
We search over a big single-stage model that contains both small child models (∼200 MFLOPs, comparable to MobileNetV3) and big child models (∼1 GFLOPs, comparable to EﬃcientNets). Diﬀerent from existing one-shot methods [1,3,22,25,35], our trained single-stage model oﬀers a much wider coverage of model capacities, and more importantly, all child models are trained in a way such that they simultaneously reach excellent performance at the end of the search phase. Architecture selection can be then carried out via a simple coarse-to-ﬁne selection strategy. Once an architecture is selected, we can obtain a child model by simply slicing the single-stage model for instant deployment

704 J. Yu et al.
Fig. 1. Comparison with several existing workﬂows. We use nested squares to denote models with shared weights, and use the size of the square to denote the size of each model. Workﬂow in the middle refers the concurrent work from [4], where submodels are sequentially induced through progressive distillation and channel sorting. We simultaneously train all child models in a single-stage model with proposed modiﬁcations, and deploy them without retraining or ﬁnetuning.
w.r.t. the given constraints such as memory footprint and/or runtime latency. The workﬂow is illustrated in Fig. 1.
The success of simpliﬁed BigNAS workﬂow relies on a single objective: how to train a high-quality single-stage model? This objective is challenging on its own. For example, we ﬁnd that the training loss explodes if a big single-stage model is not properly initialized; during the training process, big child models start to overﬁt before small ones plateau; empirically bigger child models tend to overﬁt more on the training data. To address these challenges, we systematically study and revisit conventional training techniques of stand-alone networks, and adapt them to train weight-sharing single-stage models. With the proposed techniques, we are able train a high-quality single-stage model on ImageNet and obtain a family of child models that simultaneously surpass all the state-of-the-art models in the range of 200 to 1000 MFLOPs, including EﬃcientNets B0-B2 (1.6% more accurate under 400 MFLOPs), without retraining or ﬁnetuning the child models upon the completion of search. For example, one of our child models achieves 80.9% top-1 accuracy at 1G FLOPs (4× less computation than a ResNet-50).
2 Related Work
Earlier NAS methods [20,21,27,38,39] train thousands of candidate architectures from scratch (on a smaller proxy task) and use their validation performance as the feedback to an algorithm that learns to focus on the most promising regions in the search space. More recent works have sought to amortize the cost by training a single over-parameterized one-shot model. Each architecture in the search space uses only a subset of the operations in the one-shot model; these child models can be eﬃciently ranked by using the shared weights to estimate their relative accuracies [1,3,5,18,22,25,33–35].
As a complementary direction, resource-aware NAS methods are proposed to simultaneously maximize prediction accuracy and minimize resource requirements such as latency, FLOPs, or memory footprints [2,4,9,30,31,33,35].
All the aforementioned approaches require two-stage training: once the best architectures have been identiﬁed (either through the proxy tasks or using a

BigNAS: Neural Architecture Search with Big Single-Stage Models 705
one-shot model), they have to be retrained from scratch to obtain a ﬁnal model with higher accuracy. In most of these existing works, a single search experiment only targets a single resource budget or a narrow range of resource budgets at a time.
To alleviate these issues, [4] proposed a progressive training approach (OFA) concurrently with our work. The idea is to pre-train a single full network and then progressively distill it to obtain the smaller networks. Moreover, a channel sorting procedure is required to progressively construct the smaller networks. In our proposed BigNAS, however, all the child models in the single-stage model are trained simultaneously, allowing the learning of small and big networks to mutually beneﬁt each other. During the training, we always keep lower-index channels in each layer and lower-index layers in each stage for our child models, eliminating the sorting procedure. Our BigNAS is able to handle a wider set of models (from 200 MFLOPs to 1 GFLOPs) and oﬀers a better coverage over diverse deployment scenarios and varied resource budgets.
Our work shares high-level similarities with slimmable networks [35–37] in terms of training a single shared set of weights which can be used for many child models. However, while slimmable networks are specialized to vary the number of channels only, we are able to handle a much larger space where many architectural dimensions (kernel and channel sizes, network depths, input resolutions) are searched simultaneously, subsuming and outperforming the manually-designed scaling heuristics in EﬃcientNets [32].
3 Architecture Search with Single-Stage Models
Our proposed method consists of two steps:
1. We train a big single-stage model from which we can directly sample or slice diﬀerent architectures as child models for instant inference and deployment. In contrast to previous works, our training is single-stage. In other words: the trained model weights from a search can be directly used for deployment, without any need to retrain them from scratch (e.g. [1,3,9,22,25,30,35]) or otherwise post-process them (e.g., [4]).
2. Architecture selection using a simple coarse-to-ﬁne selection method to ﬁnd the most accurate model under the given resource constraints (for example, FLOPs, memory footprint and/or runtime latency budgets on diﬀerent devices).
In the following, we will ﬁrst systematically study how to train a highquality single-stage model from ﬁve aspects: network sampling during training, inplace distillation, network initialization, convergence behavior and regularization. Then we will present a coarse-to-ﬁne approach for eﬃcient resource-aware architecture selection.

706 J. Yu et al.
3.1 Training a High-Quality Single-Stage Model
Training a high-quality single-stage model is important and highly non-trivial due to the distinct initialization and learning dynamics of small and big child models. In this section, we ﬁrst generalize two techniques originally introduced by [36] to simultaneously train a set of high-quality networks with diﬀerent channel numbers, and show that both can be extended to handle a much larger space where the architectural dimensions, including kernel sizes, channel numbers, input resolutions, network depths are jointly searched. We then present three additional techniques to address the distinct initialization and learning dynamics of small and big child models.
Sandwich Rule. In each training step, given a mini-batch of data, the sandwich rule [36] samples the smallest child model, the biggest (full) child model and N randomly sampled child models (N = 2 in our experiments). It then aggregates the gradients from all sampled child models before updating the weights of the single-stage model. As multiple architectural dimensions are included in our search space, the “smallest” child model is the one with lowest input resolution, thinnest width, shallowest depth, and smallest kernel size (the kernel of the depthwise convolutions in each inverted residual block [28]). The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.
Inplace Distillation. During the training of a single-stage model, inplace distillation [36] takes the soft labels predicted by the biggest possible child model (full model) to supervise all other child models. The beneﬁt of inplace distillation comes for free in our training setting, as we always have access to the predictions of the largest child model in each gradient update step thanks to the sandwich rule. We note that all child models are only trained with the inplace distillation loss, starting from the ﬁrst training step to the end of the training. The temperature hyper-parameter or the mixture of distillation/target loss [14] are not used in our experiments for the sake of simplicity.
During training, input images are randomly cropped as a preliminary data augmentation step. When distilling a high-resolution teacher model into a lowresolution student model, we ﬁnd that it is helpful to feed the same image patches into both the teacher and the student. In our data preparation, we ﬁrst randomly crop an image with a ﬁxed resolution (on ImageNet we use 224), and then apply bicubic interpolation to the same patch to transform it into all target resolutions (e.g., 192, 288, 320, etc.). In this case, soft labels predicted by the biggest child model (the teacher) are more compatible with the inputs seen by other child models (the students). Therefore this can serve as a more accurate distillation signal. Our preliminary results show that this leads to ∼0.3% improvement on average top-1 accuracy for child models compared with sampling diﬀerent patches.
Initialization. When we ﬁrst tried to train bigger and deeper single-stage models, we found that training was highly unstable, and that the training loss

BigNAS: Neural Architecture Search with Big Single-Stage Models 707
exploded when we used learning rates optimized for training a normal neural network. The training started to work when we reduced the learning rate to 30% of its original value, but this conﬁguration lead to much worse results (∼ 1.0%) top-1 accuracy drop on ImageNet).
While stabilize model training is in general a complex and open-ended problem, we found that in this case a simple change to our setup was suﬃcient to stabilize training. As all child models in our search space are residual networks, we initialize the output of each residual block (before skip connection) to an allzeros tensor by setting the learnable scaling coeﬃcient γ = 0 in the last Batch Normalization [19] layer of each residual block, ensuring identical variance before and after each residual block regardless of the fan-in. This initialization is originally mentioned in [8] which improves accuracy by ∼0.2% in their setting, yet is more critical in our setting (improving by ∼1.0%)). We also additionally add a skip connection in each stage transition when either resolutions or channels diﬀer (using 2 × 2 average pooling and/or 1 × 1 convolution if necessary) to explicitly construct an identity mapping [12]. Convergence Behavior. In practice, we ﬁnd that big child models converge faster while small child models converge slower. Figure 2a shows the typical learning curves during the training of a single-stage model, where we plot the validation accuracies of a small and a big child model over time. This reveals a dilemma: at training step t when the performance of big child models peaks, the small child models are not fully-trained; and at training step t when the small child models have better performance, the big child models already overﬁtted.
Fig. 2. On the left, we show typical accuracy curves during the training process for both small and big child models. It reveals a common dilemma in training big singlestage models: at training step t when the performance of big child models peaks, the small child models are not fully-trained; and at training step t when the small child models have better performance, the big child models already overﬁtted. On the right, we plot the simple modiﬁed learning rate schedules with constant ending to address this issue.

708 J. Yu et al.
To address this issue, we put our focus on the learning rate schedule. We ﬁrst plot the optimized and widely used exponentially decaying learning rate schedule for MobileNet-series [15,16,28], MNasNets [31] and EﬃcientNets [32] in Fig. 2b. We introduce a simple modiﬁcation to this learning rate schedule, named exponentially decaying with constant ending, which has a constant learning rate at the end of training when it reaches 5% of the initial learning rate (Fig. 2b). It brings two beneﬁts. First, with a slightly larger learning rate at the end, the small child models learn faster. Second, the constant learning rate at the end alleviates the overﬁtting of big child models as the weights oscillate.
Regularization. Empirically when comparing training/validation losses, we ﬁnd big child models tend to overﬁt the training data whereas small child models tend to underﬁt. In previous work, Bender et al. [1] apply the same weight decay to all child models regardless whether they are small or big. To prevent overﬁtting of larger networks, For EﬃcientNets, Tan et al. [32] found it helpful to use larger dropout [29] rates for larger neural networks. This becomes even more complicated in our context of training big single-stage models, due to the interplay among the small child models and big child models with shared parameters. Nevertheless, we introduce a simple rule that is surprisingly eﬀective for this problem: regularize only the biggest (full) child model (i.e., the only model that has direct access to the ground truth training labels since other child models are trained with inplace distillation only). We simply apply this rule to both weight decay and dropout, and empirically demonstrate its eﬀectiveness in our experiments.
Batch Norm Calibration. Batch norm statistics are not accumulated when training the single-stage model as they are ill-deﬁned with varying architectures. After the training is completed, we re-calibrate the batch norm statistics (following Yu et al. [36]) for each sampled child model for deployment without retraining or ﬁnetuning any network parameters.
3.2 Coarse-to-ﬁne Architecture Selection
After training a single-stage model, one needs to select the best architectures w.r.t. the resource budgets. Although obtaining the accuracy of a child model is cheap, the number of architecture candidates is extremely large (more than 1012). To address this issue, we propose a coarse-to-ﬁne strategy where we ﬁrst try to ﬁnd a rough skeleton of promising network candidates in general, and then sample multiple ﬁne-grained variations around each skeleton architecture of interest.
Speciﬁcally, in the coarse-grained phase, we deﬁne a limited input resolution set, depth set (global depth multipliers), channel set (global width multipliers) and kernel size set, and obtain benchmarks for all child models in this restricted space. This is followed by a ﬁne-grained search phase, where we ﬁrst pick the best network skeleton satisfying the given resource constraint found in the previous phase, and then randomly mutate its network-wise resolution, stage-wise depth, number of channels and kernel sizes to further discover better network

BigNAS: Neural Architecture Search with Big Single-Stage Models 709
architectures. Finally, we directly use the weights from the single-stage model for the induced child models without any retraining or ﬁnetuning. More details will be presented in the experiments.

4 Experiments
In this section, we ﬁrst present the details of our search space, followed by our main results compared with the previous state-of-the-arts in terms of both accuracy and eﬃciency. Then we conduct an extensive ablative study to demonstrate the eﬀectiveness of our proposed modiﬁcations. Finally, we show the intermediate results of our coarse-to-ﬁne architecture selection.

Table 1. MobileNetV2-based search space.

Stage Operator Resolution

#Channels #Layers Kernel Sizes

Conv

192 × 192–320 × 320 32–40

1

3

1

MBConv1 96 × 96–160 × 160 16–24

1–2

3

2

MBConv6 96 × 96–160 × 160 24–32

2–3

3

3

MBConv6 48 × 48–80 × 80

40–48

2–3

3, 5

4

MBConv6 24 × 24–40 × 40

80–88

2–4

3, 5

5

MBConv6 12 × 12–20 × 20

112–128 2–6

3, 5

6

MBConv6 12 × 12–20 × 20

192–216 2–6

3, 5

7

MBConv6 6 × 6–10 × 10

320–352 1–2

3, 5

Conv

6 × 6–10 × 10

1280–1408 1

1

4.1 Search Space Deﬁnition
Following previous resource-aware NAS methods [5,15,31–33], our network architectures consist of a stack with inverted bottleneck residual blocks (MBConv) [28]. We also insert a squeeze-and-excitation module [17] in each block following EﬃcientNet [32] and MobileNetV3 [15]. The detailed search space is summarized in Table 1. For the input resolution dimension, we sample from set {192, 224, 288, 320}. By training on diﬀerent input resolutions, we ﬁnd our trained single-stage model is able to generalize to unseen input resolutions during architecture search or deployment (e.g., 208, 240, 256, 272, 304, 336) after BN calibration. For the depth dimension, our network has seven stages (excluding the ﬁrst and the last convolution layer). Each stage has multiple choices of the number of layers (e.g., stage 5 can pick any number of layers ranging from 2 to 6). Following slimmable networks [37] that always keep lower-index channels in each layer, we always keep lower-index layers in each network stage (and their weights). For weight sharing on the kernel size dimension in the inverted residual

710 J. Yu et al.
blocks, a 3 × 3 depthwise kernel is deﬁned to be the center of a 5 × 5 depthwise kernel. Both kernel sizes and channel numbers can be adjusted layer-wise. The input resolution is network-wise and the number of layers is a stage-wise conﬁguration in our search space.

Group 200M FLOPs
400M FLOPs 600M FLOPs 1000M FLOPs

Model Family
MobileNetV1 0.5× MobileNetV2 0.75× AutoSlim-MobileNetV2 MobileNetV3 1.0× MNasNet A1 Once-For-All Once-For-All finetuned
BigNASModel-S
NASNet B MobileNetV2 1.3× MobileNetV3 1.25× MNasNet A3 EﬃcientNet B0
BigNASModel-M
MobileNetV1 1.0× NASNet A DARTS EﬃcientNet B1
BigNASModel-L
EﬃcientNet B2 BigNASModel-XL

Params
1.3M 2.6M 4.1M 5.4M 3.9M 4.4M 4.4M
4.5M
5.3M 5.3M 8.1M 5.2M 5.3M
5.5M
4.2M 5.3M 4.9M 7.8M
6.4M
9.2M 9.5M

FLOPs
150M 209M 207M 219M 315M 230M 230M
242M
488M 509M 350M 403M 390M
418M
569M 564M 595M 734M
586M
1050M 1040M

Top-1
63.3 69.8 73.0 75.2 75.2 76.0 76.4
76.5
72.8 74.4 76.6 76.7 77.3
78.9
70.9 64.0 73.1 79.2
79.5
80.3 80.9

Fig. 3. Main results of BigNASModels on ImageNet.

4.2 Main Results on ImageNet
We train our big single-stage model on ImageNet [7] using same settings following our strongest baseline EﬃcientNets ([32]: RMSProp optimizer with decay 0.9 and momentum 0.9; batch normalization with post-calibration [36]; weight decaying factor 1e − 5; initial learning rate 0.256 that decays by 0.97 every 2.4 epochs; swish activation [26] and AutoAugment policy [6]. We train our big single-stage model together with all techniques proposed in Sect. 3.1. The learning rate is truncated to a constant value when it reaches 5% of its initial learning rate (i.e., 0.0128) until the training ends. We apply dropout only on training the full network with dropout ratio 0.2, and weight decaying only on full network once in each training iteration. To train the single-stage model, we adopt the sandwich sampling rules and inplace distillation proposed by [36]. After the training, we use a simple coarse-to-ﬁne architecture selection to ﬁnd the best architecture under each interested resource budgets. We will show the details of coarse-toﬁne architecture selection in Sect. 4.4.
We show the performance benchmark of our model family, named BigNASModels, in Fig. 3. On the left we show the visualization of FLOPs-Accuracy benchmarks compared with the previous arts including MobileNetV1 [16], NASNet [39], MobileNetV2 [28], AutoSlim-MobileNetV2 [35], MNasNet [31],

BigNAS: Neural Architecture Search with Big Single-Stage Models 711
MobileNetV3 [15], EﬃcientNet [32] and concurrent work Once-For-All [4]. We show the detailed benchmark results on the right table. For small-sized models, our BigNASModel-S achieves 76.5% accuracy under only 240 MFLOPs, which is 1.3% better than MobileNetV3 in terms of similar FLOPs, and 0.5% better than ResNet-50 [11] with 17 × fewer FLOPs. For medium-sized models, our BigNASModel-M achieves 1.6% better accuracy than EﬃcientNet B0. For large-sized models where ImageNet classiﬁcation accuracy saturates, our BigNASModel-L still has 0.6% improvement compared with EﬃcientNet B2. Moreover, instead of individually training models of diﬀerent sizes, our BigNASModel-S, BigNASModel-M and BigNASModel-L are sliced directly from one pretrained single-stage model, without retraining or ﬁnetuning.

Learn faster Explode

Learn faster Explode

Fig. 4. Focusing on the start of training. Ablation study on diﬀerent initialization methods. We show the validation accuracy of a small (left) and big (right) child model.

4.3 Ablation Study
Ablation Study on Initialization. Previous weight initialization methods [10] are deduced from ﬁxed neural networks, where the numbers of input units is constant. However, in a single-stage model, the number of input units varies across the diﬀerent child models. In this part, we start with training a single-stage model using He Initialization [10] designed for ﬁxed neural networks. As shown in Fig. 4, the accuracy of both small (left) and big (right) child models drops to zero after a few thousand training steps during the learning rate warmingup [8]. The single-stage model is able to converge when we reduce the learning rate to the 30% of its original value. If the initialization is modiﬁed according to Sect. 3.1, the model learns much faster at the beginning of the training (shown in Fig. 4), and has better performance at the end of the training (shown in Fig. 5). Moreover, we can train the single-stage model with the original learning rate hyper-parameter, which leads to much better performance for both small (Fig. 5, left) and big (Fig. 5, right) child models.
Ablation Study on Convergence Behavior. During the training of a singlestage model, the big child models converge faster and then overﬁt, while small

712 J. Yu et al.
Fig. 5. Focusing on the end of training. Ablation study on diﬀerent initialization methods. We show the validation accuracy of a small (left) and big (right) child model.
child models converge slower and need more training. In this part, we show the performance after addressing this issue in Fig. 6. We apply the proposed learning rate schedule exponentially decaying with constant ending on the right. The detailed learning rate schedules are shown in Fig. 2b. We also tried many other learning rate schedules with an exhaustive hyper-parameter sweep, including linearly decaying [24,36] and cosine decaying [13,23]. But the performances are all worse than exponentially decaying.
Fig. 6. The validation accuracy curves during the training process for both small and big child models before (left) and after (right) our modiﬁcations.
Ablation Study on Regularization. Big child models are prone to overﬁtting on the training data whereas small child models are prone to underﬁtting. In this part, we compare the eﬀects of the regularization between two rules: (1) applying regularization on all child models [1], and (2) applying regularization only on the full network. Here the regularization techniques we consider are weight decay with factor 1e − 5 and dropout with ratio 0.2 (the same hyperparameters used in training previous state-of-the-art mobile networks). In Fig. 7, we show the performance of both small (left) and big (right) child models using diﬀerent regularization rules. On the left, the performance of small child models

BigNAS: Neural Architecture Search with Big Single-Stage Models 713
Fig. 7. The validation accuracy of a small (left) and big (right) child model using diﬀerent regularization rules.
is improved (+0.5 top-1 accuracy) as it has less regularization and more capacity to ﬁt the training data. Meanwhile on the right, we found the performance of the big child model is also improved slightly (+0.2 top-1 accuracy).
4.4 Coarse-to-ﬁne Architecture Selection
After the training of a single-stage model, we use coarse-to-ﬁne architecture selection to ﬁnd the best architectures under diﬀerent resource budgets. During the search, the evaluation metrics can be ﬂexible including predictive accuracy, FLOPs, memory footprint, latency on various diﬀerent devices, and many others. It is noteworthy that we pick the best architectures according to the predictive accuracy on training set, because we used all training data for obtaining our single-stage model (no retraining from scratch), and the validation set of ImageNet [7] is being used as “test set” in the community. In this part, we ﬁrst show an illustration of our coarse-to-ﬁne architecture selection with the trained big single-stage model in Fig. 8. The search results are based on FLOPs-Accuracy benchmarks (as FLOPs are more reproducible and independent of the software version, hardware version, runtime environments and many other factors).
During the coarse-to-ﬁne architecture selection, we ﬁrst ﬁnd rough skeletons of good candidate networks. Speciﬁcally, in the coarse selection phase, we predeﬁne ﬁve input resolutions (network-wise, {192, 224, 256, 288, ), 320} four depth conﬁgurations (stage-wise via global depth multipliers [32]), two channel conﬁgurations (stage-wise via global width multipliers [16]) and four kernel size conﬁgurations (stage-wise), and obtain all of their benchmarks (shown in Fig. 8 on the left). Then under our interested latency budget, we perform a ﬁne-grained grid search by varying its conﬁgurations (shown in Fig. 8 on the right). For example, under FLOPs near 600M we ﬁrst pick the skeleton of the red dot shown in Fig. 8. We then perform additional ﬁne-grained architecture selection by randomly varying the input resolutions, depths, channels and kernel sizes slightly. We note that the coarse-to-ﬁne architecture selection is ﬂexible and not very exhaustive in our experiments, yet it already discovered fairly good architectures as shown in Fig. 8 on the right. For the FLOPs near 650M, we ﬁnally select the child

714 J. Yu et al.

Fig. 8. Benchmark results of coarse-to-ﬁne architecture selection. The red dot in coarse-grained architecture selection is picked and mutated for ﬁne-grained architecture selection.

model with input resolution 256, depth conﬁguration , {1:2:2:2:4:4:1} channel conﬁguration {32:16:24:48:88:128:216:352:1408} and kernel size conﬁguration . {3:3:5:3:5:5:3} After training of the single-stage model, the post-search step is highly parallelizable and independent of training.

5 Analysis of BigNASModel
Finetuning Child Models Sampled from BigNASModel. In previous sections we have reported the accuracies of child models from a single trained BigNASModel without ﬁnetuning, what if we do ﬁnetune it? To understand whether the trained BigNASModel has reached relatively optimal accuracies, we conduct experiments to ﬁnetune these child models (i.e., BigNASModel-S, BigNASModel-M, BigNASModel-L, BigNASModel-XL) for additional 25 epochs under diﬀerent constant learning rates separately. Table 2 shows that ﬁnetuning in our setting no longer improves accuracy signiﬁcantly.

Table 2. Analysis on Child Models sampled from BigNASModel. We compare the ImageNet validation performance of (1) child model directly sampled from BigNASModel without ﬁnetuning (w/o Finetuning), (2) child model ﬁnetuned with various constant learning rate (w/ Finetuning at diﬀerent lr). Blue subscript indicates the performance improvement while Red subscript indicates degradation.

Child Model

w/o Finetuning w/ Fintuning w/ Fintuning w/ Fintuning
lr = 0.01 lr = 0.001 lr = 0.0001

BigNASModel-S 76.5 BigNASModel-M 78.9 BigNASModel-L 79.5 BigNASModel-XL 80.9

74.6 (−1.9) 76.7 (−2.2) 77.9 (−1.6) 79.0 (−1.9)

76.4 (−0.1) 78.8 (−0.1) 79.6 (+0.1) 80.6 (−0.3)

76.5 (0.0) 78.8 (−0.1) 79.7 (+0.2) 80.8 (−0.1)

BigNAS: Neural Architecture Search with Big Single-Stage Models 715

Table 3. Analysis on training child architectures from scratch. We compare the ImageNet validation performance of (1) child model directly sampled from BigNASModel without ﬁnetuning (w/o Finetuning), (2) child architectures trained from scratch without distillation (FromScratch w/o distill), and (3) child architectures trained from scratch with two distillation methods A [14] and B [36] (FromScratch w/ distill (A)/(B)).

Child Architecture w/o Finetuning FromScratch FromScratch FromScratch

w/o distill w/ distill (A) w/ distill (B)

BigNASModel-S BigNASModel-M BigNASModel-L BigNASModel-XL

76.5 78.9 79.5 80.9

75.3 (−1.2) 77.4 (−1.5) 78.2 (−1.3) 79.3 (−1.6)

75.3 (−1.2) 77.4 (−1.5) 77.9 (−1.5) 79.0 (−1.9)

76.3 (−0.2) 78.6 (−0.3) 79.2 (−0.3) 80.4 (−0.5)

Training the Architectures of Child from Scratch. We further study the performance when these selected child models are trained from scratch with or without distillation. We implement two distillation variants. The Distill (A) is a simple distillation [14] without temperature. The teacher network is trained with dropout and label smoothing following our training pipeline. The student network is trained with distillation loss only from soft-predictions of the teacher network. The Distill (B) is inplace distillation [36] where we jointly train a teacher and student network from scratch with weight sharing. The student network is trained with the soft-predictions of the teacher network only. The Distill (B) is most similar to the distillation used in training BigNASModel. We note that although it is commonly believed that distillation can improve regularization, we found that the simple Distill (A) method does not help in EﬃcientNet-based architectures. Table 3 shows that the accuracies of child models slightly beneﬁt from jointly training a weight-sharing single-stage model, which is consistent to the observations in previous work [36].
6 Conclusion
We presented a novel paradigm for neural architecture search by training a single-stage model, from which high-quality child models of diﬀerent sizes can be induced for instant deployment without retraining or ﬁnetuning. With several proposed techniques, we obtain a family of BigNASModels as slices in a big pretrained single-stage model. These slices simultaneously surpass all state-of-theart ImageNet classiﬁcation models ranging from 200 MFLOPs to 1 GFLOPs. We hope our work can serve to further simplify and scale up neural architecture search.
References
1. Bender, G., Kindermans, P.J., Zoph, B., Vasudevan, V., Le, Q.: Understanding and simplifying one-shot architecture search. In: International Conference on Machine Learning, pp. 549–558 (2018)

716 J. Yu et al.
2. Berman, M., Pishchulin, L., Xu, N., Blaschko, M.B., Medioni, G.: Aows: adaptive and optimal network width search with latency constraints. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020
3. Brock, A., Lim, T., Ritchie, J., Weston, N.: SMASH: one-shot model architecture search through hypernetworks. In: International Conference on Learning Representations (2018). https://openreview.net/forum?id=rydeCEhs-
4. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-all: train one network and specialize it for eﬃcient deployment. In: International Conference on Learning Representations (2020). https://openreview.net/forum?id=HylxE1HKwS
5. Cai, H., Zhu, L., Han, S.: ProxylessNAS: direct neural architecture search on target task and hardware. In: International Conference on Learning Representations (2019). https://openreview.net/forum?id=HylVB3AqYm
6. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: learning augmentation strategies from data. In: Proceedings of the IEEE Conference on Computer vision and Pattern Recognition, pp. 113–123 (2019)
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: a large-scale hierarchical image database. In: IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009, pp. 248–255. IEEE (2009)
8. Goyal, P., et al.: Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 (2017)
9. Guo, Z., et al.: Single path one-shot neural architecture search with uniform sampling. arXiv preprint arXiv:1904.00420 (2019)
10. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: surpassing humanlevel performance on imagenet classiﬁcation. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 1026–1034 (2015)
11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)
12. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0 38
13. He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., Li, M.: Bag of tricks for image classiﬁcation with convolutional neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 558–567 (2019)
14. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015)
15. Howard, A., et al.: Searching for mobilenetv3. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 1314–1324 (2019)
16. Howard, A.G., et al.: eﬃcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)
17. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141 (2018)
18. Hu, S., et al.: Dsnas: direct neural architecture search without parameter retraining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12084–12092 (2020)
19. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)
20. Liu, C., et al.: Progressive neural architecture search. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 19–34 (2018)

BigNAS: Neural Architecture Search with Big Single-Stage Models 717
21. Liu, H., Simonyan, K., Vinyals, O., Fernando, C., Kavukcuoglu, K.: Hierarchical representations for eﬃcient architecture search. In: International Conference on Learning Representations (2018). https://openreview.net/forum?id=BJQRKzbA-
22. Liu, H., Simonyan, K., Yang, Y.: DARTS: diﬀerentiable architecture search. In: International Conference on Learning Representations (2019). https://openreview. net/forum?id=S1eYHoC5FX
23. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)
24. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shuﬄenet v2: practical guidelines for eﬃcient CNN architecture design. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 116–131 (2018)
25. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Eﬃcient neural architecture search via parameter sharing. In: ICML, pp. 4092–4101 (2018). http://proceedings. mlr.press/v80/pham18a.html
26. Ramachandran, P., Zoph, B., Le, Q.V.: Searching for activation functions (2018). https://openreview.net/forum?id=SkBYYyZRZ
27. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image classiﬁer architecture search. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, pp. 4780–4789 (2019)
28. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Inverted residuals and linear bottlenecks: mobile networks for classiﬁcation, detection and segmentation. arXiv preprint arXiv:1801.04381 (2018)
29. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res. 15(1), 1929–1958 (2014)
30. Stamoulis, D., et al.: Single-path NAS: designing hardware-eﬃcient convnets in less than 4 hours. arXiv preprint arXiv:1904.02877 (2019)
31. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: platform-aware neural architecture search for mobile. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)
32. Tan, M., Le, Q.V.: Eﬃcientnet: rethinking model scaling for convolutional neural networks (2019)
33. Wu, B., et al.: FBNET: hardware-aware eﬃcient convnet design via diﬀerentiable neural architecture search. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10734–10742 (2019)
34. Yang, Z., et al.: Cars: continuous evolution for eﬃcient neural architecture search. In: The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020
35. Yu, J., Huang, T.: Network slimming by slimmable networks: towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728 (2019)
36. Yu, J., Huang, T.: Universally slimmable networks and improved training techniques. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1803–1811. IEEE (2019)
37. Yu, J., Yang, L., Xu, N., Yang, J., Huang, T.: Slimmable neural networks. In: International Conference on Learning Representations (2019). https://openreview. net/forum?id=H1gMCsAqY7
38. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016)
39. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures for scalable image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8697–8710 (2018)

