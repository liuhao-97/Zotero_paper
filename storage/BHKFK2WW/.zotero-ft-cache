2022 IEEE International Conference on Communications Workshops (ICC Workshops) | 978-1-6654-2671-8/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICCWORKSHOPS53468.2022.9814499

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

JMSNAS: Joint Model Split and Neural Architecture Search for Learning over Mobile Edge Networks
Yuqing Tian, Zhaoyang Zhang†, Zhaohui Yang, and Qianqian Yang College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China Zhejiang Provincial Key Laboratory of Info. Proc., Commun. & Netw. (IPCAN), Hangzhou, China E-mail: {tianyq, ning_ming†}@zju.edu.cn, zhaohui.yang@ucl.ac.uk, qianqianyang20@zju.edu.cn

Abstract—Deep neural networks (DNNs) enable the booming intelligent mobile applications owing to the effectiveness and reliability. With the rapid development of ﬁfth generation (5G) communication networks, edge devices can be deployed with computation capability, which makes the distributed implementation of DNN over a wireless network possible. However, the simpliﬁed model splitting method cannot guarantee the accuracy and latency performance. They call for the iterative design of model splitting and parameter updating. In this paper, a joint model split and neural architecture search (JMSNAS) framework is proposed to deploy the generated DNN model over mobile edge networks. Considering both computing and communication resource constraints, we reformulates the multi-split problem to a computational graph search problem and optimizes the objective function to realize the trade-off between model accuracy and completion latency. The experiment results reveal the superiority of the proposed JMSNAS over state-of-the-art split machine learning model designs.
I. INTRODUCTION
The ﬁfth-generation (5G) mobile networks are envisioned to support the booming mobile intelligent applications. The advancements on machine learning (ML) provide a powerful tool for stable and reliable applications. As the backbone of intelligent applications, machine learning (ML) technique can guarantee highly accurate and reliable applications. However, deep neural network (DNN) inference is computation-intensive, making it difﬁcult for mobile devices with limited resources to ﬁnish the execution process within an acceptable latency.
For example, in the campus access control face recognition scenario, the face data is collected by a terminal device, and a classiﬁcation result needs to be obtained through network inference. This process is a computationally expensive task depending on the pixel of the captured images and the size of the network. Reducing the waiting time required for each recognition while maintaining the accuracy of the network becomes an challenge.
Thus, it is important to investigate the ML model splitting technology, which can split an ML model and deploy splitted computation tasks onto multiple edge devices with high efﬁciency and low latency. Model splitting framework can partition a DNN model into several parts. Each part is calculated by one device and the calculation result is passed to the corresponding device based on the model splitting framework via wireless or wired links as shown in Fig 2. The cellular network is a native mobile edge computing
*This work was supported in part by National Key R&D Program of China under Grant 2020YFB1807101, and National Natural Science Foundation of China under Grant U20A20158 and 61725104.

structure suitable for ML model splitting. All device interconnections within a cellular network can be abstracted into two types: chain and mesh topology, involving cellular user equipment (UE), small base stations (SBSs), macro base stations (MBSs) equipped with mobile edge computing (MEC) servers, and cloud servers.
Recently, there are some works focusing on applying the ML model splitting on the mobile edge devices in a cellular network, which allows each edge device to compute part of the ML model [1]–[5]. The authors in [1] proposed a single-split method, which can partition a DNN model into two parts for two-device system. Furthermore, Hu et al. [2] utilized directed acyclic graph theory in the single-split method. The works in [1] and [2] are limited to two-device system. Considering the general multi-split problem, the work in [3] constructed a min-cost graph search problem. However, the search algorithm in [3] is limited to the linear network, which cannot be applied to the complex mesh cellular network. Besides, Teerapittayanonet al. [4] proposed to distribute the given DNN model across computing hierarchies for the purpose of reducing the communication data size. Moreover, Li et al. [5] utilized an early-exit mechanism to adjust the splitting model size to accelerate the model inference. However, the existing works [1]–[5] did not consider customizing the neural network according to edge nodes’ computation and communication abilities, which indicates that the results obtained in [1]– [5] cannot guarantee the performance including accuracy and completion latency requirements over a mobile edge network.
To realize the purpose of deploying split learning over a wireless MEC system, the DNN model splitting should be adapt to computation and communication abilities of edge nodes. However, the simpliﬁed model splitting method cannot guarantee the convergence and latency performance of DNN. As a result, deploying split learning over a wireless network calls for the iterative design of model generating and model splitting, where model generating means that the DNN model should be properly designed and chosen based on the given training accuracy and latency requirements. To solve the model generating problem, neural architecture search (NAS) can be used to automatically design the artiﬁcial neural networks. The previous work [6]–[8] utilized NAS for model generating in a centralized manner, which cannot be directly applied to the distributed mobile edge networks. This motivates us to utilize the NAS method in solving the split learning problem over a wireless MEC system.

978-1-6654-267A1ut-h8o/r2iz2ed/$li3ce1n.s0e0d u©se2l0im2it2edIEtoE: EKAUST. Downloaded on Dece1m0be3r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

Search Space ࣛ sample a model ৏ ‫ ࣛ א‬Evaluation Strategy

guide sampling Search Strategy

performance estimate of ৏

Fig. 1: Three major components of NAS models.

1

2

ࣞଵ = 1,2

3

4

ࣞଶ = 3,4

5

6

ࣞଷ = 5,6

7

8

9

澥澤

ࣞସ = {7,8,9,10}

(a) The chain MEC network

In this paper, based on the emerging technique of NAS, we propose the so-called joint model split and neural architecture search (JMSNAS) framework for deploying an ML model over the MEC. We formulate the multi-split problem by searching for a network achieving the accuracy and latency trade-off on a cellular network with given multisplit points. To our best knowledge, JMSNAS is the ﬁrst practical framework that splits the ML model for MEC systems by using the NAS method. Our main contributions are list as follows:
• We propose a multi-split algorithm, which can assign the DNN sub-model for each computation node in a given network topology.
• By using multi-objective regularizations, the multiobjective mechanism is integrated in the loss function to various practical requirements.
• The proposed JMSNAS makes full use of all the devices in the cellular network, which can be applied to different MEC topology networks including both chain and mesh.
• Experiment results indicate that the proposed JMSNAS outperforms the state-of-the-art splitting method in terms of accuracy and latency.
II. PRELIMINARIES
A. Neural Architecture Search
NAS models have outperformed manually designed architectures in many tasks, such as semantic segmentation [6] and model compression [8]. The NAS process involves three major components: search space, evaluation strategy, and search strategy (Fig. 1). Search space deﬁnes a family of candidate operations and the way operations connect. Evaluation strategy determines the quality metric of the candidate models to provide feedback that guides the search strategy. Search strategy is the method to explore the search space and generate high-quality model architectures.
B. ML Model Splitting over MEC Systems
ML model splitting partitions the DNN computing load over MEC infrastructures, to meet speciﬁc requirements such as low inference latency. The typical scenarios of ML model splitting are shown in Fig. 2.
Typically, split learning in mobile edge networks generally involves three unique properties: (1) Multi-split: The existence of multiple devices in the cellular network requires the multi-split of the DNN model so as to each device compute part of the model. The number of feasible split solution increases exponentially with the number of DNN layers and edge devices. Thus, the linear exhaustive search algorithm used in single-split problem is not practical to solve the multi-split problem. (2) Multi-object: The solution of ML model splitting needs to satisfy multiple optimization goals, such as model accuracy, completion latency, and

1 ‫ࣝك‬

‫=ݎ‬2

1

2

࣮/{‫{ = }ݎ‬3,4,5} 3 4 5

{6} ‫ࣝ ك‬ 6

Input

FC

Conv

SoftMax

Pooling

Output

MBConv

Wireless Backhaul Wired Link

(b) The mesh MEC network

Fig. 2: The typical scenarios of ML model splitting.

data privacy. (3) Complicated topology: According to the practical scenario of cellular networks, the edge network topology can be summarized into two forms: chain and mesh. Correspondingly, the DNN includes linear and nonlinear forms.
Our NAS approach for ML model splitting considers both communication and computation costs of different devices in a cellular MEC network, and explores the comprehensive search space to obtain a DNN model with low latency and high accuracy on the given network. Besides, our approach can naturally form a splitting scheme for the obtained DNN model that deploys different parts of the DNN to the devices in the target network. Finally, our approach can be applied on general structures, including chain and mesh networks.
III. SYSTEM MODEL AND PROBLEM FORMULATION
Consider a deep neural network, which can be described by a directed computational graph G(e1, . . . , eN ) as shown in Fig. 2. The nodes in Fig. 2 stand for the operations, while directed edges stand for data stream cross layers. We use [N ] to represent set {1, · · · , N }. Denote N and M as the number of layers and available devices, respectively. Layer n ∈ [N ] represents the nth node in the computational graph, and device m ∈ [M ] stands for the mth node in the MEC network. Let τnm be the execution latency of layer n on device m and εm n be the communication latency of transmitting the output of layer n between device m and the following device. Speciﬁcally, we have εM n = 0 since device M is the last device. All the MEC structures can be abstracted as chain and mesh topologies, which are considered in our model.
A. Chain Network
As shown in Fig. 2(a), consider a network with one UE, M − 2 edge devices, and a cloud server to form a M -node cellular chain. Since there are M computation nodes, we need to split the original DNN network into M parts and each node can compute one part of them. Let Dm stand for the layers deployed on device m, where D1 = {1, 2} means that both layers 1 and 2 are assigned to device 1.

Authorized licensed use limited to: KAUST. Downloaded on Dece1m0be4r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

Input Conv1x1(x6),Relu6
Dwise 3x3,Relu6
Conv1x1,Relu6
Add
stride=1 (a)

Input
Conv1x1(x6), Relu6
Dwise 3x3, stride=2,
Relu6
Conv1x1,Linear
stride=2

Input

MBConv 3x3, ratio=6, with shortcut
MBConv 5x5, ratio=3, without shortcut
MBConv 7x7, ratio=6, Ouwtpiutht out shortcut
Conv 3x3
…
Identity
(b)

Output

MBConv Module Expansion Convolution Depthwise Convolution Candidate Operation Learnable Block Input Node Output Node

1

2

݀(1)

݀(1) + 1

݀(1) + 2

݀ 2 … ݀ ݉െ1

݀ ݉െ1

… ݀(݉)

݀ ‫ܯ‬െ1

݀ ‫ܯ‬െ1

݀(‫)ܯ‬

+1

+2

+1

+2

device 1

…

…

device 2

device ݉

(c)

device ‫ܯ‬

Fig. 3: The implementation procedures. A Leader node collects the MEC network information and constructs the search space by selecting the modules in (a) and designing the candidate operations in (b). After running JMSNAS, the leader node sends the parts of the generated model to the corresponding devices in (c).

With the above notation, the completion latency [9] on chain network can be described as

T=

(

τnm + εm d(m)),

(1)

m∈[M ] n∈Dm

where d(m) = max Dm denotes the index of the last layer executed on device m.

B. Mesh Network

Consider a mesh network where a UE ﬁrst accesses an SBS through the wireless backhaul, then the SBS broadcasts the results to three MBSs, and ﬁnally the outputs of MBSs are aggregated in a cloud server, as shown in Fig. 2(b). Let C denote the set of devices connected in the chain and T denote devices connected in the tree form. For example, in Fig. 2(b), C = {1, 6}, T = {2, 3, 4, 5}. There is a root node r in tree nodes set such as device 2 in Fig. 2(b), which forwards the output data of layer d(2) to devices in set T \{r} for executing the following layers in parallel. Thus, the completion latency [9] on mesh network can be described as

T= (

τnm + εm d(m))

m∈C n∈Dm

(2)

+

max (
m∈T \{r} n∈Dr

τnr

+

εrd(r)

+

n∈Dm

τnm

+

εm d(m) ).

C. Implementation Model

The detailed implementation procedures of JMSNAS mainly include three steps. In the ﬁrst step, a leader node collects the device information in the cellular edge network, including the computing capabilities of each device, the communication capabilities between devices, and the device connection topology. Having obtained the device information, the leader node in the second step completes the initialization of the NAS search space, i.e., the connection mode of the DNN layers and the maximum number of DNN layers which can be executed by each device. In other words, the structure of computational graph (Fig. 3(c)) and parameters Dm, d(m) are determined for each m in the second step. According to the characteristics and

complexity of the DNN task, the candidate operations in the search space are artiﬁcially set. In the third step, the leader node runs JMSNAS to determine the DNN used for the speciﬁc task. Additionally, the network naturally has a split matching scheme deployed to each device to meet the constraints set by the task, which can include limited latency, limited power consumption, and so on. The leader node sends the parts of searched network to the corresponding device, and each device in the MEC network executes the computation and communication assignment in a distributed manner.

D. Problem Formulation

Our goal is to optimize the neural network weight parameter θ and architecture parameter α so as to minimize the loss function of the DNN model under given latency constraint. Mathematically, the considered optimization problem can be formulated as:

K

min L = (α, θ; (xk, yk)),

(3)

k=1

s.t. T ≤ TConst,

where α = [α11, α21, · · · , αin, · · · , αRN ] and TConst is the maximum allowed latency. The variable αin, n ∈ [N ], i ∈ [R] indicates the weight of choosing the ith candidate as the operation of layer n. In (3), xk and yk respectively indicate the kth sampled input image and the corresponding label, S is the number of sampled images. (·) represents the cross entropy loss function. Due to complicated nonconvex loss function and latency constraint, it is generally difﬁculty to solve problem (3) with the conventional convex optimization theory. To solve it, we use the advancement of ML in the following section.

IV. METHODOLOGY
In this section, we ﬁrst introduce the JMSNAS framework, which includes search spaces, evaluation strategies, and search strategies for optimized neural networks [7]. Then, we present a gradient-based algorithm to deal with the non-differential device metric: latency.

Authorized licensed use limited to: KAUST. Downloaded on Dece1m0be5r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

1

2

1

…

2… R
Input
… 1

R

2
…

update MB Conv 3x3
ߙଵ
0

… Layer 1

R Layer 2 Layer 3

(a) Search Space

Input

MB Conv 5x5

MB Conv 7x7

…

Identity

Candidate Operation The Chosen Operation

Weight Parameters

MB Conv 3x3

ߙଶ

ߙଷ … ߙୖ

Architecture Parameters

ߙଵ

1

0 …0

Binary Gate

0

Output

(b) Search Strategy

Input

MB Conv 5x5
ߙଶ 0

MB Conv 7x7

…

Identity update

ߙଷ … ߙୖ 1 …0

Output

Input

… Output

The Compact Model on Dataset

Loss

(c) Evaluation Strategy

Fig. 4: The overview of the NAS method. Search space is a full-tree structure (a) making up of all the possible DNN models. The relative parameters are updated in (b), and evaluated in (c).

A. Neural Architecture Search
Through NAS techniques, we can automatically develop a model that outperforms previous designs deployed on cellular MEC networks.
Search Space: Let Vn be the set of R candidate operations available for layer n. In the initialized computational graph with ﬁxed topology, the candidate operations of each layer together constitute a full-tree search space, as shown in Fig. 4(a). Our search space involves all combinations of R operations for N layers, which is sufﬁcient to speciﬁc task with adjustable parameters N and R.
As an example to construct search space Vn, the MobileNetV2 is a lightweight and highly efﬁcient model [10], which performs well on large-scale image classiﬁcation tasks. It is constructed by module mobile inverted bottleneck convolution (MBConv), which can be used as the backbone to build the candidate operations. The module MBConv takes low-dimension vectors as input, expands to high dimension, and is then ﬁltered with a depthwise convolution. With changeable parameters about expansion ratio and convolution kernel size, Vn includes the following types:
• 3 × 3 MBConv with expansion ratio 3 • 3 × 3 MBConv with expansion ratio 6 • 5 × 5 MBConv with expansion ratio 3 • 5 × 5 MBConv with expansion ratio 6 • 7 × 7 MBConv with expansion ratio 3 • 7 × 7 MBConv with expansion ratio 6 • Identity
In addition, each operation can choose whether to have a shortcut connection or not. Therefore, there are up to 13 optional operations. The network length can be shortened by selecting identity to skip blocks. To make the model more accurate, the framework tends to choose a large kernel and a high expansion ratio with a large amount of computation, which leads to a larger network. On the contrary, to save execution latency, the framework tends to choose a small kernel and a low expansion ratio, which makes the network thinner. As a result, in order to balance the accuracy and latency, the width and length of the model should be well designed.
Evaluation Strategy: Before demonstrating the evaluation strategy, we need to clarify how to represent the forward propagation result of the full-tree structure as shown in Fig. 4(a). To construct the full-tree structure that includes

all the combinations in the search space, we denote the
operation of each layer in the computational graph by v = [v1, · · · , vR]T , which is a mixed operation vector with R elements. The output of v is designed based on the output
of its R paths.

To simplify the description, we use a certain DNN layer

to illustrate our design for mixed operations. Without loss

of generality, we replace αin with αi in the real-valued architecture parameters. Moreover, we introduce the one-

hot binary gate g = [g1, · · · , gR]T , where gi = 1 with

probability pi, and pi = exp (αi)/ j exp (αj) indicates the

probability of choosing operation i. With input xk, the out-

put of v can be formulated as

R i=1

gi vi (xk )

=

gT

v (xk ),

where v(xk) = [v1(xk), · · · , vR(xk)]T .

To solve problem (3), we modify the objective function

as

K
(α, θ; (xk, yk)) + λ1||θ||22 + λ2(T − TConst)2, (4)
k=1

where λ1, λ2 are hyper-parameters to adjust the learning process. Note that the latency constraint is formulated as
a penalty in (4), which forces that the optimal solution satisfying T = Tconst. This is because the optimal solution of (3) is always achieved at T = Tconst as deep and timeconsuming network can lead to small loss value.

Search Strategy: There are two types of parameters in our framework, i.e., weight parameter θ and architecture parameter α. We train weight and architecture parameters in an alternating manner, as shown in Fig. 4(b). When training weight parameters, the architecture parameters are ﬁxed and the binary gates g are sampled to identify the current DNN model. Then, the sampled model is trained with forward and backward propagation. When updating architecture parameters, the weight parameters are given in the previous step and the binary gates are reset. To update the architecture parameter, the partial derivative ∂L/∂αi with respect to discrete operation choosing needs to be calculated, which is provided by the following lemma.
Lemma 1: The partial derivative ∂L/∂αi can be approximately presented by

R j=1

∂L ∂gj

pj

(δij

−

pi)

,

(5)

where δii = 1, δij

= 0 if i = j, and

∂L ∂gj

can be obtained

from the following equation (8).

Authorized licensed use limited to: KAUST. Downloaded on Dece1m0be6r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

Proof: The partial derivative of L with respect to αi is:

∂L = R ∂L ∂pj ≈ R ∂L ∂pj ∂αi j=1 ∂pj ∂αi j=1 ∂gj ∂αi

=

R

∂ ∂L

j=1 ∂gj

exp(αj )
k exp(αk )
∂αi

(6)

=

R j=1

∂L ∂gj

pj

(δij

−

pi)

,

where the ﬁrst equality follows from the chain theory and the approximation holds based on the deﬁnition of gj. The derivative ∂L/∂gj can be calculated by substituting the expression of gj into the function (4). In particular, if we consider the cross-entropy loss function, equation (4) can be further rewritten as

LCE

=

−

1 K

K
(yk log hgv(xk)

(7)

k=1

+ (1 − yk) log(1 − hgv(xk))),

where hgv(xk) = 1/(1+e−gT v(xk)) indicates the predicted probability. Thus, we can obtain

∂LCE = 1 ∂g K

K

(hgv(xk) − yk) v(xk).

(8)

k=1

This completes the proof.

Based on Lemma 1, we can update the architecture parameter through backpropagation. The architecture parameter updating procedure involves calculating and storing vj(x) for every j, which costs R times memory. To address this issue, we mask all the paths except for the sampled two in every training process so that we can reduce the memory cost from R times to 2 times.

B. Derivative of Latency Function
Since our training network dynamically chooses operations according to a probability distribution, the latency in loss function (4) is not differentiable with respect to the architecture parameter. To handle this issue, we reformulate the latency of a network to the average latency, which is a continuous function. A mixed operation v includes candidate set {v1, v2, · · · , vR} and each operation vi corresponds to a selection probability pi. We build a regression model U (·) to estimate the operation latency. For example, when layer n adopts operation vi executed on device m, we have Unm(vi) as the execution latency. In such a full-tree structure, the execution latency of layer n on device m, τnm in (1) and (2) should be reformulated as E(τnm) = i piUnm(vi). Thus, the gradient of E(τnm) with respect to architecture parameter can be given by: ∂E(τnm)/∂pi. Furthermore, we represent the overall latency Eα(T ) by replacing τnm with E(τnm).
In summary, the proposed method provides ample search space and sufﬁcient ﬂexibility to search for proper layer operations and enables high performance as the tradeoff between accuracy and latency. The cost during the NAS process is completely undertaken by the leader node, which requires the leader node to have strong computing capabilities.

TABLE I: Link settings

Transmitter UE SBS MBS

Receiver SBS MBS Cloud

Type Wireless Wireless Wired

Capacity(Mbps) 25 50 200

V. EXPERIMENT RESULTS
In this section, we ﬁrst describe our experiment setup, including the cellular network setup, and the conﬁguration for the NAS procedure. We then present our framework evaluation results on ML model splitting compared with the previous methods.
A. Cellular Network
The performance of the proposed JMSNAS framework is evaluated on both chain and mesh networks. Table I shows the communication link settings between devices.
The execution and communication latency proﬁles of different operations involved in the DNN model are the key metric of the model workload. To accurately measure the operation latency, we adopt a Pytorch [11] package (torchprof) to track latency on different devices for each involved operation and build an estimator U (·) to predict operation-wise latency during model inference.
We measure the latency proﬁles on four types of machines to represent the UE, SBS, MBS with MEC server and cloud server, respectively: (1) Raspberry Pi 4 Model with 4 Cortex-A72 1.5GHz CPUs, (2) XPS15 Laptop with Intel i7-11800H CPU, 16GB DDR 4 RAM, and Nvidia RTX3050 GPU, (3) NVIDIA Jetson AGX Xavier with 64 Tensor Core GPUs and 8-core ARM CPUs, (4) A server with two Intel Platinum 8280 CPUs, and Tesla V100 GPU.
B. Neural Architecture Search
We demonstrate the effectiveness of JMSNAS on the ImageNet dataset [12]. The training set includes 1231167 images of 1000 classes, each with dimension of 224×224× 3, while the validation set includes 50000 images.
In order to obtain a model that performs well on the given cellular network, the NAS process consists of two stages. In the ﬁrst stage, we search the full-tree structure on the training split for 20 warm-up epochs and 60 training epochs, using Adam optimizer with initial learning rate of 0.002 and batch size of 512. Warm-up training is a technique widely used in deep learning. It helps to alleviate overﬁtting on the mini-batchs, and to maintain the stability of the DNN model. At the end of every training epoch, we evaluate the performance of the current compact network on the validation set. We set up three levels of completion latency constraints on the chain network and mesh network, respectively.
In the second stage, after the architecture parameters of the full-tree structure converge, the compact model architecture is ﬁxed. Then, we further train the model on the training set for 200 epochs, with the weight parameters in the ﬁrst stage as the pretrained parameters. In this way, the derived compact model will get higher accuracy without compromising its efﬁciency.

Authorized licensed use limited to: KAUST. Downloaded on Dece1m0be7r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

WS17 IEEE ICC 2022 Workshop on Edge Artificial Intelligence for 6G

Fig. 5: In the ﬁrst NAS stage, JMSNAS on the chain and mesh networks with different TConst latency constraints.
We adopt two performance metrics of our framework, i.e., top-1 accuracy and latency. Fig. 5 shows the top-1 accuracy corresponding to different latency constraints in the ﬁrst NAS stage. Here, we ﬁnd that the models did not reach the highest accuracy during the ﬁrst stage, since the model architecture is still alternating. And when the maximum allowed latency is larger, the model accuracy tends to be higher. Compared with models on the chain network, models on the mesh network shows faster convergence and higher accuracy with the same number of training epochs.
Since the operations in our search space are mainly MBConvs, modules in MobileNetV2, we also compare the accuracy and latency of the JMSNAS-crafted models with MobileNet (V1[13], V2[10], V3[14]). Fig. 6 shows that our framework achieves higher accuracy, which conﬁrms the effectiveness of JMSNAS. As for the latency, we compare JMSNAS-crafted model performance with two baselines, cloud computing and HiveMind [3] multi-split framework. We apply the cloud computing to the chain model obtained by JMSNAS, by uploading the input data to the cloud center and completing the model inference on the cloud. The latency is mainly determined by the communication link conditions. Fig. 6 shows that JMSNAS can reduce the average latency by up to 20.1% compared with cloud computing. For DNN models with a chain structure such as MobileNet series, we can adopt linear search method like HiveMind to determine the best splitting points. Fig. 6 shows that our framework outperforms MobileNet with linear search method.
VI. CONCLUSION
In this paper, we have proposed a NAS-based multi-split framework, deploying the generated DNN model to a cellular mobile edge network to meet the accuracy and latency requirements. The proposed JMSNAS works well on largescale image classiﬁcation tasks with an ample search space dependent on the mobile edge network conditions. The automatically generated models with native split scheme outperform the previous model split method.
The proposed JMSNAS is a general framework, and it can be applied in any practical scenario to dynamically determine a customized DNN.

Fig. 6: Final accuracy vs. latency after the second NAS stage.
REFERENCES
[1] Y. Kang, J. Hauswald, C. Gao et al., “Neurosurgeon: Collaborative intelligence between the cloud and mobile edge,” ACM SIGARCH Computer Architecture News, vol. 45, no. 1, pp. 615–629, 2017.
[2] C. Hu, W. Bao, D. Wang et al., “Dynamic adaptive dnn surgery for inference acceleration on the edge,” in IEEE INFOCOM IEEE Conference on Computer Communications, 2019, pp. 1423–1431.
[3] S. Wang, X. Zhang, H. Uchiyama et al., “Hivemind: Towards cellular native machine learning model splitting,” IEEE Journal on Selected Areas in Communications, 2021.
[4] S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Distributed deep neural networks over the cloud, the edge and end devices,” in 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS), 2017, pp. 328–339.
[5] E. Li, Z. Zhou, and X. Chen, “Edge intelligence: On-demand deep learning model co-inference with device-edge synergy,” in Proceedings of the 2018 Workshop on Mobile Edge Communications, 2018, pp. 31–36.
[6] L.-C. Chen, M. D. Collins, Y. Zhu et al., “Searching for efﬁcient multi-scale architectures for dense image prediction,” arXiv preprint arXiv:1809.04184, 2018.
[7] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture search on target task and hardware,” arXiv preprint arXiv:1812.00332, 2018.
[8] Y. He, J. Lin, Z. Liu, H. Wang et al., “Amc: Automl for model compression and acceleration on mobile devices,” in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 784–800.
[9] Y.-H. Kao, B. Krishnamachari, M.-R. Ra, and F. Bai, “Hermes: Latency optimal task assignment for resource-constrained mobile computing,” IEEE Transactions on Mobile Computing, vol. 16, no. 11, pp. 3056–3069, 2017.
[10] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov et al., “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510–4520.
[11] A. Paszke, S. Gross, F. Massa et al., “Pytorch: An imperative style, high-performance deep learning library,” in Advances in Neural Information Processing Systems 32, 2019, pp. 8024–8035.
[12] O. R., J. D., H. S. et al., “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.
[13] A. G. Howard, M. Zhu, B. Chen et al., “Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications,” arXiv preprint arXiv:1704.04861, 2017.
[14] A. Howard, M. Sandler, G. Chu et al., “Searching for mobilenetv3,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1314–1324.

Authorized licensed use limited to: KAUST. Downloaded on Dece1m0be8r 04,2022 at 18:45:37 UTC from IEEE Xplore. Restrictions apply.

