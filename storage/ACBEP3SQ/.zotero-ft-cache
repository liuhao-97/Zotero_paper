Lecture Notes in Computer Science
Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen

6784

Editorial Board
David Hutchison Lancaster University, UK
Takeo Kanade Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler University of Surrey, Guildford, UK
Jon M. Kleinberg Cornell University, Ithaca, NY, USA
Alfred Kobsa University of California, Irvine, CA, USA
Friedemann Mattern ETH Zurich, Switzerland
John C. Mitchell Stanford University, CA, USA
Moni Naor Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz University of Bern, Switzerland
C. Pandu Rangan Indian Institute of Technology, Madras, India
Bernhard Steffen TU Dortmund University, Germany
Madhu Sudan Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos University of California, Los Angeles, CA, USA
Doug Tygar University of California, Berkeley, CA, USA
Gerhard Weikum Max Planck Institute for Informatics, Saarbruecken, Germany

Beniamino Murgante Osvaldo Gervasi Andrés Iglesias David Taniar Bernady O. Apduhan (Eds.)
Computational Science and Its Applications ICCSA 2011
International Conference Santander, Spain, June 20-23, 2011 Proceedings, Part III
13

Volume Editors
Beniamino Murgante Basilicata University Potenza, Italy E-mail: beniamino.murgante@unibas.it
Osvaldo Gervasi University of Perugia, Italy E-mail: osvaldo@unipg.it
Andrés Iglesias University of Cantabria, Santander, Spain E-mail: iglesias@unican.es
David Taniar Monash University, Clayton, VIC, Australia E-mail: david.taniar@infotech.monash.edu.au
Bernady O. Apduhan Kyushu Sangyo University Fukuoka, Japan E-mail: bob@is.kyusan-u.ac.jp

ISSN 0302-9743

e-ISSN 1611-3349

ISBN 978-3-642-21930-6

e-ISBN 978-3-642-21931-3

DOI 10.1007/978-3-642-21931-3

Springer Heidelberg Dordrecht London New York

Library of Congress Control Number: 2011929636

CR Subject Classiﬁcation (1998): C.2, H.4, F.2, H.3, D.2, C.2.4, F.1, H.5

LNCS Sublibrary: SL 1 – Theoretical Computer Science and General Issues

© Springer-Verlag Berlin Heidelberg 2011 This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable to prosecution under the German Copyright Law. The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface

These multiple volumes (LNCS volumes 6782, 6783, 6784, 6785 and 6786) consist of the peer-reviewed papers from the 2011 International Conference on Computational Science and Its Applications (ICCSA 2011) held in Santander, Spain during June 20-23, 2011. ICCSA 2011 was a successful event in the International Conferences on Computational Science and Its Applications (ICCSA) conference series, previously held in Fukuoka, Japan (2010), Suwon, South Korea (2009), Perugia, Italy (2008), Kuala Lumpur, Malaysia (2007), Glasgow, UK (2006), Singapore (2005), Assisi, Italy (2004), Montreal, Canada (2003), and (as ICCS) Amsterdam, The Netherlands (2002) and San Francisco, USA (2001).
Computational science is a main pillar of most of the present research, as well as industrial and commercial activities and plays a unique role in exploiting ICT innovative technologies. The ICCSA conferences have been providing a venue to researchers and industry practitioners to discuss new ideas, to share complex problems and their solutions, and to shape new trends in computational science.
Apart from the general tracks, ICCSA 2011 also included 31 special sessions and workshops, in various areas of computational science, ranging from computational science technologies to speciﬁc areas of computational science, such as computer graphics and virtual reality. We accepted 52 papers for the general track, and 210 in special sessions and workshops. These represent an acceptance rate of 29.7%. We would like to show our appreciations to the Workshop and Special Session Chairs and co-Chairs.
The success of the ICCSA conference series, in general, and ICCSA 2011, in particular, is due to the support of many people: authors, presenters, participants, keynote speakers, Session Chairs, Organizing Committee members, student volunteers, Program Committee members, International Liaison Chairs, and people in other various roles. We would like to thank them all. We would also like to thank Springer for their continuous support in publishing ICCSA conference proceedings.

June 2011

Osvaldo Gervasi David Taniar

Message from the ICCSA 2011 General Chairs
These ﬁve volumes contain an outstanding collection of refereed papers selected for the 11th International Conference on Computational Science and Its Applications, ICCSA 2011, held in Santander (Spain), June 20-23, 2011. We cordially invite you to visit the ICCSA website http://www.iccsa.org where you can ﬁnd all relevant information about this interesting and exciting event.
ICCSA 2011 marked the beginning of the second decade of this conference series. Previous editions in this series of highly successful International Conferences on Computational Science and Its Applications (ICCSA) were held in Fukuoka, Japan (2010), Suwon, Korea (2009), Perugia, Italy (2008), Kuala Lumpur, Malaysia (2007), Glasgow, UK (2006), Singapore (2005), Assisi, Italy (2004), Montreal, Canada (2003), and (as ICCS) Amsterdam, The Netherlands (2002) and San Francisco, USA (2001).
As we enter the second decade of ICCSA, we realize the profound changes and spectacular advances in the world of computational science. This discipline plays a unique role in fostering new technologies and knowledge, and is crucial for most of the present research, and industrial and commercial activities. We believe that ICCSA has contributed to this change by oﬀering a real opportunity to explore innovative approaches and techniques to solve complex problems. Reciprocally, the computational science community has enthusiastically embraced the successive editions of ICCSA, thus contributing to making ICCSA a focal meeting point for those interested in innovative, cutting-edge research about the latest and most exciting developments in the ﬁeld. We are grateful to all those who have contributed to the current success of ICCSA with their continued support over the past ten years.
ICCSA 2011 would not have been made possible without the valuable contribution from many people. We would like to thank all session organizers for their diligent work, which further enhanced the conference levels and all reviewers for their expertise and generous eﬀort which led to a very high quality event with excellent papers and presentations. We especially recognize the contribution of the Program Committee and Local Organizing Committee members for their tremendous support and for making this congress a very sucessful event.
We would like to sincerely thank our keynote speakers, who willingly accepted our invitation and shared their expertise through illuminating talks, helping us to fully meet the conference objectives.
We highly appreciate the University of Cantabria for their enthusiastic acceptance to host the conference on its main campus, their logistic assistance and additional ﬁnancial support. The conference was held in the Faculty of Sciences of the University of Cantabria. We thank the Dean of the Faculty of Sciences, Ernesto Anabitarte, for his support before and during the congress, and for providing the venue of the conference and the use of all needed facilities.

VIII

Message from the ICCSA 2011 General Chairs

ICCSA 2011 was jointly organized by the Department of Applied Mathematics and Computational Sciences and the Department of Mathematics, Statistics and Computation of the University of Cantabria, Spain. We thank both departments for their encouranging support of this conference from the very beginning. We would like to express our gratitude to the Local Organizing Committee for their persistent and enthusiastic work towards the success of this conference.
We owe special thanks to all our sponsors: the Faculty of Sciences, the University of Cantabria, the Municipality of Santander, the Regional Government of Cantabria and the Spanish Ministry of Science and Innovation, for their continuous support without which this conference would not be possible. We also thank our publisher, Springer, for their acceptance to publish the proceedings and for their kind assistance and cooperation during the editing process.
Finally, we thank all authors for their submissions and all conference attendants for making ICCSA 2011 truly an excellent forum on computational science, facilitating exchange of ideas, fostering new collaborations and shaping the future of this exciting ﬁeld. Last, but certainly not least, we wish to thank our readers for their interest in these proceedings. We really hope you ﬁnd in these pages interesting material and fruitful ideas for your future work.

June 2011

Andr´es Iglesias Bernady O. Apduhan

The Wisdom of Ancient Masters
In 1879, Marcelino Sanz de Sautuola and his young daugther Mar´ıa incidentally noticed that the ceiling of the Altamira cave was covered by images of bisons and other animals, some as old as between 25,000 and 35,000 years. They had discovered what came to be called the Sistine Chapel of Paleolithic Art. When the discovery was ﬁrst made public in 1880, many experts rejected it under the belief that prehistoric man was unable to produce such beautiful and ellaborated paintings. Once their authenticity was later conﬁrmed, it changed forever our perception of prehistoric human beings.
Today, the cave of Altamira and its paintings are a symbol of the wisdom and ability of our ancient ancestors. They remind us that our current technological development is mostly based on the work, genius and eﬀorts of our predecessors over many generations.
The cave of Altamira (UNESCO World Heritage Site) is located in the region of Cantabria, near the city of Santander (ICCSA 2011 conference venue). The original cave is closed to the public for preservation, but conference attendees visited the ”Neocave”, an exact reproduction of the original space with all its cracks and textures and the permanent exhibition ”The Times of Altamira”, which introduces visitors to the prehistory of the peninsula and rupestrian art.
“After Altamira, all is decadence” (Pablo Picasso, famous Spanish painter)

ICCSA 2011 Welcome Message
Welcome to the proceedings of the 11th International Conference on Computational Science and Its Applications, ICCSA 2011, held in Santander, Spain.
The city of Santander is located in the self-governed region of Cantabria, on the northern coast of Spain between Asturias and the Basque Country. This beautiful region of half a million inhabitants is on the shores of the Cantabrian Sea and is crossed by a mountain range. The shores and inland valleys oﬀer a wide variety of landscapes as a consequence of the mild, moist climate of so-called Green Spain. The coastal landscape of beaches, bays and cliﬀs blends together with valleys and highland areas. All along the coast there are typical traditional ﬁshing ports and innumerable diverse beaches of soft white sand.
However, Cantabria’s attractions are not limited to its natural treasures. History has provided a rich artistic and cultural heritage found in towns and villages that are outstanding in their own right. The archaeological remains and historic buildings bear the mark of a unique history starting with the world-famous Altamira cave paintings, a veritable shrine to the prehistoric age. In addition, there are remarkable remains from the Romans, the Mozarabic presence and the beginnings of the Reconquest of Spain, along with an artistic heritage of Romanesque, Gothic and Baroque styles. Examples include the Prehistoric Era (the Altamira and Puente Viesgo Caves), Roman ruins such as those of Juli´obriga, medieval settlements, such as Santillana del Mar, and several examples of the civil and religious architecture of the nineteenth and twentieth centuries.
The surrounding natural landscape and the historical importance of many of its villages and buildings make this region very appealing for tourism, especially during the spring and summer seasons, when the humid, mild weather gives the region a rich and varied nature of woods and meadows. At the time of the conference, attendees enjoyed the gentle climate (with temperatures averaging 18-20 degrees Celsius) and the longest days of the year. They found themselves waiting for sunset at the beach at about 11 pm!
Capital of the autonomous region of Cantabria, the city of Santander is also a very popular destination for tourism. Based around one of the most beautiful bays in the world, this modern city is famous for its sparkling beaches of yellow sand and clean water, the hospitality of its people and the high reputation of its celebrated gastronomy, mostly based on ﬁsh and shellﬁsh. With a population of about 200,000 inhabitants, Santander is a very safe city, with a vibrant tourist scene ﬁlled with entertainment and a high quality of life, matching the best standards in the world. The coastal side of the city boasts a long string of top– quality beaches and recreational areas, such as the Magdalena Peninsula, the Sardinero and Matalen˜as Park. There are several beaches and harbors limiting the city on the northern side, toward the southern part there is the old city

XII

ICCSA 2011 Welcome Message

center and a bit further on the green mountains. We proudly say that Santander is between the blue and the green.
The University of Cantabria (in Spanish, the Universidad de Cantabria, UC ) is the only public university in Cantabria, Spain. It was founded in 1972 and is organized in 12 faculties and schools. With about 13,000 students and 1,000 academic staﬀ, the University of Cantabria is one of the most reputed universities in the country, ranking in the highest positions of Spanish universities in relation to its size. Not surprisingly, it was selected as a Campus of International Excellence by the Spanish Government in 2009.
Besides the technical sessions and presentations, ICCSA 2011 provided an interesting, must-attend social program. It started with a Welcome Reception at the Royal Palace of the Magdalena (Sunday 19), the most emblematic building of Santander and also the most visited place in the city. The royal family used the palace during the period 1913–1930 as a base for numerous recreational and sporting activities, and the king sometimes also held government meetings at the property. Conference delegates had the wonderful opportunity to visit this splendid palace, enjoy the magniﬁcent views and see some rooms where royalty lived. The Gala Dinner (Tuesday 21) took place at the Grand Casino, in the “Sardinero” area, a regal, 1920’s building with large windows and spacious terraces oﬀering superb views of the Sardinero beach. The Casino was King Alfonso XIII and Queen Victoria Eugenia’s main place of entertainment during their summer holidays in the city between 1913 and 1930. The gala also included some cultural and musical events. Finally, a half-day conference tour (Wednesday 22) covered the “live museum” of the Middle Ages, Santillana del Mar (a medieval town with cobbled streets, declared “Site of Artistic and Historical Importance” and one of the best-known cultural and tourist centers in Cantabria) and the Altamira Neocave, an exact reproduction of the original Altamira cave (now closed to the public for preservation) with all its cracks and textures and the permanent exhibition “The Times of Altamira”, which introduces visitors to the prehistory of the peninsula and rupestrian art.
To close the conference, attendees could join the people of Santander for St. John’s day, celebrated in the night between June 23 and 24 to commemorate the summer solstice with bonﬁres on the beach.
We believe that all these attractions made the conference an unforgettable experience.
On behalf of the Local Organizing Committee members, I thank all attendees fot their visit.

June 2011

Andr´es Iglesias

Message from the Chairs of the Session: 6th International Workshop on “Geographical Analysis, Urban Modeling, Spatial Statistics”
(GEOG-AN-MOD 2011)
During the past few decades the main problem in geographical analysis was the lack of spatial data availability. Nowadays the wide diﬀusion of electronic devices containing geo-referenced information generates a great production of spatial data. Volunteered geographic information activities (e.g., Wikimapia, OpenStreetMap), public initiatives (e.g., spatial data infrastructures, geo-portals) and private projects (e.g., Google Earth, Microsoft Virtual Earth, etc.) produced an overabundance of spatial data, which, in many cases, do not help the eﬃciency of decision processes. The increase of geographical data availability has not been fully coupled by an increase of knowledge to support spatial decisions.
The inclusion of spatial simulation techniques in recent GIS software favored the diﬀusion of these methods, but in several cases led to mechanisms based on which buttons have to be pressed without having geography or processes in mind. Spatial modeling, analytical techniques and geographical analyses are therefore required in order to analyze data and to facilitate the decision process at all levels, with a clear identiﬁcation of the geographical information needed and reference scale to adopt. Old geographical issues can ﬁnd an answer thanks to new methods and instruments, while new issues are developing, challenging researchers for new solutions. This workshop aims at contributing to the development of new techniques and methods to improve the process of knowledge acquisition.
Conference themes include:
Geostatistics and spatial simulation Agent-based spatial modeling Cellular automata spatial modeling Spatial statistical models Space-temporal modeling Environmental modeling Geovisual analytics, geovisualization, visual exploratory data analysis Visualization and modeling of track data Spatial optimization Interaction simulation models Data mining, spatial data mining Spatial data warehouse and spatial OLAP Integration of spatial OLAP and spatial data mining Spatial decision support systems

XIV

GEOG-AN-MOD 2011

Spatial multicriteria decision analysis Spatial rough set Spatial extension of fuzzy set theory Ontologies for spatial analysis Urban modeling Applied geography Spatial data analysis Dynamic modeling Simulation, space-time dynamics, visualization and virtual reality.

Giuseppe Borruso Beniamino Murgante
Stefania Bertazzon

Message from the Chairs of the Session: “Cities, Technologies and Planning” (CTP 2011)
‘Share’ term has turned into a key issue of many successful initiatives in recent times. Following the advent of Web 2.0, positive experiences based on mass collaboration generated “Wikinomics” hnd ave become ‘Socialnomics”, where ‘Citizens are voluntary sensors’.
During the past few decades, the main issue in GIS implementation has been the availability of sound spatial information. Nowadays, the wide diﬀusion of electronic devices providing geo-referenced information resulted in the production of extensive spatial information datasets. This trend has led to “GIS wikiﬁcation”, where mass collaboration plays a key role in the main components of spatial information frameworks (hardware, software, data, and people).
Some authors (Goodchild, 2007) talk about ‘volunteered geographic information’ (VGI), as the harnessing of tools to create, assemble, and disseminate geographic information provided by individuals voluntarily creating their own contents by marking the locations of occurred events or by labeling certain existing features not already shown on a map. The term “neogeography” is often adopted to describe peoples activities when using and creating their own maps, geo-tagging pictures, movies, websites, etc. It could be deﬁned as a new bottom up approach to geography prompted by users, therefore introducing changes in the roles of traditional’ geographers and consumers’ of geographical contents themselves. The volunteered approach has been adopted by important American organizations, such as US Geological Survey, US Census Bureau, etc. While technologies (e.g. GPS, remote sensing, etc.) can be useful in producing new spatial data, volunteered activities are the only way to update and describe such data. If spatial data have been produced in various ways, remote sensing, sensor networks and other electronic devices generate a great ﬂow of relevant spatial information concerning several aspects of human activities or of environmental phenomena monitoring. This ‘information-explosion era’ is characterized by a large amount of information produced both by human activities and by automated systems; the capturing and the manipulation of this information leads to ‘urban computing’ and represents a sort of bridge between computers and the real world, accounting for the social dimension of human environments. This technological evolution produced a new paradigm of urban development, called ‘u-City’. Such phenomena oﬀer new challenges to scholars (geographers, engineers, planners, economists, sociologists, etc.) as well as to spatial planners in addressing spatial issues and a wealth of brand-new, updated data, generally created by people who are interested in geographically related phenomena. As attention is to date dedicated to visualization and content creation, little has been done from the spatial analytical point of view and in involving users as citizens in participatory geographical activities.

XVI

CTP 2011

Conference themes include:
SDI and planning Planning 2.0, participation 2.0 Urban social networks, urban sensing E-democracy, e-participation, participatory GIS Technologies for e-participation, policy modeling, simulation and visualization Second Life and participatory games Ubiquitous computing environment; urban computing; ubiquitous-city Neogeography Collaborative mapping Geotagging Volunteered geographic information Crowdsourcing Ontologies for urban planning City Gml Geo-applications for mobile phones Web 2.0, Web 3.0 Wikinomics, socialnomics WikiCities Maps mash up Tangible maps and planning Augmented reality, Complexity assessment and mapping

Giuseppe Borruso Beniamino Murgante

Message from the Chairs of the Session: 11th Annual International Workshop on “Computational Geometry and Applications”
(CGA 2011)
The 11th International Workshop on Computational Geometry and Applications CGA 2011, held in conjunction with the International Conference on Computational Science and Applications, took place in Santander, Spain. The workshop has run annually since it was founded in 2001, and is intended as an international forum for researchers in computational geometry and related areas, with the goal of advancing the state of research in computational geometry and related disciplines. This year, the workshop was chaired for 11th year by CGA workshop series Founding Chair Marina Gavrilova, University of Calgary, joined by co-Chair Ovidiu Daescu, University of Texas at Dallas. Selected papers from the previous CGA Workshops have appeared in special issues in the following highly regarded journals: International Journal of Computational Geometry and Applications, Springer (three special issues), International Journal of Computational Science and Engineering (IJCSE), Journal of CAD/CAM , Transactions on Computational Sciences, Springer. A special issue comprising best papers presented at CGA 2011 is currently being planned.
The workshop attracts international attention and receives papers presenting high-quality original research in the following tracks:
– Theoretical computational geometry – Applied computational geometry – Optimization and performance issues in geometric algorithms
implementation Workshop topics of interest include: – Design and analysis of geometric algorithms – Geometric algorithms in path planning and robotics – Computational geometry in biometrics – Intelligent geometric computing – Geometric algorithms in computer graphics and computer vision – Voronoi diagrams and their generalizations – 3D Geometric modeling – Geometric algorithms in geographical information systems – Algebraic geometry – Discrete and combinatorial geometry – Implementation issues and numerical precision – Applications in computational biology, physics, chemistry, geography,
medicine, education – Visualization of geometric algorithms

XVIII

CGA 2011

CGA 2011 was located in beautiful Santander, Cantabria, Spain. Santander, the capital city of Cantabria, is located on the northern coast of Spain, between Asturias and the Basque Country overlooking the Cantabrian Sea, and is surrounded by beaches. The conference preceded the Spanish Meeting on Computational Geometry, which took place in Madrid, facilitating interested researchers to attend both events. The 14 articles presented in this Springer LNCS proceeding volume represent papers selected fromi a large number of submissions to this year’s workshop. We would like to express our sincere gratitude to the following International Program Committee members who performed their duties diligently and provided constructive feedback for authors to improve on their presentation:

Tetsuo Asano (Japan Advanced Institute of Science and Technology, Japan) Sergei Bereg (University of Texas at Dallas, USA) Karoly Bezdek (University of Calgary, Canada) Ovidiu Daescu (University of Texas at Dallas, USA) Mirela Damian (Villanova University, USA) Tamal Dey (Ohio State University, USA) Marina L. Gavrilova (University of Calgary, Canada) Christopher Gold (University of Glamorgan, UK) Hisamoto Hiyoshi (Gunma University, Japan) Andr´es Iglesias (University of Cantabria, Spain) Anastasia Kurdia (Smith College, USA) Deok-Soo Kim (Hanyang University, Korea) Ivana Kolingerova (Unversity of West Bohemia, Czech Republic) Nikolai Medvedev (Novosibirsk Russian Academy of Science, Russia) Asish Mukhopadhyay (University of Windsor, Canada) Dimitri Plemenos (Universit´e de Limoges, France) Val Pinciu (Southern Connecticut State University, USA) Jon Rokne (University of Calgary, Canada) Carlos Seara (Universitat Politecnica de Catalunya, Spain) Kokichi Sugihara (University of Tokyo, Japan) Vaclav Skala (University of West Bohemia, Czech Republic) Muhammad Sarfraz (KFUPM, Saudi Arabia) Alexei Sourin (Nanyang Technological University, Singapore) Ryuhei Uehara (Japan Advanced Institute of Science and Technology, Japan) Chee Yap (New York University, USA) Kira Vyatkina (Sanct Petersburg State University, Russia)

We also would like to acknowledge the independent referees, ICCSA 2011 organizers, sponsors, volunteers, and Springer for their continuing collaboration and support.

Marina C. Gavrilova Ovidiu Daescu

Message from the Chair of the Session: 3rd International Workshop on “Software Engineering Processes and Applications”
(SEPA 2011)
The Third International Workshop on Software Engineering Processes and Applications (SEPA 2011) covered the latest developments in processes and applications of software engineering. SEPA includes process models, agile development, software engineering practices, requirements, system and design engineering including architectural design, component level design, formal methods, software modeling, testing strategies and tactics, process and product metrics, Web engineering, project management, risk management, and conﬁguration management and all those areas which are related to the processes and any type of software applications. This workshop attracted papers from leading researchers in the ﬁeld of software engineering and its application areas. Seven regular research papers were accepted as follows.
Sanjay Misra, Ibrahim Akman and Ferid Cafer presented a paper on “A Multi-Paradigm Complexity Metric(MCM)” The authors argued that there are not metrics in the literature for multi-paradigm. MCM is developed by using function points and procedural and object–oriented language’s features. In this view, MCM involves most of the factors which are responsible for the complexity of any multi-paradigm language. MCM can be used for most programming paradigms, including both procedural and object–oriented languages.
Mohamed A. El-Zawawy’s paper entitled ‘Flow Sensitive-Insensitive Pointer Analysis Based Memory Safety for Multithreaded Programs’ presented approaches for the pointer analysis and memory safety of multithreaded programs as simply structured type systems. The author explained that in order to balance accuracy and scalability, the proposed type system for pointer analysis of multithreaded programs is ﬂow-sensitive, which invokes another ﬂow-insensitive type system for parallel constructs.
Cesar Pardo, Francisco Pino, Felix Garcia, Francisco Romero, Mario Piattini, and Maria Teresa Baldassarre presented their paper entitled ‘HProcessTOOL: A Support Tool in the Harmonization of Multiple Reference Models’. The authors have developed the tool HProcessTOOL, which guides harmonization projects by supporting speciﬁc techniques, and supports their management by controlling and monitoring the resulting harmonization projects. The validation of the tool is performed by two case studies.
Wasi Haider Butt, Sameera Amjad and Farooque Azam presented a paper on ‘Requirement Conﬂicts Resolution: Using Requirement Filtering and Analysis’. The authors presented a systematic approach toward resolving software requirements spanning from requirement elicitation to the requirement analysis

XX

SEPA 2011

activity of the requirement engineering process. The authors developed a model ‘conﬂict resolution strategy’ (CRS) which employs a requirement ﬁlter and an analysis strategy for resolving any conﬂict arising during software development. They also implemented their model on a real project.
Rajesh Prasad, Suneeta Agarwal, Anuj Kumar Sharma, Alok Singh and Sanjay Misra presented a paper on ‘Eﬃcient Algorithm for Detecting Parameterized Multiple Clones in a Large Software System’. In this paper the authors have tried to solve the word length problem in a bit-parallel parameterized matching by extending the BLIM algorithm of exact string matching. The authors further argued that the extended algorithm is also suitable for searching multiple patterns simultaneously. The authors presented a comparison in support of their algorithm.
Takahiro Uchiya and Tetsuo Kinoshita presented the paper entitled ‘Behavior Analyzer for Developing Multiagent Systems on Repository-Based Multiagent Framework’. In this paper the authors proposed an interactive design environment of agent system (IDEA) founded on an agent-repository-based multiagent framework. They focused on the function of the behavior analyzer for developing multiagent systems and showed the eﬀectiveness of the function.
Jose Alfonso Aguilar, Irene Garrigos, and Jose-Norberto Mazon presented a paper on ‘Impact Analysis of Goal-Oriented Requirements in Web Engineering’. This paper argues that Web developers need to know dependencies among requirements to ensure that Web applications ﬁnally satisfy the audience. The authors developed an algorithm to deal with dependencies among functional and non-functional requirements so as to understand the impact of making changes when developing a Web application.

Sanjay Misra

Message from the Chair of the Session: 2nd International Workshop on “Software Quality” (SQ 2011)
Following the success of SQ 2009, the Second International Workshop on “Software Quality” (SQ 2011) was organized in conjunction with ICCSA 2011. This workshop extends the discussion on software quality issues in the modern software development processes. It covers all the aspects of process and product quality, quality assurance and standards, quality planning, quality control and software quality challenges. It also covers the frontier issues and trends for achieving the quality objectives. In fact this workshop covers all areas, that are concerned with the quality issue of software product and process. In this workshop, we featured nine articles devoted to diﬀerent aspects of software quality.
Roberto Espinosa, Jose Zubcoﬀ, and Jose-Norberto Mazon’s paper entitled “A Set of Experiments to Consider Data Quality Criteria in Classiﬁcation Techniques for Data Mining” analyzed data–mining techniques to know the behavior of diﬀerent data quality criteria from the sources. The authors have conducted a set of experiments to assess three data quality criteria: completeness, correlation and balance of data.
In their paper, Ivaylo Spassov, Valentin Pavlov, Dessislava Petrova-Antonova, and Sylvia Ilieva’s have developed a tool “DDAT: Data Dependency Analysis Tool for Web Service Business Processes”. The authors have implemented and shown experimental results from the execution of the DDAT over BPEL processes.
Filip Radulovic and Raul Garca-Castro presented a paper on “Towards a Quality Model for Semantic Technologies”. The authors presented some wellknown software quality models, after which a quality model for semantic technologies is designed by extending the ISO 9126 quality model.
Luis Fernandez-Sanz and Sanjay Misra authored the paper “Inﬂuence of Human Factors in Software Quality and Productivity”. The authors ﬁrst analyzed the existing contributions in the area and then presented empirical data from speciﬁc initiatives to know more about real practices and situations in software organizations.
Eudisley Anjos, and Mario Zenha-Rela presented a paper on “A Framework for Classifying and Comparing Software Architecture Tools for Quality Evaluation”. This framework identiﬁes the most relevant features for categorizing diﬀerent architecture evaluation tools according to six diﬀerent dimensions. The authors reported that the attributes that a comprehensive tool should support include: the ability to handle multiple modeling approaches, integration with the industry standard UML or speciﬁc ADL, support for trade–oﬀ analysis of

XXII

SQ 2011

competing quality attributes and the reuse of knowledge through the build-up of new architectural patterns.
Hendrik Decker presented a paper on “Causes of the Violation of Integrity Constraints for Supporting the Quality of Databases”. He presented a quality metric with the potential of more accuracy by measuring the causes. He further argued that such measures also serve for controlling quality impairment across updates.
Csaba Nagy, Laszlo Vidacs , Rudolf Ferenc, Tibor Gyimothy Ferenc Kocsis, and Istvan Kovacs’s presented a paper on “Complexity measures in a 4GL environment”. The authors discussed the challenges in adopting the metrics from 3GL environments. Based on this, they presented a complexity measure in 4GL environments. They performed the experimentations and demonstrated the results.
Lukasz Radlinski’s paper on “A Framework for Integrated Software Quality Prediction Using Bayesian Nets” developed a framework for integrated software quality prediction. His framework is developed and formulated using a Bayesian net, a technique that has already been used in various software engineering studies. The author argues that his model may be used in decision support for software analysts and managers.
Seunghun Park, Sangyoon Min, and Doohwan Bae authored the paper entitled “Process Instance Management Facilities Based on the Meta-Process Models”. Based on the metar-process models, the authors proposed a process model and two types of process instance models: the structural instance model and the behavioral instance model. The authors’ approach enables a project manager to analyze structural and behavioral properties of a process instance and allows a project manager to make use of the formalism for management facilities without knowledge of the formalism.

Sanjay Misra

Message from the Chairs of the Session: “Remote sensing Data Analysis, Modeling, Interpretation and Applications: From a Global
View to a Local Analysis” (RS 2011)
Remotely sensed data provide temporal and spatial consistent measurements useful for deriving information on the dynamic nature of Earth surface processes (sea, ice, land, atmosphere), detecting and identifying land changes, discovering cultural resources, studying the dynamics of urban expansions. Thanks to the establishment and maintenance of long-term observation programs, presently a huge amount of multiscale and multifrequency remotely sensed data are available.
To fully exploit such data source for various ﬁelds of application (environmental, cultural heritage, urban analysis, disaster management) eﬀective and reliable data processing, modeling and interpretation are required. This session brought together scientists and managers from the ﬁelds of remote sensing, ICT, geospatial analysis and modeling, to share information on the latest advances in remote sensing data analysis, product development, validation and data assimilation.
Main topics included:
Remotely sensed data – Multispectral satellite : from medium to very high spatial resolution; airborne and spaceborne Hypespectral data; open data source (Modis, Vegetation, etc..); airborne Laser Scanning; airborne and spaceborne Radar imaging; thermal imaging; declassiﬁed Intelligence Satellite Photographs (Corona, KVR); ground remote sensing Methods and procedures – change detection; classiﬁcation Data fusion / Data integration; data mining; geostatistics and Spatial statistics; image processing; image interpretation; linear and on linear statistical analysis; segmentation Pattern recognition and edge detection; time space modeling Fields of application and products – archaeological site discovery; cultural Heritage management; disaster management; environmental sciences; mapping Landscape and digital elevation models; land cover analysis; open source softwares; palaeoenvironmental studies; time series
Nicola Masini Rosa Lasaponara

Message from the Chairs of the Session: “Approximation, Optimization and Applications” (AOA 2011)
The objective of the session Approximation, Optimization and Applications during the 11th International Conference on Computational Science and Its Applications was to bring together scientists working in the areas of Approximation Theory and Numerical Optimization, including their applications in science and engineering.
Hypercomplex function theory, renamed Cliﬀord analysis in the 1980s, studies functions with values in a non-commutative Cliﬀord algebra. It has its roots in quaternionic analysis, developed as another generalization of the classic theory of functions of one complex variable compared with the theory of functions of several complex variables. It is well known that the use of quaternions and their applications in sciences and engineering is increasing, due to their advantages for fast calculations in 3D and for modeling mathematical problems. In particular, quasi-conformal 3D-mappings can be realized by regular (monogenic) quaternionic functions. In recent years the generalization of classical polynomials of a real or complex variable by using hypercomplex function theoretic tools has been the focus of increased attention leading to new and interesting problems. All these aspects led to the emergence of new software tools in the context of quaternionic or, more generally, Cliﬀord analysis.
Irene Falc˜ao Ana Maria A.C. Rocha

Message from the Chair of the Session: “Symbolic Computing for Dynamic Geometry”
(SCDG 2011)
The papers comprising in the Symbolic Computing for Dynamic Geometry technical session correspond to talks delivered at the conference. After the evaluation process, six papers were accepted for oral presentation, according to the recommendations of the reviewers. Two papers, “Equal bisectors at a vertex of a triangle” and “On Equivalence of Conditions for a Quadrilateral to Be Cyclica”, study geometric problem by means of symbolic approaches.
Another contributions deal with teaching (“Teaching geometry with TutorMates” and “Using Free Open Source Software for Intelligent Geometric Computing”), while the remaining ones propose a framework for the symbolic treatment of dynamic geometry (“On the Parametric Representation of Dynamic Geometry Constructions”) and a formal library for plane geometry (“A Coq-based Library for Interactive and Automated Theorem Proving in Plane Geometry”).
Francisco Botana

Message from the Chairs of the Session: “Computational Design for Technology Enhanced Learning” (CD4TEL 2011)
Providing computational design support for orchestration of activities, roles, resources, and systems in technology-enhanced learning (TEL) is a complex task. It requires integrated thinking and interweaving of state-of-the-art knowledge in computer science, human–computer interaction, pedagogy, instructional design and curricular subject domains. Consequently, even where examples of successful practice or even standards and speciﬁcations like IMS learning design exist, it is often hard to apply and (re)use these eﬃciently and systematically. This interdisciplinary technical session brought together practitioners and researchers from diverse backgrounds such as computer science, education, and cognitive sciences to share their proposals and ﬁndings related to the computational design of activities, resources and systems for TEL applications.
The call for papers attracted 16 high-quality submissions. Each submission was reviewed by three experts. Eventually, ﬁve papers were accepted for presentation. These contributions demonstrate diﬀerent perspectives of research in the CD4TEL area, dealing with standardization in the design of game-based learning; the integration of individual and collaborative electronic portfolios; the provision of an editing environment for diﬀerent actors designing professional training; a simpliﬁed graphical notation for modeling the ﬂow of activities in IMS learning design units of learning; and a pattern ontology-based model to support the selection of good-practice scripts for designing computer–supported collaborative learning.
Michael Derntl Manuel Caeiro-Rodr´ıguez
Davinia Hern´andez-Leo

Message from the Chair of the Session: “Chemistry and Materials Sciences and
Technologies” (CMST 2011)
The CMST workshop is a typical example of how chemistry and computer science beneﬁt from mutual interaction when operating within a grid e-science environment. The scientiﬁc contributions to the workshop, in fact, contain clear examples of chemical problems solved by exploiting the extra power oﬀered by the grid infrastructure to the computational needs of molecular scientists when trying to push ahead the frontier of research and innovation.
Ideal examples of this are the papers on the coulomb potential decomposition in the multiconﬁguration time–dependent Hartree method, on the extension of the grid–empowered simulator GEMS to the a priori evaluation of the crossed beam measurements and on the evaluation of the concentration of pollutants when using a box model version of the Community Multiscale Air Quality Modeling System 4.7. Another example of such progress in computational molecular science is oﬀered by the paper illustrating the utilization of a fault–tolerant workﬂow for the DL-POLY package for molecular dynamics studies.
At the same time molecular science studies are an excellent opportunity for investigating the use of new (single or clustered) GPU chips as in the case of the papers related to their use for computationally demanding quantum calculations of atom diatom reactive scattering. In addition, of particular interest are the eﬀorts spent to develop tools for evaluating user and service quality to the end of promoting collaborative work within virtual organizations and research communities through the awarding and the redeeming of credits.
Antonio Lagana`

Message from the Chairs of the Session: “Cloud for High Performance Computing”
(C4HPC 2011)
On behalf of the Program Committee, it is a pleasure for us to introduce the proceedings of this First International Workshop on Cloud for High–Performance Computing held in Santander (Spain) in 2011 during the 11th International Conference on Computational Science and Its Applications. The conference joined high quality researchers around the world to present the latest results in the usage of cloud computing for high–performance computing.
High–performance computing, or HPC, is a great tool for the advancement of science, technology and industry. It intensively uses computing resources, both CPU and storage, to solve technical or scientiﬁc problems in minimum time. It also uses the most advanced techniques to achieve this objective and evolves along with computing technology as fast as possible. During the last few years we have seen the introduction of new hardware isuch as multi-core and GPU representing a formidable challenge for the scientiﬁc and technical developers that need time to absorb these additional characteristics. At the same time, scientists and technicians have learnt to make faster and more accurate measurements, accumulating a large set of data which need more processing capacity. While these new paradigms were entering the ﬁeld of HPC, virtualization was suddenly introduced in the market, generating a new model for provisioning computing capacity: the cloud. Although conceptually the cloud is not completely new, because it follows the old dream of computing as a utility, it has introduced new characteristics such as elasticity, but at the cost of losing some performance.
Consequently, HPC has a new challenge: how to tackle or solve this reduction in performance while adapting methods to the elasticity of the new platform. The initial results show the feasibility of using cloud infrastructures to execute HPC applications. However, there is also some consensus that the cloud is not the solution for grand challenges, which will still require dedicated supercomputers. Although recently a cluster of more than 4000 CPUs has been deployed, there are still many technical barriers to allow technicians to use it frequently. This is the reason for this workshop which we had the pleasure of introducing.
This First International Workshop on Cloud for High–Performance Computing was an original idea of Osvaldo Gervasi. We were working on the proposal of a COST action devoted to the cloud for HPC which would link the main researchers in Europe. He realized that the technical challenges HPC has to solve in the next few years to use the Cloud eﬃciently, need the collaboration of as many scientists and technicians as possible as well as to rethink the way the applications are executed.

XXXIV C4HPC 2011
This ﬁrst workshop, which deserves in the next ICCSA conferences, joined together experts in the ﬁeld that presented high quality research results in the area. They include the ﬁrst approximations of topology methods such as cellular data system to cloud to be used to process data. Data are also the main issue for the TimeCloud front end, an interface for time series analysis based on Hadop and Hbase, designed to work with massive datasets. In fact, cloud can generate such a large amount of data when accurate information about its infrastructure and executing applications is needed. This is the topic of the third paper which introduce LISA algorithm to tackle the problem of information retrieval in cloud environment where the methods must adapt to the elasticity, scalability and possibility of failure. In fact, to understand Cloud infrastructures, researchers and technicians will need these series of data as well as the usage of tools that allow better knowledge to be gained. In this sense, iCanCloud, a novel simulator of cloud infrastructures, is introduced presenting its results for the most used and cited service: Amazon.
We strongly believe that the reader will enjoy the selected papers, which represent only a minimal, but important, part of the eﬀervescent activity in Cloud for HPC. This selection was only possible thanks to the members of the Program Committee, all of them supporting actively the initiative. We appreciate their commitment to the workshop. Also, we want to thank all of the reviewers who kindly participated in the review of the papers and, ﬁnally, to all the scientists who submitted a paper, even if it was not accepted. We hope that they will have the opportunity to join us in the next editions.
Andr´es Gomez Osvaldo Gervasi

ICCSA 2011 Invited Speakers
Ajith Abraham Machine Intelligence Research Labs, USA
Marina L. Gavrilova University of Calgary, Canada
Yee Leung The Chinese University of Hong Kong, China

Evolving Future Information Systems: Challenges, Perspectives and Applications
Ajith Abraham
Machine Intelligence Research Labs, USA ajith.abraham@ieee.org
Abstract
We are blessed with the sophisticated technological artifacts that are enriching our daily lives and society. It is believed that the future Internet is going to provide us with the framework to integrate, control or operate virtually any device, appliance, monitoring systems, infrastructures etc. The challenge is to design intelligent machines and networks that could communicate and adapt according to the environment. In this talk, we ﬁrst present the concept of a digital ecosystem and various research challenges from several application perspectives. Finally, we present some real–world applications.
Biography
Ajith Abraham received a PhD degree in Computer Science from Monash University, Melbourne, Australia. He is currently the Director of Machine Intelligence Research Labs (MIR Labs), Scientiﬁc Network for Innovation and Research Excellence, USA, which has members from more than 75 countries. He serves/has served the editorial board of over 50 international journals and has also guest edited 40 special issues on various topics. He has authored/co-authored more than 700 publications, and some of the works have also won best paper awards at international conferences. His research and development experience includes more than 20 years in industry and academia. He works in a multidisciplinary environment involving machine intelligence, network security, various aspects of networks, e-commerce, Web intelligence, Web services, computational grids, data mining, and their applications to various real-world problems. He has given more than 50 plenary lectures and conference tutorials in these areas.
Dr. Abraham is the Chair of IEEE Systems Man and Cybernetics Society Technical Committee on Soft Computing. He is a Senior Member of the IEEE, the IEEE Computer Society, the Institution of Engineering and Technology (UK) and the Institution of Engineers Australia (Australia). He is actively involved in the Hybrid Intelligent Systems (HIS), Intelligent Systems Design and Applications (ISDA), Information Assurance and Security (IAS), and Next–Generation Web Services Practices (NWeSP) series of international conferences, in addition to other conferences. More information can be found at: http://www.softcomputing.net.

Recent Advances and Trends in Biometric
Marina L. Gavrilova
Department of Computer Science, University of Calgary marina@cpsc.ucalgary.ca
Extended Abstract
The area of biometric, without a doubt, is one of the most dynamic areas of interest, which recently has displayed a gamut of broader links to other ﬁelds of sciences. Among those are visualization, robotics, multi-dimensional data analysis, artiﬁcial intelligence, computational geometry, computer graphics, e-learning, data fusion and data synthesis. The theme of this keynote is reviewing the state of the art in multi-modal data fusion, fuzzy logic and neural networks and its recent connections to advanced biometric research.
Over the past decade, multimodal biometric systems emerged as a feasible and practical solution to counterweight the numerous disadvantages of single biometric systems. Active research intoi the design of a multimodal biometric system has started, mainly centered around: types of biometrics, types of data acquisition and decision-making processes. Many challenges originating from non-uniformity of biometric sources and biometric acquisition devices result in signiﬁcant diﬀerences on which information is extracted, how is it correlated, the degree of allowable error, cost implications, ease of data manipulation and management, and also reliability of the decisions being made. With the additional demand of computational power and compact storage, more emphasis is shifted toward database design and computational algorithms.
One of the actively researched areas in multimodal biometric systems is information fusion. Which information needs to be fused and what level is needed to obtain the maximum recognition performance is the main focus of current research. In this talk I concentrate on an overview of the current trends in recent multimodal biometric fusion research and illustrate in detail one fusion strategy: rank level fusion. From the discussion, it is seen that rank level fusion often outperforms other methods, especially combined with powerful decision models such as Markov chain or fuzzy logic.
Another aspect of multi-modal biometric system development based on neural networks is discussed further. Neural networks have the capacity to simulate learning processes of a human brain and to analyze and compare complex patters, which can originate from either single or multiple biometric sources, with amazing precision. Speed and complexity have been the downsides of neural networks, however, recent advancements in the area, especially in chaotic neural networks, allow these barriers to be overcome.

XL

Recent Advances and Trends in Biometric

The ﬁnal part of the presentation concentrates on emerging areas utilizing the above developments, such as decision making in visualization, graphics, elearning, navigation, robotics, and security of web-based and virtual worlds. The extent to which biometric advancements have an impact on these emerging areas makes a compelling case for the bright future of this area.

References
1. Ross, A., Nandakumar, K., and Jain, A.K., Handbook of multibiometrics, New York, Springer (2006).
2. Jain, A.K., Ross, A., Prabhakar, S., An introduction to biometric recognition, IEEE Trans. on Circuits and Systems for Video Technology, Special Issue on Image- and Video-Based Biometrics, 14 (1): 420 (2004)
3. Nandakumar, K., Jain, A.K., Ross, A., Fusion in multibiometric identiﬁcation systems: What about the missing data?, in LNCS 5558: 743752, Springer (2009).
4. Monwar, M. M., and Gavrilova, M.L., A multimodal biometric system using rank level fusion approach, IEEE Trans. SMC - B: Cybernetics, 39(4): 867-878 (2009).
5. Monwar, M. M., and Gavrilova, M.L., Secured access control through Markov chain based rank level fusion method, in proc. of 5th Int. Conf. on Computer Vision Theory and Applications (VISAPP), 458-463, Angres, France (2010).
6. Monwar, M. M., and Gavrilova, M.L., FES: A system of combining face, ear and signature biometrics using rank level fusion, in proc. 5th IEEE Int. Conf. IT: New Generations, pp 922-927, (2008).
7. Wang, C., Gavrilova, M.L., Delaunay Triangulation Algorithm for Fingerprint Matching. ISVD’2006. pp.208 216
8. Wecker, L., Samavati, F.F., Gavrilova, M.L., Iris synthesis: a reverse subdivision application. GRAPHITE’2005. pp.121 125
9. Anikeenko, A.V., Gavrilova, M.L., Medvedev, N.N., A Novel Delaunay Simplex Technique for Detection of Crystalline Nuclei in Dense Packings of Spheres. ICCSA (1)’2005. pp.816 826
10. Luchnikov, V.A., Gavrilova, M.L., Medvedev, N.N., Voloshin, V. P., The VoronoiDelaunay approach for the free volume analysis of a packing of balls in a cylindrical container. Future Generation Comp. Syst., 2002: 673 679
11. Frischholz, R., and Dieckmann, U., BioID: A multimodal biometric identiﬁcation system, IEEE Computer, 33 (2): 64-68 (2000).
12. Latiﬁ, S., Solayappan, N. A survey of unimodal biometric methods, in proc. of Int. Conf. on Security & Management, 57-63, Las Vegas, USA (2006).
13. Dunstone, T., and Yager, N., Biometric system and data analysis: Design, evaluation, and data mining. Springer, New York (2009).
14. Ho, T.K., Hull, J.J., and Srihari, S.N., Decision combination in multiple classiﬁer systems, IEEE Trans. on Pattern Analysis and Machine Intelligence, 16 (1): 66-75 (1994)

Recent Advances and Trends in Biometric

XLI

Biography

Marina L. Gavrilova is an Associate Professor in the Department of Computer Science, University of Calgary. Prof. Gavrilova’s research interests lie in the area of computational geometry, image processing, optimization, spatial and biometric modeling. Prof. Gavrilova is founder and co-director of two innovative research laboratories: the Biometric Technologies Laboratory: Modeling and Simulation and the SPARCS Laboratory for Spatial Analysis in Computational Sciences. Prof. Gavrilova publication list includes over 120 journal and conference papers, edited special issues, books and book chapters, including World Scientiﬁc Bestseller of the Month (2007) Image Pattern Recognition: Synthesis and Analysis in Biometric and the Springer book Computational Intelligence: A Geometry-Based Approach. Together with Dr. Kenneth Tan, Prof. Gavrilova founded the ICCSA series of successful international events in 2001. She founded and chaired the International Workshop on Computational Geometry and Applications for over ten years, was co-Chair of the International Workshop on Biometric Technologies BT 2004, Calgary, served as Overall Chair of the Third International Conference on Voronoi Diagrams in Science and Engineering (ISVD) in 2006, was Organizing Chair of WADS 2009 (Banﬀ), and general chair of the International Conference on Cyberworlds CW2011 (October 4-6, Banﬀ, Canada). Prof. Gavrilova is an Editor-in-Chief of the successful LNCS Transactions on Computational Science Journal, Springer-Verlag since 2007 and serves on the Editorial Board of the International Journal of Computational Sciences and Engineering, CAD/CAM Journal and Journal of Biometrics. She has been honored with awards and designations for her achievements and was proﬁled in numerous newspaper and TV interviews, most recently being chosen together with other outstanding Canadian scientists to be featured in the National Museum of Civilization and National Film Canada production.

Theories and Applications of Spatial-Temporal Data Mining and Knowledge Discovery
Yee Leung
The Chinese University of Hong Kong, China yeeleung@cuhk.edu.hk
Abstract
Basic theories of knowledge discovery in spatial and temporal data are examined in this talk. Fundamental issues in the discovery of spatial structures and processes will ﬁrst be discussed. Real-life spatial data mining problems are employed as the background on which concepts, theories and methods are scrutinized. The unraveling of land covers, seismic activities, air pollution episodes, rainfall regimes, epidemics, patterns and concepts hidden in spatial and temporal data are employed as examples to illustrate the theoretical arguments and algorithms performances. To round up the discussion, directions for future research are outlined.
Biography
Yee Leung is currently Professor of Geography and Resource Management at The Chinese University of Hong Kong. He is also the Associate Academic Director of the Institute of Space and Earth Information Science of The Chinese University of Hong Kong. He is adjunct professor of several universities in P.R. China. Professor Leung had also served on public bodies including the Town Planning Board and the Environmental Pollution Advisory Committee of Hong Kong SAR. He is now Chair of The Commission on Modeling Geographical Systems, International Geographical Union, and Chair of The Commission on Quantitative and Computational Geography of The Chinese Geographical Society. He serves on the editorial board of several international journals such as Annals of Association of American Geographers, Geographical Analysis, GeoInformatica, Journal of Geographical Systems, Acta Geographica Sinica, Review of Urban and Regional Development Studies, etc. Professor Leung is also Council member of The Society of Chinese Geographers.
Professor Leung carried out pioneer and inﬂuential research in imprecision/uncertainty analysis in geography, intelligent spatial decision support systems, geocomputation (particularly on fuzzy sets, rough sets, spatial statistics,

XLIV

Theories and Applications

fractal analysis, neural networks and genetic algorithms), and knowledge discovery and data mining. He has obtained more than 30 research grants, authored and co-authored six books and over 160 papers in international journals and book chapters on geography, computer science, and information engineering. His landmark books are: Spatial Analysis and Planning under Imprecision (Elsevier, 1988), Intelligent Spatial Decision Support Systems (Springer-Verlag, 1997), and Knowledge Discovery in Spatial Data (Springer-Verlag, 2010).

Organization

ICCSA 2011 was organized by the University of Cantabria (Spain), Kyushu Sangyo University (Japan), the University of Perugia (Italy), Monash University (Australia) and the University of Basilicata (Italy).

Honorary General Chairs

Antonio Lagana` Norio Shiratori Kenneth C.J. Tan

University of Perugia, Italy Tohoku University, Japan Qontix, UK

General Chairs
Bernady O. Apduhan Andr´es Iglesias

Kyushu Sangyo University, Japan University of Cantabria, Spain

Program Committee Chairs

Osvaldo Gervasi David Taniar

University of Perugia, Italy Monash University, Australia

Local Arrangements Chairs

Andr´es Iglesias Akemi G´alvez Jaime Puig-Pey Angel Cobo Jos´e L. Montan˜a C´esar Otero Marta Zorrilla Ernesto Anabitarte Unal Ufuktepe

University of Cantabria, Spain (Chair) University of Cantabria, Spain University of Cantabria, Spain University of Cantabria, Spain University of Cantabria, Spain University of Cantabria, Spain University of Cantabria, Spain University of Cantabria, Spain Izmir University of Economics, Turkey

Workshop and Session Organizing Chair

Beniamino Murgante

University of Basilicata, Italy

XLVI

Organization

International Liaison Chairs

Jemal Abawajy Marina L. Gavrilova Robert C.H. Hsu Tai-Hoon Kim Takashi Naka

Deakin University, Australia University of Calgary, Canada Chung Hua University,Taiwan Hannam University, Korea Kyushu Sangyo University, Japan

Awards Chairs
Wenny Rahayu Kai Cheng

LaTrobe University, Australia Kyushu Sangyo University, Japan

Workshop Organizers

Approaches or Methods of Security Engineering (AMSE 2011)

Tai-hoon Kim

Hannam University, Korea

Approximation, Optimization and Applications (AOA 2011)

Ana Maria A.C. Rocha Maria Irene Falcao

University of Minho, Portugal University of Minho, Portugal

Advances in Web–Based Learning (AWBL 2011) Mustafa Murat Inceoglu Ege University (Turkey)

Computational Aspects and Methods in Renewable Energies (CAMRE 2011)

Maurizio Carlini Sonia Castellucci Andrea Tucci

University of Tuscia, Italy University of Tuscia, Italy University of Tuscia, Italy

Computer–Aided Modeling, Simulation, and Analysis (CAMSA 2011)

Jie Shen

University of Michigan, USA

Computer Algebra Systems and Applications (CASA 2011)

Andr´es Iglesias Akemi G´alvez

University of Cantabria (Spain) University of Cantabria (Spain)

Organization XLVII

Computational Design for Technology–Enhanced Learning: Methods, Languages, Applications and Tools (CD4TEL 2011)

Michael Derntl Manuel Caeiro-Rodriguez Davinia Hernandez-Leo

University of Vienna, Austria University of Vigo, Spain Universitat Pompeu Fabra, Spain

Computational Geometry and Applications (CGA 2011)

Marina L. Gavrilova

University of Calgary, Canada

Computer Graphics and Virtual Reality (CGVR 2011)

Osvaldo Gervasi Andr´es Iglesias

University of Perugia, Italy University of Cantabria, Spain

Chemistry and Materials Sciences and Technologies (CMST 2011)

Antonio Lagana`

University of Perugia, Italy

Consulting Methodology and Decision Making for Security Systems (CMDMSS 2011)

Sangkyun Kim

Kangwon National University, Korea

Cities, Technologies and Planning (CTP 2011)

Giuseppe Borruso Beniamino Murgante

University of Trieste, Italy University of Basilicata, Italy

Cloud for High–Performance Computing (C4HPC 2011)

Andr´es Gomez Osvaldo Gervasi

CESGA, Santiago de Compostela, Spain University of Perugia, Italy

Future Information System Technologies and Applications (FISTA 2011)

Bernady O. Apduhan Jianhua Ma Qun Jin

Kyushu Sangyo University, Japan Hosei University, Japan Waseda University, Japan

Geographical Analysis, Urban Modeling, Spatial Statistics (GEOG-AN-MOD 2011)

Stefania Bertazzon Giuseppe Borruso Beniamino Murgante

University of Calgary, Canada University of Trieste, Italy University of Basilicata, Italy

XLVIII Organization

International Workshop on Biomathematics, Bioinformatics and Biostatistics (IBBB 2011)

Unal Ufuktepe Andr´es Iglesias

Izmir University of Economics, Turkey University of Cantabria, Spain

International Workshop on Collective Evolutionary Systems (IWCES 2011)

Alfredo Milani Clement Leung

University of Perugia, Italy Hong Kong Baptist University, China

Mobile Communications (MC 2011)

Hyunseung Choo

Sungkyunkwan University, Korea

Mobile Sensor and Its Applications (MSA 2011)

Moonseong Kim

Korean Intellectual Property Oﬃce, Korea

Mobile Systems and Applications (MoSA 2011)

Younseung Ryu Karlis Kaugars

Myongji University, Korea Western Michigan University, USA

Logical, Scientiﬁc and Computational Aspects of Pulse Phenomena in Transitions (PULSES 2011)

Carlo Cattani Cristian Toma Ming Li

University of Salerno, Italy Corner Soft Technologies, Romania East China Normal University, China

Resource Management and Scheduling for Future–Generation Computing Systems (RMS 2011)

Jemal H. Abawajy

Deakin University, Australia

Remote Sensing Data Analysis, Modeling, Interpretation and Applications: From a Global View to a Local Analysis (RS 2011)

Rosa Lasaponara Nicola Masini

IRMMA, CNR, Italy IBAM, CNR, Italy

Symbolic Computing for Dynamic Geometry (SCDG 2011)

Francisco Botana

Vigo University, Spain

Organization

XLIX

Software Engineering Processes and Applications (SEPA 2011)

Sanjay Misra

Atilim University, Turkey

Software Quality (SQ 2011)

Sanjay Misra

Atilim University, Turkey

Tools and Techniques in Software Development Processes (TTSDP 2011)

Sanjay Misra

Atilim University, Turkey

Virtual Reality in Medicine and Surgery (VRMS 2011)

Giovanni Aloisio Lucio T. De Paolis

University of Salento, Italy University of Salento, Italy

Wireless and Ad-Hoc Networking (WADNet 2011)

Jongchan Lee Sangjoon Park

Kunsan National University, Korea Kunsan National University, Korea

WEB 2.0 and Social Networks (Web2.0 2011)

Vidyasagar Potdar

Curtin University of Technology, Australia

Workshop on Internet Communication Security (WICS 2011) Jos`e Maria Sierra Camara University of Madrid, Spain

Wireless Multimedia Sensor Networks (WMSN 2011)

Vidyasagar Potdar Yan Yang

Curtin University of Technology, Australia Seikei University, Japan

Program Committee
Jemal Abawajy Kenneth Adamson Michela Bertolotto Sandro Bimonte Rod Blais Ivan Blecic Giuseppe Borruso Martin Buecker Alfredo Buttari Yves Caniou Carlo Cattani Mete Celik

Deakin University, Australia Ulster University, UK University College Dublin, Ireland CEMAGREF, TSCF, France University of Calgary, Canada University of Sassari, Italy Universit`a degli Studi di Trieste, Italy Aachen University, Germany CNRS-IRIT, France Lyon University, France University of Salerno, Italy Erciyes University, Turkey

L

Organization

Alexander Chemeris
Min Young Chung Rosa Coluzzi Stefano Cozzini Jos`e A. Cardoso e Cunha Alfredo Cuzzocrea Frank D´evai Rodolphe Devillers Pasquale Di Donato Carla Dal Sasso Freitas Prabu Dorairaj Cherry Liu Fang Jos`e-Jesus Fernandez Francesco Gabellone Akemi Galvez Marina Gavrilova Jerome Gensel Andrzej M. Goscinski Shanmugasundaram
Hariharan Hisamoto Hiyoshi Fermin Huarte Andres Iglesias Peter Jimack Qun Jin Farid Karimipour Baris Kazar Ivana Kolingerova Dieter Kranzlmueller Domenico Labbate Antonio Lagana` Rosa Lasaponara Maurizio Lazzari Cheng Siong Lee Sangyoun Lee Jongchan Lee Clement Leung Chendong Li Ming Li Xin Liu Savino Longo Tinghuai Ma
Sergio Maﬃoletti

National Technical University of Ukraine “KPI”, Ukraine
Sungkyunkwan University, Korea National Research Council, Italy National Research Council, Italy Universidade Nova de Lisboa, Portugal University of Calabria, Italy London South Bank University, UK Memorial University of Newfoundland, Canada Sapienza University of Rome, Italy UFRGS, Brazil NetApp, India/USA U.S. DOE Ames Laboratory, USA National Centre for Biotechnology, CSIS, Spain National Research Council, Italy University of Cantabria, Spain University of Calgary, Canada LSR-IMAG, France Deakin University, Australia
B.S. Abdur Rahman University, India Gunma University, Japan University of Barcelona, Spain University of Cantabria, Spain University of Leeds, UK Waseda University, Japan Vienna University of Technology, Austria Oracle Corp., USA University of West Bohemia, Czech Republic LMU & LRZ Munich, Germany University of Basilicata, Italy University of Perugia, Italy National Research Council, Italy National Research Council, Italy Monash University, Australia Yonsei University, Korea Kunsan National University, Korea Hong Kong Baptist University, Hong Kong University of Connecticut, USA East China Normal University, China University of Calgary, Canada University of Bari, Italy NanJing University of Information Science and
Technology, China University of Zurich, Switzerland

Nicola Masini Nirvana Meratnia Alfredo Milani Sanjay Misra Jos`e Luis Montan˜a Beniamino Murgante Jiri Nedoma
Laszlo Neumann Kok-Leong Ong Belen Palop Marcin Paprzycki Eric Pardede Kwangjin Park Vidyasagar Potdar David C. Prosperi Wenny Rahayu Jerzy Respondek Alexey Rodionov
Jon Rokne Octavio Roncero Maytham Safar Haiduke Saraﬁan Qi Shi Dale Shires Carmelo Torre Giuseppe A. Trunﬁo Unal Ufuktepe Mario Valle
Piero Giorgio Verdini Andrea Vittadini Koichi Wada Krzysztof Walkowiak Robert Weibel Roland Wismu¨ller Markus Wolﬀ Mudasser Wyne Chung-Huang Yang Xin-She Yang Salim Zabir Albert Y. Zomaya

Organization

LI

National Research Council, Italy University of Twente, The Netherlands University of Perugia, Italy Atilim University, Turkey University of Cantabria, Spain University of Basilicata, Italy Academy of Sciences of the Czech Republic,
Czech Republic University of Girona, Spain Deakin University, Australia Universidad de Valladolid, Spain Polish Academy of Sciences, Poland La Trobe University, Australia Wonkwang University, Korea Curtin University of Technology, Australia Florida Atlantic University, USA La Trobe University, Australia Silesian University of Technology Poland Institute of Computational Mathematics and
Mathematical Geophysics, Russia University of Calgary, Canada CSIC, Spain Kuwait University, Kuwait The Pennsylvania State University, USA Liverpool John Moores University, UK U.S. Army Research Laboratory, USA Polytechnic of Bari, Italy University of Sassari, Italy Izmir University of Economics, Turkey Swiss National Supercomputing Centre,
Switzerland INFN Pisa and CERN, Italy University of Padova, Italy University of Tsukuba, Japan Wroclaw University of Technology, Poland University of Zurich, Switzerland Universita¨t Siegen, Germany University of Potsdam, Germany National University, USA National Kaohsiung Normal University, Taiwan National Physical Laboratory, UK France Telecom Japan Co., Japan University of Sydney, Australia

LII

Organization

Sponsoring Organizations

ICCSA 2011 would not have been possible without tremendous support of many organizations and institutions, for which all organizers and participants of ICCSA 2011 express their sincere gratitude:
– The Department of Applied Mathematics and Computational Sciences, University of Cantabria, Spain
– The Department of Mathematics, Statistics and Computation, University of Cantabria, Spain
– The Faculty of Sciences, University of Cantabria, Spain – The Vicerrector of Research and Knowledge Transfer, University of Cantabria,
Spain – The University of Cantabria, Spain – The University of Perugia, Italy – Kyushu Sangyo University, Japan – Monash University, Australia – The University of Basilicata, Italy – Cantabria Campus Internacional, Spain – The Municipality of Santander, Spain – The Regional Government of Cantabria, Spain – The Spanish Ministry of Science and Innovation, Spain – GeoConnexion (http://www.geoconnexion.com/) – Vector1 Media (http://www.vector1media.com/)

Table of Contents – Part III
Workshop on Computational Geometry and Applications (CGA 2011)
Optimizing the Layout of Proportional Symbol Maps . . . . . . . . . . . . . . . . . 1 Guilherme Kunigami, Pedro J. de Rezende, Cid C. de Souza, and Tallys Yunes
An Optimal Hidden-Surface Algorithm and Its Parallelization . . . . . . . . . 17 F. D´evai
Construction of Pseudo-triangulation by Incremental Insertion . . . . . . . . . 30 Ivana Kolingerov´a, Jan Trˇcka, and Ladislav Hobza
Non-uniform Geometric Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Christian Knauer, Klaus Kriegel, and Fabian Stehn
Multi-robot Visual Coverage Path Planning: Geometrical Metamorphosis of the Workspace through Raster Graphics Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Jo˜ao Valente, Antonio Barrientos, Jaime del Cerro, Claudio Rossi, Julian Colorado, David Sanz, and Mario Garzo´n
A Practical Solution for Aligning and Simplifying Pairs of Protein Backbones under the Discrete Fr´echet Distance . . . . . . . . . . . . . . . . . . . . . . 74
Tim Wylie, Jun Luo, and Binhai Zhu
k -Enclosing Axis-Parallel Square . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 Priya Ranjan Sinha Mahapatra, Arindam Karmakar, Sandip Das, and Partha P. Goswami
Tree Transformation through Vertex Contraction with Application to Skeletons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
Arseny Smirnov and Kira Vyatkina
Topology Construction for Rural Wireless Mesh Networks – A Geometric Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Sachin Garg and Gaurav Kanade
An Adapted Version of the Bentley-Ottmann Algorithm for Invariants of Plane Curves Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
Ma˘d˘alina Hodorog, Bernard Mourrain, and Josef Schicho
A Heuristic Homotopic Path Simpliﬁcation Algorithm . . . . . . . . . . . . . . . . 132 Shervin Daneshpajouh and Mohammad Ghodsi

LIV

Table of Contents – Part III

An Improved Approximation Algorithm for the Terminal Steiner Tree Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
Yen Hung Chen
Min-Density Stripe Covering and Applications in Sensor Networks . . . . . 152 Adil I. Erzin and Sergey N. Astrakov
Power Diagrams and Intersection Detection . . . . . . . . . . . . . . . . . . . . . . . . . 163 Michal Zemek and Ivana Kolingerov´a

Workshop on Approximation, Optimization and Applications (AOA 2011)
Heuristic Pattern Search for Bound Constrained Minimax Problems . . . . 174 Isabel A.C.P. Esp´ırito Santo and Edite M.G.P. Fernandes
Novel Fish Swarm Heuristics for Bound Constrained Global Optimization Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
Ana Maria A.C. Rocha, Edite M.G.P. Fernandes, and Tiago F.M.C. Martins
Quaternions: A Mathematica Package for Quaternionic Analysis . . . . . . . 200 M.I. Falca˜o and Fernando Miranda
Inﬂuence of Sampling in Radiation Therapy Treatment Design . . . . . . . . . 215 Humberto Rocha, Joana M. Dias, Brigida C. Ferreira, and Maria do Carmo Lopes
On Minimizing Objective and KKT Error in a Filter Line Search Strategy for an Interior Point Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
M. Fernanda P. Costa and Edite M.G.P. Fernandes
Modiﬁed Diﬀerential Evolution Based on Global Competitive Ranking for Engineering Design Optimization Problems . . . . . . . . . . . . . . . . . . . . . . 245
Md. Abul Kalam Azad and Edite M.G.P. Fernandes
Laguerre Polynomials in Several Hypercomplex Variables and Their Matrix Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
H.R. Malonek and G. Tomaz
On Generalized Hypercomplex Laguerre-Type Exponentials and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
I. Cac¸˜ao, M.I. Falca˜o, and H.R. Malonek
Branch-and-Bound Reduction Type Method for Semi-Inﬁnite Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
Ana I. Pereira and Edite M.G.P. Fernandes

Table of Contents – Part III

LV

On Multiparametric Analysis in Generalized Transportation Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
Sanjeet Singh, Pankaj Gupta, and Milan Vlach
On an Hypercomplex Generalization of Gould-Hopper and Related Chebyshev Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
I. Cac¸˜ao and H.R. Malonek
Nonlinear Optimization for Human-Like Movements of a High Degree of Freedom Robotics Arm-Hand System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
Eliana Costa e Silva, Fernanda Costa, Estela Bicho, and Wolfram Erlhagen
Applying an Elitist Electromagnetism-Like Algorithm to Head Robot Stabilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
Miguel Oliveira, Cristina P. Santos, Ana Maria A.C. Rocha, Lino Costa, and Manuel Ferreira
3D Mappings by Generalized Joukowski Transformations . . . . . . . . . . . . . . 358 Carla Cruz, M.I. Falca˜o, and H.R. Malonek

Workshop on Chemistry and Materials Sciences and Technologies (CMST 2011)
Evaluation of SOA Formation Using a Box Model Version of CMAQ and Chamber Experimental Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
Manuel Santiago, Ariel F. Stein, Fantine Ngan, and Marta G. Vivanco
A Fault Tolerant Workﬂow for CPU Demanding Calculations . . . . . . . . . . 387 A. Costantini, O. Gervasi, and A. Lagana`
A Grid Credit System Empowering Virtual Research Communities Sustainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
C. Manuali and A. Lagana`
A Parallel Code for Time Independent Quantum Reactive Scattering on CPU-GPU Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
Ranieri Baraglia, Malko Bravi, Gabriele Capannini, Antonio Lagan`a, and Edoardo Zambonini
Time Dependent Quantum Reactive Scattering on GPU . . . . . . . . . . . . . . 428 Leonardo Paciﬁci, Danilo Nalli, Dimitris Skouteris, and Antonio Lagan`a
Potential Decomposition in the Multiconﬁguration Time-Dependent Hartree Study of the Conﬁned H Atom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
Dimitrios Skouteris and Antonio Lagan`a

LVI

Table of Contents – Part III

An Extension of the Molecular Simulator GEMS to Calculate the Signal of Crossed Beam Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
Antonio Lagan`a, Nadia Balucani, Stefano Crocchianti, Piergiorgio Casavecchia, Ernesto Garcia, and Amaia Saracibar
Federation of Distributed and Collaborative Repositories and Its Application on Science Learning Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
Sergio Tasso, Simonetta Pallottelli, Riccardo Bastianini, and Antonio Lagana

Workshop on Mobile Systems and Applications (MoSA 2011)
HTAF: Hybrid Testing Automation Framework to Leverage Local and Global Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
Keun Soo Yim, David Hreczany, and Ravishankar K. Iyer
Page Coloring Synchronization for Improving Cache Performance in Virtualization Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
Junghoon Kim, Jeehong Kim, Deukhyeon Ahn, and Young Ik Eom
Security Enhancement of Smart Phones for Enterprises by Applying Mobile VPN Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
Young-Ran Hong and Dongsoo Kim
An Eﬃcient Mapping Table Management in NAND Flash-Based Mobile Computers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
Soo-Hyeon Yang and Yeonseung Ryu
Performance Improvement of I/O Subsystems Exploiting the Characteristics of Solid State Drives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
Byeungkeun Ko, Youngjoo Kim, and Taeseok Kim
A Node Placement Heuristic to Encourage Resource Sharing in Mobile Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
Davide Vega, Esunly Medina, Roc Messeguer, Dolors Royo, and Felix Freitag

Session on Cloud for High Performance Computing
Examples of WWW Business Application System Development Using a Numerical Value Identiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
Toshio Kodama, Tosiyasu L. Kunii, and Yoichi Seki
Building a Front End for a Sensor Data Cloud . . . . . . . . . . . . . . . . . . . . . . . 566 Ian Rolewicz, Michele Catasta, Hoyoung Jeung, Zolta´n Mikl´os, and Karl Aberer

Table of Contents – Part III

LVII

Design of a New Cloud Computing Simulation Platform . . . . . . . . . . . . . . 582 A. Nun˜ez, J.L. Va´zquez-Poletti, A.C. Caminero, J. Carretero, and I.M. Llorente

General Tracks
System Structure for Dependable Software Systems . . . . . . . . . . . . . . . . . . 594 Vincenzo De Florio and Chris Blondia
Robust Attributes-Based Authenticated Key Agreement Protocol Using Smart Cards over Home Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608
Xin-Yi Chen and Hyun-Sung Kim
AUTHHOTP - HOTP Based Authentication Scheme over Home Network Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
Hyun Jung Kim and Hyun Sung Kim
FRINGE: A New Approach to the Detection of Overlapping Communities in Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638
Camilo Palazuelos and Marta Zorrilla
Parallel Implementation of the Heisenberg Model Using Monte Carlo on GPGPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
Alessandra M. Campos, Joa˜o Paulo Pec¸anha, Patr´ıcia Pampanelli, Rafael B. de Almeida, Marcelo Lobosco, Marcelo B. Vieira, and S´ocrates de O. Dantas
Lecture Notes in Computer Science: Multiple DNA Sequence Alignment Using Joint Weight Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668
Jian-Jun Shu, Kian Yan Yong, and Weng Kong Chan
Seismic Wave Propagation and Perfectly Matched Layers Using a GFDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 676
Francisco Uren˜a, Juan Jos´e Benito, Eduardo Salete, and Luis Gavete

Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693

Optimizing the Layout of Proportional Symbol Maps
Guilherme Kunigami1, , Pedro J. de Rezende1, , Cid C. de Souza1, , and Tallys Yunes2
1 Institute of Computing, State University of Campinas, Campinas, SP, Brazil 13084-852
kunigami@gmail.com, {rezende,cid}@ic.unicamp.br 2 Department of Management Science, University of Miami
Coral Gables, FL, USA 33124-8237 tallys@miami.edu
Abstract. Proportional symbol maps are a cartographic tool to assist in the visualization and analysis of quantitative data associated with speciﬁc locations (earthquake magnitudes, oil well production, temperature at weather stations, etc.). Symbol sizes are proportional to the magnitude of the quantities that they represent. We present a novel integer programming model to draw opaque disks on a map with the objective of maximizing the total visible border of all disks (an established measure of quality). We focus on drawings obtained by layering symbols on top of each other, known as stacking drawings. We introduce decomposition techniques, and several new families of facet-deﬁning inequalities, which are implemented in a cut-and-branch algorithm. We assess the eﬀectiveness of our approach through a series of computational experiments using real demographic and geophysical data. To the best of our knowledge, we are the ﬁrst to provide provably optimal solutions to some of those problem instances.
Keywords: Computational Geometry, Symbol Maps, Integer Linear Programming, Cartography.
1 Introduction
Proportional symbol maps (PSMs) are a cartographic tool to assist in the visualization and analysis of quantitative data associated with speciﬁc locations (e.g. earthquake magnitudes, oil well production, temperature at weather stations, etc.). At each location, a symbol is drawn whose size is proportional to the numerical data collected at that point on the map (see [1,2]). For our purposes, the
Supported by CNPq – Conselho Nacional de Desenvolvimento Cient´ıﬁco e Tecnol´ogico – Grant #830510/1999-0. Partially supported by CNPq Grants #472504/2007-0, 483177/2009-1, 473867/2010-9, FAPESP – Fundac¸a˜o de Amparo a` Pesquisa do Estado de Sa˜o Paulo – Grant #07/52015-0 and a Grant from FAEPEX/Unicamp. Partially supported by CNPq Grants #301732/2007-8, 472504/2007-0, 473867/2010-9 and FAPESP Grant #07/52015-0.
B. Murgante et al. (Eds.): ICCSA 2011, Part III, LNCS 6784, pp. 1–16, 2011. c Springer-Verlag Berlin Heidelberg 2011

2

G. Kunigami et al.

symbols are scaled opaque disks (typically preferred by users [7]), and we focus on drawings obtained by layering symbols on top of each other, also known as stacking drawings. Because of overlapping, a drawing of the disks on a plane will expose some of them (either completely or partially) and potentially obscure the others. Although there have been studies about symbol sizing, it is unclear how much the symbols on a PSM should overlap (see [5,12]). The quality of a drawing is related to how easily the user is able to correctly judge the relative sizes of the disks. Intuitively, the accuracy of such a judgment is proportional to how much of the disk borders are visible. As a consequence, the objective function consists of maximizing one of two alternative measures of quality: the minimum visible border length of any disk (the Max-Min problem) – which emphasizes the local perception, or the total visible border length over all disks (the Max-Total problem) – which beneﬁts the global awareness. For n disks, Cabello et al. [1] show that the Max-Min problem can be solved in O(n2 log n) in general, or in O(n log n) if no point on the plane is covered by more than O(1) disks. The complexity of the Max-Total problem for stacking drawings is open.
The contributions of this work are: (i) proposing a novel integer linear programming (ILP) formulation for the Max-Total problem; (ii) introducing decomposition techniques, as well as several new families of facet-deﬁning inequalities; and (iii) implementing a cut-and-branch algorithm to assess the eﬀectiveness of our approach through a series of computational experiments on a set of instances that includes real geophysical data from NOAA’s National Geophysical Data Center [11]. To the best of our knowledge, we are the ﬁrst to provide provably optimal solutions to some of the Max-Total instances studied in [1,2]. We are unaware of other attempts at using ILP to solve this problem.
In Section 2, we describe the problem more formally and introduce some basic terminology. We present the ILP model in Section 3, and perform a polyhedral study of the formulation in Section 4. We describe new families of facet-deﬁning inequalities in Section 5, and introduce decomposition techniques in Section 6. The computational results obtained with our cut-and-branch algorithm appear in Section 7.

2 Problem Description and Terminology
Let S = {1, 2, . . . , n} be a set of disks with known radii and center coordinates on the Euclidean plane. Let the arrangement A be deﬁned as the picture formed by the borders of all the disks in S. A point at which two or more disk borders intersect is called a vertex of A. A portion of a disk border that connects two vertices, with no other vertices in between, is called an arc. An area of A that is delimited by arcs is called a face. A drawing of S is a subset of the arcs and vertices of A that is drawn on top of the ﬁlled interiors of the disks in S (see Figure 1).
A canonical face is a face that contains no arcs in its interior. A set of arcs on the boundary of a canonical face that belong to the same disk constitutes a canonical arc. In Figure 2, the boundary of face f is made up of canonical arcs

Optimizing the Layout of Proportional Symbol Maps

3

v rf

r1 fg
r2

r4 r3

Fig. 1. Arrangement with vertex v, arc r, Fig. 2. Three single-piece canonical arcs and face f (left), and a drawing (right) r1, r2, r3; a multi-piece canonical arc r4

r1 and r2. The boundary of face g is made up of three canonical arcs: r2, r3 and r4. Note that canonical arc r4 is composed of two pieces. From now on, arcs and faces are assumed to be canonical, unless noted otherwise.
Given an arrangement, many drawings are possible, but not all of them represent a sensible, physically feasible, placement of symbols. A stacking drawing is obtained by assigning disks to levels (a stacking order) and drawing them, in sequence, from the lowest to the highest level.

3 An Integer Linear Programming Model
Let GS = (V, E) be an undirected graph with one vertex for every disk i ∈ S (denoted V (i)) and one edge for every pair of vertices whose corresponding disks overlap. Moreover, let m − 1 be the length of the longest simple path in GS, and let K be the set of all maximal cliques of GS.
Proposition 1. The Max-Total problem for stacking drawings has an optimal solution that uses at most m levels.
Proof. Assume that a given solution assigns levels to all disks using more than m levels. Create a directed graph GS such that V (GS ) = V (GS ) and arc (i, j) is directed from i to j if disk i is at a level below disk j. Because the given solution is a stacking drawing, GS contains no directed cycles and hence admits a topological ordering of its vertices. Note that this ordering induces the same stacking order as the given solution. Because the length of the longest directed path in GS is at most m − 1, the greatest label used in the topological ordering is less than or equal to m.
Even though it may seem, at ﬁrst glance, that an optimal solution might require at most as many levels as the size of the largest clique in GS, it is easy to see that in the case where GS is a simple path with n > 2 vertices, its largest clique has size 2, while an optimal solution may require n levels.
Our ILP model uses the following data, which can be calculated in polynomial time from the set S:
– R ≡ set of all arcs; – r ≡ length of arc r ∈ R (total length if r has multiple pieces); – dr ≡ disk that contains arc r in its border; – SrI ≡ set of disks that contain arc r in their interior.

4

G. Kunigami et al.

For each r ∈ R, let the binary variable xr be equal to 1 if arc r is visible in the drawing, and equal to 0 otherwise. Then, the objective is to maximize
r∈R rxr. We assume that m ≥ 2 because it is trivial to ﬁnd the optimal solution when m = 1. For each disk i ∈ S, let the binary variable yip be equal to 1 if disk i is at level p (1 ≤ p ≤ m), and equal to 0 otherwise. A stacking
drawing has to satisfy the following constraints:

m
yip ≤ 1,
p=1 m
xr − ydrp ≤ 0,
p=1

yip ≤ 1,

i : V (i)∈K

p

m

ydra + yib + xr ≤ 2,

a=1

b=p

xr ∈ {0, 1},

yip ∈ {0, 1},

∀ i ∈ S, (1)
∀ r ∈ R, (2) ∀ 1 ≤ p ≤ m, K ∈ K, (3)
∀ r ∈ R, i ∈ SrI , 1 ≤ p ≤ m, (4) ∀ r ∈ R, (5)
∀ i ∈ S, 1 ≤ p ≤ m. (6)

We refer to the convex hull of feasible integer solutions to (1)–(6) as P . Constraint (1) states that each disk is assigned to at most one level. Constraint (2) states that a disk with a visible arc must be assigned to a level, and (3) says that overlapping disks can not be at the same level. Constraint (4) ensures that arc r is only visible if dr is above all other disks that contain r.

4 Polyhedral Study of P
In this section, we obtain the dimension of P and determine which inequalities in the original formulation (1)–(6) deﬁne facets. For the sake of brevity, we omit the proofs of Propositions 2 to 5, which are based on the direct method, that is, they essentially enumerate aﬃnely independent points belonging to a given polytope to establish its dimension. For those proofs, see [8]. We include here, however, the proofs that inequalities are facet-deﬁning whenever they employ the indirect method. Both direct and indirect methods are discussed in Theorem 3.6, Part I.4 of [10].
Proposition 2. The dimension of P is nm + |R|.
Proposition 3. Given an arc r ∈ R, the inequality xr ≥ 0 deﬁnes a facet of P , whereas the inequality xr ≤ 1 does not.
The inequality xr ≤ 1 is not facet-deﬁning for P because it is implied by the combination of (1) and (2).

Optimizing the Layout of Proportional Symbol Maps

5

Proposition 4. Given a disk i ∈ S, and a level 1 ≤ p ≤ m, the inequality yip ≥ 0 deﬁnes a facet of P , whereas the inequality yip ≤ 1 does not.
The inequality yip ≤ 1 does not deﬁne a facet of P because it is implied by (1).
Proposition 5. Given a disk i ∈ S, (1) deﬁnes a facet of P .
Proposition 6. Given an arc r ∈ R, (2) deﬁnes a facet of P .
Proof. We use the indirect method. Let x = (y, x) and let πx ≤ π0 be a valid inequality for P whose induced face contains the face F induced by (2). We will show that πx ≤ π0 is a scalar multiple of (2). Because the origin is a feasible solution that satisﬁes (2) as an equality, we have that π0 = 0. Let 1 ≤ p ≤ m and xrp satisfy ydrp = xr = 1, with all other variables equal to zero. It is easy to see that xrp is feasible and satisﬁes (2) as an equality. Then,

πxrp = πdrp + πr = π0 = 0 ,

(7)

where πdrp is the component of vector π that multiplies variable ydrp in xrp, and πr is the component that multiplies xr. Therefore, πdrp = −πr. By varying the value of p, (7) implies that

πdr1 = πdr2 = · · · = πdrm = −πr = αr .

(8)

To complete the proof, we need to show that all remaining components of π are
equal to zero.
Let r ∈ R \ {r} with dr = dr. Consider the vector x = xrp + enm+r , whose
components are all zero except ydrp, xr and xr which have value one. Clearly, x is feasible and belongs to F . Therefore, we have πr = 0. From now on, let us assume that dr = dr. For any p ∈ {1, . . . , m}, by setting ydr p = 1 and all other variables equal to zero, we obtain a feasible vector x that lies on F . As a
consequence, πx = π0, implying that πdr p = 0 for all r = r and all p. Similarly, choosing x such that ydr p = xr = 1 with all the remaining components set to zero, we generate a feasible point in F which yields πr = 0 for all r = r.

Proposition 7. Given 1 ≤ p ≤ m and K ∈ K, (3) deﬁnes a facet of P .

Proof. We use the indirect method. Let x = (y, x) and let πx ≤ π0 be a valid inequality for P whose induced face F contains the face of P induced by (3). We will show that πx ≤ π0 is a scalar multiple of (3). In this proof, the components of vector π are identiﬁed as in Proposition 6.
First let us partition the variables into ﬁve classes: (i) yjp with V (j) ∈ K; (ii) yjq with V (j) ∈ K, and q = p; (iii) yjq with V (j) ∈/ K; (iv) xr with V (dr) ∈ K; and (v) xr with V (dr) ∈/ K. We now exhibit feasible points that satisfy (3) as an equality to determine the values of the coeﬃcients of vector π for each class
of variables deﬁned above. For each choice of x given below, undeﬁned variables are assumed to be equal to zero. (i) Let x have yip = 1. Then, πx = πip = π0; (ii) Let i ∈ S be such that V (i) ∈ K, and let x have yjq = yip = 1. Then, πx = πjq + πip = π0, which implies πjq = 0 because of (i); (iii) There exists

6

G. Kunigami et al.

i ∈ S with V (i) ∈ K such that V (j) is not adjacent to V (i) (otherwise, V (j) would be a vertex of K). For each 1 ≤ q ≤ m, let x have yjq = yip = 1. Then, as in (ii), πjq = 0; (iv) If x satisﬁes ydrp = xr = 1, we have πx = πdrp + πr = π0, which implies πr = 0; (v) As in (iii), we can ﬁnd an i ∈ S with V (i) ∈ K such that V (dr) is not adjacent to V (i). Let x have ydr1 = yip = xr = 1. Then, πx = πdr1 + πip + πr = π0, which implies πr = 0.
Proposition 8. Given an arc r ∈ R, i ∈ SrI and 1 ≤ p ≤ m, (4) does not deﬁne a facet of P , but (9) does if 1 ≤ p < m.

p

m

m

ydra + yib + xr ≤ 1 + ydra

(9)

a=1

b=p

a=1

Proof. We ﬁrst show that inequality (4) does not deﬁne a facet of P . To this end,

let F denote the face deﬁned by (4) in P . Now, we claim that all feasible points

in F satisfy inequality (1) at equality for i = dr (otherwise dr is not assigned to

a level, xr is zero because of (2), and the left-hand side of (4) is at most one).

Since the P is full-dimensional, F cannot be a facet of it.

Notice that, by deﬁning the binary variable z =

m a=1

ydr

a

and

lifting

this

variable in (4), we obtain inequality (9). We now prove that the latter inequality

is facet deﬁning for P under the assumptions made in the proposition.

Initially, we observe that (9) is not facet-deﬁning for P when p = m because

it is clearly dominated by (11) or (14), depending on what kind of arc r is.

Moreover, for convenience, we rewrite (9) as:

m

m

yib −

ydra + xr ≤ 1 .

b=p

a=p+1

(10)

We use the indirect method. Let x = (y, x) and let πx ≤ π0 be a valid inequality for P whose induced face F contains the face of P induced by (10). We
will show that πx ≤ π0 is a scalar multiple of (10). In this proof, the components of vector π are identiﬁed as in Proposition 6. We partition the variables into ten
classes and establish the appropriate corresponding coeﬃcients in vector π. For
each choice of x given below, undeﬁned variables are assumed to be equal to zero
and the vector is easily shown to be feasible and to lie on F . (i) yil for p ≤ l ≤ m: Let x have yil = 1. Then, πx = πil = π0. (ii) yjm for all j ∈ S \ {dr, i}: Let x have yi(m−1) = yjm = 1. Then, πx = πi(m−1) + πjm = π0 which, from the previous result, implies that πjm = 0. (iii) yjl for all j ∈ S \ {dr, i} and 1 ≤ l ≤ m − 1: Let x have yim = yjl = 1. Then, πx = πim + πjl = π0 which, from (i), implies that πjl = 0. (iv) ydrl for 1 ≤ l ≤ p: Let x have yim = ydrl = 1. Then, πx = πim + πdrl = π0 which, from (i), implies that πdrl = 0. (v) xr: Let x have ydrp = xr = 1. Then, πx = πdrp + πr = π0 which, from (iv), implies that πr = π0. (vi) yil for 1 ≤ l ≤ p − 1: Let x have ydrp = xr = yil = 1. Then, πx = πdrp + πr + πil = π0 which, from (iv) and (v), implies that πil = 0. (vii) xq for all j ∈ S \ {dr, i} and all arcs q of disk j: Let

Optimizing the Layout of Proportional Symbol Maps

7

x have yi(m−1) = yjm = xq = 1. Then, πx = πi(m−1) + πjm + πq = π0 which, from (i) and (ii), implies that πq = 0. (viii) xq for all arcs q of disk i: Let x have
yim = xq = 1. Then, πx = πim + πq = π0 which, from (i), implies that πq = 0.
(ix) xq for all arcs q of disk dr except arc r: Let x have ydrp = xr = xq = 1. Then, πx = πdrp + πr + πq = π0 which, from (iv) and (v), implies that πq = 0. (x) ydrl for p+1 ≤ l ≤ m: Let x have yip = ydrl = xr = 1. Then, πx = πip +πdrl+πr = π0 which, from (i) and (ix), implies that πdrl = −π0.

5 Strengthening the ILP Formulation
The geometric nature of PSMs enables us to obtain new valid inequalities by observing that certain groups of arcs cannot be visible simultaneously due to a physical impossibility. In the sequel, A is an arrangement of disks on a plane. We use the following additional data sets:
– Df ≡ set of disks that contain face f . – Bf ≡ set of arcs that form the boundary of face f . Bf+ = {r ∈ Bf | dr ∈ Df }
and Bf− = Bf \ Bf+. – If ≡ set of disks whose borders contain an arc in Bf . – Cf ≡ set of disks that contain face f in their interior (Cf = Df \ If ).
Consider the arrangement in Figure 2. The boundary of face g is formed by arcs r2, r3, and r4. We have Bg = {r2, r3, r4}, Dg = {dr4 }, Bg+ = {r4}, Bg− = {r2, r3}, Ig = {dr2, dr3, dr4 }, and Cg = ∅. In the arrangement of Figure 3, the boundary of face f is formed by arcs r1, r2, and r3. Therefore, we have Bf = Bf+ = {r1, r2, r3}, Df = {dr1 , dr2 , dr3 , dr4 }, If = {dr1 , dr2 , dr3 }, and Cf = {dr4 }. If one of the arcs in Bf is visible in a drawing, the other two cannot appear. Moreover, if dr4 is assigned to the topmost level, f will not appear. This leads to the valid inequality ydr4 m + xr1 + xr2 + xr3 ≤ 1. In general, we have the following result:

r4 r1 r2 f r3

Fig. 3. Arcs r1, r2, and r3 of face f cannot be visible simultaneously

Proposition 9. Let f be a face of A with |Bf+| ≥ 1. If |Cf | ≥ 1 or |Bf+| ≥ 2, then (11) deﬁnes a facet of P .

yim +

xr ≤ 1

i∈Cf

r∈Bf+

(11)

8

G. Kunigami et al.

Proof. To prove validity, note that for every arc r ∈ Bf+, all the arcs in Bf+ \ {r} are in the interior of dr. Therefore, if r is visible, no other arc of Bf+ \ {r} can be visible, which implies r∈Bf+ xr ≤ 1. Moreover, if a disk in Cf is at the top level (m), we must have r∈Bf+ xr = 0, so it suﬃces to show that i∈Cf yim ≤ 1. Because all the disks in Cf contain f , the corresponding vertices in GS form a
clique. Hence, at most one of those disks can be assigned to level m because of (3). If |Bf+| = 0, (11) is dominated by (3). If |Cf | = 0 and |Bf+| = 1, (11) reduces to xr ≤ 1, which is not facet-deﬁning due to Proposition 3.
To prove that (11) is facet-deﬁning for P under the assumptions stated above,
we use the indirect method. Let x = (y, x) and let πx ≤ π0 be a valid inequality
for P whose induced face contains the face F induced by (11). We will show that
πx ≤ π0 is a scalar multiple of (11). As usual, this is done by exhibiting several
vectors that can be easily shown to be feasible and lying on F . Moreover, the
components of vector π are also identiﬁed as in Proposition 6. Let r ∈ Bf+, 1 ≤ p ≤ m, and let xrp satisfy ydrp = xr = 1, with all other
variables equal to zero. Clearly, xrp satisﬁes (11) as an equality, xrp ∈ P , and

πxrp = πdrp + πr = π0 .

(12)

By varying the value of p, (12) implies that, for any r ∈ Bf+,

πdr1 = πdr2 = · · · = πdrm = αr .

(13)

Let r ∈ Bf+ and q ∈/ Bf+. If pq < pr ≤ m, let xrqprpq satisfy ydrpr = ydqpq = xr = 1, with all other variables equal to zero. This gives πxrqprpq = πdrpr +
πdqpq + πr = π0 + πdqpq (using (12)), which implies πdqpq = 0. If pr < pq = m,
there are two cases: (i) dq ∈/ Cf : we can still set ydrpr = ydqm = xr = 1, which
yields πdqm = 0 as above; (ii) dq ∈ Cf : setting ydqm = 1 and all remaining
variables equal to zero, we conclude that πdqm = π0.
We now deal with coeﬃcients of π corresponding to x variables associated with arcs outside Bf+. Let q ∈/ Bf+. There are two cases to consider: (i) dq ∈ Cf : let xqm satisfy ydqm = xq = 1, with all other variables equal to zero. Then, πxqm = πdqm + πq = π0 + πq = π0. Therefore, πq = 0; (ii) dq ∈/ Cf : Take r ∈ Bf+ and let xqr21 satisfy ydq2 = ydr1 = xq = xr = 1 (even if q ∈ Bf−, both q and r will be visible). Then, πxqr21 = πdq2 + πdr1 + πq + πr = π0 + πq = π0. Hence,
πq = 0. If |Bf+| ≥ 2, let p1 > p2, r1 and r2 ∈ Bf+, and let xr1r2p1p2 satisfy ydr1 p1 =
ydr2 p2 = xr1 = 1, with all other variables equal to zero. Then, πxr1r2p1p2 = πdr1 p1 + πdr2 p2 + πr1 = αr1 + αr2 + πr1 = π0, yielding αr = 0 for all r, because of (12) and (13). Consequently, πr = π0 for all r ∈ Bf+. To achieve the same results when |Bf+| = 1, we assume |Cf | ≥ 1. Let xqrm satisfy ydqm = ydr(m−1) = 1, where dq ∈ Cf and Bf+ = {r}. Then, πxqrm = πdqm + πdr(m−1) = π0 + πdr(m−1), which implies πdr(m−1) = 0. Consequently, because of (13), πdrp = 0 for all p,
and πr = π0.

Proposition 10. Let f be a face of A with |Bf−| ≥ 1. For each r ∈ Bf−, (14) deﬁnes a facet of P .

Optimizing the Layout of Proportional Symbol Maps

9

yim + xr ≤ 1
i∈Df

(14)

Proof. The inequality is clearly valid. To prove that (14) is facet-deﬁning for P

under the assumptions stated above, we use the indirect method as in the proof

of Proposition 9. Let 1 ≤ p ≤ m, and let xrp satisfy ydrp = xr = 1, with all other variables equal to zero. Clearly, xrp satisﬁes (14) as an equality, xrp ∈ P ,

and

πxrp = πdrp + πr = π0 .

(15)

By varying the value of p, (15) implies that

πdr1 = πdr2 = · · · = πdrm = αr .

(16)

Let q = r. If pq < pr ≤ m, let xrqprpq satisfy ydrpr = ydqpq = xr = 1, with all other variables equal to zero. This gives πxrqprpq = πdrpr +πdqpq +πr = π0+πdqpq (using (15)), which implies πdqpq = 0. If pr < pq = m, there are two cases: (i) dq ∈/ Df : we can still set ydrpr = ydqm = xr = 1, which yields πdqm = 0 as above; (ii) dq ∈ Df : setting ydqm = 1 and all remaining variables equal to zero, we conclude that πdqm = π0.
We now deal with coeﬃcients of π corresponding to x variables associated with
arcs q = r. There are two cases to consider: (i) dq ∈ Df : let xqm satisfy ydqm = xq = 1, with all other variables equal to zero. Then, πxqm = πdqm +πq = π0 +πq. Therefore, πq = 0; (ii) dq ∈/ Df : Let xqr21 satisfy ydq2 = ydr1 = xq = xr = 1. Then, πxqr21 = πdq2 + πdr1 + πq + πr = π0 + πq. Hence, πq = 0.
Finally, let dq ∈ Df and let xqrm satisfy ydqm = ydr(m−1) = 1. Then, πxqrm = πdqm + πdr(m−1) = π0 + πdr(m−1), which implies πdr(m−1) = 0. Consequently, because of (16), αr = 0 and πr = π0.

A vertex of an arrangement is non-degenerate if it is an intersection point of exactly two disks or, equivalently, four arcs, as shown in Figure 4(i). Since each arc can be either visible or not, there are 16 potential assignments of values to their respective x variables. In a feasible solution, however, only the ﬁve assignments shown in Figure 4(ii)–(vi) are possible (dashed arcs are obscured). This observation gives rise to Proposition 11.

Proposition 11. Given a non-degenerate vertex of an arrangement as shown in Figure 4(i), (17)–(20) are valid and deﬁne facets of P .

xr1 ≥ xr3 xr2 ≥ xr4 xr3 + xr4 ≥ xr1 xr3 + xr4 ≥ xr2

(17) (18) (19) (20)

10

G. Kunigami et al.

r1

r2 r1

r2 r1

r2 r1

r2

r4

r3

(i)

r4

r3

(ii)

r4

r3

(iii)

r4

r3

(iv)

r1

r2 r1

r2 r1

r2 r1

r2

r4

r3

(v)

r4

r3

(vi)

r4

r3

(vii)

r4

r3

(viii)

Fig. 4. A non-degenerate vertex (i), ﬁve feasible arc conﬁgurations: (ii)–(vi), and two infeasible ones: (vii) and (viii)

Proof. It is easy to see that the ﬁve feasible conﬁgurations shown in Figure 4(ii)– (vi) satisfy (17)–(20). In addition, because of symmetry, it suﬃces to show that (17) and (19) are facet deﬁning. We will use the indirect method and deﬁne πx ≤ π0 as usual (see the proof of Proposition 9).
The zero vector satisﬁes (17) as an equality, which yields π0 = 0. Given i ∈ S and 1 ≤ p ≤ m, let xip be such that yip = 1 and all other variables are equal to zero. Clearly, xip belongs to P and satisﬁes (17) as an equality. Because πxip = πip = π0, we have that πip = 0 for all i and p. Given 1 ≤ p ≤ m and r ∈ R \ {r1, r3}, let xrp satisfy ydrp = xr = 1 and have zeroes everywhere else. Again, xrp satisﬁes (17) as an equality and xrp ∈ P . Since πxrp = πdrp +πr = π0, we have πr = 0. Finally, given 1 ≤ p ≤ m, let xr1r3 be such that xr1 = xr3 = ydr1 p = 1 (note that dr1 = dr3 ). Then, πxr1r3 = πr1 + πr3 + πdr1 p = π0. Because πdr1 p = π0 = 0, we have that πr1 = −πr3 , as desired.
We now show that (19) is facet deﬁning. By repeating the arguments of the previous paragraph, we can show that π0 = 0, πip = 0 for all i and p, and πr = 0 for all r ∈ R \ {r1, r3, r4}. Let xr1r3 be such that xr1 = xr3 = ydr1 1 = 1 and all other variables are equal to zero. Then, πxr1r3 = πr1 + πr3 + πdr1 1 = π0, which implies πr1 = −πr3 . Finally, let xr1r4 be such that xr1 = xr4 = ydr1 1 = ydr42 = 1, with all other variables equal to zero. Then, πxr1r4 = πr1 + πr4 + πdr1 1 + πdr4 2 = π0, which also implies that πr1 = −πr4 .
6 Decomposition Techniques
To reduce the size of the ILP model, we introduce decomposition techniques that allow us to consider smaller sets of disks at a time.
Without loss of generality, we assume that GS is connected. Otherwise, each of its connected components can be treated separately. In addition, we can decompose a connected component around articulation points of GS. Consider the example in Figure 5(i), in which S = {a, b, c, d, e, v}. The node corresponding to disk v, i.e. V (v), is an articulation point of GS because its removal disconnects

Optimizing the Layout of Proportional Symbol Maps

11

the graph into three connected components: {a, b}, {c, d}, and {e}. By adding v to each of these components, we get instances (ii), (iii), and (iv) of Figure 5, which are solved independently. Those three optimal solutions can be combined into an optimal solution for the entire set S by preserving the relative order of the disks in each solution. Proposition 12 formalizes this idea.

v

c

v

v

a

a

bd

b

e

e

v

c

d

(i)

(ii)

(iii)

(iv)

Fig. 5. An instance that allows for decomposition

Proposition 12. Let S be a set of disks such that GS is not 2-connected and let v be a disk corresponding to an articulation point of GS. Let Sk contain v plus the disk set of the k-th connected component obtained after the removal of V (v) from GS. The optimal solutions for each Sk can be combined into an optimal solution for S in polynomial time.
Proof. Let V (v) be an articulation point of GS and let v be its corresponding disk in S (note that articulation points can be found in O(|E|) time [3]). Using the notation introduced in the proposition, consider the disk subsets Si and Sj corresponding to any two distinct connected components of GS − V (v). By deﬁnition, the pieces of v’s border contained in Si \ {v} and in Sj \ {v} are disjoint. Hence, the optimal solutions of the problems deﬁned over Si and Sj do not inﬂuence each other. In other words, the relative order imposed by those solutions onto the disks of each such subset is optimal for the complete set of disks S. If we consider these orders as representing an orientation of the arcs of GS, we have a directed acyclic graph GS. The optimal assignment of disks to levels can be obtained in polynomial time from a topological ordering of GS.
If the graph of a connected component (GSk ) is not 2-connected and has an articulation point, the above procedure can be applied recursively.
From Figure 5(ii), it is clear that there exists an optimal solution in which a and b are drawn above v. Hence, we can consider the pair a, b as a separate instance, and v as another. Proposition 13, whose proof can be seen in [8], formalizes this idea.
Proposition 13. Let S be a set of disks and let HS be a directed graph with one node for every disk in S and an arc from node i to node j whenever a portion of the border of i’s disk is contained in the interior of j’s disk. Let Sk be the disk set of the k-th strongly connected component of HS. The optimal solutions for each Sk can be combined into an optimal solution for S in polynomial time.

12

G. Kunigami et al.

7 Computational Experiments

Our experiments are performed on the same set of instances used in the paper by Cabello et al. [1]. Instances City 156 and City 538 represent the 156 and 538 largest American cities, respectively, in which the area of each disk is proportional to the city’s population. Instances Deaths and Magnitudes represent the death count and Richter scale magnitude of 602 earthquakes worldwide, respectively. Disks are placed at the epicenters of each earthquake, and disk areas are proportional to the corresponding quantities [11]. When disks in an instance coincide, we replace them by a single disk whose border is the total border length of the original disks. This is possible because we can assume that such disks would occupy adjacent levels in an optimal solution. This pre-processing step reduces the number of disks in Deaths and Magnitudes to 573 and 491, respectively.
In Table 1, column Connected shows the number of connected components in GS for each instance, with the number of disks in the largest component in parentheses. Column Strongly Connected shows the resulting number of components (and largest component) after we apply the decomposition of Proposition 13. Proposition 12 yields further decomposition, as shown under column 2-Connected. The reductions in problem size are remarkable. City 538 can now be solved by optimizing over sets of disks no larger than one tenth of its original size. Solving the original instances is now equivalent to solving 671 signiﬁcantly smaller instances. Overall, the size of our largest instance dropped from 573 to 116 disks.

Table 1. Number of components and largest component before/after decomposition

Instance City 156 City 538 Deaths Magnitudes

# Disks 156 538 573 491

Connected 38 (57)
185 (98) 134 (141)
31 (155)

Strongly Connected 45 (56)
213 (94) 317 (85) 31 (155)

2-Connected 53 (29)
240 (53) 333 (70) 45 (116)

Our cut-and-branch algorithm uses the ILP model of Section 3, modeling (1) as SOS1, substituting (9) for (4), and adding (11), (17)–(20) at the root node. (Inequalities (14) did not help computationally.) Because |K| can be exponentially large, rather than including all of (3), we heuristically look for an edge covering of GS by maximal cliques [9]. Alternatively, we also tried replacing (3) with yip + yjp ≤ 1 for each level p and all (i, j) ∈ E. Although theoretically weaker, the latter formulation performed better in our experiments. This might be explained by the sparser coeﬃcient matrix of the weaker model, which typically yields easier-to-solve linear relaxations. Finally, instead of computing the exact value of m as in Proposition 1, which is NP-Hard [6], we use m = n in every run because the exact m is equal to n in many of the large components.
Our model was implemented in C++, using CGAL [13] for data extraction. We use XPRESS-Optimizer [4] version 20.00 to solve each problem on a 2.4GHz

Optimizing the Layout of Proportional Symbol Maps

13

Intel R CoreTM2 Quad processor, with 4GB of RAM. We limit each run to ﬁve hours of CPU time.

7.1 Numerical Results
For comparison purposes, we use the O(n2 log n) heuristic from [1,2] to ﬁnd good feasible solutions. Despite being a Max-Min heuristic, its solutions also perform well in terms of the Max-Total objective.
Out of the 671 components obtained through decomposition, all but the ﬁve or six largest ones from each original instance are easily handled by our strengthened ILP model. We will focus on them ﬁrst.
For components with |Sk| ≤ 2, the solution is trivial. For the remaining easyto-solve components, we summarize our results in Table 2. Column Comp. w/ |Sk| > 2 indicates how many easy components from the corresponding original instance have more than two disks. The next nine columns indicate the minimum, average, and maximum values of component size, followed by the number of search nodes and CPU time required to ﬁnd an optimal solution, respectively. When compared to the heuristic solutions, the optimal solutions to the 67 problems from Table 2 are 13.2% better on average (min = 0.0% and max = 158.4%).

Table 2. Average results over smallest non-trivial components of each instance

Original Instance

Comp. w/

|Sk |

Nodes

Time (in sec.)

|Sk| > 2 Min Avg Max Min Avg Max Min Avg Max

City 156

11

3 5.3 14 1 20.8 213 0 3.5 38

City 538

20

3 5.4 12 1 11.9 145 0 0.4 5

Deaths

22

3 4.7 10 1 5.8 93 0 0.1 1

Magnitudes

14

3 4.7 10 1 1.8 7 0 0.1 1

The results obtained with the ﬁve (or six) most challenging components of each original instance appear in Table 3. Component names are written as “αβ-γ (δ)”, where α identiﬁes the instance, β-γ indicates that this is the γ-th component generated by Proposition 12 when applied to the β-th component generated by Proposition 13, and δ is the number of disks. In Table 3, Base Value represents the total border length of arcs r that are visible in any feasible solution (SrI = ∅). This value is subtracted from the solution values in the remaining columns. Best Feasible and Best UB are the best lower and upper bounds on the optimal value found within the time limit, respectively (optimal solutions appear in bold). Column % Gap shows the relative diﬀerence between the lower and upper bounds, and % Above Heur. indicates how much better the best known lower bound is with respect to the heuristic solution discussed above.
Instance City 156 presented no diﬃculties, having all of its ﬁve largest components solved in less than 8 minutes. In Figure 6, we can perceive subtle differences, highlighted in light gray, between the optimal solutions for Max-Min

14

G. Kunigami et al.

Table 3. Results on largest components from each original problem instance

Component 156-18-0 (7) 156-3-2 (8) 156-3-0 (14) 156-2-0 (26) 156-2-1 (29) 538-47-2 (17) 538-3-0 (26) 538-29-1 (26) 538-1-6 (29) 538-1-0 (51) 538-24-0 (53) death-6-0 (12) death-8-0 (14) death-0-0 (24) death-3-0 (24) death-2-0 (70) mag-5-1 (25) mag-6-0 (26) mag-1-1 (39) mag-5-0 (81) mag-1-0 (113) mag-7-0 (116)

Base Value 63.97 39.84 66.15 167.22 219.36 26.75 34.27 46.48 21.98 77.37 18.98 953.08 68.05 175.78 441.75 725.28 214.92 217.21 417.32 601.79 581.41 700.37

Best Feasible 12.91 40.99 71.17 138.05 153.85 25.27 39.19 36.40 43.51 82.13 58.50 60.16 39.65 145.74 323.18 964.66 593.74 579.58 919.28 1741.24 2743.68 2622.46

Best UB 12.91 40.99 71.17 138.05 153.85 25.27 39.19 36.40 47.05 107.35 186.23 60.16 39.65 145.74 323.18 1652.02 593.74 610.99 1350.23 2317.66 -

% Gap 0 0 0 0 0 0 0 0 8.0 30.7 218.3 0 0 0 0 71.2 0 5.4 46.9 33.1 -

% Above Heur. 0 8.5 7.8 3.1 1.4 2.0 15.0 4.3 9.6 0.0 0.0 0.0 3.1 5.7 1.3 0.0 3.7 5.0 0.0 0.0 0.0 0.0

Nodes 1 7 213 5949 117 2463 23589 1143 2399 22 1 51 87 4925 3919 1 965 3385 3 1 1 1

Time (s) 0 0 39 381 10 1259 9562 1260 18000 18000 18000 1 0 199 210 18000 9609 18000 18000 18000 18000 18000

and Max-Total problems for this instance. We found optimal or near optimal solutions to the ﬁrst four of the largest components of City 538, with signiﬁcant improvements in quality with respect to the heuristic solutions. The two largest

Fig. 6. Optimal solutions for City 156 to Max-Min [2] and Max-Total problems, respectively

Optimizing the Layout of Proportional Symbol Maps

15

components of City 538 turned out to be more challenging, with sizable gaps remaining after ﬁve hours of computation. All but one of the largest earthquake death components were solved to optimality.
As was the case with component 538-24-0, the time limit was exhausted during the solution of death-2-0 even before branching started. The largest components obtained from the decomposition of earthquake magnitudes turned out to be the most challenging ones. Note that we do not have valid upper bounds for instances mag-1-0 and mag-7-0 because the time limit was not even enough to solve their ﬁrst linear relaxation. Overall, we were able to ﬁnd optimal solutions to 662 out of the 671 components derived from our original four instances.
Cutting planes (11) and (17)–(20) were essential in achieving the results in tables 2 and 3. With those cuts, the number of search nodes was 54 times smaller on average, with some cases achieving reductions of almost three orders of magnitude. (Five of the 21 hardest components — six overall — would not have been solved to optimality without cuts.) As a consequence, computation times were also drastically reduced.
Because of its direct relationship to the amount of overlapping between disks, the number of arcs in an instance/component is a better measure of diﬃculty than the number of disks. Our strengthened ILP model appears to be capable of handling about 600 to 700 arcs in ﬁve hours of CPU which, for our benchmark set, roughly corresponds to instances having between 24 and 26 disks. Table 4 contains more details about the size of our ﬁve largest components and how big their ILP formulation is before and after the inclusion of cuts. Because the number of cuts is small, we opted not to implement a branch-and-cut algorithm.

Table 4. Number of arcs and size of ILP formulation for the 5 largest components

Component 538-24-0 death-2-0 mag-5-0 mag-1-0 mag-7-0

# Disks 53 70 81 113 116

# Arcs 3753 1366 2059 4318 3759

# Cols. 6562 6266 8620 17087 17215

# Rows before cuts
3026565 620970 914490 3733407 2792468

# Rows after cuts 3035839
624115 919623 3744116 2801845

8 Conclusion
We propose a novel ILP formulation to optimize stacking drawings of proportional symbol maps (PSMs) with the objective of maximizing the total visible border of its symbols (opaque disks, in our case). By studying structural and polyhedral aspects of PSMs, we devised eﬀective decomposition techniques and new families of facet-deﬁning inequalities that greatly reduce the computational eﬀort required to solve the problem. These improvements enabled us to ﬁnd the ﬁrst provably optimal solutions to some of the real-world instances studied in [1,2]. Because solving PSM instances still pose great challenges when the

16

G. Kunigami et al.

number of arcs exceeds 1000 or so, we continue to study the PSM polyhedron in search of new families of cutting planes and/or alternative formulations.

References
1. Cabello, S., Haverkort, H., van Kreveld, M., Speckmann, B.: Algorithmic aspects of proportional symbol maps. In: Azar, Y., Erlebach, T. (eds.) ESA 2006. LNCS, vol. 4168, pp. 720–731. Springer, Heidelberg (2006)
2. Cabello, S., Haverkort, H., van Kreveld, M., Speckmann, B.: Algorithmic aspects of proportional symbol maps. Algorithmica 58(3), 543–565 (2010)
3. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms, 2nd edn. MIT Press, Cambridge (2001)
4. Fair Isaac Corporation. Xpress Optimizer Reference Manual (2009) 5. Dent, B.: Cartography – Thematic Map Design, 5th edn. McGraw-Hill, New York
(1999) 6. Garey, M.R., Johnson, D.S.: Computers and Intractability. Freeman, San Francisco
(1979) 7. Griﬃn, T.: The importance of visual contrast for graduated circles. Cartogra-
phy 19(1), 21–30 (1990) 8. Kunigami, G., de Rezende, P.J., de Souza, C.C., Yunes, T.: Optimizing the layout
of proportional symbol maps (2010), http://www.optimization-online.org/DB_HTML/2010/11/2805.html 9. Nemhauser, G.L., Sigismondi, G.: A strong cutting plane/branch-and-bound algorithm for node packing. Journal of the Operational Research Society 43(5), 443–457 (1992) 10. Nemhauser, G.L., Wolsey, L.A.: Integer and combinatorial optimization. WileyInterscience, New York (1988) 11. NOAA Satellite and Information Service. National geophysical data center (2005), http://www.ngdc.noaa.gov 12. Slocum, T.A., McMaster, R.B., Kessler, F.C., Howard, H.H.: Thematic Cartography and Geographic Visualization, 2nd edn. Prentice-Hall, Englewood Cliﬀs (2003) 13. Wein, R., Fogel, E., Zukerman, B., Halperin, D.: Advanced programming techniques applied to CGAL’s arrangement package. Computational Geometry 38(1-2), 37–63 (2007), http://www.cgal.org

An Optimal Hidden-Surface Algorithm and Its Parallelization
F. D´evai
London South Bank University 103 Borough Road, London, UK, SE1 0AA
fl.devai@lsbu.ac.uk
Abstract. Given a collection of non-intersecting simple polygons possibly with holes and with a total of n edges in three-dimensional space; parallel algorithms are given for the problems called hidden-line and hidden-surface removal in computer graphics. More precisely, algorithms are proposed to ﬁnd the portions of the edges visible from (0, 0, ∞) and to ﬁnd the upper envelope (i.e., the pointwise maximum) of the polygons. The proposed solution for the hidden-line problem is the parallelization of the optimal sequential algorithm given by D´evai in 1986. As the optimal sequential algorithm for the hidden-surface problem given by McKenna in 1987 is rather involved, a new optimal sequential algorithm is proposed, which is amenable to parallelization and might also have practical signiﬁcance in its own right. Both of the parallel hiddenline and hidden-surface algorithms take Θ(log n) time using n2/ log n CREW PRAM processors.
1 Introduction
In contemporary graphics hardware the z-buﬀer graphics pipeline has been designed to compute visibility, but eﬀects such as shadows, reﬂections, and diﬀuse lighting interactions all require more accurate visibility computations [35]. Multicore architectures promise the necessary power and ﬂexibility to implement such algorithms. Top-level systems already have tens of thousands of processors [28] and systems in the near future will have hundreds of thousands [46].
Traditionally in computer graphics two variants of the visibility problem are formulated [2,47,50]: Given a set S of pairwise disjoint, opaque and planar simple polygons possibly with holes and with a total of n edges in three-dimensional space, and a viewpoint u, u = (0, 0, ∞):
– ﬁnding each interval ξ of all the boundaries of the polygons in S such that all points of ξ are visible from u is called the hidden-line problem, and
– determining each region ρ of each polygon in S such that all points of ρ are visible from u is referred to as the hidden-surface problem.
Since u = (0, 0, ∞), a point p, p = (xp, yp, zp), of S is visible if zp is greater than the z-coordinate of any other point of S along the line through p parallel to the
B. Murgante et al. (Eds.): ICCSA 2011, Part III, LNCS 6784, pp. 17–29, 2011. c Springer-Verlag Berlin Heidelberg 2011

18

F. D´evai

z-axis. Therefore the hidden-surface problem also belongs to the topic of upper (lower) envelopes.
The visibility map of S is the subdivision of the xy-plane, also called the viewing plane or projection plane, into maximal regions such that in each region only one visible polygon is mapped or no polygon at all is mapped. The vertices of this subdivision are of two types: each vertex is either the projection of a visible vertex of a polygon, or it is the intersection of the projection of two edges. To avoid confusion between the vertices and edges of the visibility map and the vertices and edges of the polygons, we call the vertices and the edges of the visibility map nodes and arcs respectively.
It was established in 1986 [14] that the worst-case time complexity of the hidden-line problem is Θ(n2). The same upper bound was demonstrated for the hidden-surface problem by McKenna [36] in 1987. Since then spectacular progress has been reported in the computational-geometry literature [6,7,12,15,22,25,30,38,39,43,45] mainly about solutions which are output-size sensitive, i.e., solutions with running times of the function of the size of the reported output. Unfortunately, all the known output-sensitive algorithms are sub-optimal in the worst case and also are for restricted input, e.g., when each edge of the input polygons is parallel to one of the coordinate axes [22,39] or the input is fat objects [6] or objects with small union size [30] or a terrain [12,43].
Some of the solutions [7,15,37] have also been criticized in the engineering literature [31] for being “intricate”, extremely diﬃcult to implement and lacking robustness. This criticism might be unfounded, as at least one of the algorithms [15] has been tried, tested and successfully implemented in a computer-aided geometric design system.
Goodrich [20] proposed a hidden-surface algorithm taking O(n log n + k + t) time in the worst case, where k is the number of intersecting pairs of line segments and t is the number of intersecting pairs of polygons in the projection plane, but it does not allow cyclic overlap of polygons. Quoting Goodrich [20]: “Thus, the best known worst-case eﬃcient algorithm for the most general version of the hidden-surface elimination problem is still the algorithm by McKenna, which runs in O(n2) time and space” [36].
In this paper a new hidden-surface algorithm is proposed, which is simpler than the algorithm by McKenna [36] but still worst-case optimal for any set of simple polygons possibly with holes and cyclically overlapping images in threedimensional space. It is also demonstrated that, due to its simplicity, the proposed algorithm is relatively easy to parallelize.
The most widely used models of parallel computation are the variants of the Parallel Random Access Machine (PRAM) model. The PRAM model is a collection of random access machines and a global memory. All the processors have access to the global memory, and run synchronously. The global memory accesses are assumed to take unit time. The variants of the PRAM model handle concurrent reads and writes to the global memory cells diﬀerently. The major variants are the exclusive read, exclusive write (EREW), concurrent read, exclusive write (CREW) and concurrent read, concurrent write (CRCW) models.

An Optimal Hidden-Surface Algorithm and Its Parallelization

19

The most often used variant is the CREW PRAM model. In this model any number of processors can read a given global memory cell at once, but at most one processor is allowed to write into a given memory cell in one step. If more than one attempts to write, the computation is invalid.
There is a huge body of literature of PRAM algorithms [8,29,33,40] but the obstacles of regarding the PRAM as a realistic model are that a global, shared memory is diﬃcult to implement and that the memory response time is more than an order of magnitude longer than instruction execution times. In practice a hierarchy of fast cache memories is used to alleviate these problems. Some researchers [48,49] advocate that due to this memory hierarchy PRAM algorithms can directly be implemented by multicores, while others carefully adapt PRAM algorithms to the memory hierarchy [1,3].
A parallel algorithm is said to be work-optimal if the product of its running time and the number of processors used matches the sequential lower bound for the problem it solves. Thus, an optimal parallel hidden-line or hidden-surface algorithm would need to have a time-processor product of Θ(n2). The main obstacle to designing such algorithms is that the paradigms that led to eﬃcient sequential algorithms seem to be inherently sequential.
For example, one of the above-mentioned practically successful hidden-surface algorithms [15] uses depth-ﬁrst search for traversing regions of the viewing plane represented by a planar graph. Unfortunately, depth-ﬁrst search was proved to be P-complete and conjectured “inherently sequential” [41]. Though NC-algorithms were found later for performing a depth-ﬁrst search of a planar undirected graph, the best known ones take O(log3 n) time with n processors [44] or O(log n) time with n3 processors [24].
Nevertheless, Reif and Sen [42] proposed an O(log4 n)-time algorithm for the hidden-surface problem for terrains and D´evai [16] an O(log n)-time algorithm using n2 processors for the general hidden-line problem under the CREW model. In 2001 Gupta and Sen [23] proposed another O(log4 n)-time hidden-surface algorithm, also for terrains. Though these algorithms demonstrated that the hiddenline and hidden-surface problems (at least for terrains) are in the complexity class NC, none of the algorithms are work-optimal.
In Sect. 2 a new optimal sequential algorithm is proposed, which is amenable to parallelization and might also have practical signiﬁcance in its own right. In Sect. 3 the problem of union of point sets is introduced. In particular, if the point sets are n intervals of the real line, it is demonstrated that the problem requires Θ(n log n) computational work under the algebraic RAM model of computation. Then an EREW parallel algorithm is proposed that takes O(log n) time and n processors or O(log n) time and n/ log n processors if the endpoints of the intervals are sorted.
In Sect. 4 the parallel interval-union algorithm is used to develop an algorithm for hidden-line elimination. It takes Θ(log n) time with n2 processors under the EREW model, and therefore achieves a linear speedup on the O(n2 log n) worstcase time of the best known practicable sequential algorithms, where n is the total number of edges of the model. In Sect. 5 work-optimal hidden-line and

20

F. D´evai

hidden-surface algorithms are proposed that take Θ(log n) time and n2/ log n processors under the CREW model. It is demonstrated that the algorithms are also time-optimal for any PRAM without simultaneous writes. Finally in Sect. 6 some open problems are posed and directions for further work recommended.

2 A Simple Hidden-Surface Algorithm
In this section we propose a new hidden-surface algorithm, which is simpler than the one proposed by McKenna [36]. We start with the worst-case optimal hiddenline algorithm proposed earlier by the author [14]. We assume that the vertices of the outer boundary of the image of each polygon are listed in counter-clockwise order, and that the vertices of each hole in clockwise order. A hidden-line image induces a planar subdivision of the projection plane π. The input polygon visible in some regions of the subdivision can be determined by the orientation rule: If a region ρ is to the left while traversing the image e of edge e, then the polygon P containing edge e on its boundary is visible in region ρ. However some regions, called black holes, may be on the right-hand side of all edges on their boundaries, as the region shown in black in Figure 1.

Fig. 1. A black hole
The crucial observation is that, although there can be Θ(n2) black holes, it is suﬃcient to mark only one edge on the boundary of each, and cut the scene along each marked edge by a plane, called a cutting plane, perpendicular to π. The intersection of each cutting plane with the input polygons is a set of line segments in the cutting plane. The polygon that has a visible intersection in the cutting plane along the boundary of a black hole will be visible in the black hole. The proposed method can be formulated as Algorithm 1 and we can state the following result.
Theorem 1. Algorithm 1 ﬁnds the hidden surfaces of a set of non-intersecting simple polygons possibly with holes and cyclically overlapping images and with a total of n edges in three-dimensional space in optimal Θ(n2) time and space.

An Optimal Hidden-Surface Algorithm and Its Parallelization

21

Sketch of Proof : Steps (1) to (4) form a hidden-line algorithm that takes Θ(n2)

time and space [14]. There are at most O(n) cuts, and the dominant computa-

tional problem is to ﬁnd the visibility of the line segments in the cutting planes.

As the endpoint of the line segments are sorted in each cutting plane, this can

be solved in linear time for each cut by using an upper-envelope algorithm for a

planar set of line segments [4,10].

2

(1) Project each one of the n edges of the input polygons together with the straight line containing the edge into the projection plane π.
(2) For each projected line ﬁnd the list of points of intersections with the others in sorted order along the line by determining the arrangement of n lines [9,17].
(3) For each projected line ﬁnd all the intervals where the associated edge is partially or completely hidden by other polygons.
(4) Determine the visible segments of all n edges resulting in a planar subdivision G of the projection plane π.
(5) Attempt to determine the visibility of each region ρ of G by the orientation rule. If the attempt fails, put region ρ on the list of black holes.
(6) Mark one polygon edge on the boundary of each black hole. (7) For each marked edge, ﬁnd the set of line segments, called the cut, formed by
the intersection of the input polygons with the cutting plane perpendicular to π containing the marked edge. Discard the line segments collinear with the marked edge, and label each remaining line segment with the index of the input polygon containing it. (8) Find the upper envelope of the line segments in each cut. The polygon associated with the visible segment on the boundary of the black hole will be visible within the black hole.
Algorithm 1. Worst-case optimal hidden-surface elimination

3 The Interval-Union Problem

In this section we develop a parallel algorithm for the interval-union problem, and then in Sect. 4 we use this algorithm to develop a parallel hidden-line algorithm. In Sect. 5 we extend the latter to solve the hidden-surface problem in parallel.
In a wide range of application areas we are often required to ﬁnd the union of point sets: Given a collection R1, R2, ..., RN of N point sets, determine the set S deﬁned by R1 ∪ R2 ∪ ... ∪ RN . In particular, the hidden-line problem requires the determination of a subset of a line segment L contained by a collection of point sets R1, R2, ..., Rk. More precisely, if L is the image of an edge, and R1, R2, ..., Rk are images of polygons lying between the edge and an observer in three-dimensional space, then the visible subset V of L to be displayed is

V = L − {R1 ∪ R2 ∪ ... ∪ Rk}.

(1)

Surprisingly, the computation of 1 according to the deﬁnition would be both excessive and insuﬃcient at the same time. It is insuﬃcient, because a particular

22

F. D´evai

polygon may cover L along some intervals, but may not cover it along some other intervals, e.g., if the polygon is a simple polygon with holes. On the other hand, the computation of 1 is excessive, since R1 ∪ R2 ∪ ... ∪ Rk is not required; what we only need is the union of the hidden intervals of L.
The interval-union problem, as a special case of the problem of the union of point sets, can be formulated as follows: Given a list of 2n real numbers representing the endpoints of n intervals, compute the union of these intervals.
To devise an optimal algorithm which we will attempt to parallelize, we only need a counter c, initialised c = 0. For simplifying the presentation we assume that all the endpoints of the input intervals are disjoint. First we sort the endpoints of the intervals in increasing order, relabel them such that x1, x2, ..., x2n is the sorted sequence. Then we scan this sequence starting with x1, and increment c by 1 if xi, 1 ≤ i ≤ 2n, is a left endpoint, decrement c by 1 if xi is a right endpoint of an input interval. Whenever c = 1, we record xi as the left endpoint of an output interval, and whenever c = 0, we record xi as the right endpoint of an output interval.
It is not hard to demonstrate that whenever c = 1, xi must be the left endpoint, and if c = 0, xi must be the right endpoint of an input interval. Also if xi is the left endpoint of an input interval and c > 1, or if xi is the right endpoint of an input interval and c > 0, xi must be overlapped by one or more intervals. The running time of the above algorithm is dominated by the sorting step, hence the complexity of determining the union of n intervals of the real line is Θ(n log n) in the worst case, assuming the algebraic RAM model of computation [5].
Though the above algorithm is quite simple, there are two diﬃculties with its parallelization. First, scanning the sorted list seems to be inherently sequential. Second, even if we know the endpoints of the output intervals, it is not easy to store them in the memory parallely. We will use a linked list such that the elements of the list are stored in an array with mappings pred and succ, where pred provides the element preceding a given element, and succ provides the element subsequent to a given element in the list. Then overlapped endpoints are simply removed from the list.
The parallel preﬁx problem [32] is to compute all initial preﬁxes x1, x1 ◦ x2, ..., x1 ◦ x2 ◦ ... ◦ xn of n items x1, x2, ..., xn, where ◦ is an associative binary operation. By the solution of the parallel preﬁx problem we not only can assign the values of the counter c to the endpoints of the intervals, but also can attach ranks 1, 2, ..., n to the elements of a linked list, e.g., 1 to the ﬁrst, 2 to the second element etc, and the elements can be placed in an array by simply using the rank of each element as its index. Then the parallel interval-union algorithm is stated as Algorithm 2.
Lemma 1. The union of n intervals along a straight line can be determined in O(log n) time in the worst case by using n processors, or if the input is sorted, in O(log n) time with n/ log n processors under the EREW PRAM.
Proof : Step (1) can be implemented in O(log n) time by using n processors under the EREW model [11]. Step (3) and therefore step (6) take O(log n) time and n/ log n processors assuming the EREW model. Steps (2), (4) and (5)

An Optimal Hidden-Surface Algorithm and Its Parallelization

23

(1) Sort the endpoints of the intervals in increasing order, relabel them, and prepare a doubly-linked list D such that x1, x2, ..., x2n is the sorted sequence, pred (xi) = xi−1, succ(xi) = xi+1, 2 ≤ i ≤ 2n − 1, pred (x1) = nil and succ(x2n) = nil;
(2) Assign weights wi to xi, 1 ≤ i ≤ 2n, such that if xi is a left endpoint, then wi = 1, and if xi is a right endpoint, then wi = −1;
(3) Compute the parallel preﬁx sum ci = w1 + w2 + ... + wi for all xi, 1 ≤ i ≤ 2n; (4) for all xj, j = 1, 3, ..., 2n − 1, do in parallel
if ((xj is a left endpoint and cj > 1) or (xj is a right endpoint and cj > 0)) then
remove xj from the doubly linked list D; endfor; (5) Repeat step (4) for all xj, j = 2, 4, ..., 2n, in parallel; (6) Rank the doubly linked list D, and write the endpoints of the M ≤ n output intervals parallely into 2M consecutive cells of the global memory.
Algorithm 2. Interval union for the EREW PRAM

take constant time and n processors or O(log n) time with n/ log n processors.

There are no memory conﬂicts in step (2), and we can avoid memory conﬂicts

by examining and, if necessary, removing ﬁrst the odd elements of D in step

(4), then the even elements in step (5). Hence the complete algorithm can be

implemented in O(log n) time in the worst case by using n processors, or if the

input is sorted, in O(log n) time with n/ log n processors assuming the EREW

PRAM model of parallel computation.

2

4 A Parallel Hidden-Line Algorithm
Many sequential visibility algorithms published in the computer-graphics literature [18,19,26,27,34] divide polygon edges into line segments according to intersection points in the projection plane, and then test each segment for visibility against each polygon. There can be Θ(n2) line segments; each tested against Θ(n) polygons takes Θ(n3) time in the worst case.
As we have already seen in Sect. 3, any hidden-line algorithm has to determine the union of Θ(n) hidden intervals on Θ(n) edges in the worst case. Since the union of n intervals can be found optimally in Θ(n log n) time, this leads to a Θ(n2 log n) improvement in the worst-case time. Indeed, it is not easy to reduce the worst-case time below Θ(n2 log n) for any practical algorithm [15].
If we wish to achieve a sublinear running time, it follows from the sequential complexity of the problem that we need more than n processors, which may have memory conﬂicts while processing the n edges of the input. Therefore, we have to make copies of the input ﬁrst if we assume the EREW model.
Let ei be the image of edge ei in the projection plane, and let li be the straight line containing ei, 1 ≤ i ≤ n. We can assume without loss of generality that li coincides with the x-axis of the coordinate system. Then a parallel hidden-line algorithm can be formulated as Algorithm 3.

24

F. D´evai

(1) Make n copies of the description of each edge ei, 1 ≤ i ≤ n, in n consecutive blocks of memory cells.
(2) for all edge ei, 1 ≤ i ≤ n, do in parallel (2.1) for all edge ej, 1 ≤ j ≤ n, j = i, do in parallel (2.1.1) Find the intersection point xj of li and ej, where edge ej is nearer to the observer than the line containing edge ei. (2.1.2) Let aj and bj denote the endpoints of ej, such that ej is oriented from aj to bj . If aj is above li, label xj as a left, otherwise as a right endpoint, as shown in Figure 2. endfor (2.2) Let xl be a point of li to the left of the leftmost xj, let xr be a point of li to the right of the rightmost xj, xa be the left endpoint of ei, and xb be the right endpoint of ei. Label xl and xb as left, and xa and xr as right endpoints. (2.3) Determine the union of the intervals speciﬁed by the endpoints xl, xa, xb, xr and xj , 1 ≤ j ≤ n; j = i. (2.4) Insert xa and xb into the list L obtained as a result of step (2.3). If the insertion of xa fails, i.e., xa is already in L, then report interval [xa, succ(xa)) as a visible segment of ei, otherwise [xa, succ(xa)] is a hidden interval of ei. Similarly, if the insertion of xb fails, report interval (pred (xb), xb] as a visible segment of ei, otherwise [pred (xb), xb] is a hidden interval of ei. (2.5) Discard the elements of L left to xa and those right to xb. If two consecutive elements xj and xk, j = a, k = b, of L are a left and a right endpoint respectively, then [xj ,xk] is a hidden interval of ei. Otherwise if xj is a right, and xk is a left endpoint, then report (xj,xk) as a visible segment of ei. endfor
Algorithm 3. EREW hidden-line elimination
Analysing Algorithm 3, we can state the following.
Theorem 2. The hidden-line problem for a set of non-intersecting simple polygons possibly with holes and cyclically overlapping images and with a total of n edges in three-dimensional space can be solved in optimal Θ(log n) time by using n2 EREW PRAM processors.
Proof : The content of any cell of the shared memory can be copied into any block of n consecutive cells in O(log n) time by using n/ log n EREW PRAM processors. Therefore step (1) of the algorithm can be implemented in O(log n) time by using n2/ log n processors. Steps (2.1.1) and (2.1.2) take constant time and n processors (or O(log n) time with n/ log n processors).
Step (2.2) takes ﬁnding the minimum and the maximum of n numbers in O(log n) time by n/ log n processors. According to Section 3 step (2.3) can be computed in O(log n) time in the worst case by using n processors. In step (2.4) xa and xb can be inserted in L in O(log n) serial time.
Note that L is ranked after step (2.3). Discarding the elements of L left and right to the endpoints of ei and reporting the visible segments of ei takes O(log n) time and n/ log n processors. Hence step (2) of the above algorithm can be executed in O(log n) time for a single edge by using n EREW processors. Using

An Optimal Hidden-Surface Algorithm and Its Parallelization

25

y6

aj bk

xl

left

¤¤ r¤

xa

¤ xj

¤

¤

h hhrright h xk
hh

xb

-x xr

bj

ak

Fig. 2. Labelling intersection points

n2 processors, the algorithm can be executed for n edges within the same time

under the EREW model.

It follows from the deﬁnition of visibility that ﬁnding the maximum of n inte-

gers is constant-time reducible to the hidden-line problem by using n processors.

Cook, Dwork and Reischuk [13] have given an Ω(log n) lower bound for ﬁnding

the maximum of n integers allowing inﬁnitely many processors of any PRAM

without simultaneous writes.

2

The practical signiﬁcance of Algorithm 3 is that it is relatively simple and that the EREW model is the PRAM variant closest to real machines. Though Algorithm 3 is not work-time optimal, it uses only O(n2 log n) work, which is the upper bound for the best sequential algorithms used in practice.

5 A Parallel Hidden-Surface Algorithm
While Algorithm 3 is optimal in a stronger sense, i.e., its running time cannot be further improved, the question arises: would n2/ log n processors be suﬃcient to maintain O(log n) time? The sequential hidden-surface algorithm in Sect. 2 uses an optimal algorithm for the arrangement of n lines in the plane to get the intersection points in sorted order on all the projected edges. Goodrich [21] proposed an algorithm for constructing line arrangements in O(log n) time on n2/ log n CREW processors. Combining Goodrich’s result with the techniques proposed above and with a result by Chen and Wada [10], the question can be answered aﬃrmatively for the CREW model.
Theorem 3 (Goodrich, 1993). The arrangement of n lines in the plane can be constructed in O(log n) time by using n2/ log n CREW PRAM processors.
Theorem 4 (Chen and Wada, 2002). The upper envelope of n nonintersecting line segments with sorted endpoints in the plane can be found in O(log n) time by using n/ log n CREW PRAM processors.
Our main result is stated as follows.
Theorem 5. The upper envelope of a set of nonintersecting simple polygons possibly with holes and cyclically overlapping images and with a total of n edges

26

F. D´evai

in three-dimensional space can be constructed optimally in Θ(log n) time by using n2/ log n CREW PRAM processors.

Proof : An algorithm with the above time and processor bounds can be obtained

by the parallelization of Algorithm 1. For the implementation of Steps 1 and

3 to 7, the techniques developed for Algorithm 3 can be used. According to

Theorem 3, Step 2 can be implemented in O(log n) time with n2/ log n CREW

PRAM processors. By using Theorem 4, the upper envelope of the line segments

in Step 8 of Algorithm 1 and the polygons visible within the black holes can be

found in O(log n) time using n2/ log n CREW processors. The time optimality

follows by the argument at the end of the proof of Theorem 2.

2

6 Concluding Remarks
Our proposed optimal sequential hidden-surface algorithm provides a fairly simple alternative to the algorithm by McKenna. We have also developed eﬃcient parallel algorithms for solving two variants of the visibility problem of a set of pairwise disjoint polygons with a total of n edges. Our algorithms for the hiddenline problem take Θ(log n) parallel time either on n2 EREW or on n2/ log n CREW processors, and our hidden-surface algorithm takes Θ(log n) time on n2/ log n CREW processors. Θ(log n) time is the best possible under the EREW and CREW models, even if arbitrarily many processors were available.
Our algorithms for the CREW model are work-optimal, and all of our algorithms are for general input, allowing cyclically overlapping images of simple polygons possibly with holes. The EREW model is the PRAM variant closest to real machines implemented by hardware.
It is an open question if highly parallelizable hidden-line and hidden-surface algorithms, i.e., that take O(log log n) time, exist. These, however, would have to be based on concurrent-write models of parallel computation. Another possible direction for further research is the adaptation of our algorithms to the memory hierarchy of multicore architectures.

Acknowledgements
The author thanks J. M. Selig for his comments on a preliminary version of this paper and an anonymous reviewer for suggesting a simpler version of the parallel interval-union algorithm.

References
1. Ajwani, D., Sitchinava, N., Zeh, N.: Geometric algorithms for private-cache chip multiprocessors. In: de Berg, M., Meyer, U. (eds.) ESA 2010. LNCS, vol. 6347, pp. 75–86. Springer, Heidelberg (2010)
2. Appel, A.: The notion of quantitative invisibility and the machine rendering of solids. In: Proc. 1967 22nd National Conference, ACM 1967, pp. 387–393. ACM Press, New York (1967)

An Optimal Hidden-Surface Algorithm and Its Parallelization

27

3. Arge, L., Goodrich, M.T., Nelson, M., Sitchinava, N.: Fundamental parallel algorithms for private-cache chip multiprocessors. In: Proc. 20th Annual Symposium on Parallelism in Algorithms and Architectures, SPAA 2008, pp. 197–206. ACM Press, New York (2008)
4. Asano, T., Asano, T., Guibas, L., Hershberger, J., Imai, H.: Visibility of disjoint polygons. Algorithmica 1, 49–63 (1986)
5. Ben-Amram, A.M., Galil, Z.: Topological lower bounds on algebraic random access machines. SIAM J. Comput. 31, 722–761 (2001)
6. de Berg, M., Gray, C.: Computing the visibility map of fat objects. Comput. Geom. Theory Appl. 43(4), 410–418 (2010)
7. de Berg, M., Halperin, D., Overmars, M., Snoeyink, J., van Kreveld, M.: Eﬃcient ray shooting and hidden surface removal. Algorithmica 12, 30–53 (1994)
8. Blelloch, G.E., Maggs, B.M.: Parallel algorithms. In: Atallah, M.J., Blanton, M. (eds.) Algorithms and Theory of Computation Handbook, Chapman & Hall/CRC (2010)
9. Chazelle, B., Guibas, L.J., Lee, D.T.: The power of geometric duality. BIT 25, 76–90 (1985)
10. Chen, W., Wada, K.: On computing the upper envelope of segments in parallel. IEEE Transactions on Parallel and Distributed Systems 13(1), 5–13 (2002)
11. Cole, R.: Parallel merge sort. SIAM J. Comput. 17, 770–785 (1988) 12. Cole, R., Sharir, M.: Visibility problems for polyhedral terrains. Journal of Sym-
bolic Computation 7(1), 11–30 (1989) 13. Cook, S., Dwork, C., Reischuk, R.: Upper and lower time bounds for paral-
lel random access machines without simultaneous writes. SIAM J. Comput. 15, 87–97 (1986) 14. D´evai, F.: Quadratic bounds for hidden-line elimination. In: Proc. 2nd Annu. ACM Sympos. Comput. Geom, pp. 269–275 (1986) 15. D´evai, F.: An intersection-sensitive hidden-surface algorithm. In: Proc. Eurographics 1987, pp. 495–502 (1987) 16. D´evai, F.: An O(log N ) parallel time exact hidden-line algorithm. In: Advances in Computer Graphics Hardware II, Record of the Second Eurographics Workshop on Graphics Hardware, pp. 65–73 (1988) 17. Edelsbrunner, H., O’Rouke, J., Seidel, R.: Constructing arrangements of lines and hyperplanes with applications. SIAM J. Comput. 15, 341–363 (1986) 18. Franklin, W.R.: A linear time exact hidden surface algorithm. Comput. Graph. 14(3), 117–123 (1980); Proc. Siggraph 1980 19. Galimberti, R., Montanari, U.: An algorithm for hidden-line elimination. Commun. ACM 12, 206 (1969) 20. Goodrich, M.T.: A polygonal approach to hidden-line and hidden-surface elimination. CVGIP: Graph. Models Image Process. 54, 1–12 (1992) 21. Goodrich, M.T.: Constructing arrangements optimally in parallel. Discrete and Computational Geometry 9, 371–385 (1993) 22. Goodrich, M.T., Atallah, M.J., Overmars, M.H.: Output-sensitive methods for rectilinear hidden surface removal. Inform. Comput. 107, 1–24 (1993) 23. Gupta, N., Sen, S.: An eﬃcient output-size sensitive parallel algorithm for hiddensurface removal for terrains. Algorithmica 31, 179–207 (2001) 24. Hagerup, T.: Planar depth-ﬁrst search in O(log n) parallel time. SIAM J. Comput. 19, 678–704 (1990)

28

F. D´evai

25. Halperin, D., Sharir, M.: New bounds for lower envelopes in three dimensions, with applications to visibility in terrains. Discrete and Computational Geometry 12, 313–326 (1994)
26. Hornung, C.: An approach to a calculation-minimized hidden line algorithm. Computers & Graphics 6(3), 121–126 (1982)
27. Hornung, C.: A method for solving the visibility problem. IEEE Comput. Graph. Appl. 4, 26–33 (1984)
28. IBM Blue Gene Team: Overview of the IBM Blue Gene/P project. IBM J. Res. Dev. 52, 199–220 (January 2008)
29. J´aJa´, J.: An Introduction to Parallel Algorithms. Addison Wesley Longman Publishing Co., Inc, Redwood City (1992)
30. Katz, M.J., Overmars, M.H., Sharir, M.: Eﬃcient hidden surface removal for objects with small union size. Comput. Geom. Theory Appl. 2, 223–234 (1992)
31. Keeler, T., Fedorkiw, J., Ghali, S.: The spherical visibility map. Comput. Aided Des. 39, 17–26 (2007)
32. Ladner, R.E., Fischer, M.J.: Parallel preﬁx computation. J. ACM 27(4), 831–838 (1980)
33. Thomson Leighton, F.: Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. Morgan Kaufmann Publishers Inc, San Mateo (1992)
34. Loutrel, P.P.: A solution to the hidden-line problem for computer drawn polyhedra. IEEE Trans. Comput. C-19, 205–213 (1970)
35. Mark, W.: Future graphics architectures. Queue 6, 54–64 (2008) 36. McKenna, M.: Worst-case optimal hidden-surface removal. ACM Trans. Graph. 6,
19–28 (1987) 37. Mulmuley, K.: An eﬃcient algorithm for hidden surface removal. Siggraph Comput.
Graph. 23, 379–388 (1989) 38. Overmars, M.H., Sharir, M.: An improved technique for output-sensitive hidden
surface removal. Algorithmica 11, 469–484 (1994) 39. Preparata, F., Vitter, J., Yvinec, M.: Output-sensitive generation of the perspective
view of isothetic parallelepipeds. Algorithmica 8, 257–283 (1992) 40. Rajasekaran, S., Reif, J.H.: Handbook of Parallel Computing: Models, Algorithms
and Applications. Chapman & Hall/CRC, Boca Raton (2008) 41. Reif, J.H.: Depth-ﬁrst search is inherently sequential. Information Processing Let-
ters 20(5), 229–234 (1985) 42. Reif, J.H., Sen, S.: An eﬃcient output-sensitive hidden surface removal algorithm
and its parallelization. In: Proc. 4th Annual Symposium on Computational Geometry, SCG 1988, pp. 193–200. ACM Press, New York (1988) 43. Reif, J.H., Sen, S.: An eﬃcient output-sensitive hidden-surface removal algorithm for polyhedral terrains. Mathematical and Computer Modelling 21(5), 89–104 (1995) 44. Shannon, G.E.: A linear-processor algorithm for depth-ﬁrst search in planar graphs. Inf. Process. Lett. 29, 119–123 (1988) 45. Sharir, M., Overmars, M.H.: A simple output-sensitive algorithm for hidden surface removal. ACM Trans. Graph. 11, 1–11 (1992) 46. Sodan, A.C., Machina, J., Deshmeh, A., Macnaughton, K., Esbaugh, B.: Parallelism via multithreaded and multicore CPUs. Computer 43, 24–32 (2010) 47. Sutherland, I.E., Sproull, R.F., Schumacker, R.A.: A characterization of ten hiddensurface algorithms. ACM Comput. Surv. 6(1), 1–55 (1974)

An Optimal Hidden-Surface Algorithm and Its Parallelization

29

48. Vishkin, U.: A PRAM-on-chip vision (invited abstract). In: Proc. 7th International Symposium on String Processing Information Retrieval (Spire 2000), p. 260. IEEE Computer Society Press, Washington, DC, USA (2000)
49. Vishkin, U.: Using simple abstraction to reinvent computing for parallelism. Commun. ACM 54, 75–85 (2011)
50. Weiss, R.A.: Be Vision, a package of IBM 7090 FORTRAN programs to draw orthographic views of combinations of plane and quadric surfaces. J. ACM 13, 194–204 (1966)

Construction of Pseudo-triangulation by Incremental Insertion
Ivana Kolingerova´1, Jan Trˇcka2, and Ladislav Hobza3
1 University of West Bohemia, Czech Republic kolinger@kiv.zcu.cz
2 Charles University, Czech Republic jan.trcka@email.cz
3 University of West Bohemia, Czech Republic lhobza@students.zcu.cz
Abstract. A pseudo-triangulation is a planar subdivision into pseudotriangles - polygons with three convex vertices, used mainly in motion planning problems in robotics. As it is a rather new concept, not too many algorithms to construct it exist. In this paper, we propose an online version of incremental insertion, with generalized ﬂips to improve the shape of pseudo-triangles. This algorithmic paradigm is often used for Delaunay triangulations, but for pseudo-triangulations it has been used only in an oﬀ-line version (for sorted input points). We also experimented with several optimization criteria for the ﬂips and show their inﬂuence on the shape of pseudo-triangles.
Keywords: Pseudo-triangulation, Triangulation, Incremental insertion, Generalized ﬂip, Computational geometry.
1 Introduction
Planar subdivisions are geometrical structures enabling to convert some geometrical problems from global to local. In the last years, a new kind of subdivision, called a pseudo-triangulation, appeared. A pseudo-triangulation consists of pseudo-triangles - polygons with three convex vertices. An amount of knowledge about pseudo-triangulations is still limited, therefore, some research is devoted to eﬀective algorithms for them, namely, to their construction.
In this paper we propose an algorithm based on the on-line version of incremental insertion algorithmic paradigm and on the generalized ﬂip. As far as we know, such an algorithm has not been produced for pseudo-triangulations yet, although it is quite popular for the Delaunay triangulation. Our novelty is also in the use of various optimization criteria for generalized ﬂips and comparison of the shape of resulting pseudo-triangles; good shape of pseudo-triangles is important for applications such as collision detection or motion planning.
The content of the paper is as follows. Section 2 presents state of the art in pseudo-triangulations. Section 3 brings necessary terms and deﬁnitions. Section 4 presents the algorithm. Section 5 shows experiments and results, section 6 concludes the paper.
B. Murgante et al. (Eds.): ICCSA 2011, Part III, LNCS 6784, pp. 30–43, 2011. c Springer-Verlag Berlin Heidelberg 2011

Pseudo-triangulation

31

2 Related Work

The pseudo-triangulation (or geodesic triangulation) has found applications in ray casting [5], visibility problems for convex sets and simple polygons in the plane [11,12], collision detection of moving planar polygons [7,8], robot arm motion planning [17] or the guard problem [16]. Combinatorial and geometric properties have been investigated in [13]. An eﬀort has been devoted to special classes of pseudo-triangulations, such as minimum pseudo-triangulations (also called pointed) [17,6] with a minimum number of edges and faces from all pseudotriangulations on the same set of points. The total number of minimum pseudotriangulations was studied by [14,1]. [4] presents a generalized ﬂip (a greedy ﬂip) of edges in a pseudo-triangulation and its eﬃcient implementation. Other interesting results can be found in [2] (a zig-zag path in the pseudo-triangulation), [3] (a realization of a pseudo-triangulation into a polyhedral surface in E3).
Several algorithms how to construct a pseudo-triangulation have been published. Rote at al. [15] describe construction of the so-called minimal pseudotriangulations which can be created by removal a maximum number of edges from the input triangulation. Usually, this needs several linear-time runs, not counting complexity to construct the initial triangulation. Another possibility is the so-called canonical minimum pseudo-triangulation [6], see Fig.1.

Fig. 1. A canonical minimum pseudo-triangulation
To construct it, ﬁrst the input points have to be sorted according to their x coordinate. Then the ﬁrst triple of points is used to make an initial triangle and a new point is added by connecting this point to the convex hull of already constructed part of the pseudo-triangulation. The Kettner’s algorithm is an incremental insertion paradigm in the oﬀ-line version: a disadvantage is that all the points have to be present at the beginning of construction and sorted, an advantage is that location of the point in the pseudo-triangulation is not necessary as the point to be inserted next is outside the already constructed part of pseudo-triangulation.
3 Terms and Deﬁnitions
A pseudo-triangle is a planar polygon which has three convex vertices (also called corners ). Two corners are connected by a concave chain of edges (also called a

32

I. Kolingerov´a, J. Trˇcka, and L. Hobza

side), see Fig.2. A pseudo-triangulation of a ﬁnite set S of n points is a planar subdivision of the convex hull of S into pseudo-triangles whose vertex set is S, see Fig.3.

Fig. 2. Examples of pseudo-triangles

Fig. 3. A pseudo-triangulation
Geodesic path between two points in a pseudo-triangle is the shortest path between these two points inside this pseudo-triangle (including its boundary).
Two neighbouring triangles share one edge. If it is removed, we get a quadrilateral, convex or not. In case of a convex quadrilateral, there is one more possibility how to triangulate it using the so-called edge ﬂip of its diagonal. This operation is a key to local optimizations in triangulations, based on a sequence of ﬂips of non-optimal edges.
Two neighbouring pseudo-triangles can share one or two edges. If they share one edge, removal of this edge connects these two pseudo-triangles into a pseudoquadrangle - a polygon with four corners, see Fig.4. A diagonal of a pseudoquadrangle is a geodesic path connecting two its corners and lying inside the pseudo-quadrangle. This diagonal contains a part of the boundary and one line segment connecting two vertices, lying inside the pseudo-quadrangle and subdividing the pseudo-quadrangle into two pseudo-triangles. A pseudo-quadrangle has two diagonals, it means that there are two possible ways how to subdivide it into two pseudo-triangles. Let us call a ﬂip of a diagonal in a pseudo-quandrangle a generalized ﬂip .
When two neighbouring pseudo-triangles share two edges, we obtain a pseudotriangle with one isolated vertex inside.

Pseudo-triangulation

33

Lemma 1. Let us have a pseudo-triangle PT and one its inner point V, V does not lie on the boundary of PT. Then two geodesic paths from V to any two corners of PT subdivide PT into two smaller pseudo-triangles (proof omitted for space reasons).

There are maximally three ways how to split the pseudo-quadrangle into two pseudo-triangles using this vertex, see Fig.5.

Fig. 4. Pseudo-quadrangle obtained by an edge removal

Fig. 5. Subdivision of a pseudo-triangle by a geodesic path
4 The Incremental Insertion Algorithm
The algorithm proposed in this paper is based on the on-line incremental insertion, so it neither needs the input points to be present all at the beginning of work, nor to be sorted.
Let us have a ﬁnite set S of n points to be pseudo-triangulated. The algorithm consists of the following steps (details are given in the subsections):
1. Construct an initial triangle by connecting three (randomly chosen, but linearly independent) points from S. 2. For all remaining points do: a) Locate the pseudo-triangle which contains the point to be inserted b) Insert the given point and create two new pseudo-triangles. c) Improve the pseudo-triangulation by a generalized ﬂip of the newly constructed pseudo-triangles (optional).

34

I. Kolingerov´a, J. Trˇcka, and L. Hobza

Step 2c) either can be done after each point insertion or as a post-processing of all pseudo-triangles of an already constructed pseudo-triangulation.

4.1 Point Location on a Pseudo-triangulation
This problem is deﬁned as follows: a pseudo-triangulation on the given planar set S of n points and a point V are given. We want to locate a pseudo-triangle containing V if it exists.
There are many ways how to solve this problem, usually with a hierarchical data structure, enabling to locate one point in the optimal O(log n) time. We use the walk algorithm where the point is located by traversing from one pseudotriangle to another according to the sign test against the pseudo-triangle edges. Such an approach has worse time complexity than hierarchies (O(n1/2) per point location or O(n1/3) in case of the uniform distribution of input points) but no location data structures are needed and so no extra memory is consumed. However, other location technique could be used instead of the walk.
The walk algorithm works as follows. Let V be the point to be inserted into the pseudo-triangulation, let PT be the current pseudo-triangle.
1. Choose a starting pseudo-triangle for the location and take it as PT. 2. While PT does not contain the inserted point V choose the next pseudotriangle PT from the neighbours of PT.
Now PT contains V.
The starting pseudo-triangle is chosen randomly. Eﬃciency can be improved by random sampling: take about O(n1/3) randomly chosen vertices from the pseudo-triangulation, ﬁnd the vertex P nearest to V and start at a pseudotriangle containing P. This technique was introduced in [10] for triangulations. Details and experimental results for pseudo-triangulations can be found in [9].

4.2 Point Insertion and Creation of Two New Pseudo-triangles
Let the point to be inserted lies inside the pseudo-triangulation and is not incident to any vertex or edge of the pseudo-triangulation. Then the pseudo-triangle containing the point is subdivided into two pseudo-triangles by geodesic paths.
By insertion of the point and subdivision of PT by two geodesic paths we obtain three possible subdivisions to two smaller pseudo-triangles, recall Fig.5. We can choose randomly or according to preferred properties, e.g., to minimize the edge length.
If V lies on an edge of PT, we can divide PT to two smaller pseudo-triangles by a geodesic path from V to the opposite corner, see Fig.6. The same step must be done for the neighbouring pseudo-triangle sharing this edge.
If the point V is identical with some vertex, we do not do any subdivision and omit the point.

Pseudo-triangulation

35

Fig. 6. Geodesic paths for V on an edge
If V lies outside the pseudo-triangulation, a new face will be formed by connecting the new point by two tangents with the convex hull of the pseudotriangulation to form a part of a new convex hull, see Fig.7. This corresponds to the Kettner’s construction [6]. In this case, we have no choice how to make the connection as we had for the point inside.

Fig. 7. Insertion of a point outside the pseudo-triangulation
4.3 Improvement of the Pseudo-triangulation by a Generalized Flip
If the pseudo-triangulation was constructed using the algorithm as described so far, it would be formally correct but the resulting pseudo-triangles would have undesirable elongated shape and some vertices wold have a high degree, see example in Fig.8. Pseudo-triangulations made by Rote’s approach are better, see Fig.9. Therefore, we have to improve the shape of pseudo-triangles using generalized ﬂips - we ﬂip the pseudo-quadrangle diagonal if the ﬂip brings improvement in the required local optimality criterion.
The optimality criteria are based on an analogy with the triangulations. The triangle shapes are most often locally optimized by increasing the minimum angle, decreasing the maximum angle or decreasing the edges length. For pseudo-triangles, the edge length criterion can be used without any modiﬁcation, we denote it MinLength criterion. Angle criteria can be applied either to corners of a pseudo-triangle or to vertices of its convex hull (a triangle). We denote these angle criteria MaxMinAngle, MinMaxAngle, MinMaxHullAngle, MaxMinHull-Angle, respectively.

36

I. Kolingerov´a, J. Trˇcka, and L. Hobza

Fig. 8. An example of the incremental pseudo-triangulation without generalized ﬂips, 1000 points, a random choice of geodesic paths

Fig. 9. A pseudo-triangulation by Rote’s approach

Pseudo-triangulation

37

5 Experiments and Discussion

We implemented the algorithm in Java and tested on AMD Sempron 1.6 GHz computer with 512 MB, Windows XP SP3 for various input point distributions: uniform, gauss and clusters of points, 1000 to 75000 points. Each measurement was done on three diﬀerent data sets three times. We will denote the measured algorithms as follows: PST the incremental insertion without generalized ﬂips, PST-IN the version with the generalized ﬂips included in the pseudotriangulation construction and PST-POST with ﬂips as a post-processing.
First we measured time complexity of PST-IN and PST-POST. Times for PST-POST were taken as a sum of the triangulation and the post-processing time. The results showed O(n log n) time complexity for both algorithmic versions and all input types, with PST-IN being faster. The diﬀerence grows with input size to about 40 per cent.
Next, we compared the quality of pseudo-triangles obtained by PST, PST-IN and PST-POST algorithms. We tried all the criteria mentioned in the previous section: MinLength, MinMaxAngle, MaxMinAngle, MinMaxHullAngle and MaxMinHullAngle.
Lets us look at a typical result for 10000 uniform points in Fig.10-18. We can see that MinLenght improves the shapes both in PST-IN and PST-POST. The minimum angle has increased from 5 ◦ to 22 ◦, the maximum angle has decreased from 121 ◦ to 89 ◦(90 ◦) by MinLenght+ PST-POST (PST-IN). The results were similar for other types of input point distribution. MaxMinAngle brought improvement only in PST-IN and MinMaxAngle never. Due to space limitation we

Fig. 10. MinLength. Result of PST.

38

I. Kolingerov´a, J. Trˇcka, and L. Hobza

Fig. 11. MinLength. Result of PST-POST.

Fig. 12. MinLength. Result of PST-IN.

Pseudo-triangulation

39

Fig. 13. MaxMinAngle. Result of PST.

Fig. 14. MaxMinAngle. Result of PST-POST.

40

I. Kolingerov´a, J. Trˇcka, and L. Hobza

Fig. 15. MaxMinAngle. Result of PST-IN.

Fig. 16. MinMaxAngle. Result of PST.

Pseudo-triangulation

41

Fig. 17. MinMaxAngle. Result of PST-POST.

Fig. 18. MinMaxAngle. Result of PST-IN.

42

I. Kolingerov´a, J. Trˇcka, and L. Hobza

do not show the results for MaxMinHullAngle and MinMaxHullAngle; they are worse than MinLength and better than MinMaxAngle and MaxMinAngle. As angle criteria were less successful than a simple criterion of ’shorter edge’, our recommendation is to use the less time-consuming MinLength criterion.

6 Conclusion
We presented an incremental insertion algorithm with generalized ﬂips for pseudo-triangulation construction. From the tested algorithmic versions, the one with the generalized ﬂips included in the pseudo-triangulation process based on the criterion of shorter diagonal worked best.

Acknowledgment
Supported by the Grant Agency of the Czech Republic - project 201/09/0097 and by the UWB grant SGS-2010-028 Advanced Computer and Information Systems.

References
1. Aichholzer, O., Aurenhammer, F., Krasser, H., Speckmann, B.: Convexity minimizes pseudo-triangulations. In: Proceedings of the 14th Canadian Conference on Computational Geometry, pp. 158–162 (2002)
2. Aichholzer, O., Rote, G., Speckmann, B., Streinu, I.: The Zigzag Path of a PseudoTriangulation. In: Dehne, F., Sack, J.-R., Smid, M. (eds.) WADS 2003. LNCS, vol. 2748, pp. 377–388. Springer, Heidelberg (2003)
3. Aichholzer, O., Aurenhammer, F., Krasser, H., Brass, P.: Pseudo-triangulations from surfaces and novel type of edge ﬂip. SIAM Journal on Computing 32, 1621–1653 (2003)
4. Br¨onnimann, H., Kettner, L., Pocchiola, M., Snoeyink, J.: Counting and enumerating pseudo-triangulations with greedy ﬂip algorithm. SIAM Journal on Computing 36, 721–739 (2007)
5. Chazelle, B., Edelsbrunner, H., Grigni, M., Guibas, L., Hershberger, J., Sharir, M., Snoeyink, J.: Ray shooting in polygons using geodesic triangulations. Algorithmica 12, 54–68 (1994)
6. Kettner, L., Kirkpatrick, D., Mantler, A., Snoeyink, J., Speckmann, B., Takeuchi, F.: Tight degree bounds for pseudo-triangulations of points. Computational Geometry - Theory and Applications 25(1-2), 3–12 (2003)
7. Kirkpatrick, D., Snoeyink, J., Speckmann, B.: Kinetic collision detection for simple polygons. In: Proceedings of the 16th ACM Symposium on Computational Geometry, pp. 322–330 (2000)
8. Kirkpatrick, D., Speckmann, B.: Separation sensitive kinetic separation structures for convex polygons. In: Akiyama, J., Kano, M., Urabe, M. (eds.) JCDCG 2000. LNCS, vol. 2098, pp. 222–236. Springer, Heidelberg (2001)
9. Kolingerov´a, I., Trˇcka, J., Zˇalik, B.: The stochastic walk algorithms for point location in pseudo-triangulations (2011) (manuscript)

Pseudo-triangulation

43

10. Mu˝cke, E.P., Saias, I., Zhu, B.: Fast randomized point location without preprocessing in two- and three-dimensional Delaunay triangulations. In: Proceedings of the 12th Annual Symposium on Computational Geometry, pp. 274–283 (1996)
11. Pocchiola, M., Vertger, G.: Computing the visibility graph via pseudotriangulations. In: Proceedings of the 11th Annual ACM Symposium on Computational Geometry, pp. 248–257 (1995)
12. Pocchiola, M., Vertger, G.: The visibility complex. Proceedings of the International Journal of Computational Geometry and Applications, 279–308 (1996)
13. Pocchiola, M., Vertger, G.: Pseudo-triangulations: theory and applications. In: Proceedings of the 12th Annual ACM Symposium on Computational Geometry, pp. 291–300 (1996)
14. Randall, D., Rote, G., Santos, F., Snoeyink, J.: Counting triangulations and pseudo-triangulations of wheels. In: Proceedings of the 13th Canadian Conference on Computational Geometry, pp. 149–152 (2001)
15. Rote, G., Wang, C.A., Wang, L., Xu, Y.: On constrained minimum pseudotriangulations. In: Warnow, T.J., Zhu, B. (eds.) COCOON 2003. LNCS, vol. 2697, pp. 445–454. Springer, Heidelberg (2003)
16. Speckmann, B., T´oth, C.D.: Allocating vertex π-guards in simple polygons via pseudo-triangulations. In: Proceedings of the 14th ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 109–118 (2003)
17. Streinu, I.: A combinatorical approach to planar non-colliding robot arm motion planning. In: Proceedings of the 41st Annual Symposium on Foundations of Computer Science (FOCS), pp. 443–453 (2000)

Non-uniform Geometric Matchings
Christian Knauer1, Klaus Kriegel2, and Fabian Stehn1
1 Institut fu¨r Angewandte Informatik, Universit¨at Bayreuth {christian.knauer,fabian.stehn}@uni-bayreuth.de 2 Institut fu¨r Informatik, Freie Universit¨at Berlin kriegel@inf.fu-berlin.de
Abstract. In this paper we introduce a generalization of the well studied class of geometric matching problems. The input to a geometric matching problem is usually two geometric objects P , Q drawn from a class of geometric objects G, a transformation class T applicable to G and a distance measure distG : G × G → R+. The task is to compute the transformations t ∈ T minimizing distG(t(P ), Q).
Here, we extend this concept to non-uniform geometric matching problems. In this setting, a partition of P into k pieces P1, . . . , Pk is given and the task is to compute a sequence of transformations t1, . . . , tk such that distG( i ti(Pi), Q) is minimized. But instead of solving k usual geometric matching problems independently and taking the maximum of the computed distances, the objective function of a non-uniform geometric matching problem also requires the computed transformations to be similar with respect to a suitable similarity measure on T .
Computing a set of similar transformations to match an object P to Q allows to lower the inﬂuence of measurement errors and to model local deformations and has various applications, for example in medical navigation systems.
We present constant factor approximations and approximation schemes for point sequences under translations and constant factor approximations for point sets under translations.
1 Introduction
Medical navigation systems are common to the workﬂow of neurosurgical interventions since the mid-’90s. The purpose of these systems is to support surgeons during medical interventions by projecting instruments that are located in the operation theatre (and partially in the patient) at the correct position and in the correct orientation into a 3D-model of the patient. Navigation systems ﬁnd applications especially in brain biopsies and in operations where the actual spot on which the operation has to be performed on is occluded by healthy tissue.
A central component of a medical navigation system is the mapping from the operation theatre space into the model space, in this context often called registration. A registration of a pattern space to a model space is a function that maps each point of the pattern space to its corresponding point in the model space. The model space can be seen as a copy of the pattern space deformed
B. Murgante et al. (Eds.): ICCSA 2011, Part III, LNCS 6784, pp. 44–57, 2011. c Springer-Verlag Berlin Heidelberg 2011

Non-uniform Geometric Matchings

45

by noise, local distortions and by realignments (diﬀerent coordinate systems). A registration in this formulation can be seen as the concatenation of these inﬂuences.
In general, the problem of computing a registration is often intractable, as neither explicit representations of both spaces are given (especially of the operation theatre), nor of the contained objects (the patient). It is furthermore unknown from which transformation class the transformation are drawn that constitute to the registration.
Usually, a registration is approximated by ﬁrst extracting related geometric features from both spaces and subsequently solving a geometric matching problem. The features P resulting from the pattern space are called pattern and accordingly the features Q that are extracted from the model space are called model. Let P and Q be drawn from a class G of geometric objects.
The task of a geometric matching problem is then to compute a transformation t of a given transformation class T (applicable on G) minimizing distG(t(P ), Q), where distG : G × G → R+ is a suitable distance measure on G.
Computing registrations for the purpose of aligning patients to anatomic models or of aligning images that result from diﬀerent imaging processes is also intensely studied in the medical imaging community, see the surveys by Maintz and Viergever [1], Maurer et al. [2], van den Elsen et al. [3] and Dawant [4] for an introduction into the signiﬁcant amount of research in this ﬁeld.
It is out of the scope of this paper to give a comprehensive overview over the work on geometric matchings done in the computational geometry community. We recommend the survey by Alt and Guibas [5] for further reading.
An extended abstract of this paper has been published in [6].

1.1 From Geometric Matching Problems to Non-uniform Geometric Matching Problems
Geometric Matching problems hence use a single transformation to match a pattern to its model. This restriction of using just one transformation, no matter which reasonable transformation class T is considered, limits the ability to consider local deformations and makes the implied registration prone to measurement errors. Often one has to decide whether a speciﬁc region should be mapped well or whether the registration should give results that are good on average over the entire space. Another application where applying a single transformation has immediate drawbacks are soft tissue registrations as for example needed for liver biopsies and operations. Here, tissue deformations (e.g., due to respiration or physical pressure) have to be considered.

1.2 Problem Deﬁnition
We generalize the concept of geometric matchings to so-called non-uniform geometric matchings. In a non-uniform matching problem, a set of transformations is computed. Each transformation is locally valid within predeﬁned regions of the pattern space. The regions of interest form a partition of the pattern space.

46

C. Knauer, K. Kriegel, and F. Stehn

To map a point p from the pattern space to the model space one ﬁrst has to determine the cell that contains p. In a second step, the transformation that is associated to that cell is used to perform the actual mapping. The transformation for a certain cell is computed by solving a geometric matching problem that maps geometric features of that cell to the model.
Non-uniform registrations have to optimize two competing objectives: to match the pattern features close to the model features while simultaneously assuring conformity of the mapping by demanding that transformations of two neighbored cells are similar with respect to their eﬀect.
First, we are going to state non-uniform geometric matching problems in their most general form and then, in Section 2, formulate the problem for the speciﬁc case of matching point sequences under translations. In Section 5, a nongeometric matching problem is also formulated for matching point sets under translations.

Deﬁnition 1 (Non-Uniform Geometric Matching Problem)

Let P(Rd, k) denote the set of all partitions of Rd into k cells.

Given:

G a class of geometric objects

distG : G × G → R+ a distance measure in object space

P = {p1, . . . , pn} ∈ G a pattern object

Q = {q1, . . . , qm} ∈ G a model object

T a transformation class admissible on G

C = {C1, . . . , Ck} ∈ P(Rd, k) a partition of Rd into k cells,

such that ∀i ∈ [n] ∃j ∈ [k] pi ⊆ Cj distT : P(Rd, k)×T k → R+a distance measure in transformation space
f : R+ × R+ → R+ a weight function

Task: compute a set of transformations T = {t1, . . . , tk} ⊆ T minimizing

f (distG ({ti(Ci ∩ P )) | 1 ≤ i ≤ k} , Q) , distT (C, T )) .

Note, that for k = 1 Deﬁnition 1 is equal to the deﬁnition of the usual geometric
matching problem. A pattern feature pi ∈ P is mapped by the transformation tj that corresponds
to the cell Cj containing pi. The matched feature set Pˆ is hence given as

Pˆ := tj(P ∩ Cj ).
j∈[k]

The objective function that deﬁnes the quality of a matching consists of two parts: the function distG that measures the distance of the matched point set Pˆ to Q. The second factor is the function distT that measures the similarity of a transformation set T by considering the neighborhood relations of the individual transformations as induced by the partition C of the pattern space.
The remainder of this paper is organized as follows: in Section 2 a ﬁrst instance of a non-uniform geometric matching problem for point sequences and
translations is stated. In Section 3 several constant factor approximations for
this problem instance are presented depending on the structure of an associated

Non-uniform Geometric Matchings

47

graph G that encodes the neighborhood relations of the transformations. An approximation scheme for this problem is presented in Section 4 for the case that G is a tree. As stated above the problem is extended to point sets in Section 5.

2 Non-uniform Matchings for Point Sequences

For now, we consider a simple yet not trivial variant of the non-uniform matching problem where we restrict the transformation class T to translations. We further assume the geometric features to be point sequences of equal size (|P | = |Q| = n), measured in the pattern space and deﬁned in the model space. We also assume that the correspondence between the point sequences is known, that is, point pi is mapped to qi for all i ∈ [n]. As the measure in the feature space (distG) we consider the maximum Euclidean 1-to-1 distance, that is

distG (Pˆ, Q) := max ti(pi) − qi .

(1)

i∈[n]

We consider decompositions of the pattern space into n cells so that each cell contains exactly one point of P (as it is the case for the Voronoi diagram of P ). As stated above, the transformations are not computed independently from each other. To control the conformity of the registration around cell boundaries of the decomposition one has to ensure that two transformations ta and tb whose corresponding cells Ca and Cb are neighbors (share parts of their boundary) are similar with respect to their eﬀect. As a measure of similarity of two translations ta and tb we consider the Euclidean distance ta(x) − tb(x) of their images of a point x ∈ Rd. From now on, we do not distinguish between a translation and its translation vector and measure the similarity of two translations by the Euclidean norm of the translation vector diﬀerence, i.e., ta −tb (as the distance of two images of the same point does not depend on the preimage of the point).
The information about the pairs of translations that have to be similar is encoded in a graph G = (T, E) which we call neighborhood graph. The vertex set of G is the translations T that are to be computed and {ti, tj} ∈ E, if the cells corresponding to translations ti and tj share parts of their boundary. Note, that the edges of the neighborhood graph could also be selected by criteria other than the adjacency of cells and could for instance be manually chosen by the user. The algorithms presented in this chapter do not require that the neighborhood graph resembles the partition of the pattern space. For some algorithms, the approximation factors however depend on the structure of G.
To simplify notation, we deﬁne for any two translations ti, tj ∈ T :

dij =

1 0

if {ti, tj} ∈ E(G) otherwise.

As the measure distT for the similarity of the translation set T we take the maximum of the similarity of any two translations that are adjacent in G:
distT (T, G) := max dij ti − tj .
i,j∈[n]

48

C. Knauer, K. Kriegel, and F. Stehn

We chose to measure the distances in the pattern space as well as the deviations in the translation space using the Euclidean metric. This problem could might as well be studied with another reasonable underlying measure, like the Manhattan metric. Putting all this together and taking the maximum of the distance measured in object space and the similarity measure in translation space, we get the following problem description:
Problem 1 Given P , Q and G as above, compute a sequence T of translations (t1, . . . , tn) minimizing

dist(P, Q, G, T ) := max max ti(pi) − qi , max dij ti − tj , (2)

i∈[n]

i,j∈[n]

where · denotes the Euclidean norm. The ﬁrst term accounts for the distance of the matched point set P to Q by considering the L∞ norm of the vector ti(pi) − qi.
We have chosen to minimize the maximum of the distances in the pattern space and the deviations in the model space. Again, other weight functions e.g. minimizing the sum of both components could be considered as well. For translations however minimizing the maximum of both involved measures seems natural as the minimum will be achieved where both inﬂuence variables are equal. As the displacement of a point that caused by choosing either of two neighboring translations (Euclidean distance of the two images) is equal to the deviation of these two transformations (Euclidean norm of the diﬀerence vector), taking the maximum of both magnitudes results in a good balance between the two measures by not favoring one over the other.
One advantage of considering translations is that the distance of a matched point pi to its corresponding point qi and also the similarity of two translations can be measured in translation space. Consider the translations si = qi − pi for 1 ≤ i ≤ n and let S := (s1, . . . , sn). The distance ti(pi) − qi for a point pi matched with translation ti to qi can be expressed as the distance si − ti , as

ti(pi) − qi = ti + pi − qi = qi − pi − ti = si − ti .

The problem of computing a non-uniform matching for point sequences under translations can also be formulated in the following way: Consider a straight line embedding of the graph G = (S ∪ T, E ) with E = {{si, ti} | i ∈ [n]} ∪ {{ti, tj} | dij = 1}. The edge set E consists of two sorts of edges:
1. edges connecting two translations ti and tj indicating that they have to be similar,
2. n edges {si, ti} whose lengths measure the Euclidean distance of ti(pi) to qi.
Note, that the positions of all s ∈ S are already determined by the input. The problem of computing a non-uniform registration optimizing Equation 2 can be formulated as:

Non-uniform Geometric Matchings

49

Problem 2. Find a placement for all t ∈ T such that the length of the longest edge of the induced straight line embedding of G is minimal.
As the vertices of G represent translations, we also call G the translation graph of S.

2.1 Convex Programming Formulation

The problem of computing a non-uniform registration optimizing Equation 2 can be phrased as a convex optimization problem (see [7] for an introduction into this ﬁeld):

minimize subject to si − ti ≤ ,
dij ti − tj ≤

i = 1, . . . , n, 1 ≤ i < j ≤ n.

As any metric norm is convex and the maximum of two convex functions is also convex. Convex optimization problems have the property, that they have a unique minimum, i.e., any local minimum is also a global minimum. Furthermore, convex optimization problems (such as Problem 2) can be solved in polynomial time, e.g., by using the interior-point or the ellipsoid method [7].
In Sections 3 and 4 we present fast approximation algorithms that are based on geometric insights into the problem. There are two reasons for considering geometric approximation algorithms for this problem, even though the machinery of convex programming provides us with exact polynomial solutions to this problem:

1. the approximation factors of the constant-factor approximations are close to 1 and the approximate solutions can be computed in linear time with only small constants hidden in the O-Notation.
2. the geometric insights we gained during the study of the geometric nature of this problem help to develop approximation strategies for non-uniform matching variants that can not be formulated as a convex optimization problem, see Section 5.

3 Constant-Factor Approximations
Let Topt be an optimal solution and let opt := dist(P, Q, G, Topt) be the value of the objective function for Topt.
Theorem 1. Choosing ti = qi − pi for 1 ≤ i ≤ n results in a 3-approximation of opt.
Proof. Assume T to be in optimal position. For any i and j with dij = 1 we have that ti − tj ≤ opt as well as ti − si ≤ opt and tj − sj ≤ opt. Moving ti upon si and tj upon sj increases the distance ti − tj by at most 2 · opt while setting the distances ti − si and tj − sj to zero, hence ti − tj ≤ 3 · opt for all i, j with dij = 1, see Figure 1a.

50

C. Knauer, K. Kriegel, and F. Stehn

Let k be the diameter of the neighborhood graph G, i.e., the largest number of edges on a shortest path between any two vertices of G (short with respect to the number of edges on the path).
Theorem 2. Choosing t1 = t2 = · · · = tn = qi − pi for some 1 ≤ i ≤ n results in a (k + 2)-approximation of opt.
Proof. Assume T to be in optimal position and let i be the selected index. The distance of any tj to ti is at most k·opt as each edge on the path from ti to tj as a length at most opt and the number of edges of the path is bounded by k. As the distance ti−si is also bounded by opt, we have that ti(pj)−qj ≤ (k+2)opt, see Figure 1b.

a) si

≤ opt

ti

≤ opt

tj ≤ opt

sj

≤ 3 opt

si

≤ (k + 2) opt

sj

b)

ti

≤ k opt

tj

Fig. 1. a) Illustration of the 3-approximation of Theorem 1. b) Illustration of the (k + 2)-approximation of Theorem 2.

For the remainder of this section we assume P = (p1, . . . , pn) and Q = (q1, . . . , qn) to be point sequences in the plane and the neighborhood graph G to be complete, that is, any two translations have to be compared.

Lemma 1. Let Topt be an optimal choice of translations. The center copt of the smallest disc enclosing Topt provides a (1 + 1/√3)-approximation for points in the plane and complete neighborhood graphs, if copt is chosen for all t ∈ T :
dist(P, Q, G, (t1 = copt, t2 = copt, . . . , tn = copt)) ≤ (1 + 1/√3) opt.

Proof. In optimal position, the distance si − ti for any 1 ≤ i ≤ n is bounded

braydoiupsti.sAblolutnradnesdlabtyioonpst/o√f 3T,oapst

lie within the stated in the

smallest disc enclosing following lemma.

Topt

whose

Lemma 2. The radius of the smallest disc enclosing a point set of width μ in the plane is bounded by μ/√3.

Proof. Any planar point set X of at least two points contains a subset of two or three points {xi} ⊆ X such that the smallest enclosing disc δX of the subset is identical to the smallest enclosing disc of X, moreover all xi lie on the boundary of δX and deﬁne δX . If δX is described by two points x1 and x2 then x1 − x2

Non-uniform Geometric Matchings

51

√ is the diameter of δX and as x1 − x2 /2 ≤ μ/2 < μ/ 3 the lemma holds. Assume that δX is deﬁned by three points x1, x2, x3 and assume w.l.o.g. that
x1 − x2 is the longest of the pairwise distances of {x1, x2, x3} and assume that x3 lies to the left of the ray starting in x1 through x2.
A Reuleaux triangle of width μ is the intersection of three discs of radius μ

centered at the corners of an equilateral triangle with side length μ. The circum-

circle of its

δΔ of an induced

equilateral triangle of side Reuleaux triangle and has

length μ a radius

is identical to the circumcircle of μ/√3, see Figure 2 right.

Consider the rigid motion that moves x1 on the origin and x2 on the positive

x-axis, hence x3 lies in the intersection of the ﬁrst quadrant with two discs of radius x1 − x2 centered in x1 and x2 respectively. The set {x1, x2, x3} is fully

contained in the Reuleaux triangle with one corner on the origin, one corner on

the positive x-axis and the third corner in the ﬁrst quadrant and is therefore also

covered by δΔ, see Figure 2 left. This implies, that the radius of δX is bounded by the radius of δΔ.

c c

a = (0, 0)

b

b (x, 0)

μ

μ

√1 μ
3
μ

Fig. 2. left: Illustration of the proof of Lemma 2, right: the equilateral- and Reuleaux triangle containing all point sets of width x

Tthheecdeinsttearncceopotfiemapchliepsoain(t1s+∈1S/√t3o)-caopptprisoxbiomuantdioedn

by as

opt+1/√3 opt. Therefore, stated in Lemma 1.

Lemma 1 implies that there exists a single translation that results in a maximal distance of (1+1/√3) opt to any s ∈ S. But as Topt is unknown, the center copt of its smallest enclosing disc is unknown as well. On the other hand, the translation that minimizes the largest distance to any point of S can be computed in linear time [8,9].
It is easy to see, that the center c of the smallest disc enclosing S is the translation that minimizes the distance to any translation in S. We have determined the approximation factor for choosing ti = copt for i ∈ [n] and know that c is the best possible choice of a single translation. Together with Lemma 1 this implies the following constant-factor approximation:

52

C. Knauer, K. Kriegel, and F. Stehn

Theorem 3. The center c of the smallest disc enclosing the point sequence S results in a (1 + 1/√3)-approximation:
app = dist(P, Q, G, (t1 = c, t2 = c, . . . , tn = c)) ≤ (1 + 1/√3) opt.
The approximation factor can be improved to 2/3 (1 + 1/√3) ≈ 1.05157 by choosing n diﬀerent translations in the following way: Let app be the value of the approximation as presented in Theorem 3. Choose ti to be the intersection oi of the straight line si c for i ≤ 1 ≤ n with the circle δapp centered in c with radius app/3. If δapp does not intersect the line segment si c, then ti is chosen to be si. For this choice of ti, the distance si − ti is bounded by 2/3 (1 + 1/√3)opt for each 1 ≤ i ≤ n which is also the diameter of the circle δapp, implying that the distances ti − tj for 1 ≤ i < j ≤ n are also bounded by 2/3 (1 + 1/√3)opt, see Figure 3.

s1 s2
s3

δapp

o1

o4

c o2
o5 o3

t6 = s6

s4 s5

Fig. 3. The outer circle is the smallest circle enclosing S (radius app), the inner circle δapp has a radius of app/3 with the same center
Theorem 4. Let c be the center of the smallest disc enclosing the sequence S and let app be the approximation value resulting from applying Theorem 3. Choosing ti as the intersection oi of the straight line si c with the circle δapp with center c and radius app/3, or ti = si if δapp does not intersect si c, results in a 2/3 (1 + 1/√3)-approximation that can be computed in O (n) time:
app = dist(P, Q, G, (t1, t2, . . . , tn)) ≤ 2/3 (1 + 1/√3) opt ≈ 1.05157 opt.
4 Approximation Scheme for Trees
In this section, we present an (1 + )-approximation scheme for settings in which the neighborhood graph is a tree. The strategy computes an approximation based on deciding a relaxed decision problem variant for diﬀerent guesses of the value

Non-uniform Geometric Matchings

53

of opt and can be applied if P and Q are planar point sequences. By slightly abusing the notation, we impose the information of G on S, that is, we call si and sj adjacent, if {ti, tj} ∈ E(G).
The usual (unrelaxed) decision variant to Problem 2 can be stated as follows:
Problem 3. For a given μ ≥ 0 and a translation graph G = (S ∪ T, E ) that is a tree with S, T ⊂ R2, is there a placement of T such that the length of any edge in the induced straight line embedding of G is at most μ?
Before presenting an algorithm to decide Problem 3, we need to introduce some notation and mention basic geometric observations. Let δ(c, r) be a disc of radius r centered in c and let δμ be deﬁned as δ((0, 0), μ). The Minkowski sum X ⊕ Y of two sets X and Y is deﬁned as X ⊕ Y := {x + y | x ∈ X, y ∈ Y }. For X being a geometric ﬁgure, the set X ⊕ δμ is the set of all points z so that there is a point x ∈ X with z − x ≤ μ.
The following simple geometric observations hold for embeddings that meet the edge length constraint, see Figure 4:
1. for all ti ∈ T we have that ti ∈ δ(si, μ) 2. if {ta, tb} ∈ E(G) then ta ∈ δ(sb, 2μ) and tb ∈ δ(sa, 2μ) 3. if c1, . . . , ck are the children of si then ti ∈ j∈[k] δ(cj, 2μ) ∩ δ(si, μ)

si δ(s1, μ)

δ(si, μ)

t1

ti

ta

tb

sb

si

sa

c1

ti

t3

δ(c2, 2μ)

t2

c3

δ(sa, 2μ)

δ(sb, 2μ)

δ(c2, 2μ)

δ(c1, 2μ)

c2

Fig. 4. Illustration of geometric properties that every registration whose edges have a length of at most μ has to satisfy

These observations motivate the deﬁnition of the admissible region regμ (s) for a point s ∈ S. The admissible region of a point s is deﬁned as the set of all translations t for which a straight line embedding of the subtree rooted in s
exists that satisﬁes the edge length constraint.

