8354

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

Optimal Model Placement and Online Model Splitting for Device-Edge Co-Inference
Jia Yan , Member, IEEE, Suzhi Bi , Senior Member, IEEE, and Ying-Jun Angela Zhang , Fellow, IEEE

1 Abstract— Device-edge co-inference opens up new possibilities

I. INTRODUCTION

33

2 for resource-constrained wireless devices (WDs) to execute deep

3 neural network (DNN)-based applications with heavy compu- A. Motivation and Contributions

34

W 4 tation workloads. In particular, the WD executes the ﬁrst few
5 layers of the DNN and sends the intermediate features to the 6 edge server that processes the remaining layers of the DNN.

ITH recent advancements in artiﬁcial intelligence (AI) [1], [2], many deep neural network (DNN)-

35 36

7 By adapting the model splitting decision, there exists a tradeoff based applications have emerged in mobile systems [3], [4], 37

8 between local computation cost and communication overhead. such as human face recognition and augmented reality. Due 38

9 In practice, the DNN model is re-trained and updated periodically to the tremendous amount of computation workload, the 39

10 at the edge server. Once the DNN parameters are regenerated, DNN-based applications cannot be fully executed at the wire- 40

11 12

part of the updated model must be placed at the WD to facilitate on-device inference. In this paper, we study the joint

less devices (WDs) with low-performance computing units and

41

13 optimization of the model placement and online model splitting limited battery life [5]–[9]. Alternatively, the WDs can choose 42

14 decisions to minimize the energy-and-time cost of device-edge to ofﬂoad the computations to a nearby server located at the 43

15 co-inference in presence of wireless channel fading. The problem network edge, referred to as edge inference [1]. Typically, the 44

16 is challenging because the model placement and model splitting edge server can execute the whole DNN-based application on 45

17 decisions are strongly coupled, while involving two different time 18 scales. We ﬁrst tackle online model splitting by formulating an

the WD’s behalf after receiving the raw input data from the

46

19 optimal stopping problem, where the ﬁnite horizon of the problem WD. However, due to the massive original input data (e.g., 3D 47

20 is determined by the model placement decision. In addition to images and videos), the excessive communication overhead 48

21 deriving the optimal model splitting rule based on backward makes it impractical to support delay-sensitive services [10], 49

22 induction, we further investigate a simple one-stage look-ahead [11]. Such difﬁculty can be overcome by performing device- 50

23 rule, for which we are able to obtain analytical expressions of 24 the model splitting decision. The analysis is useful for us to

edge co-inference, where a large DNN is splitted into two

51

25 efﬁciently optimize the model placement decision in a larger time parts. The ﬁrst part with computation-friendly workload is 52

26 scale. In particular, we obtain a closed-form model placement executed on the WD, while the remaining part is computed 53

27 solution for the fully-connected multilayer perceptron with equal on the edge server. The WD needs to transmit the output of 54

28 neurons. Simulation results validate the superior performance the ﬁrst part (i.e., the intermediate feature) to the edge server 55

29 of the joint optimal model placement and splitting with various 30 DNN structures.

for further execution.

56

It is essential to determine at which layer the WD splits the 57

31 Index Terms— Edge inference, deep neural network, model DNN model, i.e., stops local computing and ofﬂoads the inter-

32 splitting, model placement, optimal stopping theory.

mediate feature. The prior work on model splitting [12]–[17]

58 59

Manuscript received 20 May 2021; revised 2 December 2021; accepted showed that by carefully selecting the model splitting point, 60

28 March 2022. Date of publication 15 April 2022; date of current version 11 October 2022. This work was supported in part by the National

one can strike a balance between the on-device computation

61

Key Research and Development Program under Project 2019YFB1803305, workload and the ofﬂoading communication overhead. Take 62

in part by the General Research Fund from the Research Grants Council the AlexNet [18] for example. Fig. 1 shows that a deeper 63

of Hong Kong under Project 14201920 and Project 14202421, in part by the splitting point (i.e., splitting the DNN at a later layer) in 64
National Natural Science Foundation of China under Project 61871271, in part
by the Key Project of Department of Education of Guangdong Province under the AlexNet leads to a larger local computation workload 65

Grant 2020ZDZX3050, in part by the Shenzhen Science and Technology and lower ofﬂoading data size. Besides, the model splitting 66

Program under Project JCYJ20210324093011030, and in part by the Open decisions are affected by the time-varying wireless channel 67
Research Project Program of the State Key Laboratory of Internet of Things
for Smart City (University of Macau) under Grant SKL-IoTSC(UM)-2021- fading, e.g., deep fading may lead to a large ofﬂoading cost 68

2023/ORPF/A03/2022. The associate editor coordinating the review of this from the WD to the edge server. The existing model splitting 69

article and approving it for publication was T. Q. S. Quek. (Corresponding methods [13]–[17] are based on ofﬂine optimization assuming 70
author: Suzhi Bi.)
Jia Yan and Ying-Jun Angela Zhang are with the Department of Information non-causal channel knowledge. However, in practice, it is dif- 71

Engineering, The Chinese University of Hong Kong, Hong Kong (e-mail: ﬁcult for a WD to predict the channel state information (CSI) 72

yanj@umn.edu; yjzhang@ie.cuhk.edu.hk).

at a forthcoming model splitting point. Therefore, the model

Suzhi Bi is with the College of Electronics and Information Engineering,

73

Shenzhen University, Shenzhen 518060, China, and also with the Peng Cheng splitting point selection is an online decision process, where 74

Laboratory, Shenzhen 518066, China (e-mail: bsz@szu.edu.cn).

decisions must be made based on the past and present observa- 75

Color versions of one or more ﬁgures in this article are available at tions of wireless channel conditions without any future channel 76

https://doi.org/10.1109/TWC.2022.3165824.

Digital Object Identiﬁer 10.1109/TWC.2022.3165824

knowledge.

77

1536-1276 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8355

problem [19]–[22] with ﬁnite horizon. We then solve 116

the optimal stopping problem by backward induction to 117

ﬁnd the optimal model splitting strategy (i.e., the optimal 118

stopping rule). Besides, we propose a suboptimal one- 119

stage look-ahead (1-sla) stopping rule, where the analyt- 120

ical expressions of the model splitting strategy can be 121

derived. Accordingly, we further analyze the optimality 122

probability of the 1-sla stopping rule.

123

• Optimal Model Placement Algorithm: Based on the opti- 124

mal stopping rule, we derive the long-term expected 125

cost of the WD as a function of the model placement 126

decision. Accordingly, the optimal model placement can 127

be obtained by enumerating the N + 1 possible decisions, 128

where N is the total number of layers of the DNN. 129

The brute-force search based model placement algorithm 130

Fig. 1. The intermediate feature size, computation workload and model parameter data size in the AlexNet.

is computationally expensive, mainly because evaluat- 131 ing each model placement decision involves the full 132

process of backward induction. To reduce the complexity, 133

we propose an efﬁcient 1-sla stopping rule based model 134

78 Prior work on device-edge co-inference assumes that the

placement algorithm. In particular, for a fully-connected 135

79 DNN model is completely stored at the WD. In practice,

multilayer perceptron with equal neurons at all the layers, 136

80 the DNN model needs to be updated from time to time

we show that the optimal model placement solution can 137

81 through a training process at the edge server [1], [2], [18].

be obtained in closed form.

138

82 Once the model parameters are regenerated, the WD needs • Performance Improvement: Our simulation results show 139

83 to download them from the edge server to facilitate on84 device inference. Such model parameter downloading process

that the optimality probability of 1-sla model splitting 140 strategy is as high as 0.9 in the AlexNet for any model 141

85 is time-consuming in a wireless system due to the large model

placement decision. Besides, we demonstrate that the 142

86 parameter size. For example, the total model parameter size 87 in bytes is in the order of 108 in the AlexNet. We argue

joint model placement and splitting algorithm signiﬁ- 143 cantly reduces the overall device-edge co-inference cost 144

88 that for device-edge co-inference, it is not necessary to place

under various DNN structures.

145

89 all the DNN layers at the WD. This is because the layers

90 after the model splitting point are never executed at the WD. 91 Noticeably, the model placement decision must be jointly

B. Related Work

146

92 optimized with the model splitting strategy, because the WD Existing work has extensively investigated the model split- 147

93 is not able to split at a layer that is not downloaded.

ting problem for device-edge co-inference [12]–[17]. Specif- 148

94 The model placement and splitting decisions are made on ically, [14] proposed a three-step framework including the 149

95 two different time scales. On one hand, we need to make the model splitting point selection, the communication-aware on- 150

96 model splitting decision for every DNN inference process due device model compression, and the task-oriented encoding for 151

97 to the fast variation of wireless channel conditions. On the intermediate features. The model splitting point is selected 152

98 other hand, the DNN model parameters are updated at a much via exhaustive search therein. The authors in [12] formulated 153

99 lower frequency than the DNN inference requests, and thus the model splitting problem as an integer linear programming 154

100 the model placement decision is made on a much larger time problem. In [15], a lightweight scheduler was designed to 155

101 scale [1], [2], [18].

partition the DNN. Reference [16] investigated the encoding 156

102 In this paper, we are interested in answering the following of the feature space for energy saving in edge-host partitioning 157

103 two key questions:

of DNN. The authors in [17] proposed a 2-step pruning 158

104 1) On a large time scale, how many layers of the DNN framework for DNN splitting in device-edge co-inference. The 159

105

model shall be placed at the WD, so that the expected key assumption in [12]–[17] is that the WD knows the CSI at 160

106

device-edge co-inference cost is minimized?

all the possible splitting points beforehand. This assumption, 161

107 2) On a fast time scale, how to choose the model split- however, does not hold in practice since the wireless channel 162

108

ting point to achieve the optimal tradeoff between the conditions at the forthcoming model decoupling points are 163

109

on-device computation and communication overhead random and unknown a priori.

164

110

when the future CSI is unknown?

Besides, the existing work [12]–[16] assumes that the WD 165

has already stored the whole DNN model to enable device- 166

111 The main contributions of this paper are summarized in the edge co-inference. This incurs signiﬁcant model placement 167

112 following.

cost when the DNN model is frequently updated. In [17], 168

113 • Online Model Splitting Strategy: For given model place- the WD downloads only part of the model that is needed for 169

114

ment decision, we formulate the optimal model split- device-side computation. Nevertheless, the model placement 170

115

ting point selection problem as an optimal stopping decision is pre-determined and not optimized therein. In this 171

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8356

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

Fig. 3. Representation of the DNN as a sequential task graph.

Fig. 2. An illustration of the model placement and model splitting for device-

Denote by N the total number of layers of the DNN, and 213

edge co-inference.

by M, 0 ≤ M ≤ N , the number of layers placed at the WD. 214

Suppose that the model can be used for K inference tasks 215

before it is updated. Suppose that the downloading time of 216

172 regard, the joint optimization of the model placement and the i-th DNN layer is τim. Then, the average model placement 217

173 online model partition strategy is largely overlooked in the cost per inference task is

218

174 literatures. This paper is among the ﬁrst attempts to ﬁll this 175 gap.

ψ(M ) =

M i=1

τim

.

K

(1) 219

176 C. Organization

We denote by hdi the wireless channel gain when the i-th 220

layer’s parameters are downloaded. The noise at the receiver 221

177 The rest of the paper is organized as follows. Section II is additive white Gaussian noise (AWGN) with zero mean and 222

178 179

introduces the system model and the problem formulation. The optimal model splitting and placement strategies based on

variance σ2. We denote the transmit power of the BS as Pd. Accordingly, the downloading data transmission rate for the

223 224

180 181 182 183 184

backward induction are proposed in Section III. In Section IV, we propose reduced-complexity algorithms based on the 1-sla stopping rule. We further propose a hybrid algorithm to balance the solution optimality and the computational complexity in Section IV. In Section V, simulation results are described.

i-th

layer’s

parameters

is

Rid

=

Wd log2(1 +

Pd hdi σ2

),

where

Wd is the ﬁxed downlink bandwidth. In this paper, we assume

that the downlink transmission rate Rid of the BS is ﬁxed.

In practice, due to the strong transmit power and stable power

225 226 227 228

185 Finally, we conclude the paper in Section VI.

supply at the BS, many power adaptation methods can be 229 applied to achieve ﬁxed target Rid by overcoming the effects 230

of shadowing and small scale fading of downlink wireless 231

186

II. SYSTEM MODEL AND PROBLEM FORMULATION

channels. Then, by denoting the data size of the i-th layer’s 232

187 188

A. System Model As shown in Fig. 2, we consider a mobile edge inference

parameters in bits as Di, the parameter downloading time of

layer

i

is

given

by

τim

=

. Di
Rdi

233 234

189 system with one base station (BS) and one WD. The BS In Fig. 3, we model the DNN based inference by a sequen- 235

190 191

is the gateway of the edge server and has a stable power supply. We consider a DNN-based application with the layered

tial task graph. Each vertex in the task graph represents a subtask, i.e., one layer of the DNN.1 We denote the compu-

236 237

192 network structure, where the parameters of the DNN need to tational workload of subtask i in terms of the total number 238

193 be periodically updated through a training process at the edge of CPU cycles as Li. Besides, each edge in the task graph 239

194 server. The edge server is interested in the inference result, i.e., represents that the input data of subtask i is the output of the 240

195 the output of the DNN. On the other hand, the input of the preceding subtask i − 1. We denote the input data size in bits 241

196 DNN (e.g., image for the DNN-based human face recognition) of subtask i as Ii. To reﬂect the fact that the input data is 242

197 is generated by the WD.

originated from the WD and the inference output is required 243

198 In most existing implementation, edge inference is either by the edge server, we introduce two virtual subtasks 0 and 244

199 executed on device (device-only inference) or fully ofﬂoaded N +1 as the entry and exit subtasks, respectively. In particular, 245

200 to the edge server (edge-only inference). To strike a balance subtasks 0 and N + 1 must be executed at the WD and the 246

201 between computation and communication overhead, we con- edge server, respectively. Speciﬁcally, L0 = LN+1 = 0.

247

202 sider a device-edge co-inference framework. The WD executes We deﬁne the model splitting point nk of inference task 248

203 the ﬁrst few layers of the DNN and forwards the intermediate k, k = 1, . . . , K, if subtasks 0 to nk − 1 are executed on the 249

204 features to the edge server that executes the remaining layers. WD and subtasks nk to N +1 are computed at the edge server. 250

205 To enable device-edge co-inference, the BS needs to place The model splitting point nk is determined by balancing the 251

206 the ﬁrst few layers of the DNN at the WD. Note that once local computing cost, uplink transmission cost of the input 252

207 the DNN model is updated, the BS shall re-send the model 208 to the WD. Take Fig. 2 for example. The BS deploys the 209 ﬁrst 3 layers (marked in red) at the WD. Then, the WD can

1In this paper, we consider the traditional sequential DNNs (e.g., AlexNet [18]), where input features ﬂow layer by layer straightforward. In this case, each vertex (i.e., subtask) in the task graph represents one layer

210 choose to split the DNN (i.e., stop local execution and ofﬂoad 211 the intermediate features) after computing 0, 1, 2 or all the 212 3 layers.

of such DNNs. It is worth noting that our proposed framework is applicable to all types of the DNNs [12-17]. Speciﬁcally, for the DNN models with branchy structures (e.g., ResNet [23]), each subtask in the sequential task graph represents an unit of such DNNs (e.g., one res-unit in ResNet).

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8357

253 data Ink of subtask nk, and the edge computing cost. In the B. Problem Formulation

295

254 following, we focus on a tagged inference task and drop the In this subsection, we ﬁrst formulate the optimal model 296

255 subscript k for notational brevity. Note that the model splitting splitting problem as an optimal stopping problem with ﬁnite 297

256 point is constrained by the number of layers placed at the WD. horizon M + 1. Then, we formulate the joint model placement 298

257 That is,

and splitting optimization problem in (12).

299

258

1 ≤ n ≤ M + 1.

(2) 1) Optimal Stopping Problem With Given Model Placement 300

Decision: Suppose that the ﬁrst M layers of the DNN are 301

259 In particular, n = 1 implies edge-only inference, while n = placed at the WD. That is, the WD has to ofﬂoad the 302

260 N + 1 implies device-only inference.

intermediate features for edge inference no later than stage 303

261 262

Denote by hn the channel gain when the WD ofﬂoads the input data In at the model splitting point n. We assume that

M +1. Let γ = {γ1, . . . , γM+1} denote a sequence of random variables representing the uplink SNR. Suppose that the WD

304 305

263 264

the transmit power of the WD is ﬁxed as P . Suppose that

the

instantaneous

signal-to-noise ratio

(SNR)

γn

=

P hn σ2

at

can only observe the past and current SNRs, but not the future ones. At stage n, having observed γn, the WD needs to

306 307

265 model splitting point n is random with probability density decide whether to continue computing the n-th layer locally 308

266 function (PDF) fn(·) and cumulative distribution function or stop local computing and ofﬂoad the n-th layer’s input data 309

267 (CDF) Fn(·).2 We assume that the γn is independent across (i.e., the (n − 1)-th layer’s output data) to the edge 310

268 different model splitting points [7], [24], [25]. Note that the server.

311

269 time difference between splitting points n and n + 1 is the If the WD decides to stop at stage n (i.e., split the model 312

270 local execution time of subtask n. In this paper, we assume at point n), then the total energy-time cost (ETC), deﬁned as 313

271 that the channel coherence time is shorter than or comparable the weighted sum of the WD’s energy consumption and time 314

272 with the local computing time of one DNN layer,3 such that on inference, is given by

315

273 the wireless channel conditions at two splitting points of the

274 local DNN model may change and are considered independent.

n−1

N +1

275 Accordingly, the data transmission rate from the WD to the

ηn(γn) = βt( τil +

τic + τnu(γn))

316

276 edge server at model splitting point n is

277

Rn(γn) = W log2(1 + γn),

(3)

i=0

i=n

n−1

+βe( eli + eun(γn))

317

278 where W is the ﬁxed bandwidth allocated to the WD. Then, 279 the ofﬂoading transmission time at model splitting point n is

280

τnu (γn )

=

In Rn(γn

)

,

(4) where

i=0
1 = ωn + (βtIn + βeP In) Rn(γn) ,

(9) 318
319

281 and the corresponding energy consumption is

282

eun(γn)

=

P

In Rn(γn)

.

(5)

ωn

=

n−1
βt(
i=0

Li fl

+

N +1 i=n

Li ) fc

n−1
+ βe( κLifl2).
i=0

(10) 320

Notice that ωn increases in n due to the more powerful 321

283 As for the on-device inference, we denote by fl the local computation capacity at the edge server (i.e., fl < fc). 322

284 CPU frequency for computing the subtasks. Then, the local Besides, βt and βe denote the weights of total inference 323

285 execution time for subtask i is

time and energy consumption, respectively. Note that the ETC 324

286

τil

=

Li , fl

(6) ηn(γn) is deterministically known to the WD since it observes 325

the SNR γn at stage n.

326

287 and the corresponding energy consumption is

The WD decides to continue local computing at stage n only 327

288

eli = κLifl2,

when doing so incurs a lower ETC than stopping at stage n. 328 (7) Note that the ETC of stopping at a future stage is random to 329

289 where κ is the effective switched capacitance parameter the WD, as it does not know the channel SNR in the future. 330

290 depending on the chip architecture.

A stopping rule determines the model splitting point 331

291 As for the edge inference, the edge computing time of S(M, γ) ∈ {1, 2, . . . , M + 1} based on the number of 332

292 subtask i is

downloaded layers M and the random SNR observations γ. 333

293

τic

=

Li , fc

Note that S(M, γ) is random, as it is a function of random 334 (8) variables γ. Different realizations of observations may lead 335
to different stopping decisions. Given the model placement 336

294 where fc is the CPU frequency of the edge server.

decision M , our purpose is to ﬁnd the optimal model splitting 337

2In this paper, we consider the channel statistics (i.e., fn(·) and Fn(·)) are known. Suppose that the wireless channel statistic is partially known. We can

strategy (i.e., optimal stopping rule) S∗(M, γ) to minimize the expected inference ETC, i.e., Eγ [ηS∗(M,γ)(γS∗(M,γ))].

338 339

ﬁrst apply data-driven methods to estimate the PDF and CDF of the SNR In optimal stopping theory, this problem is a stopping rule 340

from historical observed data. That is, we have a warm-up period to learn the problem with a ﬁnite horizon [19].

341

wireless channel statistics.

3In practice, the channel coherence time is about several hundred millisec-

2) Joint Optimization of Model Placement and Model Split- 342

onds, while executing one layer of the DNN usually takes several seconds. ting Strategy: Our goal is to ﬁnd the optimal model splitting 343

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8358

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

344 strategy S∗(M, γ) and select the optimal number of down345 loaded layers M, M ∈ {0, 1, . . . , N }, with the objective to 346 minimize the overall expected cost, deﬁned as the expected 347 ETC of the WD plus the weighted average model downloading 348 time cost, i.e.,

= ωM+1 + (βtIM+1 + βeP IM+1)

∞

1

×
0

RM+1(γM+1) fM+1(γM+1)dγM+1.

Inductively, at stage n,

386
(15) 387
388

349 Z(M, S∗(M, γ)) = βtψ(M ) + Eγ [ηS∗(M,γ)(γS∗(M,γ))].

E(Vn(M+1)) = E min ηn(γn), E(Vn(+M1+1))

389

350

(11)

∞

=

ηn(γn)fn(γn)dγn

390

351 Mathematically, the optimization problem is formulated as

352

(P1) min Z(M, S∗(M, γ)),

S ∗ (M ,γ ),M

γˆn(M )

+ γˆn(M) E(Vn(+M1+1))fn(γn)dγn

391

0

353

s.t. M ∈ {0, 1, . . . , N },

= ωn(1 − F (γˆn(M ))) + E(Vn(+M1+1))F (γˆn(M )) 392

354

S∗(M, γ) ∈ {1, 2, . . . , M + 1}. (12)

355 Notice that the above Problem (P1) is challenging due to the

+(βtIn + βeP In)

∞ γˆn(M )

fn(γn) Rn(γn)

dγn.

(16)

393

356 combinatorial nature of the integer model placement decision Accordingly, we can calculate E(Vn(M+1)) and γˆn(M ) in 394

357 M and the underlaying stopping rule problem given M .

(14) for all n ≤ M . According to (13), it is optimal to 395

358 III. OPTIMAL MODEL PLACEMENT AND ONLINE MODEL

359

SPLITTING STRATEGY

stop at stage n if the observed SNR γn > γˆn(M ), and to 396
continue otherwise. In other words, the optimal stopping rule 397 S∗(M, γ) is a threshold-based policy determined by γˆ(M ), 398

360 In this section, we ﬁrst assume a ﬁxed model placement where γˆ(M ) = {γˆ1(M ), . . . , γˆM (M )}. We therefore have the 399

361 decision M and investigate the optimal model splitting strategy following Proposition 3.1.

400

362 S∗(M, γ). Then, based on the analysis of the expected ETC Proposition 3.1: Given the number of downloaded layers 401

363 achieved by the optimal model splitting strategy, we propose M , the optimal stopping rule S∗(M, γ) is given by

402

364 the optimal model placement algorithm. 365 A. Backward Induction

S∗(M, γ) = min{Ψ}, Ψ = ∅ M + 1, otherwise,

(17) 403

366 Given a ﬁnite horizon M +1, we solve the optimal stopping where

404

367 problem by backward induction. Notice that when M = 0, the 368 WD has no choice but to ofﬂoad the data I1 to the edge server

Ψ = {n|1 ≤ n ≤ M, γn ≥ γˆn(M )},

(18) 405

369 at the ﬁrst stage. In the following, we consider the case where and γˆn(M ) is deﬁned in (14).

406

370 the number of downloaded layers M ≥ 1.

According to Proposition 3.1, when Ψ = ∅, our proposed 407

371 With backward induction, we ﬁrst ﬁnd the optimal model online model splitting strategy is to split the DNN model at the 408

372 splitting strategy at stage M . Then, we ﬁnd the optimal ﬁrst stage that the observed SNR γn is larger than or equal to 409

373 splitting strategy at stage M − 1, taking the decision at stage the corresponding threshold γˆn(M ). In addition, S∗(M, γ) = 410

374 M as given. The process continues backward until the ﬁrst M + 1 when Ψ = ∅.

411

375 stage. Let Vn(M+1) denote the minimum expected inference

376 ETC for splitting the model starting from stage n, n ≤ M , B. Expected Inference ETC Performance

412

377 given the current observation γn. That is,

In this subsection, we analyze the expected ETC 413

378

Vn(M+1) = min ηn(γn), E(Vn(+M1+1))

Eγ [ηS∗(M,γ)(γS∗(M,γ))] achieved by the optimal model spit- 414

379

=

ηn(γn),

γn > γˆn(M );

E(Vn(+M1+1)), γn < γˆn(M ),

ting. If M = 0, the expected ETC Eγ [ηS∗(M,γ)(γS∗(M,γ))] = 415 (13) E(V1(1)). In the following, we consider the case where M ≥ 1. 416
Speciﬁcally, we denote the probability of stopping at stage n 417

380 where

as P r{S∗(M, γ) = n}. Then, we have

418

381 382 383 384

(βt In+βeP In)
γˆn(M ) = 2 [ ] W E(Vn(M +1+1))−ωn − 1.

At the last stage M + 1, we have

E(VM(M++1 1))

∞

M

N +1

=

βt( τil +

τic + τMu +1(γM+1))

0

i=0

i=M +1

(14)

P r{S∗(M, γ⎧) = n}

=

⎪⎪⎪⎪⎨1

− F1(γˆ1(M

n−1 j=1

Fj

(γˆj

)), (M

))

⎪⎪⎪⎪⎩×

(1 − Fn(γˆn(M )))

M j=1

Fj

(γˆj

(M

)),

,

n = 1;
1 < n < M + 1; n = M + 1.

(19)

419 420

Speciﬁcally, for 1 < n < M + 1, stage n will be reached 421

M

if all the observed SNRs at its preceding stages j, ∀j < n, 422

385

+βe( eli + euM (γM+1)) fM+1(γM+1)dγM+1
i=0

are smaller than the corresponding derived threshold γˆj(M ). 423 Then, we will stop at stage n if the observed SNR γn is larger 424

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8359

425 than the threshold γˆn(M ). Besides, the expected ETC ηn(γn) statistic characteristics of wireless channels do not change. 460

426 when S∗(M, γ) = n is calculated as

We claim that although solving Problem (P1) incurs O(N 2) 461

427 Eγ [η⎧S∗(M,γ)|S∗(M, γ) = n]

⎪⎪⎪⎪⎪⎪⎪⎪⎨ωn

+ n

Eγn (βtIn ≤ M;

+

βeP

In

)

1 Rn(γn

)

|γn

>

γˆn(M )

,

computational complexity, the online implementation for the 462 model splitting follows a simple threshold-based policy with 463 low complexity. That is, we simply need to compare the 464 observed SNR related to the wireless channel gain with a 465 threshold to decide whether to ofﬂoad at the current model 466

428

=

⎪⎪⎪⎪⎪⎪⎪⎪⎩ωn

+ n

Eγn =M

(βtIn + 1,

+

βeP

In)

1 Rn(γn)

,

(20) splitting point, where the overhead of such decision making is 467 negligibly small. Once the optimal model placement decision 468 is determined, the corresponding thresholds γˆ(M ) for the 469 optimal stopping rule are simultaneously obtained and stored 470

429 where the expectation is taken over the random SNR γn at at the WD. This facilitates the online model splitting for every 471

430 stage n. That is,

DNN inference according to Proposition 3.1.

472

431 Eγ [ηS∗(⎧M,γ)|S∗(M, γ) = n]

IV. REDUCED-COMPLEXITY ALGORITHMS

473

⎪⎪⎪⎪⎪⎪⎪⎨ωn

+ (βtIn
∞ γˆn(M )

+ βeP In)×

Rn

1 (γn

)

fn

(γn

)dγn

,

n ≤ M;

The optimal model placement algorithm through exhaustive 474 search results in O(N 2) computational complexity due to the 475

432

=

⎪⎪⎪⎪⎪⎪⎪⎩ωn

1 − Fn(γˆn(M ))

+ (βtIn + βeP In)×

∞ 0

Rn

1 (γn)

fn(γn

)dγn,

(21) full process of backward induction when evaluating each feasi- 476

ble model placement decision. In this section, we are motivated 477

n = M + 1.

to investigate linear-complexity, i.e., O(N ) algorithms based 478

on a one-stage look-ahead stopping rule.

479

433 Therefore, the expected ETC given that the ﬁrst M layers are

434 downloaded to the WD is

A. One-Stage Look-Ahead Stopping Rule for Model Splitting 480

435

Eγ [ηS∗(M,γ)(γS∗(M,γ))]

First, we introduce the deﬁnition of one-stage look-ahead 481

M +1

436

= P r{S∗(M, γ) = n}

stopping rule.

482

Deﬁnition 1 (One-Stage Look-Ahead Stopping Rule): The 483

437

n=1
×Eγ [ηS∗(M,γ)|S∗(M, γ) = n], M ≥ 1.

one-stage look-ahead (1-sla) stopping rule is the one that 484 (22) stops if the cost for stopping at this stage is no more than 485

438 By substituting (19) and (21) into (22), the expected ETC the expected cost of continuing one stage and then stopping. 486

439 Eγ [ηS∗(M,γ)(γS∗(M,γ))], and hence the total expected cost Mathematically, the 1-sla stopping rule is described by the 487

440 Z(M ) in (11), can be expressed as a function of M .

stopping time [19]

488

441 C. Optimal Model Placement

S1 = min{n ≥ 1 : ηn ≤ Eγn+1[ηn+1|γ1, . . . , γn]}. (23) 489

442 In this subsection, we are ready to optimize the model place- Note that the backward-induction based optimal stopping 490

443 ment decision based on the optimal stopping rule S∗(M, γ) rule in Section III accounts for the ETC for all future stages 491

444 derived in Proposition 3.1 and the analysis of expected ETC until the last one. In contrast, the decision making at each 492

445 in Section III B. Intuitively, one can enumerate all feasible stage under the 1-sla stopping rule only depends on the current 493

446 M ∈ {0, 1, . . . , N } and select the optimal one that achieves observation and the expected performance of continuing for 494

447 the minimal expected cost Z(M ).

just one stage.

495

448 Notice that the decision threshold γˆn(M ) for the optimal In the following, we ﬁrst derive the 1-sla stopping rule for 496

449 stopping rule involves a nested expectation of ETC in future solving (P1) given the number of downloaded layers M .

497

450 stages (see (14) and (16)). For given M , it takes O(M ) Proposition 4.1: Given the number of downloaded layers 498

451 complexity to compute γˆn(M ) using backward induction. M , the 1-sla stopping rule S1(M, γ) is given by

499

452 Then, by exhausting all feasible model placement decisions, 453 the joint optimization of model placement and splitting for 454 solving Problem (P1) incurs a polynomial computational time

S1(M, γ) =

min{Ω}, Ω = ∅ M + 1, otherwise,

(24) 500

455 complexity O(N 2).

456 Remark 3.1: Notice that the optimal model placement deci- where

501

457 sion and online model splitting strategy obtained by solving 458 Problem (P1) remain the same as long as the DNN model

Ω = {n|1 ≤ n ≤ M, γn ≥ γˆn1−sla},

(25) 502

459 structure, the model update frequency parameter, and the and γˆn1−sla is given by (26), shown at the bottom of the page. 503

γˆ = 2 − 1. 1−sla
n

1 W

In+1

(βt

+βe

P

)

Ê∞ 0

βtIn +βeP In

1 Rn+1

fn+1 (γn+1 )dγn+1 +βt (

Ln fl

−

Ln fc

)+βeκLn fl2

(26)

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8360

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

504

Proof: According to Deﬁnition 1, the 1-sla stopping rule Suppose that the 1-sla calls for stopping at stage n and 552

505 is

An+1, . . . , AM also hold. First, for AM , we have

553

506 S1(M, γ)

507

= min 1 ≤ n ≤ M : ηn ≤ E[Vn(+n+1 1)]

ηM ≤ E(ηM+1|γ1, . . . , γM ) = E(VM(M++1 1)|γ1, . . . , γM ).

554

Hence,

555

508

= min 1 ≤ n ≤ M : ηn ≤ Eγn+1 [ηn+1|γ1, . . . , γn]

VM(M+1) = min{ηM , E(VM(M++1 1)|γ1, . . . , γM )} = ηM .

556

1

509

= min 1 ≤ n ≤ M : ωn + (βtIn + βeP In) Rn

Then, for AM−1, we have

557

∞

510

≤

ηn+1 (γn+1 )fn+1 (γn+1 )dγn+1

ηM−1 ≤ E(ηM |γ1, . . . , γM−1) = E(VM(M+1)|γ1, . . . , γM−1). 558

0

Hence,

559

511

= min 1 ≤ n ≤ M : γn ≥ γˆn1−sla ,

(27) VM(M−+1 1) = min{ηM−1, E(VM(M+1)|γ1, . . . , γM−1)} = ηM−1. 560

512 where γˆn1−sla is given in (26). 513 When Ω = {n|1 ≤ n ≤ M, γn ≥ γˆn1−sla} = ∅, then

Similarly, for n < k < M −1, we have Vk(M+1) = ηk. Finally,

561

514 S1(M, γ) = M + 1.

for An, we have

562

515 From Proposition 4.1, we have the following observations:

516 • The decision made in each stage n ∈ [1, M ] in the 1-sla

ηn ≤ E(ηn+1|γ1, . . . , γn) = E(Vn(+M1+1)|γ1, . . . , γn).

563

517

stopping rule only depends on the expectation of the unit

518

transmission

delay

1 Rn+1

at

the

next

stage

n+1

and

Hence,

564

519 520

the model parameters in the n-th layer (i.e., the input and output data sizes of the n-th layer In, In+1 and the

Vn(M+1) = min{ηn, E(Vn(+M1+1)|γ1, . . . , γn)} = ηn.

565

521

computation workload Ln of the n-th layer), regardless Therefore, the optimal stopping rule S∗(M, γ) also calls for 566

522

of M .

stopping at stage n.

567

523 • Compared with the calculation of γˆn in the optimal stop- Based on the above lemma, we are ready to derive the 568

524 525

ping rule via backward induction, the decision threshold γˆn1−sla in S1(M, γ) is much easier to calculate without

probability that 1-sla stopping rule is optimal. Proposition 4.2: Given the number of downloaded layers M ,

569 570

526

the nested structure for the expected ETC in all future the 1-sla stopping rule S1(M, γ) obtained by (24) is optimal 571

527

stages.

with probability

572

528 • The probability of stopping at stage n decreases when

529

the input data size In of stage n increases, the input data

530

size of the next stage In+1 is smaller, or the difference

531

between the local and edge execution costs for layer n is

532

lower.

533 According to Proposition 4.1, when Ω = ∅, the 1-sla

P r{S1(M,⎡γ) = S∗(M, γ)}

⎤

573

M+1 n−1

M

=

⎣ Fj (γˆj1−sla) [1 − Fk(γˆk1−sla)]⎦ . (28) 574

n=1 j=1

k=n

534 stopping rule based online model splitting scheme is to split

Proof: If the 1-sla stopping rule S1(M, γ) obtained by 575

535 the model at the ﬁrst point where γn ≥ γˆn1−sla occurs.

(24) calls for stopping at stage n, then it will also call for 576

536 The 1-sla stopping rule is not optimal in general. According stopping at all future stages with probability

577

537 to [19], the 1-sla rule is optimal in a ﬁnite horizon monotone

538 stopping rule problem. In the following Lemma 4.1 and P r{Stage n is reached and An, An+1, . . . , AM hold}

578

539 Proposition 4.2, we show that the proposed 1-sla based model 540 splitting strategy S1(M, γ) is optimal with certain probability.

n−1

M

=

Fj (γˆj1−sla) [1 − Fk(γˆk1−sla)]. (29) 579

541 Lemma 4.1: Let An denote the event {ηn ≤

j=1

k=n

542 543 544 545 546

E[ηn+1|γ1, . . . , γn]}, i.e., the 1-sla calls for stopping at stage n, for a given n ∈ [1, M ]. Suppose that An holds. Then, if An+1, . . . , AM also hold, the 1-sla stopping rule S1(M, γ) is optimal, i.e., S1(M, γ) = S∗(M, γ).
Proof: Recall that the optimal stopping rule is

Then, according to Lemma 4.1, we can obtain the probability P r{S1(M, γ) = S∗(M, γ)} as shown in (28).
Corollary 4.1: When the number of downloaded layers
M = 1, the 1-sla stopping rule is optimal, i.e., S1(M, γ) = S∗(M, γ).

580 581 582 583 584

547 S∗(M, γ) = min{n ≥ 1 : ηn ≤ E(Vn(+M1+1)|γ1, . . . , γn)},

Proof: According to Proposition 4.2, when M = 1, 585

we have

586

548 where VM(M++2 1) = +∞, VM(M++1 1) = ηM+1, and by backward 549 induction,

P r{S1(1, γ) = S∗(1, γ)}

587

550

Vn(M+1) = min{ηn, E(Vn(+M1+1)|γ1, . . . , γn)},

= [1 − F1(γˆ11−sla)] + F1(γˆ11−sla) = 1. (30) 588

551 for n = 1, . . . , M .

Hence, when M = 1, the 1-sla stopping rule is optimal. 589

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8361

590 B. 1-Sla Stopping Rule Based Model Placement

591 Following the similar technique in Section III

592 B, we can analyze the expected ETC performance

593 Eγ [ηS1(M,γ)(γS1(M,γ))], and hence the overall expected 594 cost Z(M ), as a function of model placement M under the

595 1-sla stopping rule. Then, we can enumerate all feasible 596 M ∈ {0, 1, . . . , N } to ﬁnd M ∗ that yields the minimal

597 expected cost Z(M ).

598 It takes O(N ) complexity to compute all possible

599 {γˆn1−sla, ∀n ∈ [1, N ]} in S1(M, γ) according to (26). Besides,

600 the complexity of enumerating all feasible model placement 601 decisions is O(N ). Notice that the decision threshold γˆn1−sla

Fig. 4. An illustration of the ﬂoating-point multiply-add calculations in one neuron of layer i, where {w1, w2, . . .} and b are the parameters corresponding

602 under the 1-sla stopping rule is independent of the model to that neuron in layer i and φ(·) is the nonlinear activation function.

603 placement M . Therefore, the overall computational complexity

604 for solving Problem (P1) reduces to linear complexity O(N )

605 under the 1-sla stopping rule based algorithm.

In the following, we assume that the number of neurons in 643

each layer remains the same (i.e., Xi−1 = Xi, ∀i). For the 644

606 C. Case Study: Fully-Connected Multilayer Perceptron

simplicity of analysis, we assume that the SNR γn in each 645

607 608 609 610 611

To obtain more engineering insights, we consider fullyconnected multilayer perceptron (MLP) networks in this subsection. We will show that under certain assumptions, the closed-form expressions of the optimal model placement M ∗ can be derived when the model splitting decision is based on

stage is identically distributed, i.e., f1(·) = f2(·) = . . . = f (·) and F1(·) = F2(·) = . . . = F (·).
Lemma 4.2: For a fully-connected MLP with Xi−1 = Xi = X, ∀i, the 1-sla stopping threshold γˆn1−sla in (26) is a constant, i.e., γˆ11−sla = . . . = γˆN1−sla = δ(X), where

646 647 648 649 650

612 the 1-sla stopping rule.

δ(X )

651

613 Fully-connected MLP [26] is a class of feedforward artiﬁcial 614 neural network, which consists of an input layer, multiple

= 2 − 1. 1 W

λ(βt

+βe

P

)

Ê∞ 0

(βt +βeP )λ

1 Rn+1

f (γn+1)dγn+1+[βt(

1 fl

−

1 fc

)+βeκfl2 ]αX

652

615 hidden layers and an output layer. Except for the nodes of 616 the input layer, each node of the other layers is a neuron that

(34) 653

617 uses a nonlinear activation function.

Lemma 4.2 indicates that for a fully-connected MLP with 654

618 Suppose that layer i has Xi neurons with input data size ﬁxed number of neurons in each layer, the 1-sla stopping rule 655

619

Ii = λXi−1,

(31)

is to compare the observed SNR γn with a constant δ(X). To derive the model placement solution, the follow-

656 657

620 where λ is the number of bytes to represent the original output ing lemma studies the relation between the expected ETC 658

621 value of each neuron. In Fig. 4, we illustrate the ﬂoating- Eγ [ηS1(M,γ)(γS1(M,γ))] and the number of downloaded layers 659

622 point multiply-add calculations in one neuron of layer i. Each M under the 1-sla stopping rule.

660

623 neuron in layer i ﬁrst performs Xi−1 multiply-add operations Lemma 4.3: With the increase of M , the expected inference 661

624 on the input data received from the previous layer using the ETC Eγ [ηS1(M,γ)(γS1(M,γ))] under the 1-sla rule decreases. 662

625 weights (e.g., {w1, w2, . . . , wXi−1 } in Fig. 4) and biases (e.g., That is,

663

626 b in Fig. 4) deﬁned by the model parameters. Then, the result is 627 further processed by a nonlinear activation function (e.g., φ(·)

Θ1−sla(M )

664

628 in Fig. 4). It can be seen that the number of model parameters

= Eγ [ηS1(M,γ)(γS1(M,γ))]

665

629 required for the inference of layer i and the total computation 630 workload of layer i depend on the number of neurons in layer 631 i − 1 and i.

−Eγ [ηS1(M−1,γ)(γS1(M−1,γ))] < 0, ∀M ∈ [1, N ].

666

(35) 667

632 From the above, we see that the computation workload of

Proof: The difference between the expected ETC when 668

633 each layer is proportional to Xi−1Xi, i.e.,

downloading M and M − 1 layers is calculated in (36), shown 669

634

Li = αXi−1Xi,

(32)

at the bottom of the next page. According to (24), ∀γM < γˆM 1−sla, we have

635 where α is the number of CPU cycles required to execute one

1

636 ﬂoating-point multiply-add operation. Likewise, the number of 637 model parameters needed by layer i, including the weights and

ωM + (βtIM + βeP IM ) RM (γM )

638 biases, is (Xi−1 + 1)Xi. Hence, the parameter downloading 639 time for layer i is

640

τim

=

μ(Xi−1 + Rd

1)Xi ,

(33)

> ωM+1 + (βtIM+1 + βeP IM+1)

∞

1

×
0

RM+1(γM+1) f (γM+1)dγM+1.

Therefore, we have Θ1−sla(M ) < 0, ∀M ∈ [1, N ].

670 671
672 673
(37) 674
675

641 where μ is the number of bytes to represent each model From Lemma 4.3, we observe that when the edge server 676

642 parameter and Rd is the ﬁxed downlink transmission data rate. places more layers to the WD (i.e., larger M ), the expected 677

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8362

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

678 ETC decreases. Nevertheless, according to (1), a larger down- Then, we have

702

679 loading time cost ψ(M ) occurs when M increases.

680 Corollary 4.2: When the model update frequency parameter

681 K is sufﬁciently large, the optimal M ∗ = N .

È 682

Proof: When K is sufﬁciently large, the average model

683 downloading time cost ψ(M ) =

M i=1

τim

K

diminishes. Then,

684 according to Lemma 4.3, we have M ∗ = N .

Δ1−sla(M )

703

=

Z(M ) − Z(M

−

1)

=

μX(X + 1) K Rd

+

Θ1−sla(M )

704

=X

[F (δ(X))]M g(δ(X))

+

μ(X + 1) K Rd

.

(41) 705

685 Based on Lemma 4.3, we are ready to derive the optimal

686 model placement solution in the following proposition. 687 Proposition 4.3: If the fully-connected MLP satisﬁes

Since g(δ(X)) < 0 and 0 < F (δ(X)) <

1,

[F (δ(X))]M g(δ(X))

+

μ(X +1) K Rd

increases

with

M.

706 707

688 Xi−1 = Xi = X, ∀i and the model splitting is based on the Accordingly, if Δ1−sla(N ) < 0, i.e., [F (δ(X))]N g(δ(X)) + 708

689 690 691 692 693
694

1-sla stopping rule, then the optimal number of downloaded layers is given by (38), shown at the bottom of the page, where · is the rounding function and g(δ(X)) is given in (40).
Proof: For the fully-connected MLP with Xi−1 = Xi = X, ∀i, we ﬁrst rewrite Θ1−sla(M ) in (35) as
Θ1−sla(M ) = X[F (δ(X))]M g(δ(X)), 1 ≤ M ≤ N, (39)

μ(X +1) K Rd

<

0,

Z(M )

decreases

with

M

and

we

have

M∗

=

N.

If Δ1−sla(1) > Z(M ) increases

0, i.e., [F (δ(X))]g(δ(X)) + with M , and thus M ∗ = 0.

μ(X +1) K Rd

>

0,

Otherwise, the root of Δ1−sla(M ) = 0 can be cal-

culated as M ∗ =

logF (δ(X))

μ(X +1) −K Rd g(δ(X ))

. We have

Δ1−sla(M ) < 0 when M ≤ M ∗ and Δ1−sla(M ) >

709 710 711 712 713 714

695 where

0 when M > M ∗. That is, Z(M ) decreases with M when 715

696 g(δ(X ))

697

=

1 αXβt( fl

−

1 fc)

+

αX βeκfl2

+

(βt

+

βeP )λ

M ≤ M ∗ and Z(M ) increases with M when M > M ∗. 716

In this case, the optimal number of downloaded layers is 717

logF (δ(X))

μ(X +1) −K Rd g(δ(X ))

.

718

∞

1

698

×(
0

RM+1(γM+1) f (γM+1)dγM+1

From Proposition 4.3, we have the following observations: 719 • When the total number of layers N increases, the BS 720

699

−

δ(X ) 0

RM

1 (γM

)

f

(γM

)dγM

)

F ⎛(δ(X ))

⎞

700

=

(βt

+

βeP

)λ

⎝

RM

1 (δ(X

))

−

δ(X ) 0

RM

1 (γM

)

f

(γM

)dγM

⎠

.

F (δ(X))

tends to place part of the model to the WD, instead 721

of downloading the whole DNN. Speciﬁcally, for a suf- 722

ﬁciently large DNN model (i.e., N → ∞), we have 723

[F (δ(X))]N g(δ(X)) → 0 and the optimal model place- 724

ment decision M ∗ < N .

725

• If the downlink data transmission rate increases (e.g., 726

701

(40)

meeting better downlink wireless channel conditions or 727

Θ1−sla(M ) = E⎛γ [ηS1(M,γ)(γS1(⎞M,γ))] − Eγ [ηS1(M−1,γ)(γS1(M−1,γ))]

M −1

=⎝

F (γˆj1−sla)⎠ (1 − F (γˆM 1−sla))

j=1

ωM + (βtIM + βeP IM )

∞ γˆM 1−sla

RM

1 (γM

)

f

(γM

)dγM

1 − F (γˆM 1−sla)

+F (γˆM 1−sla)

∞

1

ωM+1 + (βtIM+1 + βeP IM+1)
0

RM+1(γM+1) f (γM+1)dγM+1

∞1

− ωM + (βtIM + βeP IM )

⎛

⎞

0

RM (γM ) f (γM )dγM

M
= ⎝ F (γˆj1−sla)⎠
j=1

ωM+1 + (βtIM+1 + βeP IM+1)

∞ 0

RM

1 +1 (γM

+1

)

f

(γM

+1)dγM

+1

⎛ − ⎝ωM + (βtIM + βeP IM )

γˆM 1−sla 0

RM

1 (γM

)

f

(γM

)dγM

⎞ ⎠

F (γˆM 1−sla)

(36)

⎧

⎪⎪⎪⎪⎪⎨N, M ∗ = ⎪⎪⎪⎪⎪⎩0,logF (δ(X))

μ(X + 1) −K Rdg(δ(X ))

[F (δ(X))]N g(δ(X))

+

μ(X + 1) K Rd

<

0;

μ(X + 1)

F (δ(X))g(δ(X)) + KRd > 0;

, otherwise

(38)

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8363

Fig. 5. The considered autoencoder structure.

Fig. 6. The considered AlexNet structure.

728

stronger transmit power of the BS), more layers of the

729

DNN will be placed to the WD.

730 • When the model is updated less frequently

731

(i.e., a larger K), the optimal number of downloaded

732

layers M ∗ increases. Speciﬁcally, when K is sufﬁciently

733 734

large, holds,

[F (δ(X))]N g(δ(X)) + and we have M ∗ = N ,

μ(X +1)
K Rd
which is

< 0 always consistent with

735

Corollary 4.2.

736 According to Proposition 4.3, we can optimize the number 737 of downloaded layers M using the closed-form expression (38) 738 directly.

739 D. Hybrid Algorithm

Fig. 7. Optimality probability for one-stage look-ahead rule versus the number of downloaded layers M in the considered autoencoder.

740 Notice that it takes a linear computational complexity to

741 solve Problem (P1) when the model splitting decision is based ﬁrst ﬁve are convolutional and the remaining three are fully- 770

742 on the suboptimal 1-sla stopping rule. On the other hand, connected. The conﬁguration of the AlexNet follows [18], 771

743 according to Section III C, the backward-induction based where there are 1000 class lables and the input image size is 772

744 optimal model placement and splitting algorithm takes a poly- 227 ∗ 227 ∗ 3. We suppose that λ = μ = 8 Bytes, where λ and 773

745 nomial time complexity. As an alternative to balance between μ are deﬁned in (31) and (33), respectively. In addition, one 774

746 solution optimality and computational complexity, we propose ﬂoating-point multiply-add calculation requires α = 100 CPU 775

747 in this subsection a hybrid algorithm to solve the Problem cycles.

776

748 (P1). In particular, we ﬁnd the approximated optimal model The wireless channels are modeled as the Rayleigh block 777 749 placement decision M hybrid, assuming the 1-sla stopping rule. fading with large-scale path loss. In this case, the instantaneous 778

750 That is

SNR γn is exponentially distributed with PDF

779

751

M hybrid = arg min Z(M, S1(M, γ)).

(42)

M

fn(γn)

=

1

e , −

γn γ¯n

γ¯n

(43) 780

752 753

Then, we ﬁx M hybrid and ﬁnd the optimal model splitting point S∗(M hybrid, γ) using the backward induction algorithm

where

γ¯n

=

P σ2

Ad

(

3·108 4πf cd

)P

L

is

the

average

SNR

at

model

781

754 in Section III A. Based on the complexity discussions in splitting point n. Ad = 4.11 denotes the antenna gain, f c = 782

755 Section III C and Section IV B, the overall computational 915 MHz denotes the carrier frequency, d in meters denotes the 783

756 complexity of the hybrid algorithm is O(N ). The proposed distance between the WD and the BS, and P L = 3 denotes 784

757 hybrid algorithm also has a linear computational complexity, the pass loss exponent. The transmit power of the WD and 785

758 while achieving a better performance compared with the 1-sla 759 stopping rule based algorithm in Section IV B.

the BS is 100 mW and 1 W, respectively. The noise power σ2 = 10−10 W. We set the computing efﬁciency parameter κ = 10−26, and the bandwidth W = 2 MHz. The weights of

786 787 788

760

V. SIMULATION RESULTS

energy consumption and time of the WD are set as βt = βe = 789

0.5. The CPU frequencies at the WD and the edge server are 790

761 In this section, we evaluate the proposed algorithms with 108 and 1010 cycles/second, respectively.

791

762 different DNN architectures. Speciﬁcally, we consider the

763 classical autoencoder and AlexNet. As shown in Fig. 5, the

764 considered autoencoder consists of one input layer, one output A. Optimality Analysis for One-Stage Look-Ahead

792

765 layer, and seven hidden layers, where the number of neurons in Stopping Rule

793

766 the hidden layers is {Xi} = {128, 64, 32, 10, 32, 64, 128}. In Fig. 7 and Fig. 8, we plot the optimality probability 794 767 We set the number of neurons in the output layer as 784, which of the 1-sla stopping rule P r{S1(M, γ) = S∗(M, γ)} as a 795
768 is the same as that in the input layer. Likewise, as shown function of M when d = 50 meters with the autoencoder 796

769 in Fig. 6, the considered AlexNet contains eight layers: the and AlexNet, respectively. We observe that the optimality 797

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8364

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

Fig. 8. Optimality probability for one-stage look-ahead rule versus the Fig. 9. Performance gap between backward induction and one-stage look-

number of downloaded layers M in the considered AlexNet.

ahead rule when K = ∞ in the considered autoencoder.

798 probability decreases with M for both the autoencoder and

799 AlexNet. It is because the 1-sla stopping rule becomes more

800 myopic when the optimal stopping problem has a larger

801 horizon (i.e., M is large). It can be seen that when M = 1, 802 P r{S1(M, γ) = S∗(M, γ)} = 1, which means that the 1803 sla stopping rule S1(1, γ) is equal to the optimal S∗(1, γ) in

804 this case. Besides, we observe that the optimality probability is

805 larger than 0.91 for any M with the AlexNet. This conﬁrms the

806 effectiveness of the 1-sla stopping rule. Besides, the optimality

807 probability of the 1-sla stopping rule with the autoencoder is

808 larger than 0.85 when M ≤ 3. We claim the effectiveness

809 of the 1-sla stopping rule for the autoencoder under some

810 system settings. We will demonstrate this point in Fig. 13 in Fig. 10. Performance gap between backward induction and one-stage look811 Section V C. One interesting observation is that the optimality ahead rule when K = ∞ in the considered AlexNet.

812 probability of the 1-sla stopping rule for the AlexNet is

813 814

larger than that for the autoencoder. It is due to the fact that compared with the autoencoder, higher computation workloads

increases when M grows. For example, Z grows from 8.73% to 12.48% when M increases from 2 to 8 in Fig. 9.

837 838

815 816 817 818 819

of the early convolutional layers in the AlexNet dominate the decision making, which leads to an early stopping with higher probability for the backward-induction based optimal stopping rule. In this case, the myopic 1-sla stopping rule is close to the optimum by accounting for the expected ETC of continuing

Compared to the autoencoder, the performance gap between the optimal and 1-sla stopping rules in the AlexNet is smaller, e.g., 0.782% when M = 8. It is because as illustrated in Fig. 7 and Fig. 8, the optimality probability of the 1-sla stopping rule for the AlexNet is always larger than that for

839 840 841 842 843

820 for just one stage.

the autoencoder. Moreover, Fig. 10 shows that the inference 844

821 B. Performance Evaluation When K = ∞

cost in the AlexNet remains stable as M varies from 2 to 8. 845 It is due to the fact that the heavy computation workloads 846

822 In this subsection, we consider the scenario where K = ∞, of the early convolutional layers in the AlexNet dominate the 847

823 i.e., the cost of model placement is negligible. In this case, decision making for the model splitting point selection. That is, 848

824 the overall average cost Z is equal to the average inference even though more layers are downloaded, the WD still prefers 849

825 cost Eγ [ηS(M,γ)(γS(M,γ))].

to stop at an early stage.

850

826 In Fig. 9 and Fig. 10, we illustrate the total average cost

827 Z versus the number of downloaded layers M when K = ∞ C. Performance Evaluation When K < ∞

851

828 and d = 50 meters. We observe that for both the autoencoder We now consider the general scenario where the DNN 852

829 and AlexNet, Z decreases with the increase of M for both parameters need to be updated from time to time, i.e., K < ∞. 853 830 optimal (through backward induction) and 1-sla stopping rules. In Fig. 11, we plot the optimal M ∗ as a function of the 854

831 It is because when more layers are downloaded from the distance d under different model update frequencies K with 855 832 edge server, the WD has more choices of the model splitting the autoencoder. It is observed that the optimal M ∗ increases 856

833 points, which improves the average inference performance. with the distance d. This is because a larger distance leads to 857

834 In particular, when M = 1, the 1-sla stopping rule has the a higher uplink ofﬂoading cost for the intermediate feature. 858

835 same performance as the optimal stopping rule. In addition, In this case, the average inference cost can be improved by 859

836 the gap of Z between the optimal and 1-sla stopping rules downloading more layers to the WD. Besides, we observe that 860

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8365

Fig. 11. The number of downloaded layers M versus the distance between Fig. 13. Total cost versus the distance between the WD and edge server

the WD and edge server in the considered autoencoder.

when K = 50 in the considered autoencoder.

Fig. 12. The number of downloaded layers M versus the distance between the WD and edge server in the considered AlexNet.

Fig. 14. Total cost versus the distance between the WD and edge server when K = 50 in the considered AlexNet.

Jointly considering the close-to-optimal performance and the 886 861 the optimal M ∗ increases with the model update frequency complexity of the hybrid algorithm, it is a preferred choice 887

862 parameter K, which means that the BS tends to download for practical implementation. One interesting observation is 888

863 more layers to the WD when each downloaded model can be that when the distance is small (e.g., below 30 meters for the 889

864 used for more inference requests. In particular, when K = ∞, autoencoder), the 1-sla rule based algorithm is also close to 890 865 all the layers of the DNN are downloaded, i.e., M ∗ = N . the optimum. This is because a smaller distance leads to a 891 866 Moreover, given K, the optimal M ∗ under 1-sla stopping rule smaller optimal M ∗ as illustrated in Fig. 11 and 12. Under 892

867 is not larger than that under the optimal backward induction a smaller M ∗, the 1-sla stopping rule has a higher optimality 893

868 rule due to the myopic property of the 1-sla stopping rule. probability. Likewise, for the AlexNet in Fig. 14, the 1-sla 894

869 In Fig. 12, we demonstrate the impact of distance d on stopping rule based scheme and the hybrid algorithm achieve 895

870 the optimal M ∗ in the AlexNet. Comparing with Fig. 11, close-to-optimal performances even when d is large.

896

871 we can draw two conclusions from Fig. 12. First, the BS

872 tends to download smaller number of layers to the WD for

873 the AlexNet. This is due to the larger computation workloads D. Complexity of the Proposed Algorithms

897

874 and higher model parameter sizes of the convolutional layers At last, we compare the computational complexity among 898

875 in the AlexNet compared with the autoencoder. This leads the proposed algorithms under different DNN structures, 899

876 to higher costs for the local computing and model parameter where the distance d = 50 meters and the model update 900 877 downloading. Second, the optimal M ∗ computed based on the frequency parameter K = 50. Speciﬁcally, we further consider 901

878 1-sla rule is close to that computed based on the backward the VGG 16 model, where the conﬁguration follows [27]. 902

879 induction rule. It is because according to Fig. 7 and 8, the As shown in Fig. 15, the 1-sla based and hybrid algorithms 903

880 1-sla stopping rule has higher optimality probability in the require shorter runtime than the backward induction based 904

881 AlexNet compared with the autoencoder.

algorithm. In particular, for the VGG 16, around 35.82% and 905

882 In Fig. 13 and Fig. 14, we plot the total average cost Z 30.46% lower average runtime is achieved by the 1-sla based 906

883 when the distance d varies and K = 50. Fig. 13 shows that and hybrid algorithms, respectively. Therefore, the hybrid 907

884 the hybrid algorithm outperforms the 1-sla stopping rule based algorithm can achieve the near-optimal performance at the 908

885 scheme and performs very closely to the optimal solution. price of a slight increase of the computational complexity 909

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

8366

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 21, NO. 10, OCTOBER 2022

[7] S. Bi, L. Huang, and Y.-J.-A. Zhang, “Joint optimization of ser- 952

vice caching placement and computation ofﬂoading in mobile edge 953

computing systems,” IEEE Trans. Wireless Commun., vol. 19, no. 7, 954

pp. 4947–4963, Jul. 2020.

955

[8] J. Yan, S. Bi, L. Duan, and Y.-J.-A. Zhang, “Pricing-driven ser- 956

vice caching and task ofﬂoading in mobile edge computing,” 957

IEEE Trans. Wireless Commun., vol. 20, no. 7, pp. 4495–4512, 958

Jul. 2021.

959

[9] Z. Lin, S. Bi, and Y.-J. Angela Zhang, “Optimizing AI service placement 960

and resource allocation in mobile edge intelligence systems,” 2020, 961

arXiv:2011.05708.

962

[10] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garcia, and D. Scaramuzza, 963

“Event-based vision meets deep learning on steering prediction for self- 964

driving cars,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 965

Jun. 2018, pp. 5419–5427.

966

[11] L. Liu, H. Li, and M. Gruteser, “Edge assisted real-time object detection 967

Fig. 15. Comparisons of normalized computation time under different algorithms.

for mobile augmented reality,” in Proc. 25th Annu. Int. Conf. Mobile Comput. Netw., Aug. 2019, pp. 1–16. [12] H. Li, C. Hu, J. Jiang, Z. Wang, Y. Wen, and W. Zhu, “JALAD:

968 969 970

Joint accuracy-and latency-aware deep structure decoupling for edge- 971

910 compared with the 1-sla based algorithm. This demonstrates

cloud execution,” in Proc. IEEE 24th Int. Conf. Parallel Distrib. Syst. 972

(ICPADS), Dec. 2018, pp. 671–678.

973

911 the effectiveness of the low-complexity algorithms.

[13] J. Shao and J. Zhang, “BottleNet++: An end-to-end approach for 974

feature compression in device-edge co-inference systems,” in Proc. 975

912

VI. CONCLUSION

IEEE Int. Conf. Commun. Workshops (ICC Workshops), Jun. 2020, 976

pp. 1–6.

977

913

This paper has studied the joint optimization of model

[14] J. Shao and J. Zhang, “Communication-computation trade-off in resource-constrained edge inference,” IEEE Commun. Mag., vol. 58,

978 979

914 placement and online model splitting strategy to minimize the

no. 12, pp. 20–26, Dec. 2020.

980

915 expected energy and time cost of device-edge co-inference. [15] Y. Kang et al., “Neurosurgeon: Collaborative intelligence between the 981

916 Given the model placement decision, we have formulated the

cloud and mobile edge,” ACM SIGARCH Comput. Archit. News, vol. 45, 982

no. 1, pp. 615–629, 2017.

983

917 problem of online model splitting as an optimal stopping [16] J. H. Ko, T. Na, M. F. Amir, and S. Mukhopadhyay, “Edge-host 984

918 problem with ﬁnite horizon. Then, we have proposed an 919 online algorithm based on backward induction to ﬁnd the opti920 mal stopping rule (i.e., the optimal model splitting strategy).

partitioning of deep neural networks with feature space encoding 985

for resource-constrained Internet-of-Things platforms,” in Proc. 15th 986

IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Nov. 2018, 987

pp. 1–6.

988

921 To simplify the analysis, we have proposed a 1-sla stopping [17] W. Shi, Y. Hou, S. Zhou, Z. Niu, Y. Zhang, and L. Geng, “Improving 989

922 rule for model splitting, based on which an efﬁcient algorithm

device-edge cooperative inference of deep learning via 2-step pruning,” 990

2019, arXiv:1903.03472.

991

923 is proposed to optimize the model placement decision. Under [18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation 992

924 a speciﬁc DNN structure, we have derived the closed-form

with deep convolutional neural networks,” in Proc. Adv. Neural Inf. 993

925

expressions for the 1-sla stopping rule based model placement.

Process. Syst., 2012, pp. 1097–1105. [19] T. S. Ferguson. Optimal Stopping

and

994
Applications. 995

926 We have further proposed a hybrid algorithm to balance

Accessed: 2008. [Online]. Available: https://www.math.ucla.edu/ 996

927 between the solution optimality and the computational com928 plexity. Simulation results have validated the beneﬁts of jointly 929 considering the model placement and splitting under various

~tom/Stopping/Contents.html

997

[20] Y. J. Zhang, “Multi-round contention in wireless LANs with multipacket 998

reception,” IEEE Trans. Wireless Commun., vol. 9, no. 4, pp. 1503–1513, 999

Apr. 2010.

1000

930 DNN structures.

[21] Q. Gu, Y. Jian, G. Wang, R. Fan, H. Jiang, and Z. Zhong, “Mobile edge 1001 computing via wireless power transfer over multiple fading blocks: An 1002

optimal stopping approach,” IEEE Trans. Veh. Technol., vol. 69, no. 9, 1003

931

REFERENCES

pp. 10348–10361, Sep. 2020.

1004

[22] J. Jia, Q. Zhang, and X. Shen, “HC-MAC: A hardware- 1005

932 [1] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang,

constrained cognitive MAC for efﬁcient spectrum management,” 1006

933

“Edge intelligence: Paving the last mile of artiﬁcial intelligence

IEEE J. Sel. Areas Commun., vol. 26, no. 1, pp. 106–117, 1007

934

with edge computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738–1762,

Jan. 2008.

1008

935

Aug. 2019.

[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for 1009

936 [2] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artiﬁcial

image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 1010

937

neural networks-based machine learning for wireless networks: A tuto-

(CVPR), Jun. 2016, pp. 770–778.

1011

938

rial,” IEEE Commun. Surveys Tuts., vol. 21, no. 4, pp. 3039–3071, [24] J. Yan, S. Bi, Y. J. Zhang, and M. Tao, “Optimal task ofﬂoading and 1012

939

4th Quart., 2019.

resource allocation in mobile-edge computing with inter-user task depen- 1013

940 [3] R. Collobert and J. Weston, “A uniﬁed architecture for natural language

dency,” IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 235–250, 1014

941

processing: Deep neural networks with multitask learning,” in Proc. 25th

Jan. 2020.

1015

942

Int. Conf. Mach. Learn., 2008, pp. 160–167.

[25] S. Guo, B. Xiao, Y. Yang, and Y. Yang, “Energy-efﬁcient dynamic 1016

943 [4] R. Szeliski, Computer Vision: Algorithms and Applications. Springer,

ofﬂoading and resource scheduling in mobile cloud computing,” in 1017

944

2010.

Proc. IEEE 35th Annu. IEEE Int. Conf. Comput. Commun. (INFOCOM), 1018

945 [5] N. D. Lane et al., “DeepX: A software accelerator for low-power deep

Apr. 2016, pp. 1–9.

1019

946

learning inference on mobile devices,” in Proc. 15th ACM/IEEE Int. [26] D. W. Ruck, S. K. Rogers, M. Kabrisky, M. E. Oxley, and 1020

947

Conf. Inf. Process. Sensor Netw. (IPSN), Apr. 2016, pp. 1–12.

B. W. Suter, “The multilayer perceptron as an approximation to a Bayes 1021

948 [6] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, and J. Du, “On-demand deep

optimal discriminant function,” IEEE Trans. Neural Netw., vol. 1, no. 4, 1022

949

model compression for mobile devices: A usage-driven model selection

pp. 296–298, Dec. 1990.

1023

950

framework,” in Proc. 16th Annu. Int. Conf. Mobile Syst., Appl., Services, [27] K. Simonyan and A. Zisserman, “Very deep convolutional networks for 1024

951

2018, pp. 389–400.

large-scale image recognition,” in Proc. ICLR, 2015, pp. 1–14.

1025

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

YAN et al.: OPTIMAL MODEL PLACEMENT AND ONLINE MODEL SPLITTING FOR DEVICE-EDGE CO-INFERENCE

8367

1026

Jia Yan (Member, IEEE) received the B.Eng. degree

Ying-Jun Angela Zhang (Fellow, IEEE) received 1056

1027

from the School of Electronic and Information

the Ph.D. degree from the Department of Electrical 1057

1028

Engineering, South China University of Technol-

and Electronic Engineering, The Hong Kong Uni- 1058

1029

ogy, Guangzhou, China, in 2017, and the Ph.D.

versity of Science and Technology.

1059

1030

degree in information engineering from The Chinese

She joined the Department of Information Engi- 1060

1031

University of Hong Kong in 2021. He is currently

neering, The Chinese University of Hong Kong, 1061

1032

a Postdoctoral Associate with the Department of

in 2005, where she is currently a Professor. Her 1062

1033

Electrical and Computer Engineering, University of

research interests focus on optimization and learning 1063

1034

Minnesota, Twin Cities, MN, USA. His research

in wireless communication systems.

1064

1035

interests focus on optimization and machine learning

Prof. Zhang is a Member-at-Large of the IEEE 1065

1036

techniques in wireless communications and network-

ComSoc Board of Governors. She is a co-recipient 1066

1037 ing, particularly in mobile edge computing and edge intelligence.

of the 2021 and 2014 IEEE ComSoc Asia–Paciﬁc Outstanding Paper Awards, 1067

the 2013 IEEE SmartGridComm Best Paper Award, and the 2011 IEEE 1068

Marconi Prize Paper Award on Wireless Communications. As the only winner 1069

from engineering science, she has won the Hong Kong Young Scientist Award 1070

2006, conferred by the Hong Kong Institute of Science. She has served on the 1071

organizing committees for many top conferences, such as IEEE GLOBECOM, 1072

1038

Suzhi Bi (Senior Member, IEEE) received the

1039

Ph.D. degree in information engineering from

1040

The Chinese University of Hong Kong in 2013.

1041

From 2013 to 2015, he was a Postdoctoral Research

1042

Fellow with the Department of Electrical and Com-

1043

puter Engineering, National University of Singapore.

1044

Since 2015, he has been with the College of

1045

Electronics and Information Engineering, Shenzhen

1046

University, China, where he is currently an Asso-

1047

ciate Professor. His research interests mainly involve

1048

in the optimizations in wireless information and

1049 power transfer, mobile computing, and wireless sensing. He has received

1050 the 2019 IEEE ComSoc Asia–Paciﬁc Outstanding Young Researcher Award.

ICC, VTC, and SmartGridComm. She was the Founding Chair of the IEEE 1073

ComSoc Technical Committee of Smart Grid Communications. She is an 1074

Associate Editor-in-Chief of IEEE OPEN JOURNAL OF THE COMMUNICA- 1075

TIONS SOCIETY and a member of the Steering Committees of IEEE TRANS- 1076

ACTIONS ON MOBILE COMPUTING, IEEE WIRELESS COMMUNICATIONS 1077

LETTERS, and IEEE SmartgridComm Conference. Previously, she has served 1078

as the Chair for the Executive Editor Committee of IEEE TRANSACTIONS 1079

ON WIRELESS COMMUNICATIONS and many years for the editorial boards of 1080

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, IEEE TRANSAC- 1081

TIONS ON COMMUNICATIONS, Security and Communication Networks jour- 1082

nal (Wiley), IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS 1083

special issues, IEEE INTERNET OF THINGS JOURNAL special issues, and 1084

IEEE Communications Magazine special issues.

1085

1051 He was a co-recipient of the 2021 IEEE ComSoc Asia–Paciﬁc Outstanding

1052 Paper Award and the Conference Best Paper Awards of IEEE SmartGridComm

1053 2013 and IEEE/CIC ICCC 2021. He is an Editor of IEEE TRANSACTIONS ON

1054 WIRELESS COMMUNICATIONS and IEEE WIRELESS COMMUNICATIONS

1055 LETTERS.

Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:57:19 UTC from IEEE Xplore. Restrictions apply.

