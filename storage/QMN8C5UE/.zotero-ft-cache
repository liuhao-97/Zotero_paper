976

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

Trafﬁc and Computation Co-Ofﬂoading With Reinforcement Learning in Fog Computing for
Industrial Applications
Yixuan Wang , Kun Wang , Senior Member, IEEE, Huawei Huang , Member, IEEE, Toshiaki Miyazaki , Senior Member, IEEE, and Song Guo , Senior Member, IEEE

Abstract—In the past decade, network data communication has experienced a rapid growth, which has led to explosive congestion in heterogeneous networks. Moreover, the emerging industrial applications, such as automatic driving put forward higher requirements on both networks and devices. On the contrary, running computation-intensive industrial applications locally are constrained by the limited resources of devices. Correspondingly, fog computing has recently emerged to reduce the congestion of contentcentric networks. It has proven to be a good way in industry and trafﬁc for reducing network delay and processing time. In addition, device-to-device ofﬂoading is viewed as a promising paradigm to transmit network data in mobile environment, especially for autodriving vehicles. In this paper, jointly taking both the network trafﬁc and computation workload of industrial trafﬁc into consideration, we explore a fundamental tradeoff between energy consumption and service delay when provisioning mobile services in vehicular networks. In particular, when the available resource in
Manuscript received April 23, 2018; revised October 10, 2018; accepted November 8, 2018. Date of publication November 29, 2018; date of current version February 1, 2019. This work was supported in part by the NSFC under Grant 61872195, Grant 61572262, and Grant 61872310; in part by the China Postdoctoral Science Foundation under Grant 2017M610252; in part by the China Postdoctoral Science Special Foundation under Grant 2017T100297; in part by the Shenzhen Basic Research Funding Scheme under research (JCYJ20170818103849343); in part by the Strategic Information and Communications R&D Promotion Programme (SCOPE 162302008), MIC, Japan; in part by the Open Research Fund of the Jiangsu Engineering Research Center of Communication and Network Technology, NJUPT; and in part by the National Engineering Research Center of Communications and Networking (Nanjing University of Posts and Telecommunications) under Grant TXKY17014. Paper no. TII-18-0982. (Corresponding author: Kun Wang.)
Y. Wang is with the National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing 210003, China (e-mail:, yxwang.cs@gmail.com).
K. Wang is with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail:, kwang@njupt.edu.cn).
H. Huang is with the Academic Center for Computing and Media Studies, Kyoto University, Kyoto 606-8501, Japan (e-mail:, hwhuang@ media.kyoto-u.ac.jp).
T. Miyazaki is with the School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu 965-8580, Japan (e-mail:, miyazaki@u-aizu.ac.jp).
S. Guo is with the Hong Kong Polytechnic University, Shenzhen Research Institute, Shenzhen 518057, China, and also with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail:, song.guo@polyu.edu.hk).
Color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TII.2018.2883991

mobile vehicles becomes a bottleneck, we propose a novel model to depict the users’ willingness of contributing their resources to the public. We then formulate a cost minimization problem by exploiting the framework of Markov decision progress (MDP) and propose the dynamic reinforcement learning scheduling algorithm and the deep dynamic scheduling algorithm to solve the ofﬂoading decision problem. By adopting different mobile trajectory traces, we conduct extensive simulations to evaluate the performance of the proposed algorithms. The results show that our proposed algorithms outperform other benchmark schemes in the mobile edge networks.
Index Terms—Computation ofﬂoading, fog computing, industrial application, reinforcement learning (RL), trafﬁc ofﬂoading.
I. INTRODUCTION
T HE volume of mobile network trafﬁc increases drastically with the exponentially growing number of mobile devices in the recent years. Thanks to the rapid development of wireless access technologies, cellular networks are able to transmit network trafﬁc at high rates. The trafﬁc of many applications is migrated from conventional datacenter to wireless edge networks gradually for less time delay. However, the consequence is that access networks become even more congested. For example, Cisco released that the global mobile network trafﬁc showed a growth by 74% in 2016 compared with that of the previous year [1]. It is also forecasted that the global mobile network trafﬁc will exceed 30.6 exabytes each month by 2020. In addition to tablets and smartphones, Cisco also reported that the emerging device-to-device (D2D) modules will further contribute to the explosion of network trafﬁc. In the future, vehicular network also has potential demand on the network services, making the network more congested.
Moreover, lots of industrial applications, such as ﬂeet tracking and emerging automatic driving, are delay sensitive and computing hungry. As a result, network devices often exceed their capacity [2]. Although cloud computing enables ﬂexible conﬁguration of hardware resources, and allows both the network trafﬁc and the computation-intensive tasks to be uploaded to cloud. This is not an efﬁcient approach for industrial applications, as it may not only impose a huge heavy burden on congested core networks [3], but also cause intolerable transmission delay that degrades the quality of service [4], [5].

1551-3203 © 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

WANG et al.: TRAFFIC AND COMPUTATION CO-OFFLOADING WITH REINFORCEMENT LEARNING

977

As a solution to the dilemma, fog computing, which is also known as edge computing [6], has been widely adopted to direct partial workloads to be processed locally without transferring them to the centralized remote cloud. In the mobile network environment, network edge facilities, for instance, base stations and routers are equipped with computing and storage capabilities to meet devices’ requirements. Therefore, the service latency can be substantially reduced because vehicles are close to data sources.
The advantages obtained from ofﬂoading network trafﬁc and computation in fog computing include reducing both the task response time and the energy consumption for industrial applications. Edge ofﬂoading particularly beneﬁts the industrial applications, which require massive computation on a small volume of data. Nevertheless, as mentioned, there are also some industrial applications without heavy computation but with huge volume of data, e.g., product tracking. An option is to exploit the idle resources from other devices through D2D communications [7]. This manner is also called the D2D ofﬂoading. It has been shown in some studies that mobile trafﬁc can be signiﬁcantly reduced by D2D communication [8], especially when many network nodes in an event request the same content from an edge server. For instance, in vehicular networks for industrial trafﬁc, lots of local and network data and intensive computing power are needed. In addition, we cannot ignore the applications that are with both huge volume and intensive computation. It is a valuable research issue to ﬁnd the optimal policy to ofﬂoad the network tasks and trafﬁc.
In order to reduce the latency of industrial applications, devices can run the industrial applications locally because no transmission or queueing delay incurs. Nevertheless, running applications locally will consume a lot of energy, it may shorten the battery lifespan of mobile devices. On the other hand, it can reduce the energy consumption for devices to ofﬂoad the computing workloads to an edge server. But transmission delay will be induced, including the communication delay between the device and the edge server [9] as well as the waiting time at the edge server. Therefore, it is signiﬁcant to analyze the tradeoff between the service delay and the energy consumption, which has a great inﬂuence on the quality of network and industrial applications.
In general, for mobile devices, when the computation of task is not heavy, users incline to run it locally to avoid long delay [10]. By contrast, users would like to ofﬂoad the task to edge server when it is compute-intensive. However, if mobile devices invariably get the content data with high volume from the edge server, it will lead to more congested network trafﬁc and unavoidable cost [11]. Under the circumstances, it is promising to ofﬂoad trafﬁc via D2D communication, especially for intragroup users who have similar content preference. It would be helpful to improve user experience if devices or users can share their resources. However, the question is that users may not be willing to share when their resources are scarce. The sharing willingness of users is a critical factor in D2D communication. To the best of our knowledge, we have not found a model that depicts such willingness for D2D communication. Thus, a novel model should be proposed to achieve the resource sharing

in fog networks. In reality, residual energy (remaining battery level), interface bandwidth, and computing power (CPU clock frequency) are the main factors that will inﬂuence the ofﬂoading decisions [12].
In this paper, we study the co-ofﬂoading of both trafﬁc and computation for vehicular networks in industrial trafﬁc. First, we devise a feature table that records the characteristic of tasks based on the content-centric architecture. Jointly taking both the trafﬁc size and computation workload into consideration, we then propose a method that can both ofﬂoad the network tasks with large volume and intensive computation in fog computing.
We summarize our contributions, which are as follows. 1) We propose a novel formulation that jointly considers
vehicle mobility and resource constraint, aiming to meet the ofﬂoading requirement of trafﬁc vehicles. By focusing on the response latency and execution consumption, we analyze the tradeoff between the service delay and energy consumption in the edge ofﬂoading. 2) We devise two reinforcement learning (RL) based algorithms to address the proposed cost minimization problem. The results show that our proposed algorithms achieve lower energy consumption and service delay compared with other benchmark ofﬂoading schemes. The rest of this paper is organized as follows. Section II introduces the related work. Section III describes the system model of edge ofﬂoading in fog computing. RL-based algorithms are proposed in Section IV. In Section V, the proposed approach is evaluated by simulations. Section VI concludes this paper and discusses some open issues.
II. RELATED WORK
There are some studies about industrial applications in fog computing. Most of them focus on processing data and improving network services. Aazam et al. presented an architectural overview of industry 4.0 and discuss how fog can provide local computing support in the industrial Internet of Things (IoT) [13]. Fu et al. designed a ﬂexible and economical framework by integrating the fog computing and cloud computing to solve the problems about data in industrial applications [14]. Mishra et al. proposed a metaheuristic-based service allocation framework using three metaheuristic techniques, such as particle swarm optimization (PSO), binary PSO, and bat algorithm [15]. Kaur et al. presented a software-deﬁned network (SDN)-based edgecloud interplay to handle streaming big data in industrial IoT environment, wherein an SDN provides an efﬁcient middleware support [16].
In order to address the challenges of dynamic network trafﬁc, lots of methods have been proposed. For example, Ha et al. [17] proposed a pricing system architecture, which intends to handle the increasing demands in mobile networks. Zhuo et al. [18] considered utilizing delay tolerant networks and Wi-Fi to ofﬂoad trafﬁc of cellular networks. Based on this, they proposed an incentive method to simulate mobile users to balance service delay in network trafﬁc ofﬂoading.
These pioneering researches have concentrated on ofﬂoading mobile trafﬁc to Wi-Fi. On the other hand, as D2D com-

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

978

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

TABLE I NOTATIONS

Fig. 1. System model.

munication emerges, some other studies have investigated that ofﬂoading network trafﬁc via D2D communication. For example, Habak et al. [19] utilized an assemblage of colocation mobile devices to build a mobile edge system dynamically. Xie et al. [20] made use of energy efﬁcient cooperative communication when guaranteeing transmission reliability.
We ﬁnd the study that can yield the optimal policies of ofﬂoading network tasks is still missing. Many existing studies focused on only a particular type of network or type of task [21], [22]. Besides, it should not be ignored that the schemes to collect the distributed information of mobile devices and assign tasks to suitable device or edge server are the key challenges in mobile edge system [23]. Different from existing methods, the scheme we proposed not only considers reducing the energy consumption and service delay of mobile tasks, but also takes the resource condition of mobile devices into account. This work ﬁlls the gap that handles the co-ofﬂoading toward the trafﬁc and computation in mobile edge computing (MEC).

III. SYSTEM MODEL AND PROBLEM FORMULATION
A. System Model
In MEC, mobile devices can usually access to network services of industrial applications via edge networks or cellular networks. The service requirements are provisioned in either edge servers or remote cloud datacenter.
For minimizing the energy consumption and access delay, the service provider needs to make ofﬂoading decisions for each mobile device in real-life industrial trafﬁc or services [15], [24]. We assume that there is a centralized controller that makes decisions for the service provider.
As shown in Fig. 1, there are lots of mobile devices. We assume that a set U of mobile devices exists in the edge network. Each one requires a mobile service of an application during the process moving from a starting position to the next position. The mobile services are included in a set T of network tasks, whereas the location is denoted as a set L. The notations used in this paper are summarized in Table I. When a mobile device moves around, it can receive data in two manners: First, direct data transmission from edge network devices, and second, D2Dbased data transmission when meeting other devices that have

Fig. 2. Network architecture.
sufﬁcient resources and hold the requested data. The chance of ofﬂoading mobile data to edge servers or mobile devices depends greatly on the network data characteristics (i.e., trafﬁc size and computation workload). If a task is compute-intensive, then mobile device can ofﬂoad this task to an edge server for execution. When the volume of task’s data is large, controller can transmit the data though a D2D communication to avoid the network trafﬁc congestion.
As shown in Fig. 2, the fog server has a controller that can receive the data from the mobile devices. The training process takes place on the fog server with strong computing power. The controller will make decisions according to the collected information and return the results to mobile devices to help them choose the optimal actions. On the other hand, each mobile devices also has a controller that can make decisions. For example, it will decide to execute the pretrained model on the mobile device only in the case where edge network is unavailable. If

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

WANG et al.: TRAFFIC AND COMPUTATION CO-OFFLOADING WITH REINFORCEMENT LEARNING

979

needed, one mobile device will use D2D communication by exchanging data with others. As described previously, the training process of deep neural network takes place on the fog server with strong computing power. The mobile devices do not need to train the model. Therefore, the overhead is not heavy and it is acceptable.

B. Problem Formulation

We consider a scenario in a content-centric network, where

each network task has a unique identiﬁer. We use a global-

sharing feature table to record the characteristics of tasks, such

as trafﬁc size and computation workload. The mobile devices

and edge server can acquire the characteristics of a network

task easily. We assume that task i ∈ T can be measured by the

content size si and the amount of computation ci. Note that si and ci can be discrete real numbers. For example, the task data can be divided into multiple equal chunks, and the computation

is dependant on the number of instructions. When a mobile

device j ∈ U wants to transfer the ith task to an edge server or

to the other mobile devices, the time spending on transferring

the data is calculated as

Ttran(i, j, t)

=

si r(t)

∀i ∈ T

∀j ∈ U, t ∈ T

(1)

where r(t) is the network transmission rate at time slot t, and its

value depends on the environment where the device is located

at time t. T is the set of all time slots. If a device processes a

task locally, this transmission delay is not incurred. In this case,

the content size si will be set to 0. On the other hand, if a task is compute-intensive, it must be handled on mobile devices or

ofﬂoaded to an edge server for execution. It may also happen

that neither of these two modes is feasible, e.g., when energy

is insufﬁcient at the mobile device, which should execute the

task, and hence, the computation task will be postponed till

next available time slot. When the computation task is executed

locally, it will utilize the computing resource of the mobile

device, including CPU, memory, storage, battery capacity, etc.

In this paper, we focus on the CPU and battery level of mobile devices, which are denoted by fj and bj (j ∈ U ), respectively. Therefore, we can write the execution time on a local mobile device, denoted by Tcomp(i, j, t), as follows:

Tcomp(i, j, t)

=

ci fj

∀i ∈ T ∀j ∈ U, t ∈ T .

(2)

Thus, the total response time of ofﬂoading a task for execution can be calculated as follows:

Ttotal(t) =

[Ttran(i, j, t) + Tcomp(i, j, t)], t ∈ T . (3)

j ∈U i∈T

In addition, the energy consumption also stems from different procedures of the system, such as the computation and transmission. The speciﬁc facts are CPU utilization and I/O transmission. Thus, for the ith network task and the jth mobile device, we calculate the total power consumption as follows:

ptotal(i, j, t) = pcomp(i, j) + ptran(i, j, t) ∀i ∈ T ∀j ∈ U, t ∈ T (4)

where pcomp(i, j) is the power consumption when the task i is executed on the mobile device j, whereas ptran(i, j, t) is the power consumption when the task i is transferred from the mobile device j to the edge server or other mobile devices. Similar to [9], we assume that the power consumption for computation
is represented as follows:

pcomp(j) ∝ (fj )3 ∀j ∈ U.

(5)

Then, the energy consumption of execution is calculated as

Ecomp(i, j, t) = α1ci(fj )2 ∀i ∈ T ∀j ∈ U, t ∈ T (6)

where α1 is the coefﬁcient of energy consumption in computing. Moreover, according to [25], the energy consumption caused by

transmission is proportional to the size of transferred data and

the data transmission time. Hence, we can deﬁne the Etran(i, j, t) as follows:

Etran(i,

j,

t)

=

α2

si r(t)

∀i ∈ T

∀j ∈ U

∀t ∈ T

(7)

where α2 is the transformation coefﬁcient of energy consumption in transmission. The total energy consumption can be obtained as follows:

Etotal(t) =

[Ecomp(i, j, t) + Etran(i, j, t)] ∀t ∈ T .

j ∈U i∈T

(8)

As stated above, we can describe the tradeoff between the energy consumption and the service delay by tuning a knob δ ∈ [0, 1] denoting the weight of energy consumption. Finally, Q is calculated as follows:

Qt = δEtotal(t) + (1 − δ)Ttotal(t), δ ∈ [0, 1], t ∈ T (9)

where Qt denotes the system cost at time slot t. When we set δ to 1, the weighted sum will turn into energy cost. On the other hand, it will turn into time cost if δ is set to 0. Moreover, we can tune the parameter if we are more concerned about one aspect.
For all the tasks that can be ofﬂoaded, we assume they need to be accomplished before deadline D, D ∈ N . L is the set of coordinates that mobile devices could move around before D. The system state for devices and network tasks is deﬁned as S = (u, v, H), where u is the attributes of mobile devices and v is the attributes of network tasks. The set H includes the locations of other mobile devices that do not need ofﬂoad. In detail, u = (l, b, f ), where l, b, and f denote location of mobile device, battery level, and CPU clock frequency, respectively. v = (s, c, γ), where s, c, and γ are the trafﬁc size, computation workload, and delay requirement of a task. To simplify, we can partition coverage areas into discrete ones by grid method and set the delay requirement to several levels. The state space is up to O(|S||T|). The mobile controller should choose one way to handle the task at each time slot t ∈ T .
In general, the cellular network can provide seamless coverage to all the mobile devices but its quality may not meet the demands in the future. Thus, we only consider the edge networks and D2D communications, but cellular network is only being considered when a task is unﬁnished. We classify the locations by available edge networks or D2D communications at time t into the following four cases.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

980

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

1) L1t = l ∈ L: l can only access to cellular network, which is not used in most cases.

2) L2t = l ∈ L: l can access to edge network. 3) L3t = l ∈ L: l can access to D2D network. 4) L4t = l ∈ L: l can access to edge network and D2D net-
work.

There are four actions corresponding to ofﬂoading decisions

for mobile devices. Mobile controller will select one of the

actions for dealing with a network task at each decision time

slot. Accordingly, we can show the actions using the set A =

{a1, a2, a3, a4}, where a1 denotes that mobile device will wait for an opportunity to handle the task; a2 denotes that a mobile

device will dispose the task locally; a3 denotes that task will be handled through an edge network; and a4 denotes that task will be disposed through a D2D communication. Note that a3

is available when mobile devices are in the coverage of an edge

network and a4 is available when a mobile device meets other devices.

The available action at location l is denoted as a ∈ A. We de-

ﬁne the possible actions on the basis of the attributes of system.

⎧

A

=

⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩

a1, a1, a1, a1,

a2, a2, a2, a2,

a3, a4, a3,

a4,

l ∈ L1t l ∈ L2t l ∈ L3t l ∈ L4t

∀t ∈ T .

(10)

We deﬁne the cost function for every action based on the action chosen at each time slot, denoted by Ct , as follows:

Ct = Qt ∀t ∈ T .

(11)

The action cost occurs during the usage of the resource and transmission of data. In general, the consumption to ofﬂoading task using an edge network is lower than that using a D2D communication. The total cost of dealing with the task is the sum of cost units occurring at each time slot during the whole process.
However, there may be a few tasks that cannot be processed before the deadline. For the failed task (i.e., s > 0 or c > 0 when t > D), we deﬁne the penalty cost function as (12), which is calculated by the remaining trafﬁc size sri and computation workload cri .

CD +1

=

i∈T

1 γi

μ(sri )2 + (1 − μ)(cri )2, μ ∈ [0, 1]

(12)

where μ ∈ [0, 1] is a coefﬁcient balancing trafﬁc size and computation workload.
We assume that the movement mode of mobile devices is memoryless, which is deﬁned in (13). The new location l just depends on the past location l and is irrelevant with the characteristics of task. We think about a two-dimension movement mode. For mobile devices, they can stay where they are with probability λ at each decision time slot. The probability λ is called stable factor. Besides, they may move randomly to neighbor locations with probability ηk , k ∈ {east, south, west, north} indicates one of four directions that mobile devices can move to. Speciﬁcally, the stable factor λ and the probability of moving

to each direction ηk satisfy the following rules:

P (l |l) =

λ, ηk ,

l =l l =l

(13)

where

λ + ηk = 1, k ∈ {east, south, west, north}. (14)

Because mobile devices can randomly move before the deadline, we are concerned about the situation where they can encounter with each other at a certain location. The probability that mobile device n can meet with the other device m at location l at decision time slot t is denoted as Ptn (l) · Ptm (l), in which Ptn (l) is deﬁned in (15)

Ptn (l) =

1, l ∈L Ptn−1(l )P (l|l ),

Ptn (l) = P0n (ln ) Ptn (l) = P0n (ln )

.

(15)

It shows the probability during decision time slot t that mobile

device n stays in location l. ln is the starting location of mobile device before ofﬂoading procedure. Furthermore, the mobile

devices will decide whether to provide the device resources

or not. When the resources are sufﬁcient, users are ready to

offer them for better beneﬁts, such as reducing service delay

or improving energy efﬁcient. On the contrary, they will refuse

to sacriﬁce their needs if the resources are scarce. Therefore,

we devise the following indicator model, which reﬂects the

willingness of users to provide the resources for public beneﬁts.

It is reasonable that the share willingness will be higher if the

network transmission rate is faster and CPU frequency is higher.

Besides, the formula bnt (2 − bnt ) reﬂects the fact that the share willingness will drop sharply when the battery power is at a low

level, whereas it will drop slowly when the battery power is at a

high level

⎧ ⎪⎨ 1,

bnt > bthr1

Wtn = ⎪⎩ βfn r(t) 0,

bnt (2 − bnt ),

otherwise bnt < bm 2

(16)

where bnt is a battery level expressed as a percentage of device n at time t, and bthr1 and bthr2 are two different thresholds. bthr1 is the threshold that users are willing to provide the resources because the resources are abundant, whereas bthr2 is the threshold denoting users refuse to offer due to insufﬁcient resources. β is a normalized coefﬁcient to make the probability ranging from 0
to 1. Thus, the probability that one mobile device can connect
with another satisﬁes

Ptn (l, Wtn ) = Ptn (l) · Wtn , t ∈ T , n ∈ U.

(17)

As a result, the probability that two mobile users can connect with each other satisﬁes

Pt (l, n1, n2) = Ptn1 (l, Wtn1 ) · Ptn2 (l, Wtn2 ), n1, n2 ∈ U. (18)
The transition probability of system is the probability that the state of system varies from S to S in the next time slot when taking action a ∈ A. Because the movement of mobile device is independent of the characteristics of task T and action A, we

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

WANG et al.: TRAFFIC AND COMPUTATION CO-OFFLOADING WITH REINFORCEMENT LEARNING

981

get

P (u , v |u, v, a) = Pt (l, n1, n2)P (l |l) P (h |h)
h ∈H

· P (u , v |u, v)

(19)

where

1, u = u − Δu(a) and v = v−Δv(a)

P (u , v |u, v) =

.

0, otherwise

(20)

The probability that mobile device moves from location l to l is

P (l |l), and the probability that other mobile devices move from

location h to h is P (h |h). P (u , v |u, v, a) is the probability

that attributes of system vary from the current state to another

in the next decision time slot.

IV. RL-BASED ALGORITHMS

In order to solve the optimal ofﬂoading problem, we propose the dynamic RL scheduling (DRLS) algorithm (see Algorithm 1) and the deep dynamic scheduling (DDS) algorithm (see Algorithm 2), based on the framework of RL algorithm [8], [26], [27]. The policy π denotes the decision according to each system state S and decision time slot t, which is denoted as a = πt (S) in the Markov decision progress (MDP). We deﬁne the policy space of π as Π. Then, our objective is to ﬁnd the optimal policy π, which can minimize the system cost for processing the task before deadline D. Recall that the total cost during the ofﬂoading process can be calculated as follows:

D
Ctotal = Ct (S, πt (S)) + CD +1(S) ∀t ∈ T . (21)
t=1
Then, a cost minimization problem with the objective function (21) is deﬁned as follows:

min
π ∈Π

ESπ (Ctotal)

(22)

s.t. 0 ≤ t ≤ D, t ∈ T

(23)

0 ≤ bj ≤ 1, j ∈ U

(24)

0 ≤ ci ≤ cmax , i ∈ T

(25)

0 ≤ si ≤ smax , i ∈ T

(26)

where E means the expected value, cmax and smax are the constant upper bounds of ci and si (i ∈ T), respectively. Constraint (22) describes that each time slot t is before the deadline D. Constraint (23) indicates that the battery level expressed as a percentage ranges from 0 to 1. Constraints (24) and (25) show that trafﬁc size and computation workload should be within the speciﬁed ranges, respectively.

A. DRLS Algorithm

In order to solve the ofﬂoading decision problem, we propose the DRLS to obtain the optimal policy. In Algorithm 1, we deﬁne the following value function to compute the optimal policy:

Vt∗(S)

=

arg

min
a ∈A

Qt

(S,

a)

∀t ∈ T

(27)

where

Qt (S, a) = P (S |S, a)[Ct (S, a, S ) + Vt∗+1(S )]
S ∈S

= Qt +

P (S |S, a)Vt∗+1(S )

S |S

(u ,v ,H )∈S

= Qt +

P (u |u) P (h |h)P (v |u, v)

(u ,v ,H )∈S

h ∈H

· Vt∗+1(S )

= Qt +

Pt (l , Wt ) P (h |h)Vt∗+1(S ).

(l ,h )∈L

h ∈H

(28)

We should note that the ﬁrst formula in (28) indicates that Qt (S, a) includes the current cost indicated by adopting the action a as well as the future cost when system state S changes into S .
Our DRLS algorithm includes two phases: The ofﬂoading
planning phase and the ofﬂoading executing phase. The inputs
are initial states of system attributes and actions. The outputs are corresponding different Qt (S, a) values, among which the minimum value corresponds to the optimal action policy in that
state. In each iteration, an optimal policy will be generated in
the planning phase by referring to (29). In particular, we use Vtn−1(S) to calculate Vtn (S)

Vtn

(S)

=

min
a ∈A

P (S |S, a)[Ct (S, a, S ) + Vtn+−11(S )].

S ∈S

(29)

In executing phase, the controller will take the optimal action

policy in the current state. Then, the task will be dealt with the

chosen action until the completion of the task or the expiration

of the time.

B. DDS Algorithm

In an MDP model, we assume that there is an agent in the

environment. The agent is trained to run and make decision at

each decision time slot. The agent instructs the sequence with a

certain state S ∈ S to perform a given action a ∈ A. Then, the

system state moves to a new state S and receives the reward

R(S, a) correspondingly. In this process, the chosen action and

the current state jointly yield the probability that determines

which state the process will move into.

The objective of an MDP model is to ﬁnd the optimal strate-

gies for different states according to the expectation of rewards

without speciﬁc model. It does not even need to make a model.

Therefore, the model can learn new states and actions while

they become comprehensive. However, by training over time,

the system has to learn the optimal policy. In detail, what we

need is to optimize expectation reward at each decision time

slot and the discounted reward

N i=0

ϕi ri ,

where

N

is

the

step

number. Note that ri can be deﬁned as −Qi. As a discount

factor, ϕ < 1 reﬂects the importance of future rewards. The

agent will strive for a long-term reward when ϕ → 1, whereas

agent is myopic if ϕ = 0 because it does not consider the

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

982

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

future reward. If we deﬁne Q(S, a) as the discounted expecta-
tion reward when the system agent chooses action a in state S, and V (S) = maxa Q(S, a ) as the optimal value, our objective is to search an optimal policy with V (S), which is calculated as

V (S) = max R(S, a) + ϕ P r(S |S, a)V (S ) . (30)
a S

In fact, we can obtain the optimal solution by solving the

equivalent Bellman’s equation as follows. If we deﬁne f (St )

as the instantaneous revenue in state St , and a vector of utility

function V = [V (S1), V (S2) · · · ] that satisﬁes Bellman’s

equation for the proposed problem such that

⎡

⎤

φ

+

V

(St )

=

max
a ∗ ∈A

⎣f (St )

+

P r(St |St , a∗)V (St )⎦ .

St ∈S

(31)

We then can obtain the optimal revenue scalar φ = maxΩ f (Ω). Now, we present the proposed algorithms based on deep Q-
learning (DQN), which is a good choice for resource scheduling and management. Note that the inputs are states and actions. The outputs are corresponding Q-values, among which the minimum value corresponds to the optimal action policy in that state. In order to derive the correlation of different state-action pairs and corresponding Q-values, deep neural network is adopted in this algorithm. In fact, enough samples should be accumulated in the construction phase to accomplish the training process. Moreover, it could apply arbitrary policy in this process. An unsorted sum tree D stores the samples for prioritized experience replay and double DQN is constructed based on that.

In Algorithm 2, at the beginning of each decision time slot t, the controller is able to capture the current system state. There are two actions that it can choose. One is the action according to the second-level exploration with probability 1 − and the other is the action with maximum Qt (S, A). Then, the agent will carry out the chosen action and receive the corresponding rewards. In addition, the approximate Qt (S, A) could be obtained by performing inference. This may improve the performance of the Q-value initialization. The double DQN can use prioritized experience replay to update the target network. It can work well in a ﬁnite time.
V. PERFORMANCE EVALUATION
A. Experiment Settings and Metrics
We conduct experiments using both TensorFlow and MATLAB. At each decision time slot, mobile users or devices can choose a moving direction according to (13). Without loss of generality, we can assume that the battery level of mobile devices obeys normal distribution. Besides, we examine our model with different trajectory traces generated randomly. We show the average value with different network settings in terms of the locations of edge devices and attributes of network tasks in each

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

WANG et al.: TRAFFIC AND COMPUTATION CO-OFFLOADING WITH REINFORCEMENT LEARNING

983

Fig. 3. Energy cost of schemes with trafﬁc size.

Fig. 4. Energy cost with computation workload.

group of simulations. The length between two adjacent decision time slots is set to be 5 s. The network transmission rates of the edge network, D2D communication, and cellular network are 10, 1, and 2 Mb/s, respectively.
In order to evaluate the ofﬂoading performance, we focus on the following three metrics.
1) The energy cost, which is caused by mobile devices during the processes of data transmission and task computation.
2) The delay spending on data transmission and computation.
3) The ofﬂoading ratio, which represents the percentage of network trafﬁc that task transmits through the edge network or D2D communication.
We compare our edge network and D2D communication joint ofﬂoading scheme with the following three schemes.
1) Nonofﬂoading scheme (NO): Task is handled by mobile device locally and the data are received from cellular network [28].
2) Delayed D2D ofﬂoading scheme (DDO): Task is disposed only by the D2D communication and cellular networks [28].
3) Nondelayed ofﬂoading scheme (NDO): Task is dealt with by edge networks and cellular networks [29].
We should notice that one mobile device can carry several types of network tasks and lots of mobile devices may require task content data simultaneously. Hence, only several mobile devices can receive the data from a certain mobile device at some point. This is very different from edge networks and cellular networks, where the availability is out of question. Besides, we assume that the mobile devices can access to the cellular networks all the time.
B. Simulation Results
In this section, we present the simulation performance of our ofﬂoading scheme in mobile edge network. As shown in Figs. 3 and 4, we can note that the variation of the energy consumption as trafﬁc size and computation workload change. Moreover, in Fig. 3, the energy cost increases with the growth in trafﬁc

size. Besides, the DRLS and DDS that we proposed have better performance than the three other ofﬂoading schemes due to the lower energy cost for any trafﬁc size. In Fig. 4, similarly, the energy cost increases with the growth in computation workload. Additionally, the DDS also outperforms other schemes with the computation workload changing. All of the observations demonstrate the good performance of our method.
Besides, we also ﬁnd that NDO is better than NO and DDO on energy cost. From the experimental results, NO and DDO have inferior performance. NO is a scheme that mobile device does not choose to ofﬂoad trafﬁc or computation. The energy cost of mobile device will be heavy because it executes the tasks locally. On the other hand, DDO will make the mobile devices delayed to choose D2D communication. This will cause more energy cost than that of edge ofﬂoading. Both of these reasons can cause inferior performance. In addition, we observe that the computation workload has a larger effect on the energy cost than trafﬁc size. This can be explained that energy consumption caused by transmitting the data is lower than that by executing the task when the amount of instructions is large. However, several studies may show that the communication consumes more energy than that of computation. The reason of the difference is that the large amount of instructions involved in computation of our experiments. The consumption of the computation is larger than that of communication due to heavy computational cost.
In Figs. 5 and 6, we can observe that the variation of the service delay as the trafﬁc size and computation workload change. As expected, Fig. 5 shows that the service delay increases with the growth in trafﬁc size. We ﬁnd that most of time NO and DDO achieve lower service delay compared with DRLS and DDS. The reason is that we assign greater weight on the energy consumption. Consequently, it will sacriﬁce the service delay to reduce energy consumption. If we put a greater weight on the service delay of network, our DRLS and DDS will be better than others in this regard. Moreover, they will outperform other schemes when trafﬁc size is larger than 40 Mb or computation workload is greater than 65 M/I (where M/I is millions of instructions). Besides, compared with the NDO, our scheme has a better performance on saving energy consumption due to which it makes best of D2D communications. In Fig. 6, we also

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

984

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

Fig. 5. Service delay of schemes with trafﬁc size.

Fig. 8. Ofﬂoading ratio of schemes with computation workload.

Fig. 6. Service delay with computation workload.
Fig. 7. Ofﬂoading ratio of schemes with trafﬁc size.
observe that service delay is longer with the increase in computation workload. We observe that our scheme also has a better performance than NDO because it optimizes the utilization of resources. Besides, the scheme DDO outperforms NO since it looks for opportunities to use a D2D communication, and it reduces the penalty value of using cellular network.
Figs. 7 and 8 show the variation of ofﬂoading ratio as trafﬁc size and computation workload change. We can observe that ofﬂoading ratios of DDO, DRLS, and DDS decrease with the

increase of trafﬁc size while the ofﬂoading ratio of NDO does not seem to change much. The reason is that NDO ofﬂoads network tasks based on the location, ignoring current system state. Besides, with the increase of trafﬁc size, the ofﬂoading ratio of DRLS, DDS, and DDO decease because the cost is high to ofﬂoad the tasks. The difference of the experimental results is that DRLS and DDS have higher values than DDO. What makes this phenomenon is that DRLS and DDS use alternative D2D communications to ofﬂoad tasks except for edge networks. On the other hand, the ofﬂoading ratios of DRLS, DDS, and DDO rise with the increase of computation workload. This is because ofﬂoading the task with heavy computation to edge server can greatly reduce the energy consumption and execution time. We draw the conclusion that our scheme is able to achieve the lowest network cost while meeting the task completion deadline. It could also achieve the minimal energy cost or service delay compared with other schemes. Moreover, in most cases, it can outperform than others on all the three metrics. Besides, DDS has a slight advantage over DRLS shown in results, but we could not ignore the overhead of DDS, which is generally larger than that of DRLS. This is an important problem for industrial applications in fog computing.
We also analyze the effect of ofﬂoading with different trajectory traces. We generate these two traces according to formula (13) and (14). In this paper, we can get trajectory trace 1 with a distinct regularity by setting the parameter λ to 0.1, parameter ηsouth and ηnorth to 0.05, parameter ηeast and ηwest to 0.4. On the other hand, we can get trace 2 with a large randomness by setting all the parameters to 0.2. Actually, these are used to simply simulate movement in two directions and random movement. As shown in Figs. 9 and 10, for any trafﬁc size and computation workload, trace 1 has a better performance than trace 2 does on both energy cost and service delay. It means that we should consider the difference of movement modes in order to make better use of edge ofﬂoading for industrial applications.
Besides, the reliability of tasks is an important factor due to various network elements. It is generally true that the service provider can guarantee the reliability of the edge servers. However, the user-side reliability is difﬁcult to solve. Of course, we do not have to be pessimistic. In future work, we can take

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

WANG et al.: TRAFFIC AND COMPUTATION CO-OFFLOADING WITH REINFORCEMENT LEARNING

985

Fig. 9. Energy cost and service delay of two traces with trafﬁc size.

The ﬁrst issue is interoperability of edge devices. Because many edge facilities are involved in industrial applications, it is important to seek an effective and efﬁcient method to handle the interoperability of edge devices. The second issue is security and privacy of data. Industrial edge facilities are vulnerable to attacks, which can affect the availability, conﬁdentiality, and integrity of transmitted or stored data. The third issue is user friendliness in the application deployment and usage. The user interaction is very frequent in industrial applications. Therefore, it is challenging to design devices and user interfaces for applications because we should make them easy to accept for people of different backgrounds. Only by solving those challenges we can realize the true potential of industrial fog computing.

Fig. 10. Energy cost and service delay of two traces with computation workload.
advantage of 5G technology and design an incentive mechanism to alleviate this problem.
VI. CONCLUSION AND OPEN ISSUES
In this paper, we mainly focus on the problem of edge ofﬂoading in fog computing. For D2D ofﬂoading, we propose an evaluation model to depict the willingness of mobile users in industrial trafﬁc to contribute their resources to the public. Then, we devise a DRLS algorithm and a DDS algorithm to ﬁnd a ﬁnebalanced solution to ofﬂoading the tasks for mobile devices. The simulation results show that our scheme outperforms the other benchmark schemes in terms of both energy consumption and service delay. According to the results, we ﬁnd that the energy cost and service delay increase with increasing of trafﬁc size and computation workload. On the other hand, the computation workload has a larger effect on the energy cost than trafﬁc size. This can be explained that energy consumption caused by transmitting the data is lower than that by executing the task when the amount of instructions is large. The ofﬂoading ratio decreases as the trafﬁc size increases, whereas it increases as the computation workload increases. Besides, considering the different effects in the case of two traces, we should analyze the difference of movement modes in order to make better use of ofﬂoading for industrial applications.
However, to deploy real industrial applications in fog computing, we need to address several challenges. Some of them are listed here for helping carry out research in this direction.

REFERENCES
[1] C. V. N. Index, “Global mobile data trafﬁc forecast update, 2015–2020,” White Paper, CA, USA, 2016. [Online]. Available: http://goo.gl/ylTuVx
[2] Y. Cui et al., “Software deﬁned cooperative ofﬂoading for mobile cloudlets,” IEEE/ACM Trans. Netw., vol. 25, no. 3, pp. 1746–1760, Jun. 2017.
[3] K. Wang, Y. Wang, Y. Sun, S. Guo, and J. Wu, “Green industrial internet of things architecture: An energy-efﬁcient perspective,” IEEE Commun. Mag., vol. 54, no. 12, pp. 48–54, Dec. 2016.
[4] L. Vaquero and L. Rodero-Merino, “Finding your way in the fog: Towards a comprehensive deﬁnition of fog computing,” ACM SIGCOMM Comput. Commun. Rev., vol. 44, no. 5, pp. 27–32, 2014.
[5] H. Huang and S. Guo, “Service provisioning update scheme for mobile application users in a cloudlet network,” in Proc. IEEE Int. Conf. Commun., 2017, pp. 1–6.
[6] C. Xu et al., “Making big data open in edges: A resource-efﬁcient blockchain-based approach,” IEEE Trans. Parallel Distrib. Syst., vol. PP, no. 99, pp. 1–16, 2018.
[7] X. Chen, J. Wu, Y. Cai, H. Zhang, and T. Chen, “Energy-efﬁciency oriented trafﬁc ofﬂoading in wireless networks: A brief survey and a learning approach for heterogeneous cellular networks,” IEEE J. Sel. Areas Commun., vol. 33, no. 4, pp. 627–640, Apr. 2015.
[8] E. Bastug, M. Bennis, and M. Debbah, “Living on the edge: The role of proactive caching in 5G wireless networks,” IEEE Commun. Mag., vol. 52, no. 8, pp. 82–89, Aug. 2014.
[9] J. Ho, J. Zhang, and M. Jo, “Selective ofﬂoading to WiFi devices for 5G mobile users,” in Proc. IEEE 13th Int. Wireless Commun. Mobile Comput. Conf., 2017, pp. 1047–1054.
[10] Y. Lin, E. Chu, Y. Lai, and T. Huang, “Time-and-energy-aware computation ofﬂoading in handheld devices to coprocessors and clouds,” IEEE Syst. J., vol. 9, no. 2, pp. 393–405, Jun. 2015.
[11] P. Mach and Z. Becvar, “Mobile edge computing: A survey on architecture and computation ofﬂoading,” IEEE Commun. Surv. Tut., vol. 19, no. 3, pp. 1628–1656, Jul.–Sep. 2017.
[12] Y. Pan, C. Pan, H. Zhu, Q. Ahmed, M. Chen, and J. Wang, “On consideration of content preference and sharing willingness in D2D assisted ofﬂoading,” IEEE J. Sel. Areas Commun., vol. 35, no. 4, pp. 978–993, Apr. 2017.
[13] M. Aazam, S. Zeadally, and K. Harras, “Deploying fog computing in industrial internet of things and industry 4.0,” IEEE Trans. Ind. Inform., vol. 14, no. 19, pp. 4674–4682, Oct. 2018.
[14] J. Fu, Y. Liu, H. Chao, B. Bhargava, and Z. Zhang, “Secure data storage and searching for industrial IoT by integrating fog computing and cloud computing,” IEEE Trans. Ind. Inform., vol. 14, no. 10, pp. 4519–4528, Oct. 2018.
[15] S. Mishra, D. Putha, J. Rodrigues, B. Sahoo, and E. Dutkiewicz, “Sustainable service allocation using metaheuristic technique in fog server for industrial applications,” IEEE Trans. Ind. Inform., vol. 14, no. 10, pp. 4497–4506, Oct. 2018.
[16] K. Kaur, S. Garg, G. Aujla, N. Kumar, J. Rodrigues, and M. Guizani, “Edge computing in the industrial internet of things environment: Softwaredeﬁned-networks-based edge-cloud interplay,” IEEE Commun. Mag., vol. 56, no. 2, pp. 44–51, Feb. 2018.
[17] S. Ha, S. Sen, C. Joe-Wong , Y. Im, and M. Chiang, “Tube: Time-dependent pricing for mobile data,” ACM SIGCOMM Comput. Commun. Rev., vol. 42, no. 4, pp. 247–258, 2012.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

986

IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. 15, NO. 2, FEBRUARY 2019

[18] X. Zhuo, W. Gao, G. Cao, and S. Hua, “An incentive framework for cellular trafﬁc ofﬂoading,” IEEE Trans. Mobile Comput., vol. 13, no. 3, pp. 541–555, Mar. 2014.
[19] K. Habak, M. Ammar, K. Harras, and E. Zegura, “Femto clouds: Leveraging mobile devices to provide cloud service at the edge,” in Proc. IEEE 8th Int. Conf. Cloud Comput., 2015, pp. 9–16.
[20] K. Xie, J. Cao, X. Wang, and J. Wen, “Optimal resource allocation for reliable and energy efﬁcient cooperative communications,” IEEE Trans. Wireless Commun., vol. 12, no. 10, pp. 4994–5007, Oct. 2013.
[21] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource management with deep reinforcement learning,” in Proc. 15th ACM Workshop Hot Topics Netw., 2016, pp. 50–56.
[22] W. Zhang, Y. Wen, and D. Wu, “Energy-efﬁcient scheduling policy for collaborative execution in mobile cloud computing,” in Proc. IEEE INFOCOM, 2013, pp. 190–194.
[23] H. Deng and I. Hou, “Online scheduling for delayed mobile ofﬂoading,” in Proc. IEEE Conf. Comput. Commun., 2015, pp. 1867–1875.
[24] B. Baron, P. Spathis, H. Rivano, M. Amorim, Y. Viniotis, and M. Ammar, “Centrally-controlled mass data ofﬂoading using vehicular trafﬁc,” IEEE Trans. Netw. Serv. Manage., vol. 14, no. 2, pp. 401–415, Jun. 2017.
[25] H. Wu and K. Wolter, “Stochastic analysis of delayed mobile ofﬂoading in heterogeneous networks,” IEEE Trans. Mobile Comput., vol. 17, no. 2, pp. 461–474, Feb. 2018.
[26] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green resource allocation based on deep reinforcement learning in content-centric IoT,” IEEE Trans. Emerg. Topics Comput., vol. PP, no. 99, pp. 1–16, Feb. 2018.
[27] N. Liu et al., “A hierarchical framework of cloud resource allocation and power management using deep reinforcement learning,” in Proc. IEEE 37th Int. Conf. Distrib. Comput. Syst., 2017, pp. 372–382.
[28] Y. He, M. Chen, B. Ge, and M. Guizani, “On WiFi ofﬂoading in heterogeneous networks: Various incentives and trade-off strategies,” IEEE Commun. Surv. Tut., vol. 18, no. 4, pp. 2345–2385, Oct.–Dec. 2016.
[29] M. Cheung and J. Huang, “Dawn: Delay-aware Wi-Fi ofﬂoading and network selection,” IEEE J. Sel. Areas Commun., vol. 33, no. 6, pp. 1214– 1223, Jun. 2015.
Yixuan Wang received the B.S. degree in network engineering from the Nanjing University of Posts and Telecommunications, Nanjing, China, in 2017. He is currently working toward the M.S. degree in computer network at The University of Aizu, Aizuwakamatsu, Japan.
His research interests include machine learning system, edge network, and distributed and parallel computing.

Huawei Huang (M’16) received the Ph.D. degree in computer science and engineering from The University of Aizu, Aizuwakamatsu, Japan, in 2016.
He was a Visiting Scholar with The Hong Kong Polytechnic University, from 2017 to 2018. He was a Postdoctoral Research Fellow with the JSPS between October 2016 and March 2018. He is currently an Assistant Professor with the Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan. His research interests include SDN/NFV, mobile computing, and blockchain. Dr. Huang was the recipient of the Best Paper Award in the TrustCom2016.
Toshiaki Miyazaki (M’86–SM’12) received the B.E. and M.E. degrees in applied electronic engineering from the University of ElectroCommunications, Tokyo, Japan, in 1981 and 1983, respectively, and the Ph.D. degree in electronic engineering from the Tokyo Institute of Technology, Tokyo, Japan, in 1994.
He is currently a Professor with The University of Aizu, Aizuwakamatsu, Japan, and the Dean of the Undergraduate School of Computer Science and Engineering. Before joining The University of Aizu, he was with the NTT for 22 years, and engaged in research on VLSI CAD systems, telecommunications-oriented FPGAs and their applications, active networks, peer-to-peer communications, and ubiquitous network environments. He was a Visiting Professor with the Graduate School, Niigata University in 2004, and a part-time Lecturer with the Tokyo University of Agriculture and Technology from 2003 to 2007. His research interests include reconﬁgurable hardware systems, adaptive networking technologies, and autonomous systems. He is currently a senior member of the IEICE and IPSJ.

Kun Wang (M’13–SM’17) received the B.Eng. and Ph.D. degrees in computer science from the School of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China, in 2004 and 2009, respectively.
From 2013 to 2015, he was a Postdoc Fellow with the Department of Electrical Engineering, University of California, Los Angeles, Los Angeles, CA, USA. In 2016, he was a Research Fellow with the School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu , Japan. He is currently an Associate Professor with the School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China. He has authored/coauthored more than 50 papers in referred international conferences and journals. His current research interests include big data, wireless communications and networking, smart grid, energy Internet, and information security technologies. Dr. Wang was the recipient of the Best Paper Award at the IEEE GLOBECOM’2016. He serves as an Associate Editor for the IEEE ACCESS, Journal of Network and Computer Applications, EAI Transactions on Industrial Networks and Intelligent Systems and the Editor for the Journal of Internet Technology. He was the Symposium Chair/Co-Chair of the IEEE IECON16, IEEE EEEIC16, IEEE WCSP16, IEEE CNCC17, etc. He is currently a member of the ACM.

Song Guo (M’02–SM’11) received the Ph.D. degree in computer science from University of Ottawa, ON, Canada.
From 2007 to 2016, he was a professor with The University of Aizu, Aizuwakamatsu, Japan. He is currently a Full Professor with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong. His research interests include big data, cloud computing, mobile computing, and distributed systems with more than 400 papers published in major conferences and journals. Prof. Guo was an Associate Editor for the IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS and an IEEE ComSoc Distinguished Lecturer. He is currently an Associate Editor for the IEEE TRANSACTIONS ON CLOUD COMPUTING, IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING, IEEE TRANSACTIONS ON SUSTAINABLE COMPUTING, IEEE TRANSACTIONS ON GREEN COMMUNICATIONS AND NETWORKING, and IEEE NETWORK. He also served as the General and Program Chair for numerous IEEE conferences. He currently serves as a Director and a member of the Board of Governors of the IEEE ComSoc. He was the recipient of the 2018 IEEE TCGCC Best Magazine Paper Award, 2017 IEEE Systems Journal Annual Best Paper Award, and other six Best Paper Awards from IEEE/ACM conferences. His work was recognized by the 2016 Annual Best of Computing: Notable Books and Articles in Computing in ACM Computing Reviews.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:29:31 UTC from IEEE Xplore. Restrictions apply.

