SPECIAL SECTION ON CLOUD - FOG - EDGE COMPUTING IN CYBER-PHYSICAL-SOCIAL SYSTEMS (CPSS)
Received February 25, 2020, accepted March 6, 2020, date of publication March 17, 2020, date of current version March 26, 2020. Digital Object Identifier 10.1109/ACCESS.2020.2981434

Task Offloading and Resource Allocation for Mobile Edge Computing by Deep Reinforcement Learning Based on SARSA
TAHA ALFAKIH 1, MOHAMMAD MEHEDI HASSAN 1,2, (Senior Member, IEEE), ABDU GUMAEI 1, CLAUDIO SAVAGLIO 3, AND GIANCARLO FORTINO 3, (Senior Member, IEEE)
1Department of Information Systems, College of Computer and Information Sciences, King Saud University, Riyadh 11543, Saudi Arabia 2Research Chair of Smart Technologies, King Saud University, Riyadh 11543, Saudi Arabia 3Department of Informatics, Modeling, Electronics and Systems, University of Calabria, 87036 Rende, Italy
Corresponding author: Mohammad Mehedi Hassan (mmhassan@ksu.edu.sa)
This work was supported by the Deanship of Scientiﬁc Research at King Saud University through the Vice Deanship of Scientiﬁc Research Chairs. Chair of Smart Technologies. The authors also thank the RSSU at King Saud University for their technical support.

ABSTRACT In recent years, computation ofﬂoading has become an effective way to overcome the constraints of mobile devices (MDs) by ofﬂoading delay-sensitive and computation-intensive mobile application tasks to remote cloud-based data centers. Smart cities can beneﬁt from ofﬂoading to edge points in the framework of the so-called cyber–physical–social systems (CPSS), as for example in trafﬁc violation tracking cameras. We assume that there are mobile edge computing networks (MECNs) in more than one region, and they consist of multiple access points, multi-edge servers, and N MDs, where each MD has M independent real-time massive tasks. The MDs can connect to a MECN through the access points or the mobile network. Each task be can processed locally by the MD itself or remotely. There are three ofﬂoading options: nearest edge server, adjacent edge server, and remote cloud. We propose a reinforcementlearning-based state-action-reward-state-action (RL-SARSA) algorithm to resolve the resource management problem in the edge server, and make the optimal ofﬂoading decision for minimizing system cost, including energy consumption and computing time delay. We call this method OD-SARSA (ofﬂoading decision-based SARSA). We compared our proposed method with reinforcement learning based Q learning (RL-QL), and it is concluded that the performance of the former is superior to that of the latter.

INDEX TERMS Mobile devices, edge computing, mobile edge computing, edge cloud computing, virtual machines, access points.

I. INTRODUCTION In recent years, the massive growth of computationally intensive and delay sensitive mobile applications, such as online gaming, image or signal processing (e.g., facial recognition), augmented reality, and real-time translation services, have been imposing heavy computation demands on resourceconstrained mobile devices (MDs). As MDs are limited in terms of computation, battery, and storage capacity, there is a growing trend to ofﬂoad or transfer computation intensive tasks to powerful remote computing platforms. This method is referred to as computation ofﬂoading. It reduces energy
The associate editor coordinating the review of this manuscript and approving it for publication was Francesco Piccialli.

consumption for local processing and therefore prolongs battery life.
Mobile cloud computing (MCC) [1] is a well-known computation ofﬂoading model for MDs [1]. In MCC, user devices can utilize the resources of dedicated remote cloud servers for executing their tasks. These servers have high power, CPU, and storage capabilities. However, the long distance between the MDs and the cloud server lead to substantial communication costs in terms of latency and energy, negatively inﬂuencing real-time applications [2]. Therefore, in recent years, the computation and storage capabilities of the remote cloud have partially migrated to the edge server (near the MDs). This concept is called mobile edge computing (MEC) [3].
MEC provides information technology services and cloud computing capabilities at the mobile network edge. MEC is

54074

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

implemented by a dense deployment of computational servers or by strengthening already deployed edge entities, such as small cell base stations (BS) with computation and storage resources. The objective of MEC is to ensure efﬁcient network operation and service distribution, reduce latency, and offer an enhanced user experience [3], [4]. MEC ofﬂoads computation intensive applications to the cellular network edge. Smart cities can beneﬁt from ofﬂoading to edge servers in the framework of the so-called cyber–physical–social systems (CPSSs), as in trafﬁc violation tracking cameras, or drone services for delivery or geological survey purposes. Each edge node processes the data itself rather than forwarding them to a central remote cloud. Consequently, MEC can improve user experience quality (QoE) and meet service quality (QoS) requirements, such as low latency and energy consumption. Moreover, unlike MCC, MEC pursues a decentralized framework where the edge servers are deployed in a distributed manner.
Despite the great potential of MEC, there remain several challenges. As discussed before, real-time mobile applications are highly sensitive in terms of latency and energy consumption. However, owing to the randomness and dynamics of mobile edge networks, the long execution time of these applications can lead to high energy consumption. Most studies indicate that the long execution time is one of the major challenges in MEC [5], [6]. Hence, there is a need for an efﬁcient computation-ofﬂoading framework for MEC. Furthermore, MDs determine when ofﬂoading should be performed, and what part of a given task should be ofﬂoad to an edge server. However, developing an effective dynamic partitioning method for accurate ofﬂoading decision making is a challenge in MEC. Moreover, determining where to ofﬂoad a task in a multi-edge network for minimizing the latency of service computing (close proximity edge or adjacent edge network or remote cloud) is another challenge. In addition, the limited computational resources of mobile edge servers should be efﬁciently utilized so that QoS requirements may be met (e.g., latency requirement). Furthermore, user mobility, the heterogeneity of edge node resources, and the physical distribution of MDs impose additional challenges for computation ofﬂoading in edge computing.
A number of methods have been developed to overcome some of these challenges [13]–[16]. However, these studies did not consider the beneﬁt of using adjacent edges to serve ofﬂoadable tasks when the nearest edge server cannot serve these tasks. Another limitation is that all these studies used off-policy-based reinforcement learning techniques for resource allocation management, such as the Q-Learning method. This technique depends on the previous workload state, ignoring the current state. Moreover, current studies lack an efﬁcient dynamic multi-objective optimization decision scheme for selecting the tasks to be ofﬂoaded. In the present study, we will resolve these issues and improve the ofﬂoading performance by proposing a dynamic framework that considers both servers and users’ standpoints. In particular, we are concerned with 1) Computation ofﬂoading to the
VOLUME 8, 2020

mobile edge using the system utility of the MEC network to balance processing delay and energy consumption, 2) determining which part/module or process of a mobile application should be ofﬂoaded using deep reinforcement on-policy learning such as state-action-reward-state-action (SARSA), 3) determining where to ofﬂoad the part/module or process in a multi-edge network, and 4) ensuring efﬁcient resource management in the MEC servers.
In this study, we address the question of developing an efﬁcient resource management model for the selected MEC server in a multi-edge network by proposed an ofﬂoading decision-based SARSA method (OD-SARSA). Additionally, we consider the problem of managing mobility when the MDs move from one region to another. Accordingly, we should design and develop an efﬁcient resource management model to enhance MEC server utilization through task scheduling and load balancing. As MEC suffers from limited computational resources, compared with central MCC, it becomes imperative to allocate these resources efﬁciently. The proposed resource allocation will enable meeting QoS requirements (e.g., latency) with minimal effort. Therefore, the main contributions of this study as follows:
• We propose a MEC system model considering both computing time delay and power consumption, and we formulate it as an optimization problem. In particular, we propose an ofﬂoading decision-based SARSA (ODSARSA) using reinforcement learning to make the optimal ofﬂoading decision for reducing system cost in terms of energy consumption and computing time delay.
• We compared our proposed OD-SARSA with RL-QL and concluded that the former performs better than the latter.
• We analyzed the effect of optimal ofﬂoading decision factors and reduced cost by changing the main parameters and analyzing the results, leading to real-world application.
This paper is organized as follows. Section 2 reviews related work. Section 3 describes RL based on SARSA. Sections 4 describes the system model of MEC as a communication model, model of the task, and model of computation. Section 5 then describes the SARSA learning method based autonomic computation ofﬂoading. Finally, Section 6 presents the performance evaluation results and our conclusions.
II. RELATED WORK With the rapid advancement of communication technology, MEC is emerging as a promising technology. An MD uses remote execution (ofﬂoading) to enhance a mobile use’s QoS by reducing energy consumption and increasing performance. We will focus on previous studies concerned with the ofﬂoading process (how and where to ofﬂoad), the partition of mobile applications, and resource allocation, which affect ofﬂoading efﬁciency (performance) and energy consumption. Few studies have focused on computation ofﬂoading in MEC, although several options can be used on the MEC servers depending on the conditions of the mobile network.
54075

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

Thus, an efﬁcient cloud-path selection method is required to select the best resource.
Reducing execution time (T ) is one of the objectives of computation ofﬂoading in MEC. Execution time is the sum of local execution time (Tl) and remote execution time (To). The latter can be further divided into transmission delay to the ME (Tod ), processing time at the ME (Top), and receiving time from the ME (Tor ) An ofﬂoading decision is not taken unless Tl>To. The aim is to minimize computation time, as discussed in [7]. This is achieved by using a one-dimensional search method, so that an effective ofﬂoading decision can be made depending on the queuing state buffer of the application, available energy in the MD and the MEC server, and the communication status between the MEC server and the MD. This algorithm was compared with greedy ofﬂoading, local execution, and cloud execution. The simulation demonstrated that execution time can be reduced by up to 81% and 44% as the arrival of the applications. The limitation of this method is that to make a decision, the MD as a client requires feedback from the MEC. In [8], the low-complexity Lyapunov optimization dynamic computation ofﬂoading algorithm was proposed. In [9], proposed system to leverage from the ability of computing and storage capacity available in the edge servers. In [10], a new computation ofﬂoading model in MEC was introduced. Its principle is to enable the use of virtual resources in the edge cloud to reduce resource and energy consumption and improve the performance of the application. In [11], the authors proposed a novel framework for computation ofﬂoading from an MD to an edge server considering CPU availability so that execution time may be reduced in both the MD and the server. In [12], an opportunistic computation ofﬂoading scheme was proposed for data mining in MDs and the edge network to reduce execution time and power consumption. In [12], the authors developed a distributed computation ofﬂoading algorithm that can attain a Nash equilibrium so that superior performance may be achieved, and user size may be reduced through server mode selection [5]. In [13], a computation ofﬂoading method to a small cell cloud was analyzed, and its performance was evaluated.
Minimizing energy consumption (E) and achieving an acceptable execution time is one of the objectives of computation ofﬂoading in MEC. If an MD executes all computations locally, El denotes the energy consumption; otherwise, the computation is carried out remotely by ofﬂoading to the edge. In this case, (Eo) is the energy consumption and is the sum of the transmission energy to the ME (Eod ), the energy for processing at the ME (Eop), and the energy for receiving the result from the edge (Eor ) The ofﬂoading decision is not made unless El > Eo when T l > To when Tl > To. In [14], the authors proposed computation ofﬂoading to reduce energy consumption in the MD when the computation time constraint is satisﬁed. A constrained Markov decision process was proposed to solve the optimization problem. The author of [15] proposed an energy-efﬁcient computation ofﬂoading algorithm in which the decision making is
54076

performed according to the following principles, 1) the MD considers its execution time and power consumption constraints, ofﬂoading to the ME is performed when the MD cannot satisfy the computation time constraint, and local execution is selected when the power depletion is below the determined threshold and the execution constraint is satisﬁed, 2) the ofﬂoading priority is high. Third, given the radio resource allocation priorities, experiments demonstrated that this algorithm can reduce energy consumption by up to 15%. Using the cloud radio access network (C-RAN) service, the authors of [17] presented a computation ofﬂoading algorithm from mobile to remote cloud radio heads to reduce energy consumption and improve user QoE by minimizing the response time of the app. The Lyapunov optimization algorithm makes the ofﬂoading decision depending on the frequencies of the CPU-cycle for mobile execution and the transmission energy for computation ofﬂoading [8]. In [16], the authors designed an autonomous and energy-efﬁcient ofﬂoading scheme that uses a mathematical model for the energy consumption at the ME for the mobile application, considering the energy consumed by the interaction among the tasks in the same application. In [17], the authors proposed a new game theoretic approach to enhance the edge computing throughput and reduce energy consumption on the edge server.
In [20], it was proposed that the computation ofﬂoading decision should satisfy the trade-off between delay and energy consumption at the ME and UE. This study used the Nash equilibrium distributed computation ofﬂoading algorithm, in which the computation ofﬂoading decision depends on certain weight parameters, and the effective channel is chosen to transmit data. The numerical results demonstrated that this algorithm has superior performance when the application is computed at the MEC server rather than locally. In [18], the authors developed a code ofﬂoading model and decision-making process that reduce the applicatio’s response time and the MDs’ energy consumption. The ofﬂoading decision is made based on the method of Lagrange multipliers, and a nonlinear optimization solver is used instead of solving a complex linear optimizing problem.
Proper resource allocation should follow the decision-making regarding partial or full ofﬂoading. Resource allocation is inﬂuenced by the partitioned and paralleled computation ofﬂoading ability of the application. If ofﬂoading is impossible, then the partitioned and paralleled applications are allocated only one node for the computing. The number of ofﬂoaded applications to the ME should satisfy the computing time energy consumption requirements [19]. The application should determine where ofﬂoadable task should be placed, depending on the computing resources available at the ME. Reference [20] is similar to [22]; however, it not only minimizes computation time but also reduces energy consumption at the ME. The authors propose several hotspots in the density area of the UEs, which enable the MDs to access the ME using the enhanced node B (eNB). The proposed efﬁcient policy by equivalent discretion is
VOLUME 8, 2020

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA TABLE 1. Reward comparison between RL algorithms.

FIGURE 1. Reinforcement learning architecture.
called Markov decision processes (MDP). Reference [21] is similar to [19] and [20], as the main objective is to reduce computation time and energy consumption, as well as reduce channel overload, resource consumption, and computation cost of virtual machine (VM) migration. In [21], the authors use enhanced small cells (SCeNBs) as service nodes at the ME, and each MD is allocated a VM at an SCeNB. This reduces the communication delay because the SNeNBs are characterized by high-quality data transmission.
III. REINFORCEMENT LEARNING BASED ON SARSA LEARNING RL is a part of machine learning [22]. It consists of taking appropriate action to increase the reward in speciﬁc states. Various programs and machines/devices use it to ﬁnd the best behavior or possible path in a given state. RL differs from supervised learning in that the learning data contain the answer key. Thus, in supervised learning, the model is trained on the correct answer itself, whereas in RL, there is no answer, but the reinforcement agent determines how a certain task is to be carried out. When a dataset is not available, learning is performed through experience. The basic principle of RL is the following: The input must be an initial state from which the models start. The output consists of several potential results because there are several solutions to a speciﬁc problem. Training depends on the input, the model will return the value of the state, and the user will decide to punish or reward the model based on its results or output. The model learns continuously, and the best solutions are determined based on the maximum reward. RL involves an environment and agent, where the agent selects the most appropriate action from the environment states. The environment generates the next state based on an action obtained from another policy and rewards the generated state when the agent takes the action, as shown in Fig. 1. SARSA and Q-learning are two commonly used model-free RL techniques. They have different exploration policies and similar exploitation policies. Q-learning is an off-policy technique in which the agent learns based on the action by another policy, whereas SARSA is an on-policy technique, where learning is based on the current action by the current policy. RL has proved efﬁcient in resource allocation [23], cloud computing, and computation ofﬂoading [22]. The policy π estimates the next (s, a) based on the current a state-action (s, a). To do this, we use temporal-difference (TD) to update the rule applied at
VOLUME 8, 2020

every timestamp by allowing the agent to transition from one pair of state-action to another pair.
To solve complex, large state-space problems, the deep SARSA function is updated as

Q (St , At ) = R (St , At ) + γ Q (St+1, At+1)

(1)

where Q (St , At ) is the value of Q for the action A in system state S at time t, R (St , At ) is the reward when the agent selects the action At at state St , and γ denotes the discount factor; the epsilon-greedy policy is used to select the best action At+1 in the current state St+1.
Numerous traditional reinforcement learning models have

been used for computation ofﬂoading. For example, in [24],

an RL technique was used for complicated video games, and

several different RL approaches, such as SARSA learning,

Q learning, GQ, actor-critic, and R learning, were compared.

The results are shown in Table 1, which is reproduced from

that paper and shows that the SARSA outperforms other RL

algorithms, as it obtained the greatest rewards.

Markov decision processes (MDPs) are used in RL for

appropriately increasing the reward in the training task of

an agent interacting with the environment [25]. Therefore,

the future reward at time t is deﬁne as

Rt =

T k =0

αk

rt +k +1

(2)

where α ∈ (0, 1] is a discount factor, and rt is the reward when action a is taken at time t. When the agent takes the

action a under the policy π in state S at the time t, denoted by Qπ (s, a). Thus,

Qπ (s, a) = Eπ {R (t) | st = s, at = a}

= Eπ

∞ k =0

αk rt+k+1

|

st =

s,

at = a

(3)

where E is the expected reward, and π is the policy function

for the action At . The aim of the training task is to acquire
the maximum rewards and obtain the optimal state and action of Qπ (s, a). There are two methods in RL. One is called

Q-learning, and the other SARSA [26]. In this study, we will

use SARSA, as it has been demonstrated that this method can

select a safe path. This is considered appropriate in the present

study, which is concerned with the selection of an optimal

and safe path for ofﬂoading intensive tasks to the edge cloud.

SARSA is an on-policy technique, that is, the next action a∗

depends on the value of the current state st and current action at . The equation for updating state and action values is

Q (s, a) ← Q (s, a) + α[r + γ Q s∗, a∗ − Q(s, a)] (4)

In SARSA learning, the training task is a quinary (s, a, r, s∗, a∗), which is updated sequentially.

54077

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

FIGURE 2. High-level overview of computation offloading in MEC model.
IV. SYSTEM MODEL OF MOBILE EDGE COMPUTING The mobile edge system (MES) model is shown in Fig. 2. MES is constructed on a telecommunication infrastructure, such as BS/LTE. The MDs (e.g., smartphones, tablets, robots, and drones) connect to the edge computing control at the BS/LTE in the adjacent location (region) to the computation ofﬂoading. The edge computing controller in each region manages multi-edge mobile computing, receives the ofﬂoaded tasks from the MDs, and chooses an effective edge node to address them as a task model. In the mobility status, when an MD moves from one region to another, the processing results of the ofﬂoaded tasks are sent to the corresponding MD over the central edge cloud-computing controller (CE3C) and edge-computing controller for the adjacent region. The components of the edge network have high storage and computation capabilities, which are used to create a virtual server offering mobile edge services as a computing model. If a workload demands resources beyond what the edge server can support, the request is redirected over the main network (CE3C) to the cloud services on the other side of the network as the resource management model.
A. COMMUNICATION MODEL We assume that there are MEC networks (MECNs) in more than one region, as shown in Fig. 2, which consist of multiple APs, multi-edge servers, and n MDs denoted by n= {1, 2, . . . ..,n}. An MD can connect to the MECN through an AP or mobile network. Depending on certain parameters such as edge servers’ workloads, response time, or latency and energy consumption, the MDs should ﬁnd an efﬁcient location in the network to perform ofﬂoading. The ofﬂoading action is denoted by A= {a1, a2, . . . .,an}, depending on the ofﬂoading decision, where Xn represent the ofﬂoading decision Xn= {0, 1, 2, 3} (nearest the edge server: Xn = 1, adjacent to the edge server: Xn = 2, remote cloud: Xn= 3, or local computing: Xn= 0). The ofﬂoading decision is inﬂuenced by
54078

the bandwidth Bn and computing delay, which depends on the processing frequency fn.
The communication bandwidths between MDs and ofﬂoading location are denoted by Be, Ba, Bc, which represent edge server bandwidth, adjacent edge server bandwidth, and cloud bandwidth, respectively, as the end-to-end bandwidth. Moreover, the total communication delay for a certain MD is denoted by Tn. Additionally, ptn represents the power consumption for task transmission, and prn the receiving power consumption. Therefore, depending on certain parameters such as edge servers’ workloads, response time, or latency and energy consumption, the MD should ﬁnd an efﬁcient location (nearest the edge server or adjacent to the edge server or remote cloud) to ofﬂoad its tasks. Eventually, after the ofﬂoading process to the nearest edge server or adjacent edge server has been completed, an efﬁcient resource allocation method is required on the edge server.
B. TASK MODEL We assume that each MD has M independent massive real-time tasks, which can be executed locally in the MD or remotely in the MEC network by the computation ofﬂoading. Therefore, tasks cannot be partitioned into subtasks to be processed in multiple devices [27]. Task size is denoted by Dn (transferred data size), and Rn denotes the computation resources required to serve this task (CPU cycles number). Therefore, Dn and Rn are positively related:
Rn = θ Dn, θ constant.

Regardless of whether the task is executed locally by the MD or in the MEC network, Dn does not change.
C. COMPUTATION MODEL 1) LOCAL PROCESSING TIME When the decision unit decides to process a task in the MD (Xn= 0), the time processing per task is denoted by T l . This includes the computing delay of the local CPU. Therefore, the processing time is

T

l
nm=

Rnm FLN

(5)

Similarly, the corresponding power consumption for task Mn of user n is denoted by Pl and is deﬁned as

P lnm =Dinm pl

(6)

where pl denotes the power consumption when the task is processed in the MD. Therefore, the cost of local processing is the combination of the local processing time and local power consumption:

Cnl =

N n=1

(αTnl

+

β Pln )

(7)

where α and β are constant weighting parameters corresponding to the time and power cost of the task.

VOLUME 8, 2020

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

2) EDGE PROCESSING TIME When the decision unit decides to ofﬂoad the task to an edge server (Xn= 1), the time processing per task is denoted by T e. This includes the transmission delay and computing delay. The computing delay depends on the CPU frequency of the edge server and other resources. Therefore, the processing time is

T

e nm

=

1 FeBn

(Ben

Ren

+FeDn

)

(8)

where Fe and Bn denote the CPU frequency of the edge server and the communication bandwidth, respectively. Sim-
ilarly, the corresponding power cost for task Mn of user n is denoted by pe and is deﬁned as

Penm=T enmpe

(9)

Therefore, the processing cost of edge computing is the combination of edge computing time and power consumption, as follows:

C

e
n=

N n=1

(αT

en+β

P

en)

(10)

3) PROCESSING TIME OF ADJACENT EDGE SERVER
When the decision unit decides to ofﬂoad a task to an adjacent edge server (Xn = 2), the time processing per task is denoted by T a. This include the transmission delay and computing delay. The computing delay depend on the CPU frequency of the adjacent edge server and other resources. Therefore, the processing time is

Tnam

=

1 FaBn

(BanRan

+

FaDn)

(11)

where Fa and Bn represents the CPU frequency of the adjacent edge server and communication bandwidth, respectively.
Similarly, the corresponding power cost for task Mn of user n is denoted by pa and is deﬁned as

Panm = Tnampa

(12)

Therefore, the processing cost of an adjacent edge computing server is the combination the corresponding computing time and power consumption, as follows:

Cna =

N n=1

(αTna

+

β Pan )

(13)

4) REMOTE PROCESSING TIME
When it is decided to ofﬂoad a task to the remote cloud
server (Xn= 3), the time processing per task is denoted by T c. This includes the transmission delay and the computing
delay. The former corresponds to two directions: from the
MD to the edge server or adjacent edge server (Tm,e or Tm,a), and from the edge server to the remote cloud (Te,c or Ta,c). We assume that Tm,e and Tm,a are similar, and thus we neglect one of them. The computing delay depends on the CPU
frequency of the assigned remote server and other resources.

VOLUME 8, 2020

We can compute the task processing time in the cloud by the following equation, as in [28]:

Tncm

=

1 FcBn

BcnRcn + FcDn

(14)

where Fc denotes the CPU frequency for processing in the

cloud for each user. The total time cost involving the process-

ing and transmission delay is:

Tnc = Tncm + Tnem

(15)

Similarly, the corresponding power cost for task Mn of user n is denoted by pc and is deﬁned as

Pcnm = Tncmpc

(16)

Therefore, the processing cost of a remote cloud server is

the combination of computing time and power consumption, as follows:

Cnc =

N n=1

(αTnc

+

β Pcn )

(17)

The total cost Ctotal of the MEC ofﬂoading system can expressed as

Ctotal =

N ( Cln (1 − Xn) (2 − Xn) (3 − Xn)

n=1

6

Xn (2−Xn) (3−Xn) Cen Xn (Xn −1) (Xn −3) Can

+

−

2

2

+ Xn(Xn−1)(Xn−2)Ccn )

(18)

6

We assume that there are ﬁve MDs in the network. MDs 1

and 5 choose to execute tasks locally, that is, Xn= 0, MD chooses to ofﬂoad tasks to the edge point, that is, Xn = 1, MD 3 chooses to ofﬂoad tasks to an adjacent edge, that is,
Xn = 2, and MD 4 chooses to ofﬂoad tasks to the remote cloud server, that is, Xn= 3. We use formula (14) to calculate the computing time and power consumption, that is, Ctotal = Cnl +Cne + Cna + Cnc. The notations used in this study are deﬁned in Table 2.

D. OPTIMIZATION PROBLEM FORMULATION

Our objective to minimize the processing and transmission
delay and reduce the power consumption for these two opera-
tions. The minimized cost is denoted by Qmin. We assume that the transmission and receiving bandwidth are equal βnt = βnr . The optimization problem of system utilization is formulated
as follows:

Qmin = minimize

N ( Cnl (1 − Xn) (2 − Xn) (3 − Xn)

n=1

6

Xn (2 − Xn) (3−Xn) Cne Xn (Xn −1) (Xn −3) Cna

+

−

2

2

Xn +

(Xn

−

1)

(Xn

−

2)

Cnc

)

(19)

6

under the constraints

N

N

Btn ≤ βt ;

βnr

≤

β

r

,

(β

t n

,

βnr

)

≥

0, ∀n

(20)

n=1

n=1

54079

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

TABLE 2. Notation list.
where βn>0, ∀n∈N Xn∈{0,1,2,3}, ∀n∈N
Xn = {X1, X2, . . . ,Xn} is the ofﬂoading decision; it has four modes and takes four values: 0, 1, 2, and 3. Additionally, the bandwidth is limited by constraint (16) on transmission tasks and receiving results to prevent congestion on the server, which may cause signiﬁcant delays. The optimization problem (15) is considered a mixed-integer problem, which is generally difﬁcult to solve. To minimize the system utilization cost, we propose a reinforcement learning technique based on deep SARSA.
V. SARSA LEARNING AUTONOMIC COMPUTATION OFFLOADING We assume that there multiple options for executing an ofﬂoadable task at the nearest edge, at an adjacent edge, or in the remote cloud. To determine the optimal location, we used deep reinforcement learning (SARSA). Thus, the performance of the edge server (ES) depends on the resource allocation mechanism and improves the simultaneous execution of tasks. However, the scheduling and resource allocation on the edge server are NP-hard scheduling problems. Most current studies use game theory and reinforcement learning. Therefore, we will develop an efﬁcient resource allocation mechanism to enhance MEC server utilization owing to the limited power and computational resources compared with cloud-computing servers. In our mechanism, the ofﬂoading decision algorithm (OD-SARSA) will be used for solving the resource management problem on the ES based on parameters derived from its environment, such as data size, bandwidth, edge-server workload, signal strength, and energy consumption. OD-SARSA is an effective method to achieve high utilization on the ES, owing to its ability to function
54080

as an on-policy technique, that is, it considers the current resource consumption state in the ES environment, which is highly important for resource management. For example, the current state obtained from the SARSA algorithm is used to determine whether current VMs should be employed or new VMs should be created on the ES. In the latter case, the VM manager on the ES is responsible for creating the VMs and assigns VMs to each ofﬂoaded task. One approach for the VM manager could be to activate VMs only on a few servers, depending on the ofﬂoaded tasks, whereas the other servers are put into sleep mode to save energy. However, the VM manager should also consider the users’ latency requirements, as the servers may be overload with many ofﬂoaded tasks, resulting in a load balancing issue. This will be more challenging when there is uncertainty in task arrival, and there is no central controller.
A. OFFLOADING-DECISION-BASED SARSA METHOD (OD-SARSA)
We should solve the optimization problem (19) and meet the QoS (e.g., energy consumption, or delay) requirements so that a deep SARSA function may be used to make an efﬁcient decision Xnm for ofﬂoading of each task to the appropriate location . The input of the SARSA function is the uploading bandwidth βnt m and downloading bandwidth βnrm as states. The output of the system is the value of Q for each state St of the corresponding action At . Each time, the agent selects the suitable action with regard to the Q value. The result of the action is to make identical adjustment to the ofﬂoading decision Xnm and determine the appropriate location (nearest to the edge server or adjacent to the edge server or remote cloud), as well as resource allocation βnt m and βnrm.
The SARSA function considers an on-policy mechanism, which implies that the agent learns based on its up-to-date action as a consequence of the current policy. OD-SARSA is described in Algorithm 1. It performs ofﬂoading and is trained through deep leaning. In SARSA, an epsilon-greedy policy is used for state transition; the Q value in the preceding state is updated by Equation (15), where the next action is selected by an epsilon-greedy policy. In the system, there are a target network and an evaluation network. The input system is the current state, and the following or next state are obtained after the selection of an action. We can choose the action based on the epsilon greedy (ε) policy. We use a probability of 1 − ε and select the best action, and thus the output of the target network is changed according to the reward, and the parameters are updated in each state, and a new policy is imposed.
Therefore, the actions a of the agent can be deﬁned as ofﬂoading to valid locations (nearest, adjacent, and remote). We assume 10 possible actions, as a follows: Al is local processing, AN is ofﬂoading to the nearest edge, Aa is ofﬂoading to an adjacent edge, AR is ofﬂoading to the remote cloud, ANA is migration from the nearest edge to an adjacent edge, AAN is migration from an adjacent edge to the nearest edge, ANR is migration from the nearest edge to the remote cloud,
VOLUME 8, 2020

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

Algorithm 1 OD-SARSA
1: Input: Number of MDs N and task size Dn 2: Output: efﬁcient ofﬂoading decision, cost reduction
and bandwidth allocation 3: Initialize the network parameters with upload and
download bandwidth, and processing cycle number 4: Initialize the number of iterations (episodes), let I =
100 5: for iteration I < 1,2,3,. . . ., I do 6: Select ‘‘Action’’ randomly. 7: Compute ‘‘Current State’’ according to formula No. 3 8: if‘‘Current state’’< (St + 1) then 9: Set rt = 1 10: else if St > St+1 then 11: Set rt = −1 12: else 13: Set rt = 0 14: end if 15: Obtain reward rt and next state St+1 after execution
of at . 16: Set this as (St , at , rt , St+1). 17: Compute the Q-value yt from the target deep QL yt =
rt+1 + γ QSt+1, at+1 18: Execute the algorithm of gradient descent to reduce
(yt − q (st+1, at+1) ; α)2 19: Update q-value: q∗ (s, a) = (1 − α) q (s, a) +
α(Rt+1 + γ q (st+1, at+1)) 20: end for
ARN is migration from the remote cloud to the nearest edge, AAR is migration from an adjacent edge to the remote cloud, and AAR is migration from the remote cloud to an adjacent edge. Thus, the actions of the agent can be represented as A (t) = {A1 (t) , A2 (t) . . . . . . . . . . . .Ak (t)}, where Ak (t) denotes the k-th ofﬂoading decision. If Ak (t) = 0, the task is processed locally, if Ak (t) = 1, the task is ofﬂoadable and processed on the edge server, if Ak (t) = 2, task is executed at the adjacent node, and if the Ak (t) = 3, the task is processed on the remote server. The agent learning state S can be deﬁned as the resources of the edge computing: processing (Sp), memory (Sm), and network bandwidth (Sb). Thus, the current system state can be represented as (t) ={S1 (t) ,S2 (t) . . . . . .Sn (t)}, where Si=(Spi, Smi, Sbi), i= 1 . . . . . .n.
In this system, a particular learning agent does not have information regarding the overall state of all nearest edges; the agent only has information regarding its local state. There is collaboration and communication between the agents to ofﬂoad tasks to appropriate locations at the edge network (nearest or adjacent edge) or in a public cloud.
Reward function: The main objective of computation ofﬂoading is to reduce the processing delay of intensive tasks. This primarily depends on the capability of the edge network, that is, processing, memory, and bandwidth. CE3C determines its processing capability by detecting its state,
VOLUME 8, 2020

estimates the response time, and chooses the appropriate location accordingly. After an action is performed, result S (t) is obtained. If S (t) is smaller than S (t − 1), a positive reward R (t) = +1 is given. If St is larger than St+1, we give a negative reward R (t) = −1; otherwise, R (t) = 0. The reward allows the agent to learn efﬁcient decision making for resource allocation and ofﬂoading for reduced energy consumption.
To update the value of Q for the state after an action, we use the Bellman equation as follows:

q∗ (s, a) = E[Rt+1 + α q∗(st+1, at+1)

(21)

The value of Q for a given state and action should be as close to the right-hand of the Bellman equation as possible so that the Q-value will ﬁnally converge to a safe value q∗.

q∗ (st+1, at+1)−q (s, a) < 0, system state; non − ofﬂoading

(22)

E[Rr+1 + αq∗ (st+1 − at+1) − E[

∞I
∝
I =0

Rt+I +1]

(23)

The method for computing the new value of Q for the state

and action pair (s, a) at a certain time is

q∗ (s, a) = (1 − α) q (s, a) + α(Rt+1 + γ q (st+1, at+1)) (24)

B. PERFORMANCE EVALUATION
We will now evaluate the proposed OD-SARSA algorithm. The model uses N task of M users to determine if the best action at a given time is to ofﬂoad or not (local processing). We give data sizes as input and output for each user. We aim to ﬁnd an optimal policy ofﬂoading function π . The ofﬂoading size can by expressed by NM , which increase with the number of tasks M per user N in MEC networks.
We assume that the number of mobile users is N = 5, and each user has ﬁve tasks. Table 1 shows all parameters that are used in reinforcement learning. We set the local processing time for an MD to 3.75×10−7s/bit, and the corresponding power consumption to 3.55 × 10−6 J/bit. We assume that the size of all tasks is distributed between 10 and 35 MB. Regarding the other network parameters, such as bandwidth, we assume that the bandwidth for both uplink and downlink between a user and an edge server is 150 MB and may change depending on network conditions. The rate of the CPU of an edge sever is 9 ×108 cycle/s. The MDs’ transmission and receiving energy consumption are both 1.60 ×10−6 J/bit. We train the model using 100 episodes.
There is close similarity between Q-learning and SARSA, but SARSA uses an on-policy technique. This encouraged us to use it for improved ofﬂoading performance to the ES, particularly because it does not depend on explicitly learning the agent’s policy function. The results are shown in Fig. 3 and demonstrate that SARSA outperforms Q-learning, with an improvement rate of up to 8%. When the number of iterations increases, the improvement increases as well. It is noted that QL is better than SARSA in a faster training scenario

54081

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

FIGURE 3. Performance of SARSA and Q-Learning.

FIGURE 5. DO-SARSA performance under different learning rates.

FIGURE 4. System utility under different parameters γ and µ.
(when the number of iterations is less than 50), but for more than 60 iterations, SARSA is consistently better than QL. We can conclude for increased the training iterations, the gap between SARSA and Q widens, with increased gain rewards. This affects performance in favor of the SARSA method.
The system utility under the different parameters γ and µ, which denote learning rate and weight rate respectively, are shown in Fig. 4 by comparing the proposed OD-SARSA with other algorithms: Q-learning, edge processing, and local processing. The results indicate the superiority of OD-SARSA to the other algorithms. The main problem with deep learning modules is choosing a learning rate and optimizer (the hyper-parameters). Therefore, we study our algorithm under different learning rates. After 100 iterations, we notice that a learning rate of 0.001 is stable and appropriate for the proposed method (Fig. 5). In contrast, the results for other values are unstable, and large dispersion are observed, particularly when LR = 0.01.
54082

FIGURE 6. Performance of OD-SARSA under different learning rates.
Based on the comparison between various different learning rates, we studied the performance cost corresponding to 0.001 and 0.0001. We notice that the total cost of DO-SARS for a learning rate of 0.001 is signiﬁcantly lower than that for 0.0001 (Fig. 6). At the beginning of the training, we notice that the gap is large owing to the increased performance cost. Nevertheless, as iterations increase, this gap decreases, and the costs are equal in the last iteration.
We observe that Q-learning correctly selects the optimal path in several applications, but it occasionally fails in critical stages, which require an important and critical decision, owing to the ε-greedy action selection. In our study, we demonstrated that SARS is better at making decisions in critical situations, as it is considered stable, particularly because it learns the safe path. This is highly important in making critical decisions. To attain better results in practice with on-policy RL techniques, the epsilon parameter should be reduced over time. Fig. 7 shows the effect of varying epsilon on the ofﬂoading decision. We notice that when ε = 0.80, we obtain satisfactory results, and maximum rewards are achieved; thus, this value was adopted in this study. Degenerate levels (0.20: 75) of course yield suboptimal results. It is conceivable that this caused by the short timescale the agent actions.
The result of the optimization problem (eq. 19) is shown in Fig. 8, where the number of ofﬂoadable and non-ofﬂoadable tasks can be seen. We notice that as the
VOLUME 8, 2020

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

FIGURE 7. Effect of epsilon on acquiring rewards using OD-SARSA.
FIGURE 8. Number of offloaded tasks vs. non-offloadable tasks.
training iterations increase, the ‘‘ofﬂoadable’’ decisions increase, regardless of the ofﬂoading location (edge server, adjacent edge server, or a remote server). At the beginning of the training, the difference between these numbers is small, but subsequently, it gradually increases.
VI. CONCLUSION In this paper, we assumed that there are MECNs in more than one region, consisting of multiple APs, multi-edge servers, and N MDs, where each MD has independent massive realtime tasks. The MD can connect to an MECN through an AP or a mobile network. Each task can be processed locally by the MD itself or remotely. There are three ofﬂoading options: nearest edge server, adjacent edge server, and remote cloud. We propose a reinforcement-learning-based SARSA method to solve the optimization problem for making decisions regarding ofﬂoading to one of the previously mentioned locations to reduce system cost, including energy consumption and computing time delay. It was demonstrated that on this problem, OD-SARSA performed better than RL-QL. Therefore, in ofﬂoading to adjacent edge servers, the proposed method resolves most challenges faced by CPSSs and achieves optimal results in terms of volume, variety, velocity, and veracity. In future, we will consider the code ofﬂoading on edge devices with GPUs that connected with mobile devices.
VOLUME 8, 2020

ACKNOWLEDGMENT This work was supported by the Deanship of Scientiﬁc Research at King Saud University through the Vice Deanship of Scientiﬁc Research Chairs. Chair of Smart Technologies. The authors also thank the RSSU at King Saud University for their technical support.
REFERENCES
[1] M. Satyanarayanan, ‘‘Mobile computing: The next decade,’’ in Proc. 1st ACM Workshop Mobile Cloud Comput. Services, Social Netw. Beyond (MCS). New York, NY, USA: ACM, Jun. 2010, pp. 1–6.
[2] G. H. Forman and J. Zahorjan, ‘‘The challenges of mobile computing,’’ Commun. ACM, vol. 36, no. 7, pp. 75–84, 1993.
[3] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, ‘‘Fog computing and its role in the Internet of Things,’’ in Proc. 1st MCC Workshop Mobile Cloud Comput., 2012, pp. 13–16.
[4] Y. C. Hu, M. Patel, D. Sabella, N. Sprecher, and V. Young, ‘‘Mobile edge computing—A key technology towards 5G,’’ ETSI White Paper, vol. 11, no. 11, pp. 1–16, 2015.
[5] S. Ranadheera, S. Maghsudi, and E. Hossain, ‘‘Computation ofﬂoading and activation of mobile edge computing servers: A minority game,’’ 2017, arXiv:1710.05499. [Online]. Available: http://arxiv.org/abs/1710.05499
[6] D. Mazza, D. Tarchi, and G. E. Corazza, ‘‘A cluster based computation ofﬂoading technique for mobile cloud computing in smart cities,’’ in Proc. IEEE Int. Conf. Commun. (ICC), May 2016, pp. 1–6.
[7] J. Liu, Y. Mao, J. Zhang, and K. B. Letaief, ‘‘Delay-optimal computation task scheduling for mobile-edge computing systems,’’ 2016, arXiv:1604.07525. [Online]. Available: http://arxiv.org/abs/1604.07525
[8] Y. Mao, J. Zhang, and K. B. Letaief, ‘‘Dynamic computation ofﬂoading for mobile-edge computing with energy harvesting devices,’’ IEEE J. Sel. Areas Commun., vol. 34, no. 12, pp. 3590–3605, Dec. 2016.
[9] K. Habak, M. Ammar, K. A. Harras, and E. Zegura, ‘‘Femto clouds: Leveraging mobile devices to provide cloud service at the edge,’’ in Proc. IEEE 8th Int. Conf. Cloud Comput., Jun. 2015, pp. 9–16.
[10] X. Wei, S. Wang, A. Zhou, J. Xu, S. Su, S. Kumar, and F. Yang, ‘‘MVR: An architecture for computation ofﬂoading in mobile edge computing,’’ in Proc. IEEE Int. Conf. Edge Comput. (EDGE), Jun. 2017, pp. 232–235.
[11] F. Messaoudi, A. Ksentini, and P. Bertin, ‘‘On using edge computing for computation ofﬂoading in mobile network,’’ in Proc. IEEE Global Commun. Conf. (GLOBECOM), Dec. 2017, pp. 1–7.
[12] X. Chen, L. Jiao, W. Li, and X. Fu, ‘‘Efﬁcient multi-user computation ofﬂoading for mobile-edge cloud computing,’’ IEEE/ACM Trans. Netw., vol. 24, no. 5, pp. 2795–2808, Oct. 2016.
[13] J. Dolezal, Z. Becvar, and T. Zeman, ‘‘Performance evaluation of computation ofﬂoading from mobile device to the edge of mobile network,’’ in Proc. IEEE Conf. Standards for Commun. Netw. (CSCN), Oct. 2016, pp. 1–7.
[14] M. Kamoun, W. Labidi, and M. Sarkiss, ‘‘Joint resource allocation and ofﬂoading strategies in cloud enabled cellular networks,’’ in Proc. IEEE Int. Conf. Commun. (ICC), Jun. 2015, pp. 5529–5534.
[15] K. Zhang, Y. Mao, S. Leng, Q. Zhao, L. Li, X. Peng, L. Pan, S. Maharjan, and Y. Zhang, ‘‘Energy-efﬁcient ofﬂoading for mobile edge computing in 5G heterogeneous networks,’’ IEEE Access, vol. 4, pp. 5896–5907, 2016.
[16] C. Luo, S. Salinas, M. Li, and P. Li, ‘‘Energy-efﬁcient autonomic ofﬂoading in mobile edge computing,’’ in Proc. IEEE 15th Intl Conf Dependable, Autonomic Secure Comput., 15th Intl Conf Pervas. Intell. Comput., 3rd Intl Conf Big Data Intell. Comput. Cyber Sci. Technol. Congr. (DASC/PiCom/DataCom/CyberSciTech), Nov. 2017, pp. 581–588.
[17] A. Kaur and R. Kaur, ‘‘An efﬁcient framework for improved task ofﬂoading in edge computing,’’ in Proc. Int. Conf. Intell., Secure, Dependable Syst. Distrib. Cloud Environ. Cham, Switzerland: Springer, 2018, pp. 94–101.
[18] M. E. Khoda, M. A. Razzaque, A. Almogren, M. M. Hassan, A. Alamri, and A. Alelaiwi, ‘‘Efﬁcient computation ofﬂoading decision in mobile cloud computing over 5G network,’’ Mobile Netw. Appl., vol. 21, no. 5, pp. 777–792, Oct. 2016.
[19] T. Zhao, S. Zhou, X. Guo, Y. Zhao, and Z. Niu, ‘‘A cooperative scheduling scheme of local cloud and Internet cloud for delay-aware mobile cloud computing,’’ in Proc. IEEE Globecom Workshops (GC Wkshps), Dec. 2015, pp. 1–6.
[20] X. Guo, R. Singh, T. Zhao, and Z. Niu, ‘‘An index based task assignment policy for achieving optimal power-delay Tradeoff in edge cloud systems,’’ in Proc. IEEE Int. Conf. Commun. (ICC), May 2016, pp. 1–7.
54083

T. Alfakih et al.: Task Offloading and Resource Allocation for MEC by Deep Reinforcement Learning Based on SARSA

[21] V. Di Valerio and F. Lo Presti, ‘‘Optimal virtual machines allocation in mobile Femto-Cloud computing: An MDP approach,’’ in Proc. IEEE Wireless Commun. Netw. Conf. Workshops (WCNCW), Apr. 2014, pp. 7–11.
[22] A. E. Eshratifar and M. Pedram, ‘‘Energy and performance efﬁcient computation ofﬂoading for deep neural networks in a mobile cloud computing environment,’’ in Proc. Great Lakes Symp. VLSI (GLSVLSI), 2018, pp. 111–116.
[23] C.-F. Liu, M. Bennis, M. Debbah, and H. V. Poor, ‘‘Dynamic task ofﬂoading and resource allocation for ultra-reliable low-latency edge computing,’’ IEEE Trans. Commun., vol. 67, no. 6, pp. 4132–4150, Jun. 2019.
[24] A. Defazio and T. Graepel, ‘‘A comparison of learning algorithms on the arcade learning environment,’’ 2014, arXiv:1410.8620. [Online]. Available: https://arxiv.org/abs/1410.8620
[25] D. Zhao, H. Wang, K. Shao, and Y. Zhu, ‘‘Deep reinforcement learning with experience replay based on SARSA,’’ in Proc. IEEE Symp. Ser. Comput. Intell. (SSCI), Dec. 2016, pp. 1–6.
[26] G. A. Rummery and M. Niranjan, ‘‘On-line Q-learning using connectionist systems,’’ Ph.D. dissertation, Dept. Eng., Univ. Cambridge, Cambridge, U.K., 1994.
[27] L. Huang, X. Feng, L. Qian, and Y. Wu, ‘‘Deep reinforcement learningbased task ofﬂoading and resource allocation for mobile edge computing,’’ in Proc. Int. Conf. Mach. Learn. Intell. Commun. Cham, Switzerland: Springer, 2018, pp. 33–42.
[28] J. Xu, Z. Hao, and X. Sun, ‘‘Optimal ofﬂoading decision strategies and their inﬂuence analysis of mobile edge computing,’’ Sensors, vol. 19, no. 14, p. 3231, 2019.
TAHA ALFAKIH received the B.S. degree in computer science from the Computer Science Department, Hadhramout University, Yemen, and the M.Sc. degree from the Computer Science Department, King Saud University (KSU), Riyadh, Saudi Arabia. He is currently pursuing the Ph.D. degree with the Information Systems Department, KSU. He also works as a Researcher with the Computer Science College, KSU. His research interests include machine learning, mobile edge computing, and the Internet of Things (IoT).
MOHAMMAD MEHEDI HASSAN (Senior Member, IEEE) received the Ph.D. degree in computer engineering from Kyung Hee University, South Korea, in February 2011. He is currently an Associate Professor with the Information Systems Department, College of Computer and Information Sciences (CCIS), King Saud University (KSU), Riyadh, Saudi Arabia. He has authored or coauthored more than 180 publications, including refereed IEEE/ACM/Springer/Elsevier journals, conference papers, books, and book chapters. His research interests include cloud computing, edge computing, the Internet of Things, body sensor networks, big data, deep learning, mobile cloud, smart computing, wireless sensor networks, 5G networks, and social networks. Recently, his four publications have been recognized as the ESI Highly Cited Papers. He was a recipient of number of awards, including the Best Journal Paper Award from the IEEE SYSTEMS JOURNAL, in 2018, the Best Paper Award from CloudComp 2014 Conference, and the Excellence in Research Award from King Saud University, in 2015 and 2016. He has served as the Chair and a Technical Program Committee Member of numerous reputed international conferences/workshops, such as IEEE CCNC, ACM BodyNets, and IEEE HPCC.

ABDU GUMAEI received the B.S. degree from the Computer Science Department, Al-Mustansiriya University, Baghdad, Iraq, the master’s degree from the Computer Science Department, King Saud University, Riyadh, Saudi Arabia, and the Ph.D. degree from King Saud University, in 2019, all in computer science. He has worked as a Lecturer and taught many courses, such as programming languages at the Computer Science Department, Taiz University. He is currently an Assistant Professor with the College of Computer and Information Sciences, King Saud University. He has several types of research in the ﬁeld of image processing. His research interests include software engineering, image processing, computer vision, machine learning, networks, and the Internet of Things (IoT). He has received a patent from the United States Patent and Trademark Ofﬁce (USPTO), in 2013.
CLAUDIO SAVAGLIO received the Ph.D. degree in information and communication technology from the Department of Informatics, Modeling, Electronics and Systems (DIMES), University of Calabria. He is currently a Research Fellow with the Department of Informatics, Modeling, Electronics and Systems (DIMES), University of Calabria. He has been a Visiting Scholar with the Eindhoven University of Technology, The Netherlands; The University of Texas; the New Jersey Institute of Technology, USA; and the Universitat Politècnica de València, Spain. He is also the author of more than 30 articles in international journals, conferences, and book chapters. His research interests include autonomic and cognitive Internet of Things systems, cyber-physical networks, edge computing, and agent-oriented middleware and development methodologies. He has also served with different roles (the Chair, an Organizer, a Program Committee Member, a Guest Editor, and a Reviewer) in international journals, conferences, and book series.
GIANCARLO FORTINO (Senior Member, IEEE) received the Ph.D. degree in computer engineering from the University of Calabria (Unical), Italy, in 2000. He is currently a Full Professor of computer engineering with the Department of Informatics, Modeling, Electronics, and Systems, Unical. He is also a Guest Professor with the Wuhan University of Technology, Wuhan, China, a High-End Expert with HUST, China, and a Senior Research Fellow with the Italian National Research Council ICAR Institute. He is also the Director of the SPEME Laboratory, Unical, and the Co-Chair of Joint labs on IoT established between Unical and WUT and SMU Chinese universities, respectively. His research interests include agent-based computing, wireless (body) sensor networks, and the Internet of Things. He is the author of over 400 articles in international journals, conferences, and books. He is currently a member of the IEEE SMCS BoG and the IEEE Press BoG. He is also the Chair of the IEEE SMCS Italian Chapter. He is a cofounder and the CEO of SenSysCal S. r. l., a Unical spinoff focused on innovative IoT systems. He is a (founding) Series Editor of the IEEE Press Book Series on Human-Machine Systems and the EiC of Internet of Things (Springer) series and an AE of many international journals, such as the IEEE TRANSACTIONS ON AUTOMATIC CONTROL (TAC), the IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS (THMS), the IEEE INTERNET OF THINGS JOURNAL (IoTJ), the IEEE SYSTEMS JOURNAL (SJ), IEEE SMCM, Information Fusion, JNCA, and EAAI.

54084

VOLUME 8, 2020

