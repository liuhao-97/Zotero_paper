IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

1985

Deep Reinforcement Learning for Task OfÔ¨Çoading in Mobile Edge Computing Systems

Ming Tang, Member, IEEE and Vincent W.S. Wong , Fellow, IEEE

Abstract‚ÄîIn mobile edge computing systems, an edge node may have a high load when a large number of mobile devices ofÔ¨Çoad their tasks to it. Those ofÔ¨Çoaded tasks may experience large processing delay or even be dropped when their deadlines expire. Due to the uncertain load dynamics at the edge nodes, it is challenging for each device to determine its ofÔ¨Çoading decision (i.e., whether to ofÔ¨Çoad or not, and which edge node it should ofÔ¨Çoad its task to) in a decentralized manner. In this work, we consider non-divisible and delaysensitive tasks as well as edge load dynamics, and formulate a task ofÔ¨Çoading problem to minimize the expected long-term cost. We propose a model-free deep reinforcement learning-based distributed algorithm, where each device can determine its ofÔ¨Çoading decision without knowing the task models and ofÔ¨Çoading decision of other devices. To improve the estimation of the long-term cost in the algorithm, we incorporate the long short-term memory (LSTM), dueling deep Q-network (DQN), and double-DQN techniques. Simulation results show that our proposed algorithm can better exploit the processing capacities of the edge nodes and signiÔ¨Åcantly reduce the ratio of dropped tasks and average delay when compared with several existing algorithms.
Index Terms‚ÄîMobile edge computing, computation ofÔ¨Çoading, resource allocation, deep reinforcement learning, deep Q-learning
√á

1 INTRODUCTION
1.1 Background and Motivation
NOWADAYS, mobile devices are responsible for processing many computational intensive tasks, such as data processing and artiÔ¨Åcial intelligence. Despite the development of mobile devices, these devices may not be able to process all their tasks locally with a low latency due to their limited computational resources. To facilitate efÔ¨Åcient task processing, mobile edge computing (MEC) [1], also known as fog computing [2] and multi-access edge computing [3], is introduced. MEC facilitates mobile devices to ofÔ¨Çoad their computational tasks to nearby edge nodes for processing in order to reduce the task processing delay. It can also reduce the ratio of dropped tasks for those delay-sensitive tasks.
In MEC, there are two main questions related to task offloading. The Ô¨Årst question is whether a mobile device should ofÔ¨Çoad its task to an edge node or not. The second question is that if a mobile device decides to perform ofÔ¨Çoading, then which edge node should the device ofÔ¨Çoad its task to. To address these questions, some existing works have proposed task ofÔ¨Çoading algorithms. Wang et al. in [4] proposed an algorithm to determine the ofÔ¨Çoading decisions to maximize the network revenue. Bi et al. in [5] considered a wirelesspowered MEC scenario and proposed an algorithm to optimize the ofÔ¨Çoading and power transfer decisions. In the works [4], [5], the processing capacity that each device obtained from an edge node is independent of the number of tasks ofÔ¨Çoaded to the edge node.
 The authors are with the Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada. E-mail: {mingt, vincentw}@ece.ubc.ca.
Manuscript received 28 Mar. 2020; revised 31 Oct. 2020; accepted 3 Nov. 2020. Date of publication 10 Nov. 2020; date of current version 5 May 2022. (Corresponding author: Vincent W.S. Wong) Digital Object IdentiÔ¨Åer no. 10.1109/TMC.2020.3036871

In practice, however, edge nodes may have limited processing capacities, so the processing capacity that an edge node allocates to a mobile device depends on the load level at the edge node (i.e., number of concurrent tasks ofÔ¨Çoaded to the edge node). When a large number of mobile devices offload their tasks to the same edge node, the load at that edge node can be high, and hence those ofÔ¨Çoaded tasks may experience large processing delay. Some of the tasks may even be dropped when their deadlines expire. Some existing works have addressed the load levels at the edge nodes and proposed centralized task ofÔ¨Çoading algorithms. Eshraghi et al. in [6] proposed an algorithm that optimizes the ofÔ¨Çoading decisions of mobile devices, considering their uncertain computational requirements. Lyu et al. in [7] focused on delay-sensitive tasks and proposed an algorithm to minimize the task ofÔ¨Çoading energy consumption subject to the task deadline constraint. In [8], Chen et al. designed a centralized algorithm for a software-deÔ¨Åned ultra-dense network to minimize task delay. In [9], Poularakis et al. studied the joint optimization of task ofÔ¨Çoading and routing. The operation of these centralized algorithms in [6], [7], [8], [9], however, may require complete information of the system.
Other works have proposed distributed task ofÔ¨Çoading algorithms considering the load levels at the edge nodes, where each mobile device makes its ofÔ¨Çoading decision in a decentralized manner. Designing such a distributed algorithm is challenging. This is because when a device makes an ofÔ¨Çoading decision, the device does not know a priori the load levels at the edge nodes, since the load also depends on the ofÔ¨Çoading decisions of other mobile devices. In addition, the load levels at the edge nodes may change over time. To address these challenges, Lyu et al. in [10] focused on divisible tasks and proposed a Lyapunov-based algorithm to ensure the stability of the task queues. In [11], Li et al. considered the strategic ofÔ¨Çoading interaction among

1536-1233 √ü 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tps://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1986

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

mobile devices and proposed a price-based distributed algorithm. Shah-Mansouri et al. in [12] designed a potential game-based ofÔ¨Çoading algorithm to maximize the quality-of-experience of each device. Josilo et al. in [13] designed a distributed algorithm based on a Stackelberg game. Yang et al. in [14] proposed a distributed ofÔ¨Çoading algorithm to address the wireless channel competition among mobile devices. Neto et al. in [15] proposed an estimation-based method, where each device makes its ofÔ¨Çoading decision based on the estimated processing and transmission capacities. Lee et al. in [16] proposed an algorithm based on online optimization techniques to minimize the maximum delay of the tasks.
In this work, we focus on the task ofÔ¨Çoading problem in an MEC system and propose a distributed algorithm that addresses the unknown load level dynamics at the edge nodes. Comparing with the aforementioned works [10], [11], [12], [13], [14], [15], [16], we consider a different and more realistic MEC scenario. First, the existing work [10] considered divisible tasks (i.e., tasks can be arbitrarily divided), which may not be realistic due to the dependency among the bits in a task. Although the works [11], [12], [13], [14], [15] considered non-divisible tasks, they do not consider the underlying queuing systems. As a result, the processing and transmission of each task should always be accomplished within one time slot (or before the arrival of the next task), which may not always be guaranteed in practice. Different from those works [10], [11], [12], [13], [14], [15], we consider non-divisible tasks together with queuing systems and take into account the practical scenario where the processing and transmission of a task can continue for multiple time slots. This scenario is challenging to deal with, because when a new task arrives, its delay can be affected by the decisions of the tasks of other devices arrived in the previous time slots. Second, different from the related works [10], [11], [12], [13], [14], [15], [16] considered delay-tolerant tasks, we focus on delay-sensitive tasks with processing deadlines. This is challenging to address since the deadlines can affect the load levels at the edge nodes and hence the delay of the ofÔ¨Çoaded tasks.
Under the aforementioned MEC system, it is difÔ¨Åcult to apply traditional methods such as game theory and online optimization due to the complicated interaction among the tasks. To address the challenges, in this work, we propose to use deep Q-learning [17], which is a model-free deep reinforcement learning (DRL) technique. This approach enables the agents to make decisions based on local observations without the knowledge of the system modeling and dynamics. Some existing works such as [18], [19], [20] have proposed DRL-based algorithms for MEC systems, while they focused on centralized ofÔ¨Çoading algorithms. Zhao et al. in [21] proposed a DRL-based distributed ofÔ¨Çoading algorithm that addresses the wireless channel competition among mobile devices, while the algorithm at each mobile device requires the quality-of-service information of other mobile devices. Different from those works [18], [19], [20], [21], we aim to propose a DRL-based distributed algorithm that addresses the unknown load dynamics at edge nodes. It enables each mobile device to make its ofÔ¨Çoading decision without knowing the information (e.g., task models, offloading decisions) of other mobile devices.

1.2 Solution Approach and Contributions
In this work, we take into account the unknown load level dynamics at the edge nodes and propose a DRL-based distributed ofÔ¨Çoading algorithm for the MEC system. In the proposed algorithm, each mobile device can determine the ofÔ¨Çoading decision in a decentralized manner using its information observed locally, including the size of its task, the information of its queues, and the historical load levels at the edge nodes. The main contributions are as follows.
 Task OfÔ¨Çoading Problem for the MEC System: We formulate a task ofÔ¨Çoading problem for non-divisible and delay-sensitive tasks. The problem takes into account the load level dynamics at the edge nodes and aims at minimizing the expected long-term cost of the tasks (considering the delay of the tasks and the penalties for those tasks being dropped).
 DRL-based Task OfÔ¨Çoading Algorithm: To achieve the expected long-term cost minimization, we propose a model-free DRL-based distributed ofÔ¨Çoading algorithm that enables each mobile device to make its offloading decision without knowing the task models and ofÔ¨Çoading decisions of other mobile devices. To improve the estimation of the expected long-term cost in the proposed algorithm, we incorporate the long short-term memory (LSTM), dueling deep Q-network (DQN), and double-DQN techniques.
 Performance Evaluation: We perform simulations and show that when compared with the potential game based ofÔ¨Çoading algorithm (PGOA) in [14] and the user-level online ofÔ¨Çoading framework (ULOOF) in [15], our proposed algorithm can better exploit the processing capacities of the mobile devices and edge nodes, and it can signiÔ¨Åcantly reduce the ratio of dropped tasks and the average delay.
The rest of this paper is organized as follows. The system model is presented in Section 2, and the problem formulation is given in Section 3. We present the DRL-based algorithm in Section 4 and evaluate its performance in Section 5. Conclusion is given in Section 6. For notation, we use Z√æ√æ to denote the set of positive integers.
2 SYSTEM MODEL
We consider a set of edge nodes N ¬º f1; 2; . . . ; Ng and a set of mobile devices M ¬º f1; 2; . . . ; Mg in an MEC system. We focus on one episode that contains a set of time slots T ¬º f1; . . . ; T g, where each time slot has a duration of D seconds. In the following, we present the mobile device and edge node models, with an illustration given in Fig. 1.
2.1 Mobile Device Model
We focus on computational tasks of mobile devices, where each task is non-divisible such that it can either be processed locally or be ofÔ¨Çoaded to an edge node for processing. We assume that at the beginning of each time slot, a mobile device has a new task arrival with a certain probability. This assumption is consistent with some existing works (e.g., [22]). When a mobile device has a new task arrival, it Ô¨Årst needs to decide whether to process the task locally or offload it to an edge node. If the mobile device decides to

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1987

Fig. 1. An illustration of an MEC system with a mobile device m 2 M and an edge node n 2 N .
process the task locally, then its scheduler (see Fig. 1) will place the task to the computation queue for processing. Otherwise, the mobile device needs to decide which edge node the task is ofÔ¨Çoaded to. The scheduler will then place the task to the transmission queue for ofÔ¨Çoading. The ofÔ¨Çoaded task will then be sent to the chosen edge node through a wireless link. For the computation (or transmission) queue, we assume that if the processing (or transmission) of a task is completed in a time slot, then the next task in the queue will be processed (or transmitted) at the beginning of the next time slot. This assumption is consistent with some existing works considering queuing dynamics in an MEC system (e.g., [22]).
In the following, we Ô¨Årst present the task model and the task ofÔ¨Çoading decision, respectively. Then, we introduce the computation and transmission queues.
2.1.1 Task Model
At the beginning of time slot t 2 T , if mobile device m 2 M has a newly arrived task, then we deÔ¨Åne a variable km√∞t√û 2 Z√æ√æ to denote the unique index of the task. If mobile device m does not have a new task arrival at the beginning of time slot t, then km√∞t√û is set to zero for presentation simplicity.
Let m√∞t√û (in bits) denote the number of bits of the newly arrived task at the beginning of time slot t 2 T . If there exists a new task km√∞t√û at the beginning of time slot t, then m√∞t√û is equal to the size of task km√∞t√û. Otherwise, m√∞t√û is set to zero. We set the size of task km√∞t√û to be from a discrete set L , f1; 2; . . . ; jLjg with jLj available values. Hence, m√∞t√û 2 L [ f0g. In addition, task km√∞t√û requires a processing density of rm (in CPU cycles per bit), i.e., the number of CPU cycles required to process a unit of data. Task km√∞t√û has a deadline tm (in time slots). That is, if task km√∞t√û has not been completely processed by the end of time slot t √æ tm √Ä 1, then it will be dropped immediately.
2.1.2 Task OfÔ¨Çoading Decision
If mobile device m 2 M has a newly arrived task km√∞t√û at the beginning of time slot t 2 T , then it needs to make the ofÔ¨Çoading decision for task km√∞t√û as follows.
First, let binary variable xm√∞t√û 2 f0; 1g denote whether task km√∞t√û is to be processed locally or ofÔ¨Çoaded to an edge node. We set xm√∞t√û ¬º 1 (or 0) if the task is to be processed locally (or to be ofÔ¨Çoaded to an edge node). At the beginning of time slot t, m√∞t√ûxm√∞t√û is the number of bits arrived at the computation queue of mobile device m, and m√∞t√û√∞1 √Ä xm√∞t√û√û is the number of bits arrived at the transmission queue of mobile device m.

Second, if task km√∞t√û is to be ofÔ¨Çoaded to an edge node, then let binary variable ym;n√∞t√û 2 f0; 1g denote whether task km√∞t√û is ofÔ¨Çoaded to edge node n 2 N or not. We set ym;n√∞t√û ¬º 1 if task km√∞t√û is ofÔ¨Çoaded to edge node n, and ym;n√∞t√û ¬º 0 otherwise. Let ym√∞t√û ¬º √∞ym;n√∞t√û; n 2 N √û. Note that each task can be ofÔ¨Çoaded to one edge node, i.e.,

X

ym;n√∞t√û ¬º 1√∞xm√∞t√û ¬º 0√û; m 2 M; t 2 T ;

(1)

n2N

where the indicator 1√∞z 2 Z√û ¬º 1 if z 2 Z, and is equal to zero otherwise.

2.1.3 Computation Queue

The computation queue operates in a Ô¨Årst-in Ô¨Årst-out (FIFO)
manner. The arrivals are the tasks to be processed locally.
We consider one CPU which processes the tasks in the computation queue. Let fmdevice (in CPU cycles per second) denote the processing capacity of the CPU of mobile device m 2 M. The value of fmdevice is a constant. At the beginning of time slot t 2 T , if task km√∞t√û is placed in the computation queue, then we deÔ¨Åne lcmomp√∞t√û 2 T to denote the time slot when task km√∞t√û has either been processed or dropped.
Without loss of generality, if either task km√∞t√û is not placed
in the computation queue or km√∞t√û ¬º 0, then we set lcmomp√∞t√û ¬º 0.
Let dcmomp√∞t√û (in time slots) denote the number of time slots that task km√∞t√û will wait for processing if it is placed in the
computation queue. Note that mobile device m will compute the value of dcmomp√∞t√û before it decides the queue to place the task. Given lcmomp√∞t0√û for t0 < t, the value of dcmomp√∞t√û is computed as follows. For m 2 M and t 2 T ,

!√æ

dcmomp√∞t√û ¬º

max
t0 2f0;1;...;t√Ä1g

lcmomp√∞t0√û

√Ä

t

√æ

1

;

(2)

where the operator ¬Ωz¬ä√æ ¬º maxf0; zg, and we set lcmomp√∞0√û ¬º 0. SpeciÔ¨Åcally, the term maxt02f0;1;2;...;t√Ä1glcmomp√∞t0√û determines the time slot when all the tasks placed in the computation

queue before time slot t has either been processed or

dropped. Hence, dcmomp√∞t√û determines the number of time slots that task km√∞t√û should wait for processing. For exam-

ple, suppose task km√∞1√û is placed in the computation queue,

and its processing will be completed in time slot 5, i.e.,

lcmomp√∞1√û ¬º 5. Meanwhile, suppose km√∞2√û ¬º 0, i.e., lcmomp√∞2√û ¬º 0. At the beginning of time slot 3, if task km√∞3√û is placed in

the computation queue, then its processing will start after

time slot lcmomp ¬Ωmaxf5; 0g √Ä 3

√∞1√û ¬º √æ 1¬ä√æ

5. ¬º

Hence, it should 3 time slots.

wait

for

dcmomp√∞3√û

¬º

If mobile device m 2 M places task km√∞t√û in the computa-

tion queue at the beginning of time slot t 2 T (i.e.,

xm√∞t√û ¬º 1), then task km√∞t√û will have either been processed

or

dropped

in

time

slot (

lcmomp√∞t√û:

$

%

lcmomp√∞t√û ¬º min

t √æ dcmomp√∞t√û √æ )

m√∞t√û fmdeviceD=rm

√Ä 1;

(3)

t √æ tm √Ä 1 ;

where d√Åe is the ceiling function. SpeciÔ¨Åcally, the processing of task km√∞t√û will start at the beginning of time slot t √æ

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1988

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

dcmomp√∞t√û.√ÜThe number of tim√áe slots required to process the task is m√∞t√û=√∞fmdeviceD=rm√û . Hence, the Ô¨Årst term in the min operator is the time slot when the processing of task
km√∞t√û will be completed without considering the deadline of
the task. The second term is the time slot when task km√∞t√û will be dropped. As a result, lcmomp√∞t√û determines the time slot when task km√∞t√û will either be processed or dropped.

2.1.4 Transmission Queue

The transmission queue operates in a FIFO manner. The

arrivals are the tasks to be ofÔ¨Çoaded to the edge nodes. The

wireless network link interface at the mobile device sends

the tasks in the transmission queue to the chosen edge

node. The wireless network model and the transmission

rate from a mobile device to an edge node are as follows.

We consider a wireless network model where mobile devi-

ces transmit on orthogonal channels. The wireless transmis-

sion from a mobile device to an edge node suffers from path loss and small-scale fading [11], [23]. Let jhm;nj2 denote the channel gain from mobile device m 2 M to edge node n 2

N . Let P denote the transmission power of a mobile device.

The transmission rate from mobile device m to edge node n,

denoted by rtmra;nn (in bits per second), is computed as follows:

!

rtmra;nn ¬º W log2

1

√æ

jhm;nj2 s2

P

; m 2 N;n 2 N;

(4)

where W denotes the bandwidth allocated to a channel, and s2 denotes the received noise power at the edge node. The value of rtmra;nn is assumed to be a constant.
At the beginning of time slot t 2 T , if task km√∞t√û is placed
in the transmission queue for ofÔ¨Çoading, then we deÔ¨Åne a variable ltmran√∞t√û 2 T to denote the time slot when task km√∞t√û has been either sent or dropped. Without loss of generality,
if either task km√∞t√û is not placed in the transmission queue or km√∞t√û ¬º 0, then we set ltmran√∞t√û ¬º 0. Let dtmran√∞t√û (in time slots) denote the number of time slots that task km√∞t√û should
wait for transmission if it is placed in the transmission
queue. Note that mobile device m will compute the value of dtmran√∞t√û before it has decided on which queue to place the task. Given ltmran√∞t0√û for t0 < t, the value of dtmran√∞t√û is computed as follows. For m 2 M and t 2 T ,

!√æ

dtmran√∞t√û ¬º

max
t0 2f0;1;...;t√Ä1g

ltmran√∞t0√û

√Ä

t

√æ

1

;

(5)

where we set ltmran√∞0√û ¬º 0 for presentation simplicity. If device m 2 M places task km√∞t√û in the transmission

queue in time slot t 2 T (i.e., xm√∞t√û ¬º 0), then task km√∞t√û will either be sent or dropped in time slot ltmran√∞t√û:

(

&

‚Äô

ltmran√∞t√û ¬º min

t √æ dtmran√∞t√û √æ )

X ym;n√∞t√ûm√∞t√û

n2N

rtmra;nn D

√Ä 1;

(6)

t √æ tm √Ä 1 :

after an ofÔ¨Çoaded task is received by an edge node in a time slot, the task will be placed in the corresponding queue at the edge node at the beginning of the next time slot.
If a task of mobile device m 2 M is placed in its queue at edge node n 2 N at the beginning of time slot t 2 T , then we deÔ¨Åne a variable kemd;gne√∞t√û 2 Z√æ√æ to denote the unique index of the task. SpeciÔ¨Åcally, if task km√∞t0√û for t0 2 f1; 2; . . . ; t √Ä 1g is sent to edge node n in time slot t √Ä 1, then kemd;gne√∞t√û ¬º km√∞t0√û. Note that if there does not exist such a task, then we set kemd;gne√∞t√û ¬º 0. Let emd;gne√∞t√û 2 L [ f0g (in bits) denote the number of bits arrived in the queue of mobile device m at edge node n at the beginning of time slot t. If task kemd;gne√∞t√û is placed in the corresponding queue at the beginning of time slot t, then emd;gne√∞t√û is equal to the size of task kemd;gne√∞t√û. Otherwise, emd;gne√∞t√û ¬º 0.
In the following, we Ô¨Årst describe the queues. Then, we derive the task processing or dropping time.
2.2.1 Queues at Edge Nodes
The queue associated with a mobile device at an edge node operates in a FIFO manner. The arrivals of the queue are the tasks ofÔ¨Çoaded by the mobile device to that edge node. Let qmed;gne√∞t√û (in bits) denote the length of the queue of mobile device m 2 M at edge node n 2 N at the end of time slot t 2 T . Among those queues at edge node n, we refer to the queue of mobile device m as an active queue in time slot t if either there is a task of mobile device m arrived at the queue in time slot t (i.e., emd;gne√∞t√û > 0) or the queue is non-empty at the end of time slot t √Ä 1 (i.e., qmed;gne√∞t √Ä 1√û > 0). Let Bn√∞t√û denote the set of active queues at edge node n in time slot t. That is, for n 2 N and t 2 T ,
Bn√∞t√û ¬º fm j emd;gne√∞t√û > 0 or qmed;gne√∞t √Ä 1√û > 0; m 2 Mg:
(7)
Let Bn√∞t√û denote the number of active queues at edge node n in time slot t, i.e., Bn√∞t√û ¬º jBn√∞t√ûj.
We consider a scenario where the tasks of mobile devices have the same priority.1 Each edge node has one CPU for processing the tasks in the queues. In each time slot t 2 T , the active queues at edge node n 2 N (i.e., the queues in set Bn√∞t√û) equally share the processing capacity of the CPU at edge node n. This is the generalized processor sharing (GPS) model with equal processing capacity sharing [24]. Note that since the number of active queues Bn√∞t√û is timevarying and unknown a priori, the processing capacity allocated to each queue can vary across time. Meanwhile, the mobile devices and edge nodes may not have the information of this allocated processing capacity beforehand.
Let fnedge (in CPU cycles per second) denote the processing capacity of edge node n. We assume that mobile devices are aware of the value of fnedge for n 2 N . Let eemd;gne√∞t√û (in bits) denote the number of bits of the tasks dropped by the queue at the end of time slot t 2 T . Hence, the queue length is updated as follows. For m 2 M, n 2 N , and t 2 T ,

2.2 Edge Node Model
Each edge node n 2 N maintains M queues, each queue corresponding to a mobile device in set M. We assume that

1. This work can be extended to the scenario where the tasks of different mobile devices have different priorities. This can be achieved by setting different weights to the queues of different mobile devices and allocating the processing capacity based on the weights.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1989

"

qmed;gne√∞t√û ¬º qmed;gne√∞t √Ä 1√û √æ emd;gne√∞t√û

√Ä

fnedgeD rmBn√∞t√û

1√∞m

2

Bn√∞t√û√û

√Ä

#√æ eemd;gne√∞t√û :

slot t √Ä 1). It is a matrix with size T step √Ç N. Let fH√∞t√ûgi;j denote the √∞i; j√û element of H√∞t√û, which corresponds to the
(8) load level of edge node j in the ith time slot starting from time slot t √Ä T step (i.e., time slot t √Ä T step √æ i √Ä 1). That is,

fH √∞t√ûgi;j¬º Bj√∞t √Ä T step √æ i √Ä 1√û:

(12)

2.2.2 Task Processing or Dropping

If task kemd;gne√∞t√û of mobile device m 2 M is placed in the corresponding queue at edge node n 2 N at the beginning of time slot t 2 T , then we deÔ¨Åne a variable lemd;gne√∞t√û 2 T to denote the time slot when this task has either been proc-
essed or dropped by edge node n. Due to the uncertain
future load at edge node n, mobile device m and edge node n are unaware of the value of lemd;gne√∞t√û until the associated task kemd;gne√∞t√û has either been processed or dropped. Without
loss of generality, if kemd;gne√∞t√û ¬º 0, then we set lemd;gne√∞t√û ¬º 0. For the deÔ¨Ånition of variable lemd;gne√∞t√û, let blemd;gne√∞t√û denote the
time slot when the processing of task kemd;gne√∞t√û starts, i.e., for m 2 M, n 2 N , and t 2 T ,

&

'

blemd;gne√∞t√û ¬º max

t;

t0

max
2f0;1;...;t√Ä1g

lemd;gne

√∞t0

√û

√æ

1

;

(9)

where we set lemd;gne√∞0√û ¬º 0. SpeciÔ¨Åcally, the time slot when the processing of task kemd;gne√∞t√û starts is no earlier than the time slot when the task arrives in the queue or when each task
arrived earlier has been processed or dropped. Given the realization of the load levels, lemd;gne√∞t√û satisÔ¨Åes
the following constraints. For m 2 M, n 2 N , t 2 T ,

lemX d;nge √∞t√û t0 ¬ºblemd;nge √∞t√û

fnedgeD rmBn√∞t0√û

1√∞m

2

Bn√∞t0√û√û

!

emd;gne√∞t√û;

(10)

To obtain H√∞t√û, we assume that each edge node broadcasts its number of active queues at the end of each time slot. Even when all M queues at an edge node are active, the number of active queues can be represented by blog2 Mc √æ 1 bits. For example, if there are 1,000 mobile devices, then a maximum of 10 bits are required. Hence, the broadcast of

the number of active queues only incurs a small signaling overhead.
At the beginning of time slot t 2 T , each device m 2 M

observes its state information, including task size, queue
information, and the load level history at edge nodes. SpeciÔ¨Åcally, mobile device m observes the following state:2





sm√∞t√û ¬º m√∞t√û; dcmomp√∞t√û; dtmran√∞t√û; qemdge√∞t √Ä 1√û; H √∞t√û ;

(13)

where vector qemdge√∞t √Ä 1√û ¬º √∞qmed;gne√∞t √Ä 1√û; n 2 N √û. Let S denote the discrete and Ô¨Ånite state space of each mobile device, i.e., S ¬º L √Ç f0; 1; . . . ; T g2 √Ç QN √Ç f0; 1; . . . ; MgTstep√ÇN , where Q
denotes the set of the available values of the queue length at
an edge node within the T time slots. Note that mobile device m 2 M can obtain state information m√∞t√û, dcmomp√∞t√û, and dtmran√∞t√û through local observation at the beginning of time slot t. Meanwhile, mobile device m can compute qemdge√∞t √Ä 1√û locally according to (8). SpeciÔ¨Åcally, mobile device m is aware
of the number of bits that it has transmitted to an edge node in
each time slot. In addition, it can compute the number of bits
that have been processed or being dropped by an each edge node in each time slot.3

lemd;ngX e √∞t√û√Ä1 t0 ¬ºblemd;nge √∞t√û

fnedgeD rmBn√∞t0√û

1√∞m

2

Bn√∞t0√û√û

<

emd;gne√∞t√û:

(11)

SpeciÔ¨Åcally, the size of task kemd;gne√∞t√û is no larger than the total processing capacity that edge node n allocated to mobile device m from time slot blemd;gne√∞t√û to lemd;gne√∞t√û, and it is larger than that from time slot blemd;gne√∞t√û to lemd;gne√∞t√û √Ä 1.

3 TASK OFFLOADING PROBLEM IN MEC
At the beginning of each time slot, each mobile device observes its state (e.g., task size, queue information). If there is a new task to be processed, then the mobile device chooses an action for the task. The state and action will result in a cost (i.e., the delay of the task if the task is processed, or a penalty if it is dropped) for the mobile device. The objective of each device is to minimize its expected long-term cost by optimizing the policy mapping from states to actions.

3.1 State
Let matrix H√∞t√û denote the history of the load level (i.e., the number of active queues) of each edge node within the previous T step time slots (i.e., from time slot t √Ä T step to time

3.2 Action
At the beginning of time slot t 2 T , if mobile device m 2 N has a new task arrival km√∞t√û, then it will choose actions for task km√∞t√û: (a) whether to process the task locally or ofÔ¨Çoad it to an edge node, i.e., xm√∞t√û; (b) which edge node the task is ofÔ¨Çoaded to, i.e., ym√∞t√û. Hence, the action of device m in time slot t is represented by the following action vector:

am√∞t√û ¬º √∞xm√∞t√û; ym√∞t√û√û:

(14)

Let A denote the action space, i.e., A ¬º f0; 1g1√æN .

3.3 Cost
If a task has been processed, then the delay of the task is the duration between the task arrival and the time when the

2. The queuing delay of a task at an edge node is unknown at the time when the ofÔ¨Çoading decision of the task is to be made, due to the unknown load dynamics at the edge node. Thus, we did not include it as state information. In addition, the operation of our proposed algorithm does not rely on such queuing delay at the edge node.
3. We have assumed that edge nodes send the number of active queues by broadcast in each time slot. Hence, a mobile device can compute the number of its bits processed by an edge node in each time slot. It can compute the number of bits from its tasks dropped by an edge node based on the deadline of those tasks.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1990
task has been processed.4 Let Delaym√∞sm√∞t√û; am√∞t√û√û (in time slots) denote the delay of task km√∞t√û, given state sm√∞t√û and action am√∞t√û. For m 2 M and t 2 T , if xm√∞t√û ¬º 1, then

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

Delaym√∞sm√∞t√û; am√∞t√û√û ¬º lcmomp√∞t√û √Ä t √æ 1;

(15)

if xm√∞t√û ¬º 0, then

Delaym√∞sm√∞t√û; am√∞t√û√û

X XT

¬º

1√∞kemd;gne√∞t0√û ¬º km√∞t√û√ûlemd;gne√∞t0√û √Ä t √æ 1:

n2N t0¬ºt

(16)

SpeciÔ¨Åcally, consider task km√∞t√û arrived at the beginning of

time slot t. If task km√∞t√û is placed in the computation queue

for local processing, then lcmomp√∞t√û is the time slot when the

task has mission

qbueeeunepfroorceosfsÔ¨Çeoda.dIifntga,skthkemn√∞Pt√û nis2NplPacTte0¬ºdt

in the trans1√∞kemd;gne√∞t0√û ¬º

km√∞t√û√ûlemd;gne√∞t0√û is the time slot when the task has been proc-

essed. This is because 1√∞kemd;gne√∞t0√û ¬º km√∞t√û√û ¬º 1 indicates that

task km√∞t√û has arrived at the queue of edge node n 2 N at

the beginning of time slot t0, and lemd;gne√∞t0√û is the time slot

when the task of device m arrived at edge node n at the

beginning of time slot t0 has been processed.

There is a cost cm√∞sm√∞t√û; am√∞t√û√û associated with task km√∞t√û.

If task km√∞t√û has been processed, then

cm√∞sm√∞t√û; am√∞t√û√û ¬º Delaym√∞sm√∞t√û; am√∞t√û√û:

(17)

On the other hand, if task km√∞t√û has been dropped, then

cm√∞sm√∞t√û; am√∞t√û√û ¬º C;

(18)

where C > 0 is a constant penalty. Without loss of generality, if task km√∞t√û ¬º 0, then we set cm√∞sm√∞t√û; am√∞t√û√û ¬º 0. In the remaining part of this work, we use the short form cm√∞t√û to denote cm√∞sm√∞t√û; am√∞t√û√û. Note that in practical systems, there may be other kinds of costs, such as energy consumption and the subscription fee charged by the edge nodes. These costs can be incorporated into this work by including the corresponding terms in Equations (17) and (18). As the proposed DRL-based algorithm is a model-free approach, it will still be applicable to the extended scenario.
3.4 Problem Formulation
A policy of device m 2 M is a mapping from its state to its action, i.e., pm : S ! A. Let g 2 √∞0; 1¬ä denote the discount factor that characterizes the discounted cost in the future. We aim to Ô¨Ånd the optimal policy p√Ém for each device m such that its expected long-term cost is minimized, i.e.,

4. The delay of a task includes the queuing delay, processing delay, and transmission delay (if the task has been ofÔ¨Çoaded). Instead of computing these delays separately and then summing them up, the mobile device can determine the delay of the task by computing the duration between the task arrival and when the task has been processed. This is reasonable because in practical systems, when a task has been processed, the mobile device knows both time instances.

Fig. 2. The neural network of mobile device m 2 M with parameter vec-
tor um, which maps from state sm√∞t√û 2 S to the Q-value of each action a 2 A.

p√Ém

¬º

arg

minimize
pm

" X
E

gt√Ä1cm√∞t√û



# pm

t2T

(19)

subject to constraints √∞1√û √Ä √∞6√û; √∞8√û √Ä √∞11√û;

√∞15√û √Ä √∞18√û:

The expectation E¬Ω√Å¬ä is with respect to the time-varying system parameters, e.g., the task arrivals and the computational requirements of the tasks of all mobile devices as well as the decisions of the mobile devices other than device m.

4 DRL-BASED OFFLOADING ALGORITHM
In this section, we propose a DRL-based ofÔ¨Çoading algorithm that enables the distributed ofÔ¨Çoading decision making of each mobile device. This algorithm is based on deep Qlearning [17]. As deep Q-learning is a model-free approach, the proposed algorithm can address the complicated system setting and interaction among the mobile devices without a priori knowledge of the system and interaction dynamics. Meanwhile, the proposed algorithm can handle the potentially large state space of the system.
In the proposed algorithm, each mobile device aims to learn a mapping from each state-action pair to a Q-value, which characterizes the expected long-term cost of the stateaction pair. The mapping is determined by a neural network. Based on the mapping, each device can select the action inducing the minimum Q-value under its state to minimize its expected long-term cost. In the following, we present the neural network and the DRL-based algorithm, respectively.

4.1 Neural Network
The objective of the neural network is to Ô¨Ånd a mapping from each state to a set of Q-values of the actions. Fig. 2 shows an illustration of the neural network of mobile device m 2 M. SpeciÔ¨Åcally, the state information is passed to the neural network through an input layer. Then, we use an LSTM layer to predict the load levels (at the edge nodes) in the near future based on the load level history. After that, the mapping from all the states (except the load level history) and the predicted load levels to the Q-values are learned through two fully-connected (FC) layers. Meanwhile, dueling DQN technique [25] is applied to improve the learning efÔ¨Åciency of the mapping from states to Q-values through an advantage and value (A&V) layer.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1991

Fig. 3. An LSTM network with T step LSTM units.
Finally, the Q-values of the actions are determined in the output layer. Let um denote the parameter vector of the neural network of device m, which includes the weights of all connections and the biases of all neurons from the input layer to the A&V layer. The details of each layer are as follows.
4.1.1 Input Layer
This layer is responsible for taking the state as input and passing them to the following layers. For mobile device m 2 M, the state information m√∞t√û, dcmomp√∞t√û, dtmran√∞t√û, and qemdge√∞t √Ä 1√û will be passed to the FC layer, and H√∞t√û will be passed to the LSTM layer for load level prediction.
4.1.2 LSTM Layer
This layer is responsible for learning the dynamics of the load levels at edge nodes and predicting the load levels in the near future. This is achieved by including an LSTM network [26], which is a widely used approach for learning the temporal dependence of sequential observations and predicting the future variation of time series.5
SpeciÔ¨Åcally, the LSTM network takes the matrix H√∞t√û as input so as to learn the load level dynamics. Fig. 3 shows the structure of an LSTM network. The LSTM network contains T step LSTM units, each of which contains a set of hidden neurons. Each LSTM unit takes one row of H√∞t√û as input, we let fH√∞t√ûgi denote the ith row of H√∞t√û in Fig. 3. These LSTM units are connected in sequence so as to keep track of the variations of the sequences from fH√∞t√ûg1 to fH√∞t√ûgTstep , which can reveal the variations of the load levels at the edge nodes among time slots. The LSTM network will output the information that indicates the dynamics of the load levels in the future in the last LSTM unit, where the output will be passed to the next layer for further learning.
4.1.3 FC Layers
The two FC layers are responsible for learning the mapping from the state and the learned load level dynamics to the Q-values of the actions. Each FC layer contains a set of neurons with rectiÔ¨Åed linear unit (ReLU), which are connected with the neurons in the previous and following layers.
5. In this work, we use the conventional LSTM network. This work can be extended by applying variants of LSTM network (e.g., gated recurrent units), which may further enhance the algorithm performance and reduce the computational complexity.

4.1.4 A&V Layer and Output Layer

The A&V layer and the output layer implement the duelingDQN technique [25] and determine the Q-value of each action as output. The main idea of the dueling-DQN is to Ô¨Årst separately learn a state-value (i.e., the portion of the Q-value resulting from the state) and action-advantage values (i.e., the portion of the Q-value resulting from the actions). It then uses the state-value and action-advantage values to determine the Q-values of state-action pairs. This technique can improve the estimation of the Q-values through separately evaluating the expected long-term cost resulting from a state and an action.
The A&V layer contains two networks, denoted by network A and network V (see Fig. 2). The network A contains an FC network, and it is responsible for learning the actionadvantage value of each action a 2 A. For mobile device m 2 M, let Am√∞sm√∞t√û; a; um√û denote the action-advantage value of action a under state sm√∞t√û 2 S with network parameter vector um. The network V contains an FC network, and it is responsible for learning the state-value. For mobile device m, let Vm√∞sm√∞t√û; um√û denote state-value of state sm√∞t√û with network parameter vector um. The values of Am√∞sm√∞t√û; a; um√û and Vm√∞sm√∞t√û; um√û are determined by the parameter vector um and the neural network structure from the input layer to the A&V layer, where vector um is adjustable and will be trained in the DRL-based algorithm.
Based on the A&V layer, for mobile device m 2 M, the resulting Q-value of action a 2 A under state sm√∞t√û 2 S in the output layer is given as follows [25]:



Qm√∞sm√∞t√û; a; um√û ¬ºVm√∞sm√∞t√û; um√û √æ Am√∞sm√∞t√û; a; um√û

√Ä

1 jAj

X
a 0 2A

Am

√∞sm

√∞t√û;

a0

;

u

m

 √û;

(20)

which is the sum of the state-value under the corresponding state and the additional action-advantage value of the corresponding action (with respect to the average actionadvantage value under the state over all actions).

4.2 DRL-Based Algorithm
In our proposed DRL-based algorithm, we let edge nodes help mobile devices to train the neural network to alleviate the computational loads at the mobile devices. SpeciÔ¨Åcally, for each mobile device m 2 M, there is an edge node nm 2 N which helps device m with the training. This edge node nm can be the edge node that has the maximum transmission capacity with mobile device m. For presentation convenience, let Mn & M denote the set of mobile devices whose training is performed by edge node n 2 N , i.e., Mn ¬º fme jnem ¬º n; m 2 Mg.
The DRL-based algorithms to be executed at mobile device m 2 M and edge node n 2 N are given in Algorithms 1 and 2, respectively. The key idea of the algorithms is to train the neural network using the experience (i.e., state, action, cost, and next state) of the mobile device to obtain the mapping from state-action pairs to Qvalues, based on which the mobile device can select the action leading to the minimum Q-value under the observed state to minimize its expected long-term cost.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1992

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

Algorithm 1. DRL-Based Algorithm at Device m 2 M

1: for episode = 1; 2; . . . ; E do

2: Initialize sm√∞1√û;

3: for time slot t 2 T do

4: if device m has a new task arrival km√∞t√û then

5:

Send a parameter_request to edge node nm;

6:

Receive network parameter vector um;

7:

Select an action am√∞t√û according to (22);

8: end if

9: Observe the next state sm√∞t √æ 1√û;

10:

Observe a set of costs fcm√∞t0√û; t0 2 Te m;tg;

11:

for each task km√∞t0√û with t0 2 Te m;t do

12:

Send √∞sm√∞t0√û; am√∞t0√û; cm√∞t0√û; sm√∞t0 √æ 1√û√û to nm;

13: end for

14: end for

15: end for

In the DRL-based algorithm, the edge node n 2 N maintains a replay memory Dm for device m 2 Mn. The replay memory Dm stores the observed experience √∞sm√∞t√û; am√∞t√û; cm√∞t√û; sm√∞t √æ 1√û√û of mobile device m for some t 2 T , where we refer √∞sm√∞t√û; am√∞t√û; cm√∞t√û; sm√∞t √æ 1√û√û as experience t of mobile device m. Meanwhile, the edge node n 2 N maintains two neural networks for device m 2 Mn, including an evaluation network, denoted by Netm, and a target network, denoted by Target_Netm. The evaluation network Netm is used for action selection. The target network Target_Netm is used for characterizing a target Q-value, which approximates the expected long-
term cost of an action under the observed state. This target Q-
value will be used for updating Netm by minimizing the difference between the Q-value under Netm and the target Q-value. Note that both Netm and Target_Netm have the same neural network structure, as presented in Section 4.1, while they have different network parameter vectors um and u√Äm, respectively. Hence, the Q-values of Netm and Target_Netm are represented by Qm√∞sm √∞t√û; a; um√û and Qm√∞sm√∞t√û; a; u√Äm√û under observed state sm√∞t√û 2 S and action a 2 A, respectively. The initialization of the replay memory Dm and two neural networks are given in steps 1√Ä3 in Algorithm 2.

4.2.1 Algorithm 1 at Mobile Device m 2 M
We consider multiple episodes, where E denotes the number of episodes. At the beginning of each episode, mobile device m 2 M initializes the state, i.e.,

sm√∞1√û ¬º √∞m√∞1√û; dcmomp√∞1√û; dtmran√∞1√û; qemdge√∞0√û; H √∞1√û√û;

(21)

where we set qmed;gne√∞0√û ¬º 0 for all n 2 N , and H√∞1√û is a zero matrix with size T step √Ç N. At the beginning of time slot t 2 T , if mobile device m has a new task arrival km√∞t√û, then it can send a parameter_request to edge node nm in order to update the parameter vector um that it uses for making the task offloading decision. To reduce the communication overhead and address the potential scalability issue due to the neural network parameter transmission, the mobile device may not request the parameter vector in every time slot when it has a new task arrival. That is, steps 5-6 in Algorithm 1 can be omitted in some time slots. Intuitively, when the frequency that a mobile device requests the parameter vector is small, the communication overhead will also be small. However, it may reduce the rate of convergence of the proposed algorithm. We

Algorithm 2. DRL-Based Algorithm at Edge Node

n 2 N 1: Initialize replay memory Dm for m 2 Mn and Count :¬º 0;

2: Initialize Netm with random um for m 2 Mn; 3: Initialize Target_Netm with random u√Äm for m 2 Mn; 4: while True do

5: if receive a parameter_request from m 2 Mn then

6: Send um to device m;

7: end if

8: if receive an experience √∞sm√∞t√û; am√∞t√û; cm√∞t√û; sm√∞t √æ 1√û√û from

m 2 Mn and Converge_Indicator = 0 then

9: Store √∞sm√∞t√û; am√∞t√û; cm√∞t√û; sm√∞t √æ 1√û√û in Dm;

10: Sample a set of experiences (denoted by I ) from Dm; 11: for each experience i 2 I do

12:

Obtain experience √∞sm√∞i√û; am√∞i√û; cm√∞i√û; sm√∞i √æ 1√û√û;

13:

Compute Q^Tma;irget according to (26);

14: end for

15:

Set vector Q^ Tmarget :¬º √∞Q^Tma;irget; i 2 I √û;

16:

Update um to minimize L√∞um; Q^ Tmarget√û in (24);

17: Count :¬º Count √æ 1;

18: if mod(Count, Replace Threshold) ¬º 0 then

19:

u

√Ä m

:¬º

um;

20: end if

21: end if

22: end while

empirically evaluate how such a frequency affects the algo-

rithm convergence in Section 5.1.

Based on the parameter vector um of Netm, mobile device m will choose its action for task km√∞t√û as follows:

&

am√∞t√û ¬º

a random action from A; w.p. ; arg mina2A Qm√∞sm√∞t√û; a; um√û; w.p. 1 √Ä ;

(22)

where ‚Äòw.p.‚Äô is the short-form for ‚Äúwith probability‚Äù, and 

is the probability of random exploration. Intuitively, with

probability 1 √Ä , the device chooses the action that leads to

the minimum Q-value under state sm√∞t√û based on Netm.

At the beginning of the next time slot (i.e., time slot t √æ 1),

mobile device m observes the next state sm√∞t √æ 1√û. On the

other hand, as the processing and the transmission of a task

may continue for multiple time slots, the cost cm√∞t√û, which

depends on the delay of task km√∞t√û, may not be observed at

the beginning of time slot t √æ 1. Instead, mobile device m

may observe a set of costs belonging to some tasks km√∞t0√û with time slot t0 t. To address this, for device m, we deÔ¨Åne Te m;t & T as the set of time slots such that each task km√∞t0√û associated with time slot t0 2 Te m;t has been processed or dropped in time slot t. Set Te m;t is deÔ¨Åned as follows:

Te m;t

( ¬º t0



t0

¬º 1; 2; . . . ; t;

m√∞t0√û

>

0;

lcmomp√∞t0√û ¬º t

X Xt

)

(23)

or

1√∞kemd;gne√∞i√û ¬º km√∞t0√û√ûlemd;gne√∞i√û ¬º t :

n2N i¬ºt0

In (23), m√∞t0√û > 0 implies that there is a newly arrived task km√∞t0√û in time slot t0. SpeciÔ¨Åcally, set Te m;t contains a time slot t0 2 f1; 2; . . . ; tg if task km√∞t0√û has been processed or dropped

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1993

in time slot t. Hence, at the beginning of time slot t √æ 1, mobile device m can observe a set of costs fcm√∞t0√û; t0 2 Te m;tg, where set Te m;t can be an empty set for some m 2 M and t 2 T . Then, for each task km√∞t0√û with t0 2 Te m;t, device m sends its experience √∞sm√∞t0√û; am√∞t0√û; cm√∞t0√û; sm√∞t0 √æ 1√û√û to edge node nm. Note that when device m transmits state information sm√∞t0√û and sm√∞t0 √æ 1√û, it does not need to send the load level history H√∞t0√û and H√∞t0 √æ 1√û. This is because the number of active queues at
the edge nodes in each time slot has been sent by broadcast.
Thus, each edge node knows the load level history at all edge nodes during the past time slots.6 In this case, when an edge node receives experience √∞sm√∞t0√û; am√∞t0√û; cm√∞t0√û; sm√∞t0 √æ 1√û√û, it can include H√∞t0√û and H√∞t0 √æ 1√û to the experience for training.

performing backpropagation (see Section 6 in [27]) on the
neural network using iterative optimization algorithms such
as gradient descent algorithm. The target Q-value Q^Tma;irget for experience i 2 I is deter-
mined based on double-DQN technique [28], which can
improve the estimation of the expected long-term cost when
compared with the traditional method (e.g., [17]). To derive this target Q-value, let aNi ext denote the action with the minimum Q-value given state sm√∞i √æ 1√û under Netm, i.e.,

aNi ext

¬º

arg

min
a2A

Qm√∞sm√∞i

√æ

1√û;

a;

um√û:

(25)

The value of Q^Tma;irget for experience i is derived as follows:

4.2.2 Algorithm 2 at Edge Node n 2 N
After initializing the replay memory Dm as well as the neural networks Netm and Target_Netm for device m 2 Mn, edge node n 2 N will wait for the request messages from the mobile devices in set Mn. If edge node n receives a parameter_request from mobile device m 2 Mn, then it will send the current parameter vector um of Netm to device m. On the other hand, if edge node n receives an experience √∞sm√∞t√û; am√∞t√û; cm√∞t√û; sm√∞t √æ 1√û√û from mobile device m 2 Mn, then it will store the experience in memory Dm. The reply of the parameter vector (steps 5-7 in Algorithm 2) and the training of the network (steps 8-21 in Algorithm 2) can be operated in parallel. That is, when an edge node receives the parameter_request from mobile device m, it will immediately send the current vector um to the device regardless of whether there is training in progress or not.
The edge node will train the neural network (in steps 10√Ä20 in Algorithm 2) to update the parameter vector um of Netm as follows. The edge node will randomly sample a set of experiences from the memory (in step 10), denoted by I . Let jI j denote the number of experiences in set I .7 Based on these experience samples, the key idea of the update of Netm is to minimize the difference between the Q-values under Netm and the target Q-values computed based on the experience samples under Target_Netm. SpeciÔ¨Åcally, for the experience samples in set I, the edge node will compute Q^ Tmarget ¬º √∞Q^Tma;irget; i 2 I √û and update um in Netm by minimizing the following loss function:

L√∞um;

Q^ Tmarget√û

¬º

1 jI j

X
i2I

 Qm√∞sm√∞i√û;

am√∞i√û;

um√û

√Ä

Q^Tma;irget 2 :

(24)

Loss function (24) characterizes the gap between the Q-value
of action am√∞i√û given state sm√∞i√û under the current network parameter vector um and a target Q-value Q^Tma;irget for each experience i 2 I (to be explained in the next paragraph).
The minimization of the loss function is accomplished by

6. As we focus on addressing the load level dynamics at the edge nodes, we consider a scenario where the edge nodes are located in neighboring areas. We assume that any two edge nodes are within one hop from each other. In the scenario where this assumption does not hold, an edge node can send the number of active queues to other edge nodes through backhaul links.
7. When all other factors are Ô¨Åxed, a smaller batch size (i.e., the number of experiences sampled in each round) incurs a shorter time for one round of training.

Q^Tma;irget ¬º cm√∞i√û √æ gQm√∞sm√∞i √æ 1√û; aNi ext; u√Äm√û:

(26)

Intuitively, target-Q value Q^Tma;irget reveals the expected longterm cost of action am√∞i√û given state sm√∞i√û, i.e., the sum of the cost in experience i and a discounted Q-value of the
action that is likely to be selected given the next state in
experience i under network Target_Netm. Let Replace_Threshold denote the number of training
rounds after which Target_Netm has to be updated. That is, for every Replace_Threshold training rounds, Target_Netm has to be updated by copying the parameter vector of Netm, where mod(√Å) is the modulo operator (in step 18 in
Algorithm 2). The objective of this step is to keep the network parameter u√Äm in Target_Netm up-to-date, so that it can better approximate the expected long-term cost in the com-
puting of the target Q-values in (26).

4.2.3 Computational Complexity and Convergence
To determine the computational complexity, let L denote the number of multiplication operations in the neural network. The computational complexity of backpropagation for the training of one experience is O√∞L√û. Recall that jI j is the number of experiences sampled in each round of training. Let K denote the expected number of tasks in each episode. Since there are E episodes, the computational complexity of the proposed algorithm is O√∞LKEjI j√û.
Regarding the convergence, as mentioned in many existing works (e.g., [29]), the convergence guarantee of a DRLalgorithm is still an open problem. Despite the fact that the convergence of a reinforcement learning algorithm can be proven, a DRL algorithm requires function approximation (e.g., the approximation of the Q-values in deep Q-learning algorithm) using neural networks, under which the convergence may no longer be guaranteed. In this work, we empirically evaluate the convergence performance of the proposed algorithm in Section 5.1.

5 PERFORMANCE EVALUATION
We consider a scenario with 50 mobile devices and Ô¨Åve edge nodes. The parameter settings are given in Table 1. The neural network settings are as follows. The batch size is set to 16. The learning rate is equal to 0.001, and the discount factor is equal to 0.9. The probability of random exploration is gradually decreasing from 1 to 0.01. Meanwhile, we use RMSProp optimizer. In these simulations, we focus on a scenario with stationary environment, i.e., the transition function (from the

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1994

TABLE 1 Parameter Settings

Parameter
D fmdevice; m 2 M fnedge; n 2 N rtmra;nn; m 2 M; n 2 N m√∞t√û; m 2 M; t 2 T rm; m 2 M tm; m 2 M Task arrival probability

Value
0.1 second 2.5 GHz [15]
41.8 GHz [15]
14 Mbps [30]
{2.0, 2.1, ..., 5.0} Mbits [4] 0.297 gigacycles per Mbits [4] 10 time slots (i.e., 1 second) 0.3

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

state and action to the next state) and the cost function (from the state and action to cost) do not vary across time. Under a non-stationary environment, if the environment has changed, then the proposed algorithm can adapt to it by resetting the probability of random exploration to be one so as to enable the random exploration again.
5.1 Performance and Convergence
The neural network in the proposed algorithm is trained online, where the real-time collected experience is used to train the neural network and update the task ofÔ¨Çoading decision. We evaluate the convergence of the proposed algorithm under different neural network hyperparameters and algorithm settings. We consider 1,000 episodes, where each episode has 100 time slots. The simulation results are shown in Fig. 4. In the subÔ¨Ågures, the x-axis shows the episode, and the y-axis shows the average cost among the mobile devices and the time slots in each episode. We plot the performance of the proposed algorithm under different settings and the random policy (denoted by ‚ÄúRand.‚Äù), where the actions are randomly selected.
Fig. 4a shows the convergence of the proposed algorithm under different values of learning rate (denoted ‚Äúlr‚Äù), where the learning rate is the step size in each iteration for moving towards the minimum of the loss function. In Fig. 4a, lr ¬º 10√Ä3 leads to a relatively fast convergence and small converged cost. When the learning rate is small (i.e., 10√Ä4), the convergence is slow. When the learning rate is large (i.e., 10√Ä2, 10√Ä1), the converged cost increases, which may be even higher than that of the random policy.
Fig. 4b shows the algorithm performance under different batch sizes, i.e., the number of experiences sampled in each training round. As the batch size increases from 2 to 8, the convergence speed increases. As it further increases from 8 to 32, the performance of the proposed algorithm does not have a signiÔ¨Åcant improvement in terms of the convergence speed and the converged result. Thus, we can choose a small batch size (e.g., 8) to reduce the time for one round of training without signiÔ¨Åcantly reducing the performance of the proposed algorithm.
Fig. 4c shows the algorithm performance under different optimizers, including gradient descent (denoted by ‚ÄúGD‚Äù), RMSProp, and adaptive moment estimation (Adam) optimizers. These optimizers provide different approaches to update the neural network for minimizing the loss function in (24). As shown in Fig. 4c, RMSProp and Adam optimizers lead to similar convergence speed and result.

Fig. 4. Convergence of the proposed algorithm under different: (a) learning rate; (b) batch size; (c) optimizer; (d) the frequency that a mobile device sends a parameter_request.
In Fig. 4d, we consider a setting where a mobile device sends the parameter_request to update its network parameter every Ô¨Åxed number of time slots (instead of every time slot when the mobile device has a task arrival). As shown in the Ô¨Ågure, sending the request every 100 time slots does not have much impact on the algorithm performance when compared with sending it every time slot. This is because the training of the neural network in the proposed algorithm is based on randomly sampled experiences from experience replay rather than newly obtained experience. Hence, the algorithm can tolerate a certain degree of delay in terms of the update of the neural network for action selection. As a result, in order to reduce the communication overhead, we can reduce the frequency that the parameter_request is sent without signiÔ¨Åcantly affecting the algorithm performance.
5.2 Method Comparison We compare our proposed DRL-based method with several benchmark methods, including no ofÔ¨Çoading (denoted by No OfÔ¨Ç.), random ofÔ¨Çoading (denoted by R. OfÔ¨Ç.), PGOA in [14], and ULOOF in [15]. The PGOA is designed based on the best response algorithm for the potential game, which considers the strategic interaction among mobile devices.8 The ULOOF is designed based on the capacity estimation according to historical observations. We choose PGOA [14] and ULOOF [15] for comparison because similar to our work, those schemes considered non-divisible tasks and multiple edge nodes, and they did not consider the involvement of any centralized entity. We consider two
8. PGOA operates under the assumption that the processing of each task can be Ô¨Ånished within each time slot. Hence, the ofÔ¨Çoading decision of each task can be made based on the feedback (e.g., delay) of the tasks arrived in the previous time slots. In our work, we do not impose this assumption. To evaluate the performance, we consider the ofÔ¨Çoading decision of each task is made based on the feedback of the tasks that have been processed in the previous time slots.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1995

Fig. 5. Performance evaluation under different task arrival probabilities: (a) ratio of dropped tasks; (b) average delay.

Fig. 7. Performance under different number of mobile devices: (a) ratio of dropped tasks; (b) average delay.

Fig. 6. Performance under different task deadlines: (a) ratio of dropped tasks; (b) average delay.
performance metrics: the ratio of dropped tasks (i.e., the ratio of the number of dropped tasks to the number of total task arrivals) and the average delay (i.e., the average delay of the tasks which have been processed).
In Fig. 5a, as the task arrival probability increases, the proposed DRL-based algorithm can always maintain a lower ratio of dropped tasks when compared with the benchmark methods. When the task arrival probability is small (i.e., 0.1), most of the methods can achieve a ratio of dropped tasks of around zero. As the task arrival probability increases from 0.1 to 0.5, the ratio of dropped tasks of the proposed algorithm remains less than 0.2, while those of the benchmark methods increase to more than 0.5. In Fig. 5b, as the task arrival probability increases from 0.1 to 0.4, the average delay of our proposed DRL-based algorithm increases by 26.1 percent, while those of the benchmark methods increase by at least 34.5 percent. This implies that as the load of the system increases, the average delay of the proposed algorithm increases less dramatically than those of the benchmark methods. As the task arrival probability increases to 0.6, the average delay of some of the methods decrease, because an increasing number of tasks are dropped and hence are not accounted in the average delay. For the same reason, when the load of the system is high, the proposed algorithm may have a larger average delay than the other methods, as it has less tasks dropped.
As shown in Fig. 6a, the proposed algorithm always achieves a lower ratio of dropped tasks than the benchmark methods, especially when the deadline is small. When the task deadline is 0.6 second, the proposed algorithm reduces the ratio of dropped tasks by 65.8-79.3 percent when compared with the benchmark methods. In Fig. 6b, as the task deadline increases, the average delay of each method increases and gradually converges. This is because when the deadline is larger, the tasks requiring longer processing (and

transmission) time can be processed and are accounted in the average delay. When the deadline is large enough, no task is dropped, so further increasing the deadline makes no difference. As shown in Fig. 6b, the average delay of the proposed algorithm converges (i.e., achieves a marginal increase of less than 0.05) after the deadline increases to 1.4 seconds, and the converged average delay is around 0.54 second. In comparison, the converged average delay of the other methods are larger than 0.84 second.
In Fig. 7a, the proposed algorithm achieves a lower ratio of dropped tasks than the other methods, especially when the number of devices is large. This is because the proposed algorithm can effectively address the unknown load dynamics at the edge nodes. When the number of mobile devices increases to 80, the proposed algorithm maintains a ratio of dropped tasks of less than 0.05. In Fig. 7b, as the number of mobile devices increases, the average delay of each method (except no ofÔ¨Çoading) increases due to the potentially increasing load at the edge nodes. Since the proposed algorithm can effectively deal with the unknown edge load dynamics, when the number of mobile devices increases to 150, it achieves an average delay of 9.0 percent lower than those of PGOA and ULOOF.
6 CONCLUSION
In this work, we studied the computational task ofÔ¨Çoading problem with non-divisible and delay-sensitive tasks in the MEC system. We designed a distributed ofÔ¨Çoading algorithm that enables mobile devices to make their ofÔ¨Çoading decisions in a decentralized manner, which can address the unknown load level dynamics at the edge nodes. Simulation results showed that when compared with several benchmark methods, our proposed algorithm can reduce the ratio of dropped tasks and average delay. The beneÔ¨Åt is especially signiÔ¨Åcant when the tasks are delay-sensitive or the load levels at the edge nodes are high.
There are several directions to extend this work. First, it is interesting to extend the simpliÔ¨Åed wireless network model to incorporate the transmission error and the interference among mobile devices. Second, the algorithm performance can be evaluated in a demo system, under which many practical issues (e.g., real computational tasks) should be addressed. Third, the computational complexity of the proposed algorithm can be reduced. This can be achieved by incorporating deep compression [31] to reduce the number of multiplication operations in the neural network and incorporating transfer learning [32] to accelerate the

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

1996

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 21, NO. 6, JUNE 2022

convergence. In addition, as we let edge nodes help mobile devices to train the neural networks, the communication overhead and scalability issue due to the neural network parameter transmission may be a concern when the neural network is large. To extend this work, deep compression [31] can be applied to reduce the number of weights of the neural network and the number of bits required to represent each weight. Furthermore, to enhance the performance of the proposed algorithm under non-stationary environment, it is interesting to incorporate techniques such as the reinforcement learning for non-stationary environment (e.g., [33]) and lifelong reinforcement learning [34]. Last but not least, game-theoretic and multi-agent reinforcement learning techniques can be applied to further understand the strategic interactions among mobile devices. Those techniques can be incorporated into the proposed algorithm to further address the load level dynamics at the edge nodes.
ACKNOWLEDGMENTS
This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC).
REFERENCES
[1] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, ‚ÄúA survey on mobile edge computing: The communication perspective,‚Äù IEEE Commun. Surveys Tuts., vol. 19, no. 4, pp. 2322‚Äì2358, Fourthquarter 2017.
[2] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, ‚ÄúFog computing and its role in the Internet of Things,‚Äù in Proc. 1st Ed. MCC Workshop Mobile Cloud Comput., 2012, pp. 13‚Äì16.
[3] P. Porambage, J. Okwuibe, M. Liyanage, M. Ylianttila, and T. Taleb, ‚ÄúSurvey on multi-access edge computing for Internet of Things realization,‚Äù IEEE Commun. Surveys Tuts., vol. 20, no. 4, pp. 2961‚Äì2991, Fourthquarter 2018.
[4] C. Wang, C. Liang, F. R. Yu, Q. Chen, and L. Tang, ‚ÄúComputation ofÔ¨Çoading and resource allocation in wireless cellular networks with mobile edge computing,‚Äù IEEE Trans. Wireless Commun., vol. 16, no. 8, pp. 4924‚Äì4938, Aug. 2017.
[5] S. Bi and Y. J. Zhang, ‚ÄúComputation rate maximization for wireless powered mobile-edge computing with binary computation ofÔ¨Çoading,‚Äù IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177‚Äì4190, Jun. 2018.
[6] N. Eshraghi and B. Liang, ‚ÄúJoint ofÔ¨Çoading decision and resource allocation with uncertain task computing requirement,‚Äù in Proc. IEEE Conf. Comput. Commun., 2019, pp. 1414‚Äì1422.
[7] X. Lyu, H. Tian, W. Ni, Y. Zhang, P. Zhang, and R. P. Liu, ‚ÄúEnergyefÔ¨Åcient admission of delay-sensitive tasks for mobile edge computing,‚Äù IEEE Trans. Commun., vol. 66, no. 6, pp. 2603‚Äì2616, Jun. 2018.
[8] M. Chen and Y. Hao, ‚ÄúTask ofÔ¨Çoading for mobile edge computing in software deÔ¨Åned ultra-dense network,‚Äù IEEE J. Sel. Areas Commun., vol. 36, no. 3, pp. 587‚Äì597, Mar. 2018.
[9] K. Poularakis, J. Llorca, A. M. Tulino, I. Taylor, and L. Tassiulas, ‚ÄúJoint service placement and request routing in multi-cell mobile edge computing networks,‚Äù in Proc. IEEE Conf. Comput. Commun., 2019, pp. 10‚Äì18.
[10] X. Lyu et al., ‚ÄúDistributed online optimization of fog computing for selÔ¨Åsh devices with out-of-date information,‚Äù IEEE Trans. Wireless Commun., vol. 17, no. 11, pp. 7704‚Äì7717, Nov. 2018.
[11] L. Li, T. Q. S. Quek, J. Ren, H. H. Yang, Z. Chen, and Y. Zhang, ‚ÄúAn incentive-aware job ofÔ¨Çoading control framework for multiaccess edge computing,‚Äù IEEE Trans. Mobile Comput., early access, Sep. 17, 2019, doi: 10.1109/TMC.2019.2941934.
[12] H. Shah-Mansouri and V. W. S. Wong, ‚ÄúHierarchical fog-cloud computing for IoT systems: A computation ofÔ¨Çoading game,‚Äù IEEE Internet Things J., vol. 5, no. 4, pp. 3246‚Äì3257, Aug. 2018.

[13] S. Josilo and G. Dan, ‚ÄúWireless and computing resource allocation for selÔ¨Åsh computation ofÔ¨Çoading in edge computing,‚Äù in Proc. IEEE Conf. Comput. Commun., 2019, pp. 2467‚Äì2475.
[14] L. Yang, H. Zhang, X. Li, H. Ji, and V. Leung, ‚ÄúA distributed computation ofÔ¨Çoading strategy in small-cell networks integrated with mobile edge computing,‚Äù IEEE/ACM Trans. Netw., vol. 26, no. 6, pp. 2762‚Äì2773, Dec. 2018.
[15] J. L. D. Neto, S.-Y. Yu, D. F. Macedo, M. S. Nogueira, R. Langar, and S. Secci, ‚ÄúULOOF: A user level online ofÔ¨Çoading framework for mobile edge computing,‚Äù IEEE Trans. Mobile Comput., vol. 17, no. 11, pp. 2660‚Äì2674, Nov. 2018.
[16] G. Lee, W. Saad, and M. Bennis, ‚ÄúAn online optimization framework for distributed fog network formation with minimal latency,‚Äù IEEE Trans. Wireless Commun., vol. 18, no. 4, pp. 2244‚Äì2258, Apr. 2019.
[17] V. Mnih et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù Nature, vol. 518, no. 7540, pp. 529‚Äì533, Feb. 2015.
[18] L. Huang, S. Bi, and Y. J. Zhang, ‚ÄúDeep reinforcement learning for online computation ofÔ¨Çoading in wireless powered mobile-edge computing networks,‚Äù IEEE Trans. Mobile Comput., vol. 19, no. 11, pp. 2581‚Äì2593, Nov. 2020.
[19] J. Luo, F. R. Yu, Q. Chen, and L. Tang, ‚ÄúAdaptive video streaming with edge caching and video transcoding over software-deÔ¨Åned mobile networks: A deep reinforcement learning approach,‚Äù IEEE Trans. Wireless Commun., vol. 19, no. 3, pp. 1577‚Äì1592, Mar. 2020.
[20] Y. Liu, H. Yu, S. Xie, and Y. Zhang, ‚ÄúDeep reinforcement learning for ofÔ¨Çoading and resource allocation in vehicle edge computing and networks,‚Äù IEEE Trans. Veh. Technol., vol. 68, no. 11, pp. 11 158‚Äì11 168, Nov. 2019.
[21] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, ‚ÄúDeep reinforcement learning for user association and resource allocation in heterogeneous cellular networks,‚Äù IEEE Trans. Wireless Commun., vol. 18, no. 11, pp. 5141‚Äì5152, Nov. 2019.
[22] J. Liu, Y. Mao, J. Zhang, and K. B. Letaief, ‚ÄúDelay-optimal computation task scheduling for mobile-edge computing systems,‚Äù in Proc. IEEE Int. Symp. Inf. Theory, 2016, pp. 1451‚Äì1455.
[23] Y. Mao, J. Zhang, S. Song, and K. B. Letaief, ‚ÄúStochastic joint radio and computational resource management for multi-user mobileedge computing systems,‚Äù IEEE Trans. Wireless Commun., vol. 16, no. 9, pp. 5994‚Äì6009, Sep. 2017.
[24] A. K. Parekh and R. G. Gallager, ‚ÄúA generalized processor sharing approach to Ô¨Çow control in integrated services networks: The single-node case,‚Äù IEEE/ACM Trans. Netw., vol. 1, no. 3, pp. 344‚Äì357, Jun. 1993.
[25] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas, ‚ÄúDueling network architectures for deep reinforcement learning,‚Äù in Proc. 33rd Int. Conf. Mach. Learn., 2016, pp. 1995‚Äì2003.
[26] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural Comput., vol. 9, no. 8, pp. 1735‚Äì1780, Nov. 1997.
[27] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.
[28] H. van Hasselt, A. Guez, and D. Silver, ‚ÄúDeep reinforcement learning with double Q-learning,‚Äù in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 2094‚Äì2100.
[29] H. Van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil, ‚ÄúDeep reinforcement learning and the deadly triad,‚Äù 2018, arXiv:1812.02648v1[cs.AI].
[30] Speedtest intelligence, ‚ÄúSpeedtest market reports: Canada average mobile upload speed based on Q2-Q3 2019 data,‚Äù Accessed: Aug. 5, 2020. [Online]. Available: https://www.speedtest.net/ reports/canada/
[31] S. Han, H. Mao, and W. J. Dally, ‚ÄúDeep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding,‚Äù 2016, arXiv:1510.00149v5[cs.CV].
[32] K. Weiss, T. M. Khoshgoftaar, and D. Wang, ‚ÄúA survey of transfer learning,‚Äù J. Big Data, vol. 3, no. 1, pp. 1345‚Äì1359, May 2016.
[33] E. Lecarpentier and E. Rachelson, ‚ÄúNon-stationary markov decision processes, a worst-case approach using model-based reinforcement learning,‚Äù in Proc. 33rd Int. Conf. Neural Inf. Process. Syst., 2019, pp. 7214‚Äì7223.
[34] D. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman, ‚ÄúPolicy and value transfer in lifelong reinforcement learning,‚Äù in Proc. 35th Int. Conf. Mach. Learn., 2018, pp. 20‚Äì29.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

TANG AND WONG: DEEP REINFORCEMENT LEARNING FOR TASK OFFLOADING IN MOBILE EDGE COMPUTING SYSTEMS

1997

Ming Tang (Member, IEEE) received the PhD degree from the Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China, in 2018. She is currently a postdoctoral research fellow with the Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada. Her research interests include mobile networking and network economics.

Vincent W.S. Wong (Fellow, IEEE) received the BSc degree from the University of Manitoba, Winnipeg, MB, Canada, in 1994, the MASc degree from the University of Waterloo, Waterloo, ON, Canada, in 1996, and the PhD degree from the University of British Columbia (UBC), Vancouver, BC, Canada, in 2000. From 2000 to 2001, he worked as a systems engineer at PMC-Sierra Inc. (now Microchip Technology Inc.). He joined the Department of Electrical and Computer Engineering, University of British Columbia, Canada, in 2002 and is currently a professor. His research areas include protocol design, optimization, and resource management of communication networks, with applications to wireless networks, smart grid, mobile edge computing, and Internet of Things. Currently, he is an executive editorial committee member of the IEEE Transactions on Wireless Communications, an area editor of the IEEE Transactions on Communications and the IEEE Open Journal of the Communications Society, and an associate editor of the IEEE Transactions on Mobile Computing. He is a technical program co-chair of the IEEE 92nd Vehicular Technology Conference (VTC2020-Fall). He has served as a guest editor of the IEEE Journal on Selected Areas in Communications and the IEEE Wireless Communications. He has also served on the editorial boards of the IEEE Transactions on Vehicular Technology and the Journal of Communications and Networks. He was a tutorial co-chair of IEEE Globecom‚Äô18, a technical program co-chair of IEEE SmartGridComm‚Äô14, as well as a symposium co-chair of IEEE ICC‚Äô18, IEEE SmartGridComm (‚Äô13, ‚Äô17) and IEEE Globecom‚Äô13. He is the chair of the IEEE Vancouver Joint Communications Chapter and has served as the chair of the IEEE Communications Society Emerging Technical Sub-Committee on Smart Grid Communications. He is an IEEE Communications Society distinguished lecturer (2019 - 2020).

" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.

Authorized licensed use limited to: KAUST. Downloaded on June 05,2022 at 11:30:40 UTC from IEEE Xplore. Restrictions apply.

