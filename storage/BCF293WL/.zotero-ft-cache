IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

5539

SplitPlace: AI Augmented Splitting and Placement of Large-Scale Neural Networks
in Mobile Edge Environments

Shreshth Tuli , Giuliano Casale , and Nicholas R. Jennings

Abstract‚ÄîIn recent years, deep learning models have become ubiquitous in industry and academia alike. Deep neural networks can solve some of the most complex pattern-recognition problems today, but come with the price of massive compute and memory requirements. This makes the problem of deploying such large-scale neural networks challenging in resource-constrained mobile edge computing platforms, speciÔ¨Åcally in mission-critical domains like surveillance and healthcare. To solve this, a promising solution is to split resource-hungry neural networks into lightweight disjoint smaller components for pipelined distributed processing. At present, there are two main approaches to do this: semantic and layer-wise splitting. The former partitions a neural network into parallel disjoint models that produce a part of the result, whereas the latter partitions into sequential models that produce intermediate results. However, there is no intelligent algorithm that decides which splitting strategy to use and places such modular splits to edge nodes for optimal performance. To combat this, this work proposes a novel AI-driven online policy, SplitPlace, that uses Multi-Armed-Bandits to intelligently decide between layer and semantic splitting strategies based on the input task‚Äôs service deadline demands. SplitPlace places such neural network split fragments on mobile edge devices using decision-aware reinforcement learning for efÔ¨Åcient and scalable computing. Moreover, SplitPlace Ô¨Åne-tunes its placement engine to adapt to volatile environments. Our experiments on physical mobile-edge environments with real-world workloads show that SplitPlace can signiÔ¨Åcantly improve the state-of-the-art in terms of average response time, deadline violation rate, inference accuracy, and total reward by up to 46, 69, 3 and 12 percent respectively.
Index Terms‚ÄîMobile edge computing, neural network splitting, container orchestration, artiÔ¨Åcial intelligence, QoS optimization
√á

1 INTRODUCTION
MODERN Deep Neural Networks (DNN) are becoming the backbone of many industrial tasks and activities [2]. As the computational capabilities of devices have improved, new deep learning models have been proposed to provide improved performance [3], [4]. Moreover, many recent DNN models have been incorporated with mobile edge computing to give low latency services with improved accuracies compared to shallow networks, particularly in complex tasks like image segmentation, high frame-rate gaming and trafÔ¨Åc surveillance [5]. The performance of such neural models reÔ¨Çects directly on the reliability of application domains like self-driving cars, healthcare and manufacturing [2], [6]. However, to provide high accuracy, such neural models are becoming increasingly demanding in terms of data and compute power, resulting in many challenging problems. To accommodate these increasing demands, such massive models are often hosted as web services deployed on the public cloud [7], [8].
 Shreshth Tuli and Giuliano Casale are with the Department of Computing, Imperial College London, SW7 2BX London, U.K. E-mail: {s.tuli20, g.casale}@imperial.ac.uk.
 Nicholas R. Jennings is with the Department of Computing, Imperial College London, SW7 2BX London, U.K., and also with Loughborough University, LE11 3TU Loughborough, U.K. E-mail: n.r.jennings@lboro.ac.uk.
Manuscript received 22 Oct. 2021; revised 6 May 2022; accepted 20 May 2022. Date of publication 24 May 2022; date of current version 4 Aug. 2023. (Corresponding author: Shreshth Tuli.) Digital Object IdentiÔ¨Åer no. 10.1109/TMC.2022.3177569

Challenges. Recently, application demands have shifted from either high-accuracy or low-latency to both of these together, termed as HALL (high-accuracy and low-latency) service delivery [2]. Given the prevalence and demand of DNN inference, serving them on a public cloud with tight bounds of latency, throughput and cost is becoming increasingly challenging [9]. In this regard, recent paradigms like mobile edge computing seem promising. Such approaches allow a robust and low-latency deployment of Internet of Things (IoT) applications close to the edge of the network. SpeciÔ¨Åcally, to solve the problem of providing HALL services, recent work proposes to integrate large-scale deep learning models with modern frameworks like edge computing [9], [10], [11]. However, even the most recent approaches either provide a low Service Level Agreement (SLA) violation mode or a high-accuracy mode [9], [10] and struggle to provide the beneÔ¨Åts of both modes at the same time.
Another challenge of using edge computing is that mobile edge devices face severe limitations in terms of computational and memory resources as they rely on low power energy sources like batteries, solar or other energy scavenging methods [12], [13]. This is not only because of the requirement of low cost, but also the need for mobility in such nodes [5]. In such systems, it is possible to handle the processing limitations of massive DNN models by effective preemption and prolonged job execution. However, memory bottlenecks are much harder to solve [14]. In a distributed edge environment, storage spaces are typically mapped to network-attached-storage (NAS) media. Thus, prior work that runs inference on a pre-trained DNN without memory-

1536-1233 ¬© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5540

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

aware optimizations leads to high network bandwidth over- infrastructure to optimize both accuracy and SLA together.

heads due to frequent overÔ¨Çow of memory and the use of vir- The placement of such split models is non-trivial consider-

tual memory (swap space) on NAS media, making high ing the diverse and complex dynamism of task distribution,

Ô¨Ådelity inference using DNNs hard [10], [15]. To deploy an model usage frequencies and geographical placement of

upgraded AI model, tech giants like Amazon, NetÔ¨Çix and mobile edge devices [30].

Google usually consider completely revamping their infra- Research Contributions. This work proposes a novel neural

structures and upgrading their devices, raising many sus- splitting and placement policy, SplitPlace, for enhanced dis-

tainability concerns [2]. This has made the integration of tributed neural network inference at the edge. SplitPlace

massive neural network models with such devices a chal- leverages a mobile edge computing platform to achieve low

lenging and expensive ordeal.

latency services. It allows modular neural models to be inte-

Solution. A promising solution for this problem is the grated for best result accuracies that could only be provided

development of strategies that can accommodate large-scale by cloud deployments. SplitPlace is the Ô¨Årst splitting policy

DNNs within legacy infrastructures. However, many prior that dynamically decides between semantic and layer-wise

efforts in this regard [16], [17], [18] have not yet tackled the splits to optimize both inference accuracy and the SLA vio-

challenge of providing a holistic strategy for not only dis- lation rate. This decision is taken for each incoming task

tributed learning, but also inference in such memory-con- and remains unmodiÔ¨Åed until the execution of all split frag-

strained environments. Recently, research ideas have been ments of that task are complete. The idea behind the pro-

proposed like Cloud-AI, Edge-AI and Federated learning posed splitting policy is to decide for each incoming task

that aim to solve the problem of running enormous deep whether to use the semantic or layer-wise splitting strategy

learning models on constrained edge devices by splitting based on its SLA demands. Due to their quick adaptability,

them into modular fragments [17], [18]. However, in Cloud- SplitPlace uses Multi-Armed-Bandits to model the decision

AI where AI systems are deployed on cloud machines, the strategy for each application type by checking if the SLA

high communication latency leads to high average response deadline is higher or lower than an estimate of the response

times, making it unsuitable for latency-critical applications time for a layer split decision [31]. Further, SplitPlace opti-

like healthcare, gaming and augmented reality [6], [19], [20]. mizes the placement decision of the modular neural net-

Instead, Edge-AI provides low-latency service delivery, work fragments using a split decision aware surrogate

thanks to edge devices being in the same Local Area Net- model. Compared to a preliminary extended abstract of this

work (LAN), where the input data from multiple edge work [1], this paper provides a substantially expanded

nodes are combined to a single Ô¨Åxed broker node for proc- exposition of the working of MABs in SplitPlace. We also

essing. Edge-AI based methods aim at scheduling deep neu- present techniques to dynamically adapt to non-stationary

ral networks for providing predictable inference [21], [22]. workloads and mobile environments. We present a gradi-

However, due to the centralized collection of data, these sol- ent-based optimization approach for task placement deci-

utions typically suffer from high bandwidth overheads and sion conditioned on split decisions. Experiments on real-

poor service quality [17]. Federated learning depends on world application workloads on a physical edge testbed

data distribution over multiple nodes where the model show that the SplitPlace approach outperforms the baseline

training and inference are performed in a decentralized approaches by reducing the SLA violation rate and improv-

fashion. However, this paradigm assumes that neural mod- ing the average inference accuracy.

els with data batches can be accommodated in the system Outline. The rest of the paper presents a brief back-

memory. This is seldom the case for common edge devices ground with motivation and related work in Section 2.

like Arduinos or Raspberry Pis [23].

Section 3 presents the system model assumptions and for-

Other recent works offer lower precision models that can mulates the problem. Sections 4 and 5 give the model

Ô¨Åt within the limited memory of such devices by using details of the proposed SplitPlace approach. We then vali-

methods like Model Compression or Model Pruning [9], date and show the efÔ¨Åcacy of the placement policy in

[24], [25]. However, compressed and low-precision models Section 6. Finally, Section 7 concludes the work and pro-

lose inference accuracy, making them unsuitable for accu- poses future directions. Additional experimental results

racy-sensitive applications like security and intrusion detec- are given in the Appendix A, which can be found on

tion [26]. Recently, split neural network models have been the Computer Society Digital Library at http://doi.

proposed. They show that using semantic or layer-wise ieeecomputersociety.org/10.1109/TMC.2022.3177569, in

splitting, a large deep neural network can be fragmented the supplementary text.

into multiple smaller networks for dividing network param-

eters onto multiple nodes [16], [27], [28], [29]. The former partitions a neural network into parallel disjoint models

2

BACKGROUND AND RELATED WORK

that produce a part of the result. The latter partitions a neu- As discussed in Section 1, there is a need for frameworks

ral network into sequential models that generate intermedi- that can exploit the low latency of edge nodes and also high

ate results. We illustrate the accuracy and response time inference performance of DNNs to provide HALL services.

trade-offs through sample test cases in Section 2. Our However, complete neural models with the input batch can

experiments show that using layer and semantic splitting seldom be accommodated in the random-access-memory

gives higher inference accuracies than previously proposed (RAM) of edge devices. Thus, ideas like model compression

model compression techniques (see Section 6). However, no or splitting are required to make inference over large-scale

appropriate scheduling policies exist that can intelligently neural networks plausible in such environments. Frame-

place such modular neural fragments on a distributed works that aim at achieving this must maintain a careful
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5541

Fig. 1. Overview of layer and semantic splitting strategies.

balance between accuracy requirements and response times

for different user tasks. For such a framework, real-time analysis of the incoming tasks is required for quick decision Fig. 2. Comparison of layer and semantic splits.

making of task placement. This requires robust algorithms

to seamlessly integrate different paradigms and meet the proposed SplitPlace approach as it is better than model

user‚Äôs service level agreements.

compression or early-exit strategies for quick inference.

Semantic and Layer Splitting. In this work, we leverage the This is acceptable in many industrial applications [25], [36]

only two available splitting schemes for neural networks: where latency and service level agreements are more impor-

layer and semantic splitting [16], [32]. An overview of these tant performance metrics than high-Ô¨Ådelity result delivery.

two strategies is shown in Fig. 1. Semantic splitting divides In this work, we consider a system with both SLA violation

the network weights into a hierarchy of multiple groups rates and inference accuracy as optimization objectives.

that use a different set of features (different colored models This makes the combination of layer and semantic splitting

in Fig. 1). Here, the neural network is split based on the a promising choice for such use cases.

data semantics, producing a tree structured model that has

no connection among branches of the tree, allowing paralle- 2.1 Related Work

lization of input analysis [16]. Due to limited information We now analyze the prior work in more detail. We divide our

sharing among the neural network fragments, the semantic literature review into three major sections based on the strat-

splitting scheme gives lower accuracy in general. Semantic egy used to allow DNN inference on resource-constrained

splitting requires a separate training procedure where pub- mobile-edge devices: model compression, layer splitting and

licly available pre-trained models cannot be used. This is semantic splitting. Moreover, we compare prior work based

because a pre-trained standard neural network can be split on whether they are suitable for edge-only setups (i.e., with-

layer wise without affecting output semantics. For semantic out leveraging cloud nodes), consider heterogeneous and

splitting we would need to Ô¨Årst split the neural network mobile nodes and work in settings with adaptive Quality of

based on data semantics and re-train the model. However, Service (QoS). See Table 1 for an overview.

semantic splitting provides parallel task processing and Model Compression. EfÔ¨Åcient compression of DNN models

hence lower inference times, more suitable for mission-criti- has been a long studied problem in the literature [43]. Several

cal tasks like healthcare and surveillance. Layer-wise split- works have been proposed that aim at the structural pruning

ting divides the network into groups of layers for sequential of neural network parameters without signiÔ¨Åcantly impact-

processing of the task input, shown as different colored ing the model‚Äôs performance. These use approaches like ten-

models in Fig. 1.

sor decomposition, network sparsiÔ¨Åcation and data

Layer splitting is easier to deploy as pre-trained models quantization [43]. Such pruning and model compression

can be just divided into multiple layer groups and distrib- approaches have also been used by the systems research

uted to different mobile edge nodes. However, layer splits community to allow inference of massive neural models on

require a semi-processed input to be forwarded to the sub- devices with limited resources [44]. Recently, architectures

sequent edge node with the Ô¨Ånal processed output to be like BottleNet and Bottlenet++ have been proposed [37], [38]

sent to the user, thus increasing the overall execution time. to enable DNN inference on mobile cloud environments and

Moreover, layer-wise splitting gives higher accuracy com- reduce data transmission times. BottleNet++ compresses the

pared to semantic splitting. Comparison of accuracies and intermediate layer outputs before sending them to the cloud

average response times for the two strategies is shown in layer. It uses a model re-training approach to prevent the

Fig. 2. The Ô¨Ågure shows results for 10 edge worker nodes inference being adversely impacted by the lossy compres-

using popular image classiÔ¨Åcation datasets: MNIST, Fash- sion of data. Further, BottleNet++ classiÔ¨Åes workloads in

ionMNIST and CIFAR100 [33], [34], [35] averaged over terms of compute, memory and bandwidth bound categories

ResNet50-V2, MobileNetV2 and InceptionV3 neural mod- and applies an appropriate model compression strategy.

els [9]. As is apparent from the Ô¨Ågure, layer splits provide Other works propose to efÔ¨Åciently prune network channels

higher accuracy and response time, whereas semantic splits in convolution neural models using reinforcement learn-

provide lower values for both. SplitPlace leverages this con- ing [39], [45]. Other efforts aim to prune the weights of the

trast in traits to trade-off between inference accuracy and neural models to minimize their memory footprint [46]. Such

response time based on SLA requirements of the input methods aim at improving the accuracy per model size as a

tasks. Despite the considerable drop in inference accuracy metric in contrast to the result delivery time as in BottleNet+

when using semantic splitting scheme, it is still used in the +. However, model compression does not leverage multiple
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5542

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

TABLE 1 Comparison of Related Works With Different Parameters (‚úìMeans That the Corresponding Feature is Present)

Work
[37], [38] [39] [27], [36], [40], [41] [9] [11], [42] [16], [25], [28], [36] [32] SplitPlace

Edge Only
‚úì
‚úì ‚úì ‚úì ‚úì ‚úì

Mobility ‚úì ‚úì

Heterogeneous Environment
‚úì
‚úì
‚úì ‚úì ‚úì

Adaptive QoS
‚úì ‚úì ‚úì ‚úì ‚úì

Layer Split
‚úì ‚úì ‚úì
‚úì ‚úì

Semantic Split
‚úì ‚úì

Model Compression
‚úì ‚úì
‚úì

Optimization Parameters

Accuracy SLA Reward

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

compute nodes and has poor inference accuracy in general Semantic Splitting. The early efforts of semantic splitting

compared to semantic split execution (discussed in Section 6). only split the neural network at the input layer to allow

Thus, SplitPlace does not use the model compression model parallelization and size reduction [29]. Some methods

technique.

divide the data batch itself across multiple nodes addressing

Layer Splitting. Many other efforts aim at improving the computational contention problems but not memory limita-

inference time or accuracy by efÔ¨Åcient splitting of the DNN tions of Ô¨Åtting neural networks in the RAM [47]. Other meth-

models. Some methods aim to split the networks layer-wise ods use progressive slicing mechanisms to partition neural

or vertically, viz, that the different fragments correspond to models into multiple components to Ô¨Åt in heterogeneous

separate layer groups and hence impose the constraint of devices [25]. Such methods produce the complete output

sequential execution. Most work in this category aims at from each split or fragment of the DNN, adversely impacting

segregating these network splits into distinct devices based the scalability of such methods to high-dimensional output

on their computational performance [27], [36], [40], [41], spaces such as image segmentation applications [16], [28].

[42]. In heterogeneous edge-cloud environments, it is fairly Moreover, these methods require signiÔ¨Åcant cross-commu-

straightforward to split the network into two or three frag- nication among network splits, signiÔ¨Åcantly increasing

ments each being deployed in a mobile device, edge node the communication overheads. Recently, more intelligent

or a cloud server. Based on the SLA, such methods provide approaches have been developed which hierarchically split

early-exits if the turnaround time is expected to be more neural networks such that each fragment produces a part of

than the SLA deadline. This requires a part of the inference the output using an intelligently chosen sub-part of the

being run at each layer of the network architecture instead input [16]. Such schemes use the ‚Äúsemantic‚Äù information of

of traditionally executing it on the cloud server. Other the data to create the corresponding links between input and

recent methods aim at exploiting the resource heterogeneity output sub-parts being given to each DNN fragment, hence

in the same network layer by splitting and placing DNNs the name semantic splitting. Such splitting schemes require

based on user demands and edge worker capabilities [9]. minimal to no interaction among network fragments elimi-

Such methods can not only split DNNs, but also choose nating the communication overheads and increased latency

from different architectural choices to reach the maximum due to stragglers. As semantic splitting can provide results

accuracy while agreeing to the latency constraints. Other quickly, albeit with reduced accuracy, SplitPlace uses it for

works aim at accelerating the model run-times by appropri- tasks with tight deadlines (Section 4).

ate scheduling of a variety of DNN models on edge-clus-

ters [11]. The state-of-the-art method, Gillis uses a hybrid model, wherein it employs either model-compression or

3

SYSTEM MODEL AND PROBLEM FORMULATION

layer-splitting based on the application SLA demands [32]. In this work, we assume a scenario with a Ô¨Åxed number of

The decision is taken using a reinforcement-learning model multiple heterogeneous edge nodes in a broker-worker fash-

which continuously adapts in dynamic scenarios. As the ion, which is a typical case in mobile-edge environments [27],

model paritioning is also performed dynamically, the Gillis [38], [48], [49]. Here, the broker node takes all resource man-

model cannot work with semantic splitting strategy that agement related decisions, such as neural network splitting

requires a new model to be trained for each partitioning and task placement. The processing of such tasks is carried

scheme. It is a serverless based model serving system that out by the worker nodes. Examples of broker nodes include

automatically partitions a large model across multiple serv- personal laptops, small-scale servers and low-end worksta-

erless functions for faster inference and reduced memory tions [48]. Example of common worker nodes in edge envi-

footprint per function. The Gillis method employs two ronments include Raspberry Pis, Arduino and similar

model partitioning algorithms that respectively achieve System-on-Chip (SoC) computers [2]. All tasks are received

latency optimal serving and cost-optimal serving with ser- from an IoT layer that includes sensors and actuators to col-

vice-level agreement compliance. However, this method lect data from the users and send it to the edge broker via the

cannot jointly optimize both latency and SLA. Moreover, it gateway devices. Akin to typical edge conÔ¨Ågurations [50],

does not consider the mobility of devices or users and hence the edge broker then decides which splitting strategy to use

is ineffective in efÔ¨Åciently managing large DNNs in mobile and schedules these fragments to various edge nodes based

edge computing environments.

on deployment constraints like sequential execution in a

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5543

Fig. 3. SplitPlace system model.
Notation It Tt Nt H i ¬º fbi; slai; aig di 2 fL; Sg Ci Ct ¬º [i2T t Ci St Pt : Ct √Ç H Ot

TABLE 2 Table of Main Notation
Description
t-th scheduling interval Active tasks in It
New tasks received at the start of It Set of workers in the edge layer
Task as a collection input batch, SLA deadline and application type Splitting decision for input task i
Container realization of task i based on decision di Set of all active containers in the interval It State of the system at the start of It
Placement decision of Ct to H as an adjacency matrix Objective score for interval It

layer-decision. The data to be processed comes from the IoT execution. We assume a Ô¨Åxed number of worker machines

sensors/actuators, which with the decision of which split in the edge layer and denote them as H. We also consider

fragment to use is forwarded by the broker to each worker that new tasks created at the interval It are denoted as N t, node. Some worker nodes are assumed to be mobile, with all active tasks being denoted as T t (and N t  T t). whereas others are considered to be Ô¨Åxed in terms of their Each task i 2 T t consists of a batch input bi, SLA deadline

geographical location. In our formulation, we consider slai and a DNN application ai. The set of all possible DNN
mobility only in terms of the variations in terms of the net- applications is denoted by A. For each new task i 2 N t, the work channels and do not consider the worker nodes or edge broker takes a decision di, such that di 2 fL; Sg, with L

users crossing different networks. We assume that the CPU, denoting layer-wise splitting and S denoting semantic split

RAM, Bandwidth and Disk capacities of all nodes are known strategy. The collection of all split decisions for active tasks

in advance, and similarly the broker can sample the resource consumption for each task in the environment at any time

in interval sion di for

It is task

denoted as i, this task

Dt ¬º fdigi2N t is realized as

. Based on the decian execution work-

(see Resource Monitor in Fig. 3). As we describe later, the bro- Ô¨Çow in the form of containers Ci. Similar to a VM, a

ker periodically measure utilizations of CPU, RAM, Band- container is a package of virtualized software that contains

width and Disk for each task in the system. The broker is all of the necessary elements to run in any environment.

trusted with this information such that it can make informed The set of all containers active in the interval It is denoted resource management decisions to optimize QoS. Moreover, as Ct ¬º [i2T t Ci. The set of all utilization metrics of CPU,
we consider that tasks include a batch of inputs that need to RAM, Network Bandwidth and Disk for all containers and

be processed by a DNN model. Further, for each task, a ser- workers at the start of the interval It deÔ¨Ånes the state of the

vice level deadline is deÔ¨Åned at the time the task is sent to the system, denoted as St. A summary of the symbols is given edge environment. We give an overview of the SplitPlace in Table 2.

system model in Fig. 3. We decompose the problem into deciding an optimal splitting strategy and a fragment place- 3.1 Split Nets Placement

ment for each application (motivation in Appendix A.6, We partition the problem into two sub-problems of decid-

available in the online supplemental material, and more ing the optimal splitting strategy for input tasks and that of

details in Section 1).

placement of active containers in edge workers (see Fig. 4).

Workload Model. We consider a bounded discrete time Considering the previously described system model, at the

control problem where we divide the timeline into equal start of each interval It, the SplitPlace model takes the split duration intervals, with the tth interval denoted as It. Here, decision di for all i 2 N t. Moreover, it also takes a placement
t 2 f0; . . . ; Gg, where G √æ 1 is the number of intervals in an decision for all active containers Ct, denoted as an adjacency
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5544

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

impose a high overhead on network bandwidth at runtime.

This sharing of containers for each splitting strategy and

dataset type are transferred to the worker nodes is per-

formed at the start of the run. At run-time, only the decision

of which split fragment to be used is communicated to the

worker nodes, which executes a container from the corre-

sponding image. The placement of task on each worker is

based on the resource availability, computation required to

be performed in each section and the capabilities of the

nodes (obtained by the Resource Monitor). For intensive com-

putations with large storage requirements (Gated Recurrent

Units or LSTMs) or splits with high dimension size of

input/output (typically the Ô¨Ånal layers), the splits are sent

to high-resource edge workers. The management of alloca-

tion and migration of neural network splits is done by the

Container Orchestrator. Other attention based sub-layer

extensions can be deployed in either edge or cloud node

Fig. 4. SplitPlace decision and placement problems.

based on application requirements, node constraints and

user demands. Based on the described model assumptions,

matrix Pt : Ct √Ç H. This is realized as a container allocation we now formulate the problem of taking splitting and place-

for new tasks and migration for active tasks in the system. ment decisions to optimize the QoS parameters. Implemen-

The main idea behind the layer-wise split design is Ô¨Årst to tation speciÔ¨Åc details on how the results of layer-splits are

divide neural networks into multiple independent splits, forwarded and outputs of semantic splits combined across

classify these splits in preliminary, intermediate and Ô¨Ånal edge nodes are given in Section 5.

neural network layers and distribute them across different

nodes based on the node capabilities and network hierarchy. 3.2 Problem Formulation

This exploits the fact that communication across edge nodes The aim of the model is to optimize an objective score Ot (to in the LAN with few hop distances is very fast and has low be maximized), which quantiÔ¨Åes the QoS parameters of the

latency and jitter [51]. Moreover, techniques like knowledge interval It, such as accuracy, SLA violation rate, energy condistillation can be further utilized to enhance the accuracy of sumption and average response time. This is typically in the

the results obtained by passing the input through these dif- form of a convex combination of energy consumption,

ferent classiÔ¨Åers. However, knowledge distillation needs to response time, SLO violation rates, etc. [50], [52]. The con-

be applied at the training stage, before generating the neural straints in this formulation include the following. First, the

network splits. As there are many inputs in our assumed container decomposition for a new task i 2 N t should be large-scale deployment, the execution can be performed in a based on di. Second, containers corresponding to the layer-

pipelined fashion to further improve throughput over and split decisions fCijdi ¬º Lg should be scheduled as per the

above the low response time of the nodes at the edge of the linear chain of precedence constraints. This means that a

network. For the semantic split, we divide the network container later in the neural inference pipeline should be

weights into a set or a hierarchy of multiple groups that use scheduled only after the complete execution of the previous

disjoint sets of features. This is done by making assignment containers in the pipeline. This is because the output of an

decisions of network parameters to edge devices at deploy- initial layer in an inference pipeline of a neural network is

ment time. This produces a tree-structured network that required before we can schedule a latter layer in the pipe-

involves no connection between branched sub-trees of line. Third, the placement matrix Pt : Ct √Ç H should adhere semantically disparate class groups. Each sub-group is then to the allocation constraints, i.e., it should not allocate/

allocated to an edge node. The input is either broadcasted migrate a container to a worker where the worker does not

from the broker or forwarded in a ring-topology to all nodes have sufÔ¨Åcient resources available to accommodate the con-

with the network split corresponding to the input task. We tainer. Thus, the problem can be formulated as

use standard layer [32] and semantic splitting [16] methods

as discussed in Section 2. We now outline the working of the proposed distributed
deep learning architecture for edge computing environ-

maximize
Pt ;Dt

XT Ot
t

ments. Fig. 3 shows a schematic view of its working. As shown, there is a shared repository of neural network parameters which is distributed by the broker to multiple edge nodes. The layer and semantic splits are realized as Docker container images that are shared by the broker to

subject to

8 t; 8 i 2 N t; Ci containers created based on splitting decision di; 8 t; Pt is feasible; 8 di ¬º L; Ci follow precedence chain: (1)

the worker nodes at the start of each execution trace. The

placement of tasks is realized as spinning up a Docker container using the corresponding image on the worker. As the

4

SPLITPLACE POLICY

process of sharing container images is a one-time event, We now describe the SplitPlace decision and placement pol-

transferring all semantic and layer split fragments does not icy. For the Ô¨Årst sub-problem of deciding optimal splitting
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5545

strategy, we employ a Multi-Armed Bandit model to greater than the estimate of the response time for a layer
dynamically enforce the decision using external reward sig- decision, 2) when SLA is less than this estimate. The motivanals.1 Our solution for the second sub-problem of split tion behind these two contexts is that in case of the SLA

placement uses a reinforcement-learning based approach that speciÔ¨Åcally utilizes a surrogate model to optimize the placement decision (agnostic to the speciÔ¨Åc implementation).2 This two-stage approach is suboptimal since the response time of the splitting decision depends on the placement decision. In case of large variation in terms of the

deadline being lower than the execution time of layer split, a ‚Äúlayer‚Äù decision would be more likely to violate the SLA as result delivery would be after the deadline. However, the exact time it takes to completely execute all containers corresponding to the layer split decision is apriori unknown. Thus, for every application type, we maintain estimates of

computational resources, it is worth exploring joint optimi- the response time, i.e, the total time it takes to execute all zation of both decisions. However, in our large-scale edge containers corresponding to this decision.

settings, this segregation helps us to make the problem tractable as we describe next.
The motivation behind this segregation is two-fold. First, having a single reinforcement-learning (RL) model that takes both splitting and placement decisions makes the state-space explode exponentially, causing memory bottle-

Let us denote the tasks leaving the system at the end of It as Et. Now, for each task i 2 Et, we denote response time and inference performance using ri and pi. We denote the layer response time estimate for application a 2 A as Ra. To
quickly adapt to non-stationary scenarios, for instance due
to the mobility of edge nodes in the system, we update our

necks in resource-constrained edge devices [55]. Having a estimates using new data-points as exponential moving simple RL model does not allow it to scale well with several averages using the multiplier f 2 ¬Ω0; 1¬ä for the most recent devices in modern IoT settings (see Section 6 with 50 edge response time observation. Moving averages presents a low

devices the and Gillis RL baseline). One of the solutions that we explore in this work is to simplify this complex problem by decomposing it into split decision making and task placement. Second, the response time of an application depends primarily on the splitting choice, layer or semantic, making it a crucial factor for SLA deadline based decision making. To minimize the SLA violation rates we only use the response time based context for our Multi-Armed bandit model. Other parameters like CPU or RAM utilization have high variability in a volatile setting and are not ideal choices for deciding which splitting strategy to opt. Instead, the

computational cost and consequently low latency compared to more sophisticated smoothing functions
Ra f √Å ri √æ √∞1 √Ä f√û √Å Ra; 8i 2 Et ^ di ¬º L; 8a 2 A: (2)
Compared to simple moving average, the above equation gives higher weights to the latest response times, allowing the model to quickly respond to recent changes in environment and workload characteristics.

inference accuracy is another key factor in taking this decision. Thus, SLA violation and inference accuracy are apt objectives for the Ô¨Årst sub-problem. Further, the energy consumption and average response time largely depend on the task placement, making them an ideal objective for optimization in the task placement sub-problem.

4.1.2 Context Based MAB Model
Now, for any input task i 2 N t, we divide it into two cases: slai ! Rai and slai < Rai . Considering that the response time of a semantic-split decision would likely be lower than the layer-split decision, in the Ô¨Årst case both decisions would most likely not lead to an SLA violation (high SLA setting). However, in the second case, a layer-split decision

4.1 Multi-Armed Bandit Decision Module
Multi-Armed Bandit, in short MAB, is a policy formulation where a state-less agent is expected to take one of many decisions with each decision leading to a different reward. The objective of such an agent is to maximize the expected longterm reward [31]. However, in our case, the most important factor to consider when making a decision of whether to use layer or semantic splits for a task is its SLA deadline.

would likely lead to an SLA violation but not the semanticsplit decision (low SLA setting). To tackle the problem for these different contexts, we maintain two independent MAB models denoted as MABh and MABl. The former represents a MAB model for the high-SLA setting and the latter for the low-SLA setting.
For each context and decision d 2 fL; Sg, we deÔ¨Åne reward metrics as

4.1.1 Estimating Response Time of Layer-Splits
The idea behind the proposed SplitPlace approach is to maintain MABs for two different contexts: 1) when SLA is

Oh;d Ol;d

¬º ¬º

P
i2Et
P
i2Et

√∞1√∞ri 2√Å
√∞1√∞ri 2√Å

P slai√û √æ pi√û √Å 1√∞slai i2Et 1√∞slai ! Rai ^

! di

Rai ^ ¬º d√û

di

¬º

d√û

;

P slai√û √æ pi√û i2Et 1√∞slai

√Å 1√∞slai < < Rai ^ di

Rai ^ ¬º d√û

di

¬º

d√û

:

(3) (4)

1. Compared to other methods like A/B testing and Hill Climbing

search [53], Multi-Armed Bandits allow quick convergence in scenarios The Ô¨Årst term of the numerator, i.e., 1√∞ri slai√û quantiÔ¨Åes

when different cases need to be modelled separately, which is the case in our setup. Thus, we use Mult-Armed Bandits for deciding the optimal splitting strategy for an input task.
2. In contrast to Monte Carlo or Evolutionary methods, Reinforce-

SLA violation reward (one if not violated and zero otherwise). The second term, i.e., pi corresponds to the inference accuracy of the task. These two objectives have been moti-

ment learning allows placement to be goal-directed, i.e., aims at opti- vated at the start of Section 4. Thus, each MAB model gets

mizing QoS using it as a signal, and allows the model to adapt to changing environments [54]. Hence, we use a RL model, speciÔ¨Åcally using a surrogate model due for its scalability, to decide the optimal

the reward function for its decisions allowing independent training of the two. The weights of the two metrics, i.e.,

task placement of network splits.

accuracy and SLA violation can be set by the user to modify

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5546

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

Fig. 5. MAB decision workÔ¨Çow.

the relative importance between the metrics as per applica- Here, the probability  decays using the reward feedback,

tion requirements. In our experiments, the weight parame- starting from 1. We maintain a reward threshold r that is

ters of both metrics are set to be equal based on grid-search, maximizing the average reward.
Now, for each decision context c 2 fh; lg and d 2 fL; Sg,

initialized as a small age reward OMAB ¬º

1pPositive
4 c2fh;lg

cPonstant
d2fL;Dg

k< Oc;d

1, and use averto update  and

r using the rules

we maintain a decision count Nc;d and a reward estimate Qc;d which is updated using the reward functions Oh;d or Ol;d as follows:
Qc;d Qc;d √æ g√∞Oc;d √Ä Qc;d√û; 8d 2 fL; Sg; 8c 2 fh; lg;
(5) where g is the decay parameter. Thus, each reward-estimate

(

decay√∞√û; OMAB > r



;

(7)

;

otherwise

(

r

increment√∞r√û; OMAB > r :

(8)

r;

otherwise

is updated by the corresponding reward metric.

Here decay√∞√û ¬º √∞1 √Ä k√û √Å  and increment√∞r√û ¬º √∞1 √æ k√û √Å r.

Note that OMAB > r refers to the current value of r prior to

Algorithm 1. SplitPlace Decision and Placement Module the update. The k value controls the rate of convergence of

Require: Pre-trained MAB models MABh; MABl Discounting factor g 2 √∞0; 1√û

the model. The  value controls the exploration of the model at training time allowing the model to visit more states and obtain precise estimates of layer-split response times.

1: procedure SPLITPLACE(scheduling interval It)

2: Get new tasks N t

3: Get leaving tasks Et 4: Calculate Oc;d 8c 2 fh; lg; d 2 fL; Sg using (3) and (4)

5: Update gain estimates Qc;d 8c; d using (5) "Q update

6: Update decision counts Nc;d 8c; d

"Count update

7: for i 2 N t do

8:

i ¬º 8fbi; slai; aig

qÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É

9:

di

< arg maxd2fL;SgQh;d √æ cqÔ¨ÉNlÔ¨ÉoÔ¨ÉÔ¨ÉhgÔ¨É;Ô¨Édt ; : arg maxd2fL;SgQl;d √æ c Nlogl;dt;

slai ! Rai slai < Rai

"UCB based decision

10:

Dt fdigi2N t

11: Get St from resource-monitor

"Current State

12: 13:

OPtMABDAS14OP√∞¬ΩcS2ft;h;Plgt√ÄP1;dD2ftL¬ä√û;Dg Oc;d "Placement Decision

14: Fine-tune DASO using OP calculated using (10)

15: returnPt

However, at test time we already have precise estimates of the response times; thus exploration is only required to adapt in volatile scenarios. For this, -greedy is not a suitable approach as decreasing  with time would prevent exploration as time progresses. Instead, we use an Upper-ConÔ¨Ådence-Bound (UCB) exploration strategy that is more suitable as it takes decision counts also into account [58], [59]. Thus, at test time, we take a deterministic decision using the rule

8

qÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É

di

¬º

< :

arg maxd2fL;SgQh;d √æ cqÔ¨ÉÔ¨ÉNÔ¨ÉlÔ¨ÉoÔ¨ÉhgÔ¨É;td; argmaxd2fL;SgQl;d √æ c Nlogl;dt;

slai ! Rai ;
slai < Rai

(9)

where t is the scheduling interval count and c is the exploration factor. An overview of the complete split-decision making workÔ¨Çow is shown in Fig. 5. We now discuss the RL based placement module. It is worth noting that both components are independent of each other and can be improved

For both these MAB models, we use a parameter-free feedback-based -greedy learning approach that is known to be versatile in adapting to diverse workload characteris-

separately in future; however, our experiments show that the MAB decision module accounts for most of the performance gains (see Section 6).

tics [56]. Unlike other strategies, this is known to scale asymptotically as the long-term Q estimates become exact under mild conditions [57, Section 2.2]. To train the model, we take the contextual decision

8

di

¬º

>>< >>:

random decision; arg maxd2fL;SgQh;d;
random decision; arg maxd2fL;SgQl;d;

with prob.  otherwise ;

with prob. otherwise



;

slai ! Rai :
slai < Rai

4.2 Reinforcement Learning Based Placement Module
Once we have the splitting decision for each input task i 2 N t, we now can create containers using pre-trained layer and semantic split neural networks corresponding to the application ai. This can be done ofÔ¨Çine on a resource rich system, where the split models can be trained using existing datasets. Once we have trained models, we can generate container images corresponding to each split fragment and

(6) distribute to all worker nodes [60]. Then we need to place
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5547

containers, translating to the worker node initializing a container for the corresponding image, of all active tasks Ct to workers H. To do this, we use a learning model which pre-

L√∞f√∞x; u√û; y√û ¬º 1

Xb √∞y √Ä f√∞x; u√û√û2;

where

√∞x; y√û 2 L:

b t¬º0

(11)

dicts the placement matrix Pt using the state of the system St, decisions Dt ¬º di8i 2 T t and a reward signal OP . To deÔ¨Åne OP , we deÔ¨Åne the following metrics [50]:

is minimized as in [50]. To do this, we use AdamW optimizer [61] and update u up till convergence. This allows the surrogate model f to predict an QoS objective score for a

1) Average Energy Consumption (AEC) is deÔ¨Åned for any given system state St, split-decisions Dt and task placement

interval It as the mean energy consumption of all Pt. Once the surrogate model is trained, starting from the

edge workers in the system.

placement decision from the previous interval Pt ¬º Pt√Ä1, we

2) Average Response Time (ART) is deÔ¨Åned for any inter- leverage it to optimize the placement decision using the fol-

val It as mean response time (in scheduling inter- lowing rule:

vals) of all leaving tasks Et. The choice of these two objectives for the placement sub-

Pt Pt √Ä h √Å rPt f√∞¬ΩSt; Pt; Dt¬ä; u√û;

(12)

problem has been motivated at the start of Section 4. Using

these metrics, for any interval It, OP is deÔ¨Åned as

for a given state and decision pair St; Dt. Here, h is the learn-

ing rate of the model. The above equation is iterated till con-

OP ¬º OMAB √Ä a √Å AECt √Ä b √Å ARTt:

(10)

vergence, i.e., the L2 norm between the placement matrices of two consecutive iterations is lower than a threshold

Here, a and b (such that a √æ b ¬º 1) are hyper-parameters that can be set by users as per the application requirements. Higher a aims to optimize energy consumption at the cost

value. Thus, at the start of each interval It, using the output of the MAB decision module, the DASO model gives us a placement decision Pt.

of higher response times, whereas low a aims to reduce average response time. Thus, a RL model f, parameterized by u takes a decision Pi, where the model uses the reward estimate as the output of the function f√∞¬ΩSt; Pt; Dt¬ä; u√û, where the parameters u are updated based on the reward signal OP . We call this learning approach ‚Äúdecision-aware‚Äù as part of the input is the split-decision taken by the MAB model.
Clearly, the proposed formulation is agnostic to the underlying implementation of the learning approach. Thus, any policy like Q-learning or Actor-Critic Learning could be used in the SplitPlace model [57]. However, recently developed techniques like GOBI [50] use gradient-based optimization of the reward to quickly converge to a localmaximum of the objective function. GOBI uses a neural-network based surrogate model to estimate the reward from a given input state, which is then used to update the state by calculating the gradients of the reward estimates with respect to the input. Moreover, advances like momentum, annealing and restarts allow such models to quickly reach a global optima [50].
DASO Placement Module. In the proposed framework, we use decision-aware surrogate based optimization method (termed as DASO) to place containers in a distributed mobile edge environment. This is motivated from

4.3 SplitPlace Algorithm
An overview of the SplitPlace approach is given in Algorithm 1. Using pre-trained MAB models, i.e., Q-estimates Qc;d and decision counts Nc;d, the model decides the optimal splitting decision using the UCB metric (line 9). To adapt the model in non-stationary scenarios, we dynamically update the Q-estimates and decision counts (lines 5 and 6). Using the current state and the split-decisions of all active tasks, we use the DASO approach to take a placement decision for the active containers (line 12). Again, we Ô¨Åne-tune the DASO‚Äôs surrogate model using the reward metric to adapt to changes in the environment, for instance the changes in the latency of mobile edge nodes and their consequent effect on the reward metrics (line 14). However, the placement decision must conform to the allocation constraints as described in Section 3.2. To relax the constraint of having only feasible placement decisions, in SplitPlace we allocate or migrate only those containers for which it is possible. Those containers that could not be allocated in a scheduling interval are placed to nodes corresponding to the highest output of the neural network f. If no worker placement is feasible the task is added to a wait queue, which are considered again for allocation in the next interval.

prior neural network based surrogate optimization methods [50]. Here, we consider a Fully-Connected-Network

5

IMPLEMENTATION

(FCN) model f√∞x; u√û that takes an x as a tuple of input state To implement and evaluate the SplitPlace policy, we need a

St, split-decision Dt and placement decision Pt, and out- framework that we can use to deploy containerized neural

puts an estimate of the QoS objective score Ot. This is network split fragments on an edge computing environbecause FCNs are agnostic to the structure of the input ment. One such framework is COSCO [50]. It enables the

and hence a suitable choice for modeling dependencies development and deployment of integrated edge-cloud

between QoS metrics and model inputs like resource utili- environments with structured communication and platform

zation and placement decision [49], [50]. Exploration of independent execution of applications. It connects various

other styles of neural models, such as graph neural net- IoT sensors, which can be healthcare sensors with gateway

works that can take the network topology graph as an devices, to send data and tasks to edge computing nodes,

input are part of future work. Now, using existing execu- including edge or cloud workers. The resource management

tion trace dataset, L ¬º f¬ΩSt; Pt; Dt¬ä; Otgb, the FCN model is and task initiation is undertaken on edge nodes in the brotrained to optimize its network parameters u such that the ker layer. The framework uses HTTP RESTful APIs for com-

Mean-Square-Error (MSE) loss

munication and seamlessly integrates a Flask based web-

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5548

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

TABLE 3 Edge Worker Characteristics of Azure Edge Environment

Name

Qty

Core

MIPS

count

RAM

RAM Bandwidth

Ping time

Network Bandwidth

Disk Bandwidth

Cost Model

Worker Nodes

B2ms

20

2

4029

4295 MB

372 MB/s

2 ms 1000 MB/s

13.4 MB/s

0.0944 $/hr

E2asv4

10

2

4019

4172 MB

412 MB/s

2 ms 1000 MB/s

10.3 MB/s

0.148 $/hr

B4ms

10

4

8102

7962 MB

360 MB/s

3 ms 2500 MB/s

10.6 MB/s

0.189 $/hr

E4asv4

10

4

7962

7962 MB

476 MB/s 3 ms 2500 MB/s 11.64 MB/s 0.296 $/hr

Broker Node

L8sv2

1

8

16182 17012 MB

945 MB/s

1 ms 4000 MB/s

17.6 MB/s

0.724 $/hr

environment to deploy and manage containers in a distrib- 256 GB size.3 The worker nodes have 4-8 GB of RAM,

uted setup [62].

whereas the broker has 16 GB RAM. To factor in the mobil-

We use only the edge-layer deployment in the frame- ity of the edge nodes, we use the NetLimiter tool to

work and use the Docker container engine to containerize tweak the communication latency with the broker node

and execute the split-neural networks in various edge work- using the mobility model described in [66]. SpeciÔ¨Åcally,

ers [60]. We uses the Checkpoint/Restore In Userspace we use the latency and bandwidth parameters of workers

(CRIU) [63] tool for container migration. Further, the DASO from the traces generated using the Simulation of Urban

approach is implemented using the Autograd package in Mobility (SUMO) tool [67] that emulates mobile vehicles

the PyTorch module [64].

in a city like environment. SUMO gives us the parameters

To implement SplitPlace in the COSCO framework, we like ping time and network bandwidth to simulate in our

extend the Framework class to allow constraints for testbed using NetLimiter. The moving averages and

sequential execution of layer-splits. The function getPla- periodic Ô¨Åne-tuning allow our approach to be robust

cementPossible() was modiÔ¨Åed to also check for con- towards any kind of dynamism in the edge environment,

tainers of layer-split partitioning scheme to be scheduled including the one arising from mobility of worker nodes.

sequentially. Moreover, we implemented data transferring Our Azure environment is such that all devices are in the

pipeline for broadcasting inputs in semantic-split decision same LAN with 10 MBps network interface cards to avoid

and forwarding the outputs in layer-split decision. Finally, network bottlenecks while transferring inputs, outputs and

the inference outputs were synchronized and brought to the intermediate results across neural network splits. Even if

broker to calculate the performance accuracy and measure the nodes are in the same LAN with high bandwidth con-

the workÔ¨Çow response time. For synchronization of outputs nections, the SUMO model would emulate the affects of

and execution of network splits, we use the HTTP NotiÔ¨Åca- mobility as is common in prior work on mobile edge com-

tion API.

puting [65], [68]. Further, we use the cPickle4 Python

6 PERFORMANCE EVALUATION
To test the efÔ¨Åcacy of the SplitPlace approach and compare it against the baseline methods, we perform experiments on a heterogeneous edge computing testbed. To do this we emulate a setting with mobile edge devices mounted on self-driving cars, that execute various image-recognition tasks.

module to save the intermediate results using bzip2 compression and rsync5 Ô¨Åle-transfer utility to minimize the communication latency. For containers corresponding to a layer-split workload that are deployed in different nodes, the intermediate results are forwarded using the scp utility to the next container in the neural network pipeline. Similarly, for semantic splitting, the cPickle outputs are collected using rsync and concatenated using the torch. cat function.

6.1 Experiment Setup

We use the Microsoft Azure pricing calculator to obtain the cost of execution per hour (in US Dollars).6 The power

As in prior work [49], [50], [65], we use a ¬º b ¬º 0:5 in (10) consumption models are taken from the Standard Perfor-

for our experiments (we consider other value pairs in mance Evaluation Corporation (SPEC) benchmarks reposiAppendix A.2, available in the online supplemental mate- tory.7 The Million-Instruction-per-Second (MIPS) of all VMs rial). Also, we use the exploration factor c ¬º 0:5 for the are computed using the perf-stat8 tool on the SPEC

UCB exploration and the exponential moving average

parameter f ¬º 0:9, chosen using grid-search using the

3. Azure Managed Disks https://docs.microsoft.com/en-

cumulative reward as the metric to maximize. We create a us/azure/virtual-machines/disks-types#premium-ssd.

testbed of 50 resource-constrained VMs located in the

4. cPickle module https://docs.python.org/2/library/

same geographical location of London, United Kingdom

pickle.html#module-cPickle. 5. rsync tool https://linux.die.net/man/1/rsync.

using Microsoft Azure. The worker resources are shown in

6. Microsoft Azure pricing calculator for South UK https://

Table 3. All machines use Intel i3 2.4 GHz processor cores azure.microsoft.com/en-gb/pricing/calculator/.

with processing capacity of no more than a Raspberry Pi 4B device. To keep storage costs consistent, we keep Azure

7. SPEC benchmark repository https://www.spec.org/cloud_ iaas2018/results/.
8. perf-stat tool https://man7.org/linux/man-pages/

P15 Managed disk with 125 MB/s disk throughput and man1/perf-stat.1.html.

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5549

Fig. 6. MAB training curves.

benchmarks. We run all experiments for 100 scheduling material. The split fragments for MNIST, FashionMNIST and

intervals, i.e., G ¬º 100, with each interval being 300 seconds CIFAR100 lead to container images of sizes 8-14 MB, 34-56

long, giving a total experiment time of 8 hours 20 minutes. MB and 47-76 MB, respectively. To calculate the inference

We average over Ô¨Åve runs and use diverse workload types accuracy to feed in the MAB models and perform UCB explo-

to ensure statistical signiÔ¨Åcance in our experiments. We con- ration, we also share the ground-truth labels of all datasets

sider variations of the experimental setup in Appendix A.3, with all worker nodes at the time of sharing the neural models

available in the online supplemental material.

as Docker container images. We also compare edge and cloud

6.2 Workloads

setups in Appendix A.5, available in the online supplemental material, to establish the need for edge devices for latency crit-

Motivated from prior work [32], we use three families of pop- ical workloads.

ular DNNs as the benchmarking models: ResNet50-V2 [69], MobileNetV2 [70] and InceptionV3 [71]. Each family has 6.3 MAB Training

many variants of the model, each having a different number To train our MAB models, we execute the workloads on the

of layers in the neural model. For instance, the ResNet model test setup for 200 intervals and use feedback-based -greedy

has 34 and 50 layers. We use three image-classiÔ¨Åcation data exploration to update the layer-split decision response time

sets: MNIST, FashionMNIST and CIFAR100 [33], [34], [35]. estimates, Q-estimates and decision counts. Fig. 6 shows the

MNIST is a hand-written digit recognition dataset with 28 √Ç training curves for the two models.

28 gray-scale images to 10-dimensional output. Fashion- Fig. 6a shows how the response time estimates for the

MNIST has 28 √Ç 28 RGB images with 10 dimensional output. layer-split decision are learned starting from zero using

CIFAR100 has 32 √Ç 32 RGB images with 100-dimensional moving averages. Fig. 6d shows how the reward-threshold r

output. Thus the application set A becomes {MNIST, Fashion- and decay parameter  change with time. We use the decay

MNIST, CIFAR100}. These models have been taken directly and increment multipliers as 0.9 and 1.1 (k ¬º 0:1 in (7)) for 

from the AIoTBench workloads [72]. This is a popular suite and r respectively, as done in [56]. Figs. 6b and 6c show the

of AI benchmark applications for IoT and Edge computing decision counts for high and low SLA settings for both deci-

solutions. The three speciÔ¨Åc datasets used in our experiments sions. Figs. 6e and 6f show the Q-estimates for high and low

are motivated from the vertical use case of self-driving cars, SLA settings. The dichotomy between the two settings is

which requires DNN-based applications to continuously rec- reÔ¨Çected here. When the slai of the input task i is less than ognize images with low latency requirements. Herein, an the estimate Rai (low setting) there is a clear distinction

image recognition software is deployed that reads speed between the rewards of the two decisions as layer-split is

signs (digit recognition, MNIST), recognizes humans likely to lead to SLA violation and hence lower rewards.

(through apparel and pose [73], FashionMNIST), identiÔ¨Åes However, when slai is greater than the estimate Rai (high set-

other objects like cars and barriers (object detection, ting), both decisions give relatively high rewards with layer-

CIFAR100). We use the implementation of neural network split decision slightly surpassing the semantic-split due to

splitting from prior work [16], [32].

higher average accuracy as discussed in Section 2.

We use the inference deadline from the work [32] as our The feedback-based -greedy training allows us to obtain

SLA. To create the input tasks, we use batch sizes sampled close estimates of the average response times of the layer-

uniformly from 16; 000 √Ä 64; 000. At the beginning of each split executions for each application type and average

scheduling interval, we create Poisson√∞√û tasks with  ¬º 6 rewards for both decisions in high and low SLA settings.

tasks for our setup, sampled uniformly from one of the three Thus, in our experiments, we initialize the expected reward

applications [50]. We consider other  values and single work- (Q) and layer-split response time (R) estimates by the values

load type (from MNIST, FashionMNIST and CIFAR100) in we get from this training approach. At test time, we dynam-

Appendices A.1 and A.4, available in the online supplemental ically update these estimates using (2) and (5).
Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5550

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

6.4 Performance Metrics

we replace one or both of the MAB or DASO components

We use the following evaluation metrics in our experiments with simpler versions as described below.

as motivated from prior works [2], [49], [50]. We also use AEC and ART as discussed in Section 4.

 Semantic+GOBI (S+G): Semantic-split decision only with vanilla GOBI placement module.

1) Average Accuracy is deÔ¨Åned for an execution trace as

 Layer+GOBI (L+G): Layer-split decision only with

the average accuracy of all tasks run in an experi-

vanilla GOBI placement module.

ment, i.e.,

 Random+DASO (R+D): Random split decision with

PP Accuracy ¬º Pt i2Et pi :
t jEtj

DASO placement module.

(13)

 MAB+GOBI (M+G): MAB based split decider with

vanilla GOBI placement module.

The Ô¨Ånal SplitPlace approach is represented as MAB

2) Fraction of SLA violation is deÔ¨Åned for an execution +DASO or M+D in shorthand notation in the graphs. These

trace as the fraction of all tasks run in an experiment ablated baselines help us determine the relative improve-

for which the response time is higher than the SLA ments in performance by the two components of MAB and

deadline, i.e.,

DASO separately.

PP

SLA Violations ¬º

t

t2PEt 1√∞slai t jEtj

!

ri√û :

(14) 6.6 Results and Ablation Analysis We now provide comparative results showing the perfor-

3)

Average Reward is deÔ¨Åned for an execution trace as follows:

mance of the proposed SplitPlace approach against the baseline models and argue the importance of the MAB and decision-aware placement using ablation analysis. We train

PP Reward ¬º t t2Et 1P√∞slai ! ri√û √æ pi :
2 √Å t jEtj

the GOBI and DASO models using the execution trace data-
(15) set used to train the MAB models. The learning rate (h) was set to 10√Ä3 from [50].

4)

Execution Cost is deÔ¨Åned for an execution trace as the
total cost incurred during the experiment, i.e., XZ

Fig. 7 shows the average reward and related performance metrics, i.e., accuracy, response time and SLA violation rate. As expected, the L+G policy gives the highest accuracy of 93:17% as all decisions are layer-wise only with a higher infer-

Cost ¬º

Ch√∞x√ûdx:

h2H x

(16) ence performance than semantic-split execution. The S+G policy gives the least accuracy of 89:04%. However, due to layer-

where Ch√∞x√û is the cost function for worker h with splits only the L+G policy also has the highest average

time.

response time, subsequently giving the highest SLA violation

5) Average Wait Time is the average time a task had to rate. On the other hand, S+G policy has the least average

wait in the wait queue till it could be allocated to a response time. However, due to the intelligent decision mak-

worker for execution.

ing in SplitPlace, it is able to get the highest total reward of

6) Average Execution Time is the response time minus 0.9418. Similar trends are also seen when comparing across

the wait time, averaged for all tasks run in an models for each application. The accuracy is the highest for

experiment.

the MNIST dataset and lowest for CIFAR100. Average

7) Fairness is deÔ¨Åned as the Jain‚Äôs fairness index for exe- response time is highest for the CIFAR100 and lowest for

cution on tasks over the edge workers [50].

MNIST in general. Among the baselines, the Gillis approach

has the lowest SLA violation rate of 22% and SplitPlace

6.5 Baselines and Ablated Models
We compare the performance of the SplitPlace approach against the state-of-the-art baselines Gillis and BottleNet++ Model Compression (denoted as MC in our graphs) [32], [37], [38]. Gillis refers to the reinforcement learning method proposed in [32] that leverages both layer-splitting and compression models to achieve optimal response time and inference accuracy. Note that contrary to the original Gillis‚Äô work, our implementation does not leverage serverless functions. MC is a model-compression approach motivated from BottleNet++ that we implement using the PyTorch Prune library.9 Further details in Section 2. We do not include results for other methods discussed in Section 2 as MC and Gillis give better results empirically for all comparison metrics. We also compare SplitPlace with ablated models, where

improves upon this by giving 14% lower SLA violations (only 8%). Gillis has higher accuracy between the baselines of 91:9%, with SplitPlace giving an average improvement of 0:82%. Overall, the total reward of SplitPlace is higher than the baselines by at least 10:1%, giving the reward of 94:18%.
Fig. 8 shows the performance of all models for other evaluation metrics like energy, execution time and fairness. Compared to the baselines, SplitPlace can reduce energy consumption by up to 4:41% √Ä 5:03% giving an average energy consumption of 1.0867 MW-hr. However, the SplitPlace approach has higher scheduling time and lower fairness index (Table 4). The Gillis baseline has the highest fairness index of 0.89, however this index for SplitPlace is 0.73. SplitPlace has a higher overhead of 11:8% compared to the Gillis baseline in terms of scheduling time. Fig. 8i compares the average execution cost (in USD) for all models. As

9. PyTorch Prune. https://pytorch.org/docs/stable/ generated/torch.nn.utils.prune.ln_structured.html.

SplitPlace is able to run the maximum number of containers in the 100 intervals, it has the least cost of 3.07 USD/con-

Accessed 10 October 2021.

tainer. The main advantage of SplitPlace is the intelligent

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5551

Fig. 7. Comparison of SplitPlace against baselines and ablated models on physical setup with 50 edge workers.

Fig. 8. Additional results comparing SplitPlace with baselines and ablated models. Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5552

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

TABLE 4 Comparison of SplitPlace With Baseline and Ablated Models

Model
Model Compression Gillis
Semantic+GOBI Layer+GOBI Random+DASO MAB+GOBI
MAB+DASO

Energy
1.1368 1.1442
1.1112 1.1517 1.1297 1.1290
1.0867

Scheduling Time
8.84√Ü0.02 8.22√Ü0.01
8.68√Ü0.03 8.72√Ü0.01 8.86√Ü0.01 9.12√Ü0.02
9.32√Ü0.02

Fairness

Wait Time

Baselines

0.65√Ü0.01 1.15√Ü0.09 0.89√Ü0.03 1.40√Ü0.18

Ablation

0.68√Ü0.04 0.88√Ü0.03 0.62√Ü0.05 0.78√Ü0.08

1.08√Ü0.00 1.52√Ü0.21 1.00√Ü0.07 1.13√Ü0.13

SplitPlace Model

0.73√Ü0.01 1.09√Ü0.08

Response Time
6.85√Ü1.20 8.39√Ü0.95
3.70√Ü0.57 9.92√Ü0.91 5.55√Ü1.05 5.64√Ü1.02
4.50√Ü1.00

SLA Violations
0.26√Ü0.02 0.22√Ü0.03
0.14√Ü0.04 0.62√Ü0.07 0.29√Ü0.09 0.10√Ü0.03
0.08√Ü0.02

Accuracy
89.93 91.90
89.04 93.17 90.71 91.45
92.72

Average Reward
83.98 84.17
83.91 64.87 81.62 90.18
94.18

The best achieved value for each metric is shown in bold. Units: Energy (MW-hr), Scheduling Time (seconds), Fairness (Jain‚Äôs Index), Wait Time (Intervals), Response Time (Intervals).

splitting decisions facilitate overcoming the memory bottlenecks in edge environments, giving up to 32% lower RAM utilization compared to Gillis and Model Compression.
In terms of the initial communication time of the Docker container images, the SplitPlace method takes 30 seconds at the start of an execution. Gillis and MC have such communication times of 20 and 18 seconds, respectively. This demonstrates that SplitPlace has a low one-time overhead (up to 12 seconds) compared to the baselines when compared to the gains in response time (up to 46%) that linearly scales as the number of workloads increase.
A summary of comparisons with values of main performance metrics for all models is given in Table 4. The best values achieved for each metric are highlighted in bold.
7 CONCLUSION

may be hard for deep learning models like attention based neural networks or transformer models [75]. Finally, the model only considers splits and their placement as containers, more Ô¨Åne-grained methods involving Neural Architecture Search and cost efÔ¨Åcient deployment methods may be explored like serverless frameworks [76]. Other considerations such as privacy concerns and non-stationary number of active edge nodes with extreme levels of heterogeneity such that the placement decision has a signiÔ¨Åcant impact on response time is also part of future work.
SOFTWARE AVAILABILITY
The code is available at https://github.com/imperialqore/SplitPlace. The Docker images used in the experiments are available at https://hub.docker.com/u/ shreshthtuli.

In this work, we present SplitPlace, a novel framework for
efÔ¨Åciently managing demanding neural network based ACKNOWLEDGMENTS

applications. SplitPlace exploits the trade-off between layer and semantic split models where the former gives higher accuracy, but the latter gives much lower response times. This allows SplitPlace to not only manage tasks to maintain high inference accuracy on average, but also reduce SLA violation rate. The proposed model uses a Multi-ArmedBandits based policy to decide which split strategy to use

Shreshth Tuli was grateful to the Imperial College London for funding his Ph.D. through the President‚Äôs Ph.D. Scholarship scheme. We thank Feng Yan for helpful discussions. A preliminary version of this work was presented at the Student Research Competition in ACM SIGMETRICS Conference 2021 [1].

according to the SLA deadline of the incoming task. Moreover, it uses a decision-aware learning model to take appropriate placement decisions for those neural fragments on mobile edge workers. Further, both MAB and learning models are dynamically tuned to adapt to volatile scenarios. All

REFERENCES
[1] S. Tuli, ‚ÄúSplitPlace: Intelligent placement of split neural nets in mobile edge environments,‚Äù SIGMETRICS Perform. Eval. Rev., vol. 49, pp. 63‚Äì65, 2021.
[2] S. S. Gill et al., ‚ÄúTransformative effects of IoT, blockchain and arti-

these contributions allow SplitPlace to out-perform the baseline models in terms of average response time, SLA violation rate, inference accuracy and total reward by up to 46:3%, 69:2%, 3:1% and 12:1% respectively in a heterogeneous edge environment with real-world workloads.
We propose the following future directions for this work. An extension of the current work may be developed that

Ô¨Åcial intelligence on cloud computing: Evolution, vision, trends and open challenges,‚Äù Internet Things, vol. 8, pp. 100‚Äì118, 2019. [3] H. Zhu et al., ‚ÄúBenchmarking and analyzing deep neural network training,‚Äù in Proc. IEEE Int. Symp. Workload Characterization, 2018, pp. 88‚Äì100. [4] E. Li, L. Zeng, Z. Zhou, and X. Chen, ‚ÄúEdge AI: On-demand accelerating deep neural network inference via edge computing,‚Äù IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 447‚Äì457, Jan. 2020. [5] A. Khanna, A. Sah, and T. Choudhury, ‚ÄúIntelligent mobile edge

dynamically updates the splitting conÔ¨Åguration to adapt to

computing: A deep learning based approach,‚Äù in Proc. Int. Conf.

more heterogeneous and non-stationary edge environments [74]. Moreover, the current model assumes that all

Adv. Comput. Data Sci., 2020, pp. 107‚Äì116. [6] F. A. Kraemer, A. E. Braten, N. Tamkittikhun, and D. Palma, ‚ÄúFog
computing in healthcare‚ÄìA review and discussion,‚Äù IEEE Access,

neural models are divisible into independent layers. This

vol. 5, pp. 9206‚Äì9222, 2017.

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

TULI ET AL.: SPLITPLACE: AI AUGMENTED SPLITTING AND PLACEMENT OF LARGE-SCALE NEURAL NETWORKS IN MOBILE EDGE...

5553

[7] L. Zhang and L. Zhang, ‚ÄúDeep learning-based classiÔ¨Åcation and reconstruction of residential scenes from large-scale point clouds,‚Äù IEEE Trans. Geosci. Remote Sens., vol. 56, no. 4, pp. 1887‚Äì1897, Apr. 2018.
[8] M. Roopaei, P. Rad, and M. Jamshidi, ‚ÄúDeep learning control for complex and large scale cloud systems,‚Äù Intell. Autom. Soft Comput., vol. 23, no. 3, pp. 389‚Äì391, 2017.
[9] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, M. T. Kandemir, and C. R. Das, ‚ÄúImplications of public cloud resource heterogeneity for inference serving,‚Äù in Proc. 6th Int. Workshop Serverless Comput., 2020, pp. 7‚Äì12.
[10] S. Laskaridis, S. I. Venieris, M. Almeida, I. Leontiadis, and N. D. Lane, ‚ÄúSPINN: Synergistic progressive inference of neural networks over device and cloud,‚Äù in Proc. 26th Annu. Int. Conf. Mobile Comput. Netw., 2020, pp. 1‚Äì15.
[11] Q. Liang, P. Shenoy, and D. Irwin, ‚ÄúAI on the edge: Characterizing AI-based IoT applications using specialized edge architectures,‚Äù in Proc. IEEE Int. Symp. Workload Characterization, 2020, pp. 145‚Äì156.
[12] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, ‚ÄúMobile edge computing: A survey,‚Äù IEEE Internet Things J., vol. 5, no. 1, pp. 450‚Äì465, Feb. 2018.
[13] Y. Mao, J. Zhang, and K. B. Letaief, ‚ÄúDynamic computation ofÔ¨Çoading for mobile-edge computing with energy harvesting devices,‚Äù IEEE J. Sel. Areas Commun., vol. 34, no. 12, pp. 3590‚Äì3605, Dec. 2016.
[14] J. Shao and J. Zhang, ‚ÄúCommunication-computation trade-off in resource-constrained edge inference,‚Äù IEEE Commun. Mag., vol. 58, no. 12, pp. 20‚Äì26, Jan. 2021.
[15] J. Liu, M. L. Curry, C. Maltzahn, and P. Kufeldt, ‚ÄúScale-out edge storage systems with embedded storage nodes to get better availability and cost-efÔ¨Åciency at the same time,‚Äù in Proc. 3rd USENIX Workshop Hot Topics Edge Comput., 2020, pp. 1‚Äì7.
[16] J. Kim, Y. Park, G. Kim, and S. J. Hwang, ‚ÄúSplitNet: Learning to semantically split deep networks for parameter reduction and model parallelization,‚Äù in Proc. 34th Int. Conf. Mach. Learn., 2017, pp. 1866‚Äì1874.
[17] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, ‚ÄúCommunicationefÔ¨Åcient edge AI: Algorithms and systems,‚Äù IEEE Commun. Surveys Tuts., vol. 22, no. 4, pp. 2167‚Äì2191, Oct.‚ÄìDec. 2020.
[18] W. Y. B. Lim et al., ‚ÄúFederated learning in mobile edge networks: A comprehensive survey,‚Äù IEEE Commun. Surveys Tuts., vol. 22, no. 3, pp. 2031‚Äì2063, Jul.‚ÄìSep. 2020.
[19] Y. Siriwardhana, P. Porambage, M. Liyanage, and M. Ylianttila, ‚ÄúA survey on mobile augmented reality with 5G mobile edge computing: Architectures, applications, and technical aspects,‚Äù IEEE Commun. Surveys Tuts., vol. 23, no. 2, pp. 1160‚Äì1192, Apr.‚ÄìJun. 2021.
[20] S. Tuli et al., ‚ÄúHealthFog: An ensemble deep learning based smart healthcare system for automatic diagnosis of heart diseases in integrated IoT and fog computing environments,‚Äù Future Gener. Comput. Syst., vol. 104, pp. 187‚Äì200, 2020.
[21] W. Xiao et al., ‚ÄúGandiva: Introspective cluster scheduling for deep learning,‚Äù in Proc. 13th USENIX Symp. Oper. Syst. Des. Implementation, 2018, pp. 595‚Äì610.
[22] A. Gujarati et al., ‚ÄúServing DNNs like clockwork: Performance predictability from the bottom up,‚Äù in Proc. 14th USENIX Symp. Oper. Syst. Des. Implementation, 2020, pp. 443‚Äì462.
[23] J. Chen and X. Ran, ‚ÄúDeep learning with edge computing: A review,‚Äù Proc. IEEE, vol. 107, no. 8, pp. 1655‚Äì1674, Aug. 2019.
[24] A. Capotondi, M. Rusci, M. Fariselli, and L. Benini, ‚ÄúCMix-NN: Mixed low-precision CNN library for memory-constrained edge devices,‚Äù IEEE Trans. Circuits Syst., II, Exp. Briefs, vol. 67, no. 5, pp. 871‚Äì875, May 2020.
[25] J. Huang, C. Samplawski, D. Ganesan, B. Marlin, and H. Kwon, ‚ÄúCLIO: Enabling automatic compilation of deep learning pipelines across IoT and cloud,‚Äù in Proc. 26th Annu. Int. Conf. Mobile Comput. Netw., 2020, pp. 1‚Äì12.
[26] Q. Le, L. Miralles-Pechuan, S. Kulkarni, J. Su, and O. Boydell, ‚ÄúAn overview of deep learning in industry,‚Äù in Data Anal. AI. Boca Raton, FL, USA: CRC, 2020, pp. 65‚Äì98.
[27] Y. Matsubara, S. Baidya, D. Callegaro, M. Levorato, and S. Singh, ‚ÄúDistilled split deep neural networks for edge-assisted real-time systems,‚Äù in Proc. Workshop Hot Topics Video Anal. Intell. Edges, 2019, pp. 21‚Äì26.
[28] Y. A. Ushakov, P. N. Polezhaev, A. E. Shukhman, M. V. Ushakova, and M. V. Nadezhda, ‚ÄúSplit neural networks for mobile devices,‚Äù in Proc. 26th Telecommun. Forum, 2018, pp. 420‚Äì425.

[29] V. S. Gordon and J. Crouson, ‚ÄúSelf-splitting modular neural network-domain partitioning at boundaries of trained regions,‚Äù in Proc. IEEE Int. Joint Conf. Neural Netw., 2008, pp. 1085‚Äì1091.
[30] E. Ahmed and M. H. Rehmani, ‚ÄúMobile edge computing: Opportunities, solutions, and challenges,‚Äù Future Gener. Comput. Syst., vol. 70, pp. 59‚Äì63, 2017.
[31] D. Bouneffouf, I. Rish, and C. Aggarwal, ‚ÄúSurvey on applications of multi-armed and contextual bandits,‚Äù in Proc. IEEE Congr. Evol. Comput., 2020, pp. 1‚Äì8.
[32] M. Yu, Z. Jiang, H. C. Ng, W. Wang, R. Chen, and B. Li, ‚ÄúGillis: Serving large neural networks in serverless functions with automatic model partitioning,‚Äù in Proc. IEEE 41st Int. Conf. Distrib. Comput. Syst., 2021, pp. 138‚Äì148.
[33] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ‚ÄúGradient-based learning applied to document recognition,‚Äù Proc. IEEE, vol. 86, no. 11, pp. 2278‚Äì2324, Nov. 1998.
[34] H. Xiao, K. Rasul, and R. Vollgraf, ‚ÄúFashion-MNIST: A novel image dataset for benchmarking machine learning algorithms,‚Äù 2017, arXiv: 1708.07747.
[35] A. Krizhevsky et al., ‚ÄúLearning multiple layers of features from tiny images,‚Äù M.S. thesis, Univ. Toronto, Toronto, ON, Canada, 2009.
[36] A. Goli, O. Hajihassani, H. Khazaei, O. Ardakanian, M. Rashidi, and T. Dauphinee, ‚ÄúMigrating from monolithic to serverless: A FinTech case study,‚Äù in Proc. Companion ACM/SPEC Int. Conf. Perform. Eng., 2020, pp. 20‚Äì25.
[37] A. E. Eshratifar, A. Esmaili, and M. Pedram, ‚ÄúBottleNet: A deep learning architecture for intelligent mobile cloud computing services,‚Äù in Proc. IEEE/ACM Int. Symp. Low Power Electron. Des., 2019, pp. 1‚Äì6.
[38] J. Shao and J. Zhang, ‚ÄúBottleNet++: An end-to-end approach for feature compression in device-edge co-inference systems,‚Äù in Proc. IEEE Int. Conf. Commun. Workshops, 2020, pp. 1‚Äì6.
[39] S. Gao, F. Huang, J. Pei, and H. Huang, ‚ÄúDiscrete model compression with resource constraint for deep neural networks,‚Äù in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 1896‚Äì1905.
[40] S. Teerapittayanon, B. McDanel, and H. T. Kung, ‚ÄúDistributed deep neural networks over the cloud, the edge and end devices,‚Äù in Proc. IEEE 37th Int. Conf. Distrib. Comput. Syst., 2017, pp. 328‚Äì339.
[41] Y. Kang et al., ‚ÄúNeurosurgeon: Collaborative intelligence between the cloud and mobile edge,‚Äù ACM SIGARCH Comput. Archit. News, vol. 45, no. 1, pp. 615‚Äì629, 2017.
[42] S. Zhang, S. Zhang, Z. Qian, J. Wu, Y. Jin, and S. Lu, ‚ÄúDeepSlicing: Collaborative and adaptive CNN inference with low latency,‚Äù IEEE Trans. Parallel Distrib. Syst., vol. 32, no. 9, pp. 2175‚Äì2187, Sep. 2021.
[43] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, ‚ÄúModel compression and hardware acceleration for neural networks: A comprehensive survey,‚Äù Proc. IEEE, vol. 108, no. 4, pp. 485‚Äì532, Apr. 2020.
[44] P. S. Chandakkar, Y. Li, P. L. K. Ding, and B. Li, ‚ÄúStrategies for retraining a pruned neural network in an edge computing paradigm,‚Äù in Proc. IEEE Int. Conf. Edge Comput., 2017, pp. 244‚Äì247.
[45] L. Wang et al., ‚ÄúContext-aware deep model compression for edge cloud computing,‚Äù in Proc. IEEE 40th Int. Conf. Distrib. Comput. Syst., 2020, pp. 787‚Äì797.
[46] F. Yu, L. Cui, P. Wang, C. Han, R. Huang, and X. Huang, ‚ÄúEasiEdge: A novel global deep neural networks pruning method for efÔ¨Åcient edge computing,‚Äù IEEE Internet Things J., vol. 8, no. 3, pp. 1259‚Äì1271, Feb. 2021.
[47] A. Kaplunovich and Y. Yesha, ‚ÄúAutomatic tuning of hyperparameters for neural networks in serverless cloud,‚Äù in Proc. IEEE Int. Conf. Big Data, 2020, pp. 2751‚Äì2756.
[48] S. Tuli, R. Mahmud, S. Tuli, and R. Buyya, ‚ÄúFogBus: A blockchainbased lightweight framework for edge and fog computing,‚Äù J. Syst. Softw., vol. 154, pp. 22‚Äì36, 2019.
[49] D. Basu, X. Wang, Y. Hong, H. Chen, and S. Bressan, ‚ÄúLearn-asyou-go with Megh: EfÔ¨Åcient live migration of virtual machines,‚Äù IEEE Trans. Parallel Distrib. Syst., vol. 30, no. 8, pp. 1786‚Äì1801, Aug. 2019.
[50] S. Tuli, S. R. Poojara, S. N. Srirama, G. Casale, and N. R. Jennings, ‚ÄúCOSCO: Container orchestration using co-simulation and gradient based optimization for fog computing environments,‚Äù IEEE Trans. Parallel Distrib. Syst., vol. 33, no. 1, pp. 101‚Äì116, Jan. 2022.
[51] S.-H. Park, O. Simeone, and S. Shamai, ‚ÄúJoint cloud and edge processing for latency minimization in fog radio access networks,‚Äù in Proc. IEEE 17th Int. Workshop Signal Process. Adv. Wireless Commun., 2016, pp. 1‚Äì5.

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

5554

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 9, SEPTEMBER 2023

[52] R. A. C. da Silva and N. L. S. da Fonseca, ‚ÄúResource allocation mechanism for a fog-cloud infrastructure,‚Äù in Proc. IEEE Int. Conf. Commun., 2018, pp. 1‚Äì6.
[53] R. Kohavi and R. Longbotham, ‚ÄúOnline controlled experiments and A/B testing,‚Äù Encyclopedia Mach. Learn. Data Mining, vol. 7, no. 8, pp. 922‚Äì929, 2017.
[54] M. Wiering and M. Van Otterlo, ‚ÄúReinforcement learning,‚Äù Adapt. Learn. Optim., vol. 12, no. 3, 2012, Art. no. 729.
[55] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, ‚ÄúOptimized computation ofÔ¨Çoading performance in virtual edge computing systems via deep reinforcement learning,‚Äù IEEE Internet Things J., vol. 6, no. 3, pp. 4005‚Äì4018, Jun. 2019.
[56] A. Maroti, ‚ÄúRBED: Reward based epsilon decay,‚Äù 2019, arXiv: 1910.13701.
[57] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 2018.
[58] Y. Zhang, P. Cai, C. Pan, and S. Zhang, ‚ÄúMulti-agent deep reinforcement learning-based cooperative spectrum sensing with upper conÔ¨Ådence bound exploration,‚Äù IEEE Access, vol. 7, pp. 118 898‚Äì118 906, 2019.
[59] S. Gupta, G. Joshi, and O. Yagan, ‚ÄúCorrelated multi-armed bandits with a latent random source,‚Äù in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., 2020, pp. 3572‚Äì3576.
[60] A. Ahmed and G. Pierre, ‚ÄúDocker container deployment in fog computing infrastructures,‚Äù in Proc. IEEE Int. Conf. Edge Comput., 2018, pp. 1‚Äì8.
[61] I. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regularization,‚Äù in Proc. Int. Conf. Learn. Representations, 2018, pp. 1‚Äì19.
[62] M. Grinberg, Flask Web Development: Developing Web Applications With Python. Sebastopol, CA, USA: O‚ÄôReilly Media, Inc., 2018.
[63] R. S. Venkatesh, T. Smejkal, D. S. Milojicic, and A. Gavrilovska, ‚ÄúFast in-memory CRIU for docker containers,‚Äù in Proc. Int. Symp. Memory Syst., 2019, pp. 53‚Äì65.
[64] A. Paszke et al., ‚ÄúAutomatic differentiation in pytorch,‚Äù in Proc. Int. Conf. Neural Inf. Process. Syst. 2017 Workshop Autodiff, 2017.
[65] S. Tuli, S. Ilager, K. Ramamohanarao, and R. Buyya, ‚ÄúDynamic scheduling for stochastic edge-cloud computing environments using A3C learning and residual recurrent neural networks,‚Äù IEEE Trans. Mobile Comput., vol. 21, no. 3, pp. 940‚Äì954, Mar. 2022.
[66] K. Gilly, S. Alcaraz, N. Aknin, S. Filiposka, and A. Mishev, ‚ÄúModelling edge computing in urban mobility simulation scenarios,‚Äù in Proc. IFIP Netw. Conf., 2020, pp. 539‚Äì543.
[67] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, ‚ÄúRecent development and applications of SUMO-simulation of urban mobility,‚Äù Int. J. Adv. Syst. Meas., vol. 5, no. 3/4, 2012, pp. 128‚Äì138.
[68] C. Wang, Y. Xu, J. Zhang, and B. Ran, ‚ÄúIntegrated trafÔ¨Åc control for freeway recurrent bottleneck based on deep reinforcement learning,‚Äù IEEE Trans. Intell. Transp. Syst., early access, pp. 1‚Äì14, Jan. 20, 2022, doi: 10.1109/TITS.2022.3141730.
[69] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770‚Äì778.
[70] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ‚ÄúMobileNetV2: Inverted residuals and linear bottlenecks,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 4510‚Äì4520.
[71] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, ‚ÄúInception-v4, inception-ResNet and the impact of residual connections on learning,‚Äù in Proc. AAAI Conf. Artif. Intell., 2017, pp. 4278‚Äì4284.
[72] C. Luo et al., ‚ÄúAIoT bench: Towards comprehensive benchmarking mobile and embedded device intelligence,‚Äù in Proc. Int. Symp. Benchmarking Measuring Optim., 2018, pp. 31‚Äì35.
[73] P. R. Tupe, P. M. Vibhute, and M. A. Sayyad, ‚ÄúAn architecture combining convolutional neural network (CNN) with batch normalization for apparel image classiÔ¨Åcation,‚Äù in Proc. IEEE Int. Symp. Sustain. Energy, Signal Process. Cyber Secur., 2020, pp. 1‚Äì6.

[74] K. A. Bonawitz et al., ‚ÄúTowards federated learning at scale: System design,‚Äù in Proc. Mach. Learn. Syst., 2019, pp. 374‚Äì388.
[75] A. Vaswani et al., ‚ÄúAttention is all you need,‚Äù in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5998‚Äì6008.
[76] G. Casale et al., ‚ÄúRADON: Rational decomposition and orchestration for serverless computing,‚Äù SICS Softw.-Intensive Cyber-Phys. Syst., vol. 35, no. 1, pp. 77‚Äì87, 2020.
Shreshth Tuli received the undergraduate degree from the Department of Computer Science and Engineering, Indian Institute of Technology Delhi, India. He is a president‚Äôs PhD scholar with the Department of Computing, Imperial College London, U.K. He has worked as a visiting research fellow with the CLOUDS Laboratory, School of Computing and Information Systems, University of Melbourne, Australia. He is a national level Kishore Vaigyanik Protsahan Yojana (KVPY) scholarship holder from the Government of India for excellence in science and innovation. His research interests include fog computing and deep learning.
Giuliano Casale joined the Department of Computing, Imperial College London, in 2010, where he is currently a reader. Previously, he worked as a research scientist and consultant in the capacity planning industry. He teaches and does research in performance engineering and cloud computing, topics on which he has published more than 100 refereed papers. He has served on the technical program committee of more than 80 conferences and workshops and as co-chair for several conferences in the area of performance and reliability engineering, such as the ACM SIGMETRICS/Performance and IEEE/ IFIP DSN. His research work has received multiple awards, recently the Best Paper Award at ACM SIGMETRICS. He serves on the editorial boards of the IEEE Transactions on Network and Service Management and ACM Transactions on Modeling and Performance Evaluation of Computing Systems and as current chair of ACM SIGMETRICS.
Nicholas R. Jennings is the vice-chancellor and president of Loughborough University. He is an internationally-recognised authority in the areas of AI, autonomous systems, cyber-security, and agent-based computing. He is a member of the UK government‚Äôs AI Council, the governing body of the Engineering and Physical Sciences Research Council, and chair of the Royal Academy of Engineering‚Äôs Policy Committee. Before Loughborough, he was the vice-provost for research and enterprise and professor of artiÔ¨Åcial intelligence with Imperial College London, the UK‚Äôs Ô¨Årst regius professor of computer science (a post bestowed by the monarch to recognise exceptionally high quality research) and the UK Government‚Äôs Ô¨Årst chief scientiÔ¨Åc advisor for national security.
" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.

Authorized licensed use limited to: KAUST. Downloaded on January 14,2024 at 08:22:58 UTC from IEEE Xplore. Restrictions apply.

