Power Consumption and Delay in Wired Parts of Fog Computing Networks
Bartosz Kopras, Filip Idzikowski, Paweł Kryszkiewicz Faculty of Electronics and Telecommunications, Poznan University of Technology, Poland

Abstract—In the last decade Cloud computing has seen a surge of popularity. Clouds, with their scale and high functionality, are used to outsource various infrastructure, platform, and software services. However, relaying solely on distant Cloud Data Centers (DCs) can be inefﬁcient for many applications concerning mobile devices and Internet of Things (IoT) in general. A more decentralized Fog computing paradigm has been proposed to augment Cloud availability and execution.
This work addresses latency and power consumption in Fog computing networks. Models for power consumption and delay are proposed. Performance of Fog computing is estimated using parameters setting based on real-world equipment and trafﬁc. Our results tackle the balance between Fog and Cloud. Applications requiring heavy computations (relative to size of ofﬂoaded data) are best served by Cloud DCs, while it is faster (and more power-efﬁcient) to compute “lighter” requests in the Fog Nodes (FNs). However, where is the trade-off between power consumption and delay in the context of Fog and Cloud? We answer this question modeling multiple architectures and using various network scenarios.
Index Terms—power, energy, latency, fog, cloud, green
I. INTRODUCTION
IoT is rapidly expanding as the number of connected devices continues to grow [1]. This creates new challenges for the Information and Communication Technology (ICT) sector. On one hand, the “Things” often have limited storage, computational power, and battery life related to their small sizes. These limitations hinder their ability to process gathered data. On the other hand, sending all of this data to be processed at a remote Cloud would introduce unprecedented amount of trafﬁc [2] in the backbone of the Internet, which would inﬂuence the total energy consumed by the Internet. Moreover, some applications such as video surveillance [3] or augmented reality [4] require low delays. Tasks performed for these applications are often too complex (and battery-draining) to be processed by Mobile Devices (MDs). Ofﬂoading these tasks to a Cloud DC could introduce unacceptable delays.
Fog computing [5] addresses these problems. The main idea behind Fog computing is to augment the Cloud by providing computational, networking, and storage resources closer to end users. This is achieved by introducing the Fog tier between the Cloud tier and the Things tier [6] as shown in Figs. 1 and 2. Various types of devices are present in the lowest (Things) tier. They include sensors, security cameras as well as “smart” devices commonly associated with IoT (vehicles, home appliances, personal equipment). The Fog tier consists of
The presented work has been funded by the National Centre of Research and Development, Poland within the FAUST project (no. PL-TW/V/3/2018).

Backbone Core Network

Cloud Data Center

Cloud tier

Backhaul

BS FN

BS FN

BS FN

Fronthaul

Fog tier

RRH

RRH

RRH

Wired connection
Wireless conection

Things tier

Smartphone

Tablet

Sensor

Vehicle

Smartphone

Fig. 1: Example architecture of Fog computing network with fronthaul, backhaul, and backbone parts captioned.

elements called FNs located near the end users, “on the edge” of the network. Thanks to this fact, data transmission between “Things” and FNs is quicker and less energy-consuming than alternative Thing-to-Cloud communication. “Things” can also communicate with FNs via Remote Radio Heads (RRHs). FNs can be interconnected and share ofﬂoaded workload. A detailed description of a FN, including its role in a network can be found in Sec. 5.5 of [6]. The Cloud tier contains DCs organized in multiple specialized clusters. Their high computation performance allows for extensive data analysis and storage. DCs are highly virtualized so multiple Virtual Machines (VMs) on a single physical cluster (let alone a single DC) can simultaneously serve multiple different applications.
We focus on the wired parts of the Fog computing network consisting of fronthaul, backhaul, and backbone as depicted in Fig. 1. The concept of fronthaul originates from Cloud Radio Access Networks (C-RANs) [7] and Fog Radio Access Networks (F-RANs). There is little agreement between the researchers on what constitutes a fronthaul. We deﬁne it in this work as wired links between RRHs and FNs. Consequently, backhaul consists of the links connecting FNs to the core network (Fig. 1). While the interconnections within the core network (including connections to the Cloud Data Centers) are regarded as backbone.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

Cloud tier

Fog tier

Cloud Data Center

Computation

Storage Long-term analysis

Ofﬂoaded tasks

Results

Fog Node Computation

Request

Fog Node

Caching Fog Node

Results

Ofﬂoaded tasks
Results Request

Requested content
Requested content

Extracted features Records Fog Node
Computation
Data

Smartphone
Ofﬂoading of computations

Tablet

Content distrubution

Smartphone

Sensor Network

Data processing and aggregation

Fig. 2: Tasks performed by Fog computing network.

Things tier

Three broad types of services provided by Fog network are shown in Fig. 2. They include (1) ofﬂoading, (2) content distribution, and (3) data aggregation. The idea of ofﬂoading data/computation/task from a device with limited capabilities to a more powerful one is not new. Othman et al. [8] show how to increase battery life of a laptop by putting some of its computational load to a nearby Base Station (BS). This idea is reused in the context of Fog computing. An overview of the ofﬂoading schemes is provided in [9]. Fog computing is based on moving resources closer to the edge of a network, i.e., Fog can be regarded as an “extension” of the traditional Content Delivery Network (CDN). An overview of content distribution methods used to save energy in Fog is also provided in [9]. Eventually, extensive data collected by IoT devices can be reduced at Fog nodes. E.g., processing raw Electrocardiogram (ECG) data in a nearby FN reduces the data sent to the Cloud by over 90% [10].
Starting from a survey of articles estimating power consumption and latency, we propose power consumption and latency models of a network using Fog computing. We consider costs related to both communication and computation in and between devices belonging to the Fog and the Cloud tiers of the network. Differently from other works, we take a holistic approach for the wired parts of the network, i.e., we take into account power spent by each device through which the data ﬂows. Furthermore, we provide an extensive comparison of efﬁciency of utilizing Fog/Cloud or a mix of them using realistic network scenarios.
Our work is structured as follows. An overview of related work is provided in Section II. Mathematical models describing Fog computing are developed in Section III. In Section IV the performance of Fog computing is tested under various conditions using models from Section III. Finally, Section V concludes the work.

II. RELATED WORK
In this section models used by researchers for estimation of power consumption and latency in Fog computing networks are discussed.
Table I provides an overview of power consumption and latency models used in related work on Fog computation networks. In all of the surveyed works there is a distinction between power/time spent on computation and on communication. In Table I entries in columns labeled “End devices”, “Fog nodes”, and “Cloud data centers” mean that these devices spend time/power performing computations. The remaining columns contain information regarding communications between devices. We point out that the focus of this work is set on power and latency occurring in the wired part of the network, i.e., in the Fog and Cloud tiers (Fig. 1).
The works [11] and [12] examine this problem from the point of view of a MD. They only consider the power consumed by these devices (on both computation and transmission to FNs). For latency, they consider contributions from the whole network.
On the other hand, in [13], the ofﬂoaded tasks are considered as they enter the Fog tier of the network. Computations in the FNs and Cloud DCs consume both power and time, while the communication between FNs and DCs introduces only delay.
The authors of [14] use an unorthodox network architecture, where all computational and storage resources are located in the lowest tier (each end user has a personal FN – a nano DC). Latency is not considered. However, they develop detailed models for power consumption related to communication (which include idle power of devices and number of nodes data ﬂows through). These ideas are further developed in our work.
Extensive modeling is performed in the works of Sarkar et al. [15], [16]. Power consumption and delay is caused by both communication and computation in multiple tiers of the network. It coincides with the topic of this paper. However, their models are heavily skewed towards showing that processing data in the Fog tier is more efﬁcient than doing it in the Cloud which yields dubious results (e.g., they show that processing 25% of requests in the Fog tier and 75% in the Cloud tier results in over 40% lower power consumption than processing 100% of requests in the Cloud tier).
As shown in Table I, our work has different focus from [11], [12], [14], i.e., we focus on power and latency due to FNs and Cloud DCs as well as communication between them. We contribute with a model of latency and power consumption of a Fog network including its interaction between the Fog and Cloud tiers. Differently from [13], we cover power consumption needed for communications between FNs and Cloud DCs. Differently from [15], [16], our model covers complexity of computation tasks and power spent on data transmission through each single node in the network. The concept of computational complexity is studied regarding its impact on ofﬂoading efﬁciency. Parameter values from credible sources are used to describe devices used in our models.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

TABLE I: Overview of power consumption and latency models. Which parts of network are considered?

Research work (Year) This work
Dinh et al. [11] (2017) Liu et al. [12] (2018) Deng et al. [13] (2016) Jalali et al. [14] (2016) Sarkar and Misra [15] (2016) Sarkar et al. [16] (2018)

End devices
not considered power & latency power & latency not considered not considered not considered not considered

Comm. between end devices & FNs power & latency* (incl. downlink) power & latency (incl. downlink) power & latency
negligible
power
power & latency (incl. downlink) power & latency*

Fog nodes (FNs) power & latency latency latency power & latency power power & latency power & latency

Comm. between FNs negligible
not considered not considered negligible
not considered negligible
negligible

Comm. between FNs & Cloud DCs power & latency (incl. downlink) latency (incl. downlink) latency
latency
not considered
power & latency (incl. downlink) power & latency

Cloud data centers (DCs) power & latency
latency
latency
power & latency
not considered
power & latency
power & latency

* – gateways between end devices and FNs are considered rather than end devices themselves

III. NETWORK MODEL
This section deﬁnes models used for estimating delay and power consumption related to the ofﬂoading of computations in Fog computing network. Computational requests are modeled in Section III-B. The power consumption of the network is modeled in Section III-C, and delay is modeled in Section III-D.

A. Network Description
In the bottom tier of the network there are end devices (e.g., smartphones, sensors) which may require ofﬂoading computational tasks. These tasks can be processed in either the Fog tier (consisting of a set F of FNs) or the Cloud tier (set C of DCs). Data is sent to the FNs through the RRHs as shown in Fig. 1 and Fig. 3. Then the results are transmitted back to the MD.
The FNs are capable of sharing the computational load between themselves. To simplify the model and following calculations, they do so with inducing neither additional delays nor power consumption. These costs have also been left out by other researchers [13], [15], [16].

B. Ofﬂoaded Tasks – Computational Requests

Let there be a total of N requests ofﬂoaded during analyzed

time period T . RF and RC are the sets of requests ofﬂoaded

to Fog tier and Cloud tier respectively. Let LRi be the size

(in bits) of the i-th request Ri. Total amount of ofﬂoaded data

L, amount of data ofﬂoaded to the Fog tier X and Cloud tier

Y are therefore:

X

X

L = X + Y, X =

LRi , Y =

LRi (1)

Ri 2RF

Ri 2RC

Concepts discussed in this section are illustrated in Fig. 3. It is important to note that, in this work, the requests are examined as they enter the Fog tier of the network. The cost of transmitting a single request from MDs to RRH/FN is not considered.
The volume of ofﬂoaded data is not the only thing that matters when it comes to processing data. For certain applications,

RRH RRH

N Requests L=X+Y Bits

M Requests Y Bits

Fog Node

Fronthaul

Fog Node

Backhaul + kcore backbone nodes

Cloud Data Centers

N Results
L·oR Bits X·θR FLOPs

M Results Y·oR Bits

Y·θR FLOPs

Fig. 3: Diagram explaining the ﬂow of data through the considered Fog computing network.

the amount of required computations can be disproportionate

to the size of an input, e.g., a Portable Game Notation (PGN)

ﬁle representing a chess game [17] is minuscule (< 1 kB)

compared to the amount of computation used to analyze the

game. Let ✓Ri be the computational complexity of task Ri

deﬁned as a ratio of single precision Floating Point Operations

(FLOPs) required to process this task to its size in bits.

When the data has been successfully processed, the results

are

transmitted

back

to

the

MD.

Let

R
o

be

the

average

ratio

of

size

of

the

result

to

the

size

of

the

ofﬂoaded

task.

If

R
o

=

0,

then there is no transmission of the results.

C. Power Consumption
The power consumption model is divided into two parts: communication (transmission and reception of the data) and computation (processing of data). In this work, the power consumption of end devices is not considered.
1) Communication: For the power consumption of networking equipment, the linear model from [14] is used which includes idle power Pidle and active power that scales with load C (in bits/second) by parameter b (in Joules/bit):
P (C) = Pidle + C Pmax Pidle = Pidle + C b (2) Cmax

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

where Pidle and Pmax denote idle and maximum power consumption respectively. Cmax is maximum load. Energy-perbit cost of transmitting data through the core, backhaul, and fronthaul networks is equal to the number of devices through which data ﬂows (kcore, kbck, kfr respectively) multiplied by the average b parameter, i.e., b, of devices in this part of network:

b core = kcore b, b bck = kbck b, b fr = kfr b (3)

The number kcore can vary from 1 if the Cloud DC is

directly connected to the Fog tier of the network to 5-20 in

a more realistic scenario of Cloud being few hundred/thousand

kilometers away [18]. For backhaul transmission it is assumed

that there is only a single hop/node from the FNs to the core

network. kfr is equal to 2 (RRH and FN).

Total

energy

comm
Eact

used

for

“active”

transmission

is

equal:

comm
Eact

=

b frL (1 + oR) + ( b core +

b bck) Y (1 + oR) .

(4)

Eq. (4) clearly shows that L bits go through the fronthaul to

the Fog and, from these L bits, Y bits are transmitted further

(via backhaul and core) to the Cloud.

We also consider idle power consumption of devices in the

Fog tier of the network. For fronthaul communication it is

assumed that there is one networking device in each of F FNs

and

H

RRHs.

Idle

power

consumption

f ront
Pidle

of

devices

in

fronthaul is deﬁned as:

f ront
Pidle

=

(F

+

H

)

f
Pidle

(5)

If there is a ﬁxed number of B networking devices in the

backhaul (e.g., one per FN), then the backhaul idle power

consumption

back
Pidle

is

given

as:

back
Pidle

=

B

·

b
Pidle

(6)

f
Pidle

and

b
Pidle

denote

idle

power

of

a

single

networking

device in fronthaul and backhaul respectively.

Total power Pcomm spent on transmission is given as:

comm

Pcomm = Eact

+

f ront
Pidle

+

back
Pidle

(7)

T

Respective values b, Pidle, and Cmax are taken from [19]

and [20]. For IP routers we calculate

b

=

Pmax Pidle
Cmax

assuming that Pidle is equal to 90% of Pmax [20].

2) Computation: Different power consumption models for

Cloud DCs and FNs should be used as their scale of operation

is different. FNs are utilized solely by the tasks from within

examined Fog computing network while Clouds are full of

computational requests from various devices across the Web.

For FNs we assume that they consume Pact watts of power

when performing computations and Pidle when in idle-state.

Values of Pact and Pidle depend on type and model of the

device as well as parameters such as clock frequency.

RFi (subset of RF ) is the set of requests processed in the

FN i. Assuming the clock frequency f Fi of FNs is ﬁxed while

performing calculations, then FN i is actively computing for:

tFacit

=

P
Ri 2RFi

LRi ✓Ri

f Fi sFi

,

(8)

where sFi is the FLOPs per cycle parameter of FN i. tFacit

and tFidile are the time-lengths in which FN i is in active and

idle state respectively, T = tFacit + tFidile. Therefore total power

consumption

f og
Pcp

spent

on

computations

in

Fog

equals:

f og
Pcp

=

P
Fi 2F

⇣ tFacit PaFcit

⌘ + tFidilePiFdile

(9)

T

For Cloud DC, it is not feasible to estimate direct impact

of ofﬂoaded workload on the clock frequency (and number of

running servers). Instead, a simpler measure called Floating

Point Operations per Second (FLOPS) per watt is used, this is

a benchmark that is often used in comparing performance of

supercomputers [21]. Let Cj be the FLOPS per watt value

characterizing DC j. RCj (subset of RC ) is the set of requests

processed

in

the

Cloud

DC

j.

Power

Cj
Pcp

consumed

in

Cloud

DC j is calculated as follows:

PcCpj

=

P
Ri 2RCj

LRi ✓Ri

Cj

(10)

Power

cld
Pcp

consumed

in

whole

Cloud

tier

is

equal

to

the

sum

of power consumption of each Cloud DC:

X

cld
Pcp

=

PcCpj

(11)

Cj 2C

Total power P consumed in the network is the sum of power

consumption spent on transmission of data (Pcomm) and on

performing

computations

in

Fog

–

f og
Pcp

and

in

Cloud

–

cld
Pcp

.

Ptot

=

Pcomm

+

f og
Pcp

+

cld
Pcp

(12)

D. Latency

The delay model is also divided into two parts: com-

munication and computation. The wireless communication

channel between end devices and FNs/RRHs is not considered.

Wireless transmission from MDs is pivotal to other research

works [11], [12].

1) Communication: For delays associated with communica-

tion the Round-Trip Time (RTT) [22] metric is used. 7.5µs/km

is chosen as a numerical value assigned to RTT based on [18].

It can be seen that for data computed in the Fog tier the RTT

value is negligibly low as FNs are meant to be located close to

the end users. On the other hand, when it comes to ofﬂoading

computation to the Cloud tier, the RTT value is signiﬁcant as

the Cloud can be a few hundred (thousand) kilometers away

from the source of the request.

The average-per-request transmission delay in the fronthaul

f ront
Dcomm

is

deﬁned

as:

f ront
Dcomm

=

L (1 + oR) N · rb,front

(13)

where L/N is the average size (in bits) of a computational request sent to be processed in either the Fog tier or the Cloud tier and rb,front is the bitrate of a fronthaul link between a RRH and a FN. RTT estimated by the length of the fronthaul link is assumed to be negligible.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

Requests sent to the Cloud traverse multiple links and hops

in the network. It is assumed that the fronthaul link is the

slowest in terms of bitrate out of all these links. Only the

fronthaul delay caused by packet size is considered as it is seen

as the “bottleneck” in this network scenario. However, back-

bone/backhaul network can introduce signiﬁcant “distance”

delay – RTT. Let d be the average distance between Cloud DC

and FNs which forward requests to this DC. The average-per-

request transmission delay in the backhaul and the backbone

equals

back
Dcomm

=

d

·

7.5µs/km

(14)

Total (average-per-request) communication delay Dcomm is calculated based on the premise that all N requests go through the fronthaul while M requests are transmitted through the backhaul and backbone network:

Dcomm

=

N

·

f ront
Dcomm

+M

·

back
Dcomm

N

(15)

2) Computation: For estimating computational delays in

the Fog tier of the network we assume that there is a queueing

system. In [13] each FN is modeled as an M/M/1 queue. As it

is assumed that FNs can balance the load between themselves,

the queueing for each FN should not be independent – as long

as there is an unutilized node the request should not “wait in

queue” of another node. Therefore, in this work, the entire

Fog tier is modeled as an M/M/n queue.

f og

=

(N M ) T

is

the average request arrival rate. n = F is equal to the number

of

FNs.

The

service

rate

f og
µ

is

calculated

as

the

ratio

of

FLOPS performance of an FN1 and an average number of

FLOPs that are needed to process a request sent to the Fog

tier:

µ = f s fog

Fi Fi

P
Ri 2RF

LRi ✓Ri

NM

(16)

Then,

the

average

delay

f og
Dcp

of

a

request

caused

by

comput-

ing (and queueing) in the Fog tier equals:

f og
Dcp

=

C (n,

f

og

f
/µ

og

)

n · µfog fog

+

1 µf og

(17)

where C(n,

f

og

f
/µ

og

)

is

the

Erlang

C

formula.

For computations in a Cloud DC, it is assumed that the

computational resources are vast and there is never a need for

queueing tasks. It is modeled as an M/M/1 queue. So the

average

latency

cld
Dcp

depends

only

on

the

computation

and

is

equal

to

, 1
µcld

where:

cld cld

cld
µ

=

fs

P
Ri 2RC

LRi ✓Ri

M

(18)

is analogous to Eq. (16). It is worth noting that by assuming inﬁnite computational resources at each Cloud DC the number

1All FNs are modeled to have the same frequency and FLOPS measure to allow the use of the M/M/n model.

of DCs in the network does not impact the performance. Total (average-per-request) computational delay Dcp is equal:

(N Dcp =

M)

·

f og
Dcp

+

M

·

cld
Dcp

(19)

N

Total (average-per-request) delay Dtot is calculated as a sum of computational and communicative delays:

Dtot = Dcp + Dcomm

(20)

IV. RESULTS
We look at power consumption and delay in various scenarios for Fog without Cloud and for full Fog computing. Evaluation has been performed using GNU Octave [23].

A. Fog Without Cloud

First, let us consider a network with H = 20 RRHs, F =

10 FNs and no possibility of ofﬂoading to Cloud DCs. Each

request

has

size

1

MB,

and

values

R
✓

=

10

and

R
o

=

0.1.

The length of examined time period T is set to 60 s.

1) Computations: Let us examine the computational delay

and power consumption in the FNs. We assume each FN is

equipped with Intel Core2 Duo E6850 as its Central Processing Unit (CPU). sFi = 4 as that is the maximum number of double

precision FLOPs that can be calculated in a single cycle of

this processor [24]. Pact and Pidle are parametrized using the

power consumption model from [25]:

Pcpu = 8.4503⌫procVc2pufcpu + 36.3851Vcpu 33.9503 (21)

where ⌫proc is the total utilization of the processor and the

units of Pcpu, Vcpu, and fcpu are W, V, and GHz respectively.

It is assumed that for the idle state ⌫proc = 0 and for the

active state ⌫proc = 1. We consider power consumption of a

FN at two different frequency-voltage levels measured in [25]:

fcpu = 3.006 GHz, Vcpu = 1.28 V later called as ’3 GHz’ and

fcpu = 2.004 GHz, Vcpu = 1.104 V later called as ’2 GHz’. At

3

GHz

Fi
Pact

=

54.241

W

and

Fi
Pidle

=

12.6226

W.

At

2

GHz

Fi
Pact

=

26.859

W

and

Fi
Pidle

=

6.2189

W.

These

values

are

then inserted into Eq. (9) to calculate the computational power

consumption of FNs. Delay and power consumption of FNs

are plotted against the number of computational requests in

Fig. 4a. It shows that decreasing the CPU frequency lowers

power consumption, but increases delay. Results in Fig. 4a

are calculated for the number of FNs F = 10. Fig. 4b plots

power consumption and delay against the number of FNs

for number of requests N = 50000. It can be seen that

higher number of FNs decreases delay. There are diminishing

returns as the delay caused by queueing quickly disappears

and processing delay does not change with number of FNs.

Moreover, utilizing more FNs increases power consumption.

2) Communications: The communications delay in the

fronthaul depends on the bit rate of fronthaul links and the size

of the requests (and responses) as in Eq. (13). The term home

gateway is used in [19] to describe access interface composed

of several components, namely a processor plus memory, a

Wide Area Network (WAN) interface and several Local Area

Network (LAN) interfaces. We use it to model interfaces

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

Power [W] Delay [s] Power [W] Delay [s]

400

0.013

500

0.025

350

0.012

450

300

0.011

0.02

400

250

0.01

350

0.015

200

0.009

300

150

0.008

0.01

100

0.007

250

50

0.006

10000 20000 30000 40000

Number of requests

(a) Dependency on number of requests, number of FNs F = 10.

200 6

0.005 8 10 12 14 16 18 20
Number of FNs

(b) Dependency on number of FNs, number of requests N = 50000.

Fig. 4: Power consumption and delay related to computation in the Fog tier (dashed line – 2 GHz, solid line – 3 GHz).

TABLE II: Power consumption of networking equipment.

Equipment Gateways [19] 1G EPON 10/10G EPON Core routers [20] Juniper T1600

Capacity
1 Gb/s 10 Gb/s
640 Gb/s

Pact
3.3 W 5.5 W
6572 W

Pidle

b

3.0 W 0.3 nJ/bit 3.5 W 0.2 nJ/bit

5915 W 1.03 nJ/bit

of RRHs and FNs. Parameters of Ethernet Passive Optical Network (EPON) home gateways are detailed in Table II.
3) Overall performance: The values calculated in Sections IV-A1 and IV-A2 are added and presented in Fig. 5. Fig. 5 shows total (stemming from computations and transmission) power consumption and delay in a ,,Fog without Cloud” network. The following conclusions can be drawn. The average delay is low – hovering around 10-20 ms. There is also a visible trade-off between power consumption and delay – higher clock frequency and interface bandwidth results in shorter delay and greater power consumption. With that in mind, a combination of FNs running at 2 GHz frequency and 10 Gbit fronthaul outperforms FNs running at 3 GHz and 1 Gbit fronthaul in both estimated metrics at given trafﬁc conditions.
B. Full Fog Computing Network
Let us consider a network with H = 20 RRHs, F = 10 FNs and one Cloud DC. RRHs and FNs are connected through 1G EPON. FNs are connected through the 10G EPON backhaul to the core network. For estimating delays and power consumption occurring in the core network the following three distance scenarios are chosen. In near scenario the Cloud DC is located 100 km and 6 nodes away from the Fog tier of the network. The other scenarios are medium – 2000 km, 12 nodes and far – 8000 km, 18 nodes. There is a Juniper T1600 router (Table II) installed at each node. Cloud computational power efﬁciency is assumed to be 1 GFLOPS/W, its processors are running at 1.5 GHz frequency, and can calculate up to 32 FLOPs in a single cycle (scld = 32). FNs are assumed to work at 2 GHz with sFi = 4 FLOPs per cycle as in the previous subsection. Each request has size 1 MB, and values

R
✓

=

10

and

R
o

=

0.1.

The

length

of

time

period

remains

T = 60 s.

Let us vary the number of requests M which are sent to

Cloud DC while the total number of ofﬂoaded requests stays

the same (N = 50000). Power consumption and average delay

are plotted in

Fig. 6 against the fraction

of

requests

M N

sent

to Cloud. There are a few interesting observations. First, the

distance (physical and logical) to the Cloud DC plays a key

role in determining whether ofﬂoading data all the way to the

Cloud DC is beneﬁcial. In the near scenario utilizing Cloud

reduces both latency and power consumption. More interest-

ing results can be seen in the medium scenario: performing

computations in Cloud decreases power consumption while

performing them in Fog results in lower average latency. Figs.

7a-d show how network performance changes when values of

certain parameters are modiﬁed.

Power efﬁciency of Cloud DCs contributes signiﬁcantly to total power consumption. As shown in Fig. 7a, if power efﬁciency is decreased (from 1 to 0.5 GFLOPS/W), then sending requests to Cloud is more expensive than computing them in Fog for all distance scenarios. On the other hand, at 10 GFLOPS/W (world-class efﬁciency [21]), sending requests to Cloud consumes less power (Fig. 7b). Delay does not depend on power efﬁciency.

The

impact

of

computational

complexity

of

requests

R
✓

on network performance is also studied. The results for

R
✓

=

1

and

R
✓

=

100

are

shown

in

Figs.

7c

and

7d.

Corresponding share of total delay and power consumption

spent on computation and communication is plotted in Fig. 8.

For

R
✓

=

1

computing

requests

in

the

Fog

tier

is

faster

and

consumes less power than processing them in the Cloud tier.

The delay and power consumption caused by data sent through

the core network outweighs the fact that Cloud DCs are more

efﬁcient at computing. On the contrary, when the ofﬂoaded

requests require heavy computations, it is beneﬁcial to send

these

requests

all

the

way

to

the

Cloud

as

high

R
✓

means

lower communication cost compared to computation cost.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

500

0.025

500

0.025

450

450

0.02

0.02

400

400

Delay [s]

Delay [s] Power [W]

Power [W]

350

350

0.015

0.015

300

300

250

250

0.01

0.01

200

200

150

0.005

10000 20000 30000 40000

Number of requests

150

0.005

10000 20000 30000 40000

Number of requests

(a) Dashed line: f =2 GHz, r=1 Gbps; solid: f =3 GHz, r=10 Gbps. (b) Dashed line: f =3 GHz, r=1 Gbps; solid: f =2 GHz, r=10 Gbps.

Fig. 5: Total power consumption and delay in “Fog without Cloud” network. f – FN clock frequency, r – fronthaul bit rate.

400

0.08

0.07 380
0.06

360

0.05

Delay [s]

Power [W]

340

0.04

0.03
320 0.02

300

0.01

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

Fig. 6: Total power cons. and delay, full Fog computing. Dashed, solid, dotted lines – near, medium, far scenarios resp.

V. CONCLUSION
This work surveys several research articles which model the power consumption and delay in the Fog computing network. The common concept is that the computation and communications are modeled separately. Following up on the survey, the models for total power consumption and average delay in the Fog computing network are developed. The concept of computational complexity is introduced to differentiate ofﬂoading requests depending on how computationally intensive they are to process.
Impact of different parameters on power consumption and delay in the Fog computing is examined. It is observed that there is often a trade-off between delay and power consumption. E.g., Fog Nodes (FNs) working at higher frequency consume more power, but provide lower latency. High computational complexity of ofﬂoaded requests favors (as expected) processing in Cloud. Delay and power consumption caused by transmitting data through the core network is offset by the high processing speed and computational power efﬁciency. Conversely, requests which require relatively few operations to process are best served by nearby FNs.
Future work includes analyzing wireless transmission between Mobile Devices (MDs) and the Fog tier of the network.

We are going to examine under what conditions it is beneﬁcial
to ofﬂoad computations from the MDs. Moreover, we would
like to confront our results with a small scale testbed.
REFERENCES
[1] P. Cerwall et al., “Ericsson mobility report,” Ericsson, Tech. Rep., June 2018.
[2] Cisco, “Cisco global cloud index: Forecast and methodology, 2016–2021 white paper,” Cisco, Tech. Rep., 2018.
[3] N. Chen, Y. Chen, Y. You, H. Ling, P. Liang, and R. Zimmermann, “Dynamic urban surveillance video stream processing using fog computing,” in Proc. BigMM, Apr. 2016.
[4] J. K. Zao, T. T. Gan, C. K. You, S. J. R. Me´ndez, C. E. Chung, Y. T. Wang, T. Mullen, and T. P. Jung, “Augmented brain computer interaction based on fog computing and linked data,” in Proc. IE, June 2014.
[5] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, “Fog computing and its role in the Internet of Things,” in Proc. MCC, Aug. 2012.
[6] OpenFog Consortium, “OpenFog reference architecture for fog computing, OPFRA001.020817,” 2017.
[7] M. Peng, S. Yan, K. Zhang, and C. Wang, “Fog-computing-based radio access networks: issues and challenges,” IEEE Network, vol. 30, no. 4, pp. 46–53, July 2016.
[8] M. Othman and S. Hailes, “Power conservation strategy for mobile computers using load sharing,” SIGMOBILE Mob. Comput. Commun. Rev., vol. 2, no. 1, pp. 44–51, Jan. 1998.
[9] C. Mouradian, D. Naboulsi, S. Yangui, R. H. Glitho, M. J. Morrow, and P. A. Polakos, “A comprehensive survey on fog computing: State-of-theart and research challenges,” IEEE Communications Surveys Tutorials, vol. 20, no. 1, pp. 416–464, Q1 2018.
[10] T. N. Gia, M. Jiang, A. Rahmani, T. Westerlund, P. Liljeberg, and H. Tenhunen, “Fog computing in healthcare Internet of Things: A case study on ECG feature extraction,” in Proc. WNM, Oct. 2015.
[11] T. Q. Dinh, J. Tang, Q. D. La, and T. Q. S. Quek, “Ofﬂoading in mobile edge computing: Task allocation and computational frequency scaling,” IEEE Transactions on Communications, vol. 65, no. 8, pp. 3571–3584, Aug. 2017.
[12] L. Liu, Z. Chang, and X. Guo, “Socially aware dynamic computation ofﬂoading scheme for fog computing system with energy harvesting devices,” IEEE Internet of Things Journal, vol. 5, no. 3, pp. 1869–1879, June 2018.
[13] R. Deng, R. Lu, C. Lai, T. H. Luan, and H. Liang, “Optimal workload allocation in fog-cloud computing toward balanced delay and power consumption,” IEEE Internet of Things Journal, vol. 3, no. 6, pp. 1171– 1181, Dec. 2016.
[14] F. Jalali, K. Hinton, R. Ayre, T. Alpcan, and R. S. Tucker, “Fog computing may help to save energy in cloud computing,” IEEE Journal on Selected Areas in Communications, vol. 34, no. 5, pp. 1728–1739, May 2016.
[15] S. Sarkar and S. Misra, “Theoretical modelling of fog computing: a green computing paradigm to support IoT applications,” IET Networks, vol. 5, no. 2, pp. 23–29, Mar. 2016.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

0.08

0.08

450

450

0.07

0.07

400

0.06

400

0.06

Delay [s]

Delay [s] Power [W]

Power [W]

0.05

0.05

350

350

0.04

0.04

300

0.03

300

0.03

0.02

250

0.01

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(a) 0.5 GFLOPS/W
0.14 350
0.12

0.02

250

0.01

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(b) 10 GFLOPS/W
0.14 350
0.12

Delay [s]

Delay [s] Power [W]

Power [W]

0.1

0.1

300

300

0.08

0.08

0.06

0.06

250

0.04

250

0.04

0.02

0.02

200

0

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(c) ✓R = 1

200

0

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(d) ✓R = 100, N = 5000

Fig. 7: Total power consumption and delay in full Fog computing network depending on fraction of requests sent to Cloud,

Cloud power efﬁciency (GFLOPS/W) and computational complexity of requests ✓R. Dashed line – near scenario, solid line –

medium, dotted line – far.

1

1

1

1

Share in delay

Share in delay Share in power cons.

Share in power cons.

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0

0

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(a) ✓R = 1

0

0

0.2

0.4

0.6

0.8

Fraction of requests sent to Cloud

(b) ✓R = 100, N = 5000

Fig. 8: Share of computation (dashed line) and communication (solid line) in power consumption and delay in full Fog computing network, medium scenario.

[16] S. Sarkar, S. Chatterjee, and S. Misra, “Assessment of the suitability of fog computing in the context of internet of things,” IEEE Transactions on Cloud Computing, vol. 6, no. 1, pp. 46–59, Jan. 2018.
[17] S. J. Edwards, S. D. Forsyth, J. Stanback, and A. Saremba, “Standard: Portable Game Notation speciﬁcation and implementation guide,” 1994, http://www.saremba.de/chessgml/standards/pgn/pgn-complete.htm.
[18] M. Olbrich, F. Nadolni, F. Idzikowski, and H. Woesner, “Measurements of path characteristics in PlanetLab,” Technical University Berlin, Tech. Rep. TKN-09-005, July 2009.
[19] P. Bertoldi, “EU code of conduct on energy consumption of broadband equipment: Version 6,” 2017.
[20] W. Van Heddeghem et al., “Evaluation of power rating of core network equipment in practical deployments,” in Proc. OnlineGreenComm, Sep. 2012.

[21] W. Feng and T. Scogland, “Green500 list for November 2018,” 2018. [Online]. Available: https://www.top500.org/green500/lists/2018/11/
[22] G. Almes, S. Kalidindi, and M. Zekauskas, “A round-trip delay metric for IPPM,” 1999, RFC 2681.
[23] “GNU octave.” [Online]. Available: www.gnu.org/software/octave/ [24] R. Dolbeau, “Theoretical peak FLOPS per instruction set: a tutorial,”
The Journal of Supercomputing, vol. 74, no. 3, pp. 1341–1377, Mar 2018. [25] S. Park, J. Park, D. Shin, Y. Wang, Q. Xie, M. Pedram, and N. Chang, “Accurate modeling of the delay and energy overhead of dynamic voltage and frequency scaling in modern microprocessors,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 32, no. 5, pp. 695–708, May 2013.

Authorized licensed use limited to: KAUST. Downloaded on October 27,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

