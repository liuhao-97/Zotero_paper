5870

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

EdgeAdaptor: Online ConÔ¨Åguration Adaption, Model Selection and Resource Provisioning
for Edge DNN Inference Serving at Scale

Kongyange Zhao , Zhi Zhou , Member, IEEE, Xu Chen , Senior Member, IEEE, Ruiting Zhou , Member, IEEE, Xiaoxi Zhang , Member, IEEE, Shuai Yu , Member, IEEE, and Di Wu , Senior Member, IEEE

Abstract‚ÄîThe accelerating convergence of artiÔ¨Åcial intelligence and edge computing has sparked a recent wave of interest in edge intelligence. While pilot efforts focused on edge DNN inference serving for a single user or DNN application, scaling edge DNN inference serving to multiple users and applications is however nontrivial. In this paper, we propose an online optimization framework EdgeAdaptor for multi-user and multi-application edge DNN inference serving at scale, which aims to navigate the three-way trade-off between inference accuracy, latency, and resource cost via jointly optimizing the application conÔ¨Åguration adaption, DNN model selection and edge resource provisioning on-the-Ô¨Çy. The underlying long-term optimization problem is difÔ¨Åcult since it is NP-hard and involves future uncertain information. To address these dual challenges, we fuse the power of online optimization and approximate optimization into a joint optimization framework, via i) decomposing the long-term problem into a series of single-shot fractional problems with a regularization technique, and ii) rounding the fractional solution to a near-optimal integral solution with a randomized dependent scheme. Rigorous theoretical analysis derives a parameterized competition ratio of our online algorithms, and extensive trace-driven simulations verify that its empirical value is no larger than 1.4 in typical scenarios.
Index Terms‚ÄîEdge intelligence, edge computing, DNN inference serving, online optimization
√á

1 INTRODUCTION
DRIVEN by the burgeoning as well as accelerating convergence of artiÔ¨Åcial intelligence (AI) and Internet-ofThings (IoT), we have recently witnessed an unprecedented booming of AI-of-Things or AI-empowered IoT applications. This new trend is known as AIoT [1], which has gained sparking interest from both academia and industrial. To materialize the vision of AIoT, edge intelligence has emerged as an enabling paradigm to address the last mile delivery issue faced by AI [2]. With edge intelligence, dataand computation-intensive AI tasks are pushed from the centralized cloud to the network edges to serve the ubiquitous IoT users and devices in closer proximity. This closer
 Kongyange Zhao, Zhi Zhou, Xu Chen, Xiaoxi Zhang, Shuai Yu, and Di Wu are with the School of Computer Science and Engineering, Sun Yat-sen University (SYSU), Guangzhou 510006, China. E-mail: zhaokyg@mail2. sysu.edu.cn, {zhouzhi9, chenxu35, zhangxx89, yushuai, wudi27}@mail. sysu.edu.cn.
 Ruiting Zhou is with the School of Cyber Science and Engineering, Wuhan University, Wuhan 430072, China. E-mail: ruitingzhou@whu.edu.cn.
Manuscript received 23 November 2021; revised 17 May 2022; accepted 22 June 2022. Date of publication 7 July 2022; date of current version 31 August 2023. This work was supported in part by the National Natural Science Foundation of China under Grants 62172454, 62072344, U20A20159, 61972432, 62102460, 62002397, U20A20177, and U1911201, in part by the Guangdong Basic and Applied Basic Research Foundation under Grants 2021B151520008 and 2021A1515011912, and in part by the Science and Technology Planning Project of Guangdong Province under Grants 2018B030322004 and 2021 A0505110008. (Corresponding author: Zhi Zhou.) Digital Object IdentiÔ¨Åer no. 10.1109/TMC.2022.3189186

proximity spurs prominent beneÔ¨Åts including reduced transmission delay, energy consumption, and wide-areanetwork (WAN) bandwidth usage [3].
While recognizing the prominent advantages of edge intelligence, we should also note that edge nodes are typically provisioned with limited resources (e.g., low-end and weak CPU/GPU). Such resource limitations deteriorate the performance of state-of-the-art AI models based on deep neural network (DNN) (e.g., VGG [4] and ResNet [5] for computer vision tasks), since the latter is far more resourcehungry and delay-sensitive in AIoT scenarios. To address this performance degradation incurred by the mismatch between resource supply and demand, model compression is widely applied to practical resource-constrained edge deployment [6], [7]. After applying model compression methods, such as model weight pruning and quantization [8], the compressed model is with less number of weights, smaller model size, and thus less resource consumption as shown in Fig. 1. In orthogonal to model compression, application conÔ¨Åguration adaption can be also exploited to reduce the resource footprint of DNN inference serving. For computer vision tasks with image input, the resolution of the image can be degraded (e.g., from 720p to 240p as shown in Fig. 1) to accelerate the model inference [9].
Notably, pilot efforts on edge intelligence have mostly focused on energy-efÔ¨Åcient DNN inference serving for a single user or application [10], [11], [12], the problem of edge DNN inference serving to multi-user and multi-application deployment at scale has been largely overlooked. Orchestrating DNN inference serving for multiple users and

1536-1233 ¬© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5871

Ô¨Årst relax the integer constraint and regularize the time-cou-

pling term in the long-term problem. Next, we temporally

decompose it into a series of single-shot fractional subpro-

blems that do not involve future information and thus can

be readily solved. To maintain the feasibility of the frac-

tional solution, we further round it to a feasible integer solu-

tion by carefully designing a randomized dependent

rounding scheme. Essentially, at the core of the dependent

rounding scheme, is to compensate each rounded-down

model instance variable by another rounded-up model

instance variable. Since the compensation avoids provision-

Fig. 1. An illustration of application conÔ¨Åguration adaption and model ing excessive model instances, the dependent rounding

compression for edge DNN inference serving.

scheme enables a signiÔ¨Åcant reduction of the total cost, and

meanwhile maintains the feasibility of the solution.

applications is however nontrivial due to the following dif- Our main contributions are highlighted as follows.

Ô¨Åculties. For each application, its request can be served by multiple DNN models. Take the application of object detection task as an example, its request can be served by DNN model such as YOLO [13], SSD [14], or R-FCN [15], and such different models have a diverse resource-accuracy tradeoff. While for different applications, their requests can also share the same DNN model. For trafÔ¨Åc analytic applications such as object counting (e.g., pedestrians, cars, bikes), trafÔ¨Åc violations detection (e.g., jaywalking), and collision analysis (e.g., the collision between vehicles and pedestrians), they can share the same model instance (e.g., a container or a VM) for object detection to reduce the resource cost. To balance accuracy improvement brought by different models and resource cost saved by model sharing, how to choose the best model for each application and dynamically provision the number of model instances for each DNN model? Besides, launching a new model instance (e.g., a container or a VM) would incur instance switching cost which is associated with the hardware wear-and-tear. When the request arrival diminishes, destroying an idle model instance does not necessarily improve the cost-efÔ¨Åciency, as it would surge the instance switching cost if the request arrival grows shortly after.
To address the above difÔ¨Åculties, in this paper we advocate an online optimization framework EdgeAdaptor for joint conÔ¨Åguration adaption, model selection and resource provisioning for cost-efÔ¨Åcient edge DNN inference serving at scale. It minimizes the long-term resource cost and cumulative accuracy loss under predeÔ¨Åned real-time performance requirements. In particular, such a problem can be formulated as a mixed integer linear programming (MILP). However,

 We cast the joint problem of conÔ¨Åguration adaption, model selection, and resource provisioning for edge DNN inference serving as a mixed integer linear programming, which judiciously arbitrates the threeway tradeoff between resource cost, performance (i.e., service latency), and inference accuracy.
 We propose a regularization-based algorithm to address the challenge of uncertain future information by time-coupling switching cost. Through a carefully designed randomized dependent rounding scheme, the feasibility of the solution is maintained without introducing expensive cloud outsourcing costs.
 We rigorously analyze and derive a parameterized competitive ratio for our proposed online algorithms, by incorporating the competitive ratio of the online regularization method as well as the rounding gap of the randomized dependent rounding scheme. Extensive experiments driven by realistic workload traces further verify that the empirical value of the parameterized competitive ratio is no larger than 1.40 in typical scenarios.
The rest parts of the paper are organized as follows. Section 2 reviews the related literature on edge intelligence model inference. Section 3 introduces the system model and problem formulation for cost-efÔ¨Åcient edge DNN inference serving at scale. Section 4 presents an online optimization algorithm and we analyze its worst-case performance rigorously in Section 5. Section 6 conducts extensive trace-driven simulations to empirically assess the performance of the proposed online optimization algorithm. Finally, Section 7 concludes this paper.

solving the above MILP problem is rather challenging. On

the one hand, as the switching cost is incurred when launching new model instances, the resource provisioning deci-

2

RELATED WORK

sions are temporally coupled over consecutive time slots. To push the AI capabilities from the centralized cloud to the

This temporal correlation makes the long-term cost minimi- network edge which is in closer proximity to the wide-

zation time-coupling and involving future system informa- spread IoT users and devices, an essential problem is how

tion. On the other hand, even with an ofÔ¨Çine setup in which to facilitate low-latency and energy-efÔ¨Åcient DNN model

the future system parameters are given beforehand, the inference at edge nodes that are typically limited by both

long-term problem is still proven to be NP-hard.

energy and resource capacity. To answer this problem in

By exploiting the structural properties of the problem, we the positive, Neurosurgeon [17] attempts to partition the

simultaneously cope with the above dual challenges via fus- DNN model layers into two parts and ofÔ¨Çoading the more

ing the regularization method for online algorithm design resource-demanding part from the IoT device to the power-

and a dependent rounding technique for approximation ful cloud for inference acceleration. Following Neuro-

algorithm design. In particular, by leveraging the regulari- surgeon, Edgent [10] augments model right-sizing to model

zation method from the online learning literature [16], we partitioning to further reduce the inference latency. By
Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5872
Notation I ; J ; K; T i; j; k; t Ai√∞t√û; Li ej; Ej aijk djk; fjk bj√∞t√û; ci; sj

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023
TABLE 1 Main Notations
Describtion
sets of applications, models, conÔ¨Ågurations, and time slots indexes of applications, models, conÔ¨Ågurations, and time slots workload and average latency requirement for application i in time slot t workload capacity and resource capacity for DNN model j accuracy loss for application i served by model j with conÔ¨Åguration k inference latency and resource consumption for model j serving with conÔ¨Åguration k instance operational cost, unit cloud outsourcing cost, and switching cost

pre-training a master AI model embedding with multiple formulated as a set of continuous rather than discrete varia-

sizes (each size corresponds to a branch of the master bles, and thus the optimization problem does not necessar-

DNN), Edgent adaptively selects the best model size to opti- ily to be NP-hard. Besides, the DRL method is difÔ¨Åcult to

mize the latency-accuracy tradeoff against varying compu- converge during the training phase, especially for continu-

tation resources and network bandwidth.

ous variables with a large action space, which is still an

For model partitioning approaches, the vulnerable wire- open problem at present. Our online algorithm can directly

less network link between the device and the edge server obtain the online decision without the training phase and

may become the performance bottleneck of model inference. provide performance guarantees through rigorous theoreti-

To cope with this issue, the recent works CLIO [11] and cal analysis compared to DRL method. The most related

SPINN [12] further compress the DNN layer corresponds to work to us is probably [23], in which Wang et al. leverage

the partitioning points, and thus reduce the transmission Lyapunov optimization to jointly optimize the video conÔ¨Åg-

latency of the intermediate model data transfer between the uration and network bandwidth allocation to balance the

device and the edge server.

three-way tradeoff between accuracy, latency and energy

Different from the above research attempts which consumption. Our work is different from and complemen-

relieves resource mismatch at the model serving side, tary to [23] in at least the following three aspects. First, we

another stream of recent efforts exploit conÔ¨Åguration adap- further consider model selection and model sharing to opti-

tion at the application request side to accelerate DNN infer- mize the holistic system cost. Second, unlike [23] assumes

ence. For computer vision-based applications such as video Ô¨Åxed amount of computation resource, we further considers

analytics and image recognition, the application conÔ¨Ågura- the problem of dynamical resource provisioning. Finally, in

tion ‚Äî frame rates and resolution ratio for videos, compres- [23], the challenge of time-coupling is incurred by the long-

sion ratio and frame size for images ‚Äî can be adaptively term time-averaged latency constraint, while in our work,

optimized to balance the tradeoff between inference latency, the challenge of time-coupling is incurred by the switching

accuracy, and resource quota [18]. For large-scale edge- cost of dynamically resource provisioning. Note that how-

coordinated video analytics, Yang et al. [19] jointly optimize ever, Lyapunov optimization is unable to address the time-

the video conÔ¨Åguration and edge resource allocation to coupling term in the objective function and thus is not

maximize the long-term inference accuracy, while satisfying applicable to our problem.

the real-time delay requirement of various video streams,

by applying deep reinforcement learning (DRL). For the real-world implementation, Crankshaw et al. [20] propose a inference serving system with a two-layer architecture, which contains the model selection and the model abstraction to achieve low latencies, high throughputs, and improved accuracy. And Francisco et al. [21] propose a distributed inference serving system, which selects appropriate

3 SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we present the system model for multi-application and multi-model edge inference serving at scale, and formulate the problem which navigates the three-way tradeoff between inference accuracy, latency, and resource cost. Table 1 lists the main notations in our paper.

model variants for each query with different service level

objectives (SLOs), i.e., latency requirements, to achieve a 3.1 Overview of Edge DNN Inference

trade-off between accuracy, latency and cost.

Serving System

While existing efforts on edge intelligence has mostly We consider a realistic edge intelligence paradigm as illus-

focused on energy-efÔ¨Åcient DNN inference serving for a sin- trated in Fig. 2. The edge server which is typically attached

gle user or DNN application, the problem of scaling edge to an access point serves diverse emerging AIoT applica-

DNN inference serving to multi-user and multi-application tions as exempliÔ¨Åed by smart retail, intelligent transporta-

deployment has been largely overlooked. Recently, Wu tion and smart factory. These AIoT applications persistently

et al. [22] investigate collaborative DNN inference between sense the surroundings of IoT devices and generate huge

device and edge server for multiple AI services. To mini- amounts of multi-modal data such as streams of videos,

mize the service delay under a time-averaged accuracy audios and images. To extract knowledge from the sensed

requirement, the authors apply DRL method to jointly opti- data, it is required to perform model inference (such as

mize the knobs of task sampling rate selection, task ofÔ¨Çoad- VGG [4], YOLO [24] for object recognition in intelligent

ing, and edge computing resource allocation. Our work is transportation). Considering the resource scarcity of the IoT

different from [22], since the resource allocation decision is devices, such DNN model inference service is deployed at
Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5873

application conÔ¨Åguration adaption [18] is also widely applied

to Ô¨Çexibly navigate the resource-latency-accuracy tradeoff in

edge computing paradigm. For example, for inference request

with image input, the resolution can be adapted (e.g., from

1080p to 480p) to tune the resource consumption, inference

latency and accuracy. While for inference request with video

input, the frame rate can be further adjusted (e.g., from 30fps

to 10fps) to balance the resource-latency-accuracy tradeoff. In

this paper, we use a set K ¬º f1; 2; . . . ; Kg to denote the set of

conÔ¨Ågurations can be chosen for each application. Then, to

model the resource-latency-accuracy tradeoff, we use aijk, djk

Fig. 2. An illustration of multi-application and multi-model edge DNN and fjk to denote the accuracy loss, inference latency and

inference serving system.

resource consumption (i.e., the CPU FLOPS) of using DNN

model j 2 J to serve the inference request of application i 2

I with conÔ¨Åguration k 2 K.

the edge servers (e.g., Huawei Atlas 500 Edge Server) which For the envisioned edge DNN inference serving system

is equipped with moderate computing capacity and AI as shown in Fig. 2, it also incorporates collaborative model

accelerators such as GPU and NPU (neural processing inference between the edge server and the remote public

units). To fully utilize the expensive edge server resources, cloud, i.e., an inference request can be served either by the

an edge server is typically virtualized into a set of contain- edge server or the public cloud, due to the following consid-

ers or virtual machines (VM) to co-locate multiple DNN erations. First, the price of the edge resource typically Ô¨Çuc-

models to serve diverse AIoT applications.

tuates over time, since the time-varying electricity price

Based on the above setup, we use a set I ¬º f1; 2; . . . ; Ig to dominates the operational cost. This indicates that when the

denote diverse AIoT applications served by the edge server. edge resource price peaks, we can ofÔ¨Çoad the inference

Considering the fact that the inference requests of some request to the public cloud for processing. Second, the

diverse applications can share the same DNN model, and arrival of inference requests also Ô¨Çuctuates over time, which

the inference requests of a speciÔ¨Åc application can be even may overwhelm the capacity of the edge server. For each

served by different DNN models, we use another set J ¬º inference request ofÔ¨Çoaded to the cloud, it is often served

f1; 2; . . . ; Jg to denote the DNN models held on the edge by the most accurate but resource-demanding model, due

server. The model set J contains different types of DNN to the powerful computing capacity of the cloud.

models (e.g., VGG [4], ResNet [5]), as well as model variants

with different sizes (e.g., VGG-16 and VGG-19), as a result of the aforementioned model compression technique in the 3.2 Decision Variables

second paragraph of Section 1. Without loss of generality, to For the edge DNN inference serving system, its primary

capture the system dynamics such as time-varying inference objective is to jointly optimize the inference accuracy, latency

request arrivals, we assume that the edge DNN inference and monetary cost. Towards this goal, it should judiciously

serving system works in a time-slotted fashion within a exploit the model heterogeneity as well as resource heteroge-

large time span of T . Each time slot of t 2 T ¬º f1; 2; . . . ; T g neity (i.e., edge versus cloud) to make the following deci-

represents a decision interval that matches the change of the sions: (i) application conÔ¨Åguration adaption, for each

system dynamics. In practice, the length of a time slot (i.e., application type of inference requests, which conÔ¨Ågurations

10 minutes) is much longer than a typical end-to-end delay (e.g., the resolution of a input image) it should choose? (ii)

of DNN model inference. Then we use Ai√∞t√û to denote the DNN model selection, for the inference requests of each

number of inference requests generated by the AIoT appli- application, which set of DNN models should be chosen to

cation i 2 I . For each DNN model j 2 J we assume that serve them? (iii) resource provisioning, for each DNN model,

each DNN model j 2 J is instantiated in a VM or container how many instances should be provisioned for it? We now

on the edge server, and this VM or container running a formulated the above decisions in the following.

DNN model is referred to a DNN model instance. Deter- Joint ConÔ¨Åguration Adaption and Model Selection. While it is

mined by the resource capacities of the underlying physical feasible to introduce two independent variables of conÔ¨Ågura-

resources (e.g., CPU, GPU and memory), the workload tion adaption and model selection for each inference request,

capacity of each instance of DNN model j 2 J in each time it would greatly complicate our problem. Instead, a more

slot is denoted as ej, i.e., the amount of CPU FLOPS can be straightforward and simple way is to use a joint conÔ¨Åguraoffered by an instance of DNN model j 2 J in each time tion adaption and model selection variable xijk√∞t√û to denote

slot. Considering the limited amount of the resources on the amount of inference request of application i 2 I and

edge server, we further use Ej to denote the resource capac- with conÔ¨Åguration k 2 K to be served by DNN model j 2 J ity, i.e., the maximal number of available instances of DNN at time slot t. Note that this simpliÔ¨Åcation beneÔ¨Åts from the

model j 2 J on the edge server.

identity of different inference requests by a given application

In serving a speciÔ¨Åc inference request, different DNN type. Furthermore, given the potentially huge amount of

models offer varying resource-latency-accuracy tradeoff. inference request arrival Ai√∞t√û, the decision variable xijk√∞t√û

Therefore, for each inference request, we should choose the can be relaxed to a continuous variable. Such relaxation is in

best model to improve the accuracy with low inference consistent with the recent lit√Äerature√Å on data center power

latency

and

resource
Authorized

footprint.
licensed use

In addition to
limited to: KAUST.

model selection,
Downloaded on January

3m0,a2n02a4gaetm12e:3n2t:5[82U5T].CLfreotmxI√∞EtE√ûE¬ºXploxreij.k√∞Rte√ûst8riic2tiIo;n8sj2aJpp;8lyk.2K;8t2T

be the

5874

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

set of joint conÔ¨Åguration adaption and model selection variables, then it satisÔ¨Åes:

&

XX

X ¬º x√∞t√û j

xijk√∞t√û

j2J k2K

Ai√∞t√û and xijk√∞t√û ! 0; '

8i 2 I ; 8j 2 J ; 8k 2 K; 8t 2 T . (1)

Here xijk√∞t√û Ai√∞t√û indicates that part of the inference requests can be ofÔ¨Çoaded to the cloud.
Resource Provisioning. We use yj√∞t√û to denote the number of model instances launched for DNN model j 2 J at time slot t. Since the resource capacity on edge server is highly

limited, it is impractical to relax the non-negative integer yj√∞t√û to a continuous one. Instead we enforce that yj√∞t√û 2 f0; 1; . . . ; Ejg, where Ej is the aforementioned maximal numbe√Är of a√Åvailable instances for DNN model j 2 J . Let y√∞t√û ¬º yj√∞t√û 8j2J ;8t2T be the set of feasible resource provisioning variables, then it satisÔ¨Åes:

n

o

Y ¬º y√∞t√û j yj√∞t√û 2 f0; 1; . . . ; Ejg; 8j 2 J ; 8t 2 T .

(2)

3.3 Cost Structure

Given the above decision variables, we are now ready to formulate the overall cost incurred by the edge DNN inference serving system, which includes the edge operational cost,

cloud outsourcing cost and instance switching cost. Edge Operational Cost. Edge service providers rent server

resources to run DNN model instances, and multiple inference requests in the same time slot will be processed con-

currently in a model instance. For an edge server, it is widely recognized that its operational cost is dominated by the energy cost and the amortized capital expenditure [26].

Moreover, since the electricity price in the real-time electricity market typically Ô¨Çuctuates over time, we use bj√∞t√û to denote the operational cost of running an instance of DNN model j 2 J at time slot t. It contains the processing cost of multiple inference requests and is jointly determined by

DNN size and electricity price. Then the total edge operational cost in each time slot t is given by:

X

CEO√∞y√∞t√û√û ¬º bj√∞t√ûyj√∞t√û.

(3)

j2J

are served by the most accurate model under the highest

application conÔ¨Åguration. Here we use ci to denote the

aggregated cost of outsourcing one inference request of

application i 2 I from the edge server to the remote central

cloud. The unit cloud outsourcing cost ci uniÔ¨Åes the process-

ing cost (resource usage) of the remote cloud, the transmis-

sion cost (bandwidth usage) and the performance penalty

incurred by the latency of the WAN connecting to the remote

cloud. Measurements by internet giants suggest that there is

the numerical relationship betweePn servPice latency and price [28]. Given the amount Ai√∞t√û √Ä j2J k2K xijk√∞t√û of inference requests of application i 2 I outsourced to the public

cloud from edge server, the total cloud outsourcing cost in

time slot t can be computed by:

" X

# XX

CCO√∞x√∞t√û√û ¬º ci Ai√∞t√û √Ä

xijk√∞t√û .

(4)

i2I

j2J k2K

Instance Switching Cost. Note that when we launch new models instance when updating the instance provisioning decisions, another type of cost which is referred to as instance switching cost would be incurred. SpeciÔ¨Åcally, launching a new instance in practical systems (e.g., Docker) involves Ô¨Årst loading the image Ô¨Åle and booting it to a new VM instance, and then loading the model parameters to the instance. This process not only incurs additional delay and energy consumption for preparing resources but also brings hardware wear-and-tear which is often very costly. In practice, the instance switching cost can be on a par with the edge operation cost [29]. We use sj to denote the cost of launching a new instance of DNN model j 2 J , then the total instance switching cost at time slot t is given by:

CItS

√∞y√∞t√û;

y√∞t

√Ä

1√û√û

¬º

X

√Ç sj yj√∞t√û

√Ä

yj√∞t

√Ä

1√û√É√æ;

(5)

j2J

where

√Ç yj√∞t√û

√Ä

yj√∞t

√Ä

1√û√É√æ

¬º

√à max yj√∞t√û

√Ä

yj√∞t

√Ä

1√û;

√â 0,

denot-

ing the number of newly launched instances of DNN model

j 2 J in time slot t. Without loss of generally, we let yj√∞0√û ¬º 0. In summary, the overall cost incurred in time slot t in the

system is:

C√∞x√∞t√û; y√∞t√û√û ¬º CEO√∞y√∞t√û√û √æ CCO√∞x√∞t√û√û √æ CItS√∞y√∞t√û; y√∞t √Ä 1√û√û.

(6)

Cloud Outsourcing Cost. As we discussed in Section 3.1, the

edge server itself is unable to serve all the inference request 3.4 Inference Accuracy and Latency

arrival when the latter burst suddenly, or un-economical to Application conÔ¨Åguration adaption and DNN model selec-

serve all the inference request arrival when the real-time tion pose a three-way tradeoff among inference accuracy,

electricity price peaks and overweighs the cost of the cloud. latency and resource consumption. Recall that in Section 3.1

In these two cases, the edge DNN inference serving system we use aijk and djk to denote the accuracy loss and inference

can exploit the cloud resource to improve robustness and latency when using DNN model j 2 J to serve the inference

cost-efÔ¨Åciency. Following the recent trend on large-scale request of application i 2 I with conÔ¨Åguration k 2 K. More-

DNN model inference serving with serverless resource in over, since we assume that the outsourced inference

the cloud [27], we also consider serving the outsourced infer- requests are served by the most accurate model under the

ence request with serverless (e.g., AWS Lambda and Google highest conÔ¨Åguration, their accuracy loss is thus zero. Then

Cloud Functions). A salient feature of serverless is that it is the total accuracy loss of all inference request at each time

charged in a ‚Äúpay-as-you-go‚Äù manner, i.e., we pay for the slot t can be expressed as:

actual resource usage rather than the resource occupation

XXX

(like VM). Considering the powerful computing capacity of

Acc√∞t√û ¬º

aijkxijk√∞t√û:

(7)

the cloud, we assume that the outsourced inference requests

i2I j2J k2K

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5875

In practice, requests‚Äô latency typically follows the ‚Äúlong-

tail‚Äù effect [30], [31], and a predeÔ¨Åned hard-deadline for

each request is non-trivial to satisfy, due to the unpredict-

able reliability issues of the underlying hardware platform

as well as the operating system. At the edge, the latency dis-

tribution has a longer tail than the cloud, and it has shown

that given the same latency constraints, the average latency

guarantee is more beneÔ¨Åcial to improve CPU utilization

[32]. For different AIoT applications, they usually have dif-

ferent latency requirements. For example, object detection

tasks for dynamic recognition (e.g., pedestrian or vehicle)

often have a high real-time requirement. Therefore, we use

Li to denote the average inference latency requirements of each applications i 2 I in each time slot. Therefore, the

latency constrPaint Pcondition on edge server can be

expressed as

Pj2J Pk2K xijk√∞t√ûdjk
j2J k2K xijk√∞t√û

Li; 8i 2 I . For inference

requests processed by cloud outsourcing, the latency con-

straint is implicitly concluded in the unit cost ci. SpeciÔ¨Åcally,

applications with a stricter latency requirements have a

higher unit cost.

3.5 Problem Formulation
By jointly consider the holistic cost, inference accuracy and latency, we aim at minimizing the long-term holistic cost and inference accuracy loss over the time horizon T , and under the real-time latency requirement Li. Formally, this problem can be cast as:

P: min s.t.

XT 



C√∞x√∞t√û; y√∞t√û√û √æ v √Å Acc√∞t√û ;

X t¬º1 X

xijk√∞t√û Ai√∞t√û;

X j2J X k2K xijk√∞t√ûfjk

yj√∞t√ûej;

Pi2I k2KP

Pj2J Pk2K xijk√∞t√ûdjk j2J k2K xijk√∞t√û

Li;

xijk√∞t√û ! 0;

yj√∞t√û 2 f0; 1; . . . ; Ejg;

8t; 8i;

(8a)

8t; 8j;

(8b)

8t; 8i;

(8c)

8t; 8i; 8j; 8k; (8d)

8t; 8j:

(8e)

dynamics which typically Ô¨Çuctuates over time and thus hard to precisely estimate. 2) Even if the future information is known as a priori knowledge, the minimization problem P is also NP-hard. SpeciÔ¨Åcally, we can reduce the classical minimum knapsack problem (MKP) [33] which is known to be NP-hard to our problem. The detailed reduction from the MKP can be found in Appendix A, which can be found on the Computer Society Digital Library at http://doi. ieeecomputersociety.org/10.1109/TMC.2022.3189186. These challenges call for an online approach that can jointly optimize conÔ¨Åguration adaption, model selection and resource provision for application requests without requiring future information.

4 ONLINE ALGORITHM DESIGN
We now present the online optimization framework of EdgeAdaptor. SpeciÔ¨Åcally, to address the dual challenges of time-coupling effect and NP-hardness of long-term objective value minimization problem P, we Ô¨Årst relax the integer variables yj√∞t√û and regularize the switching cost CItS, then decouple the long-term problem into a series of single-shot fractional problems. These problems can be easily solved through our fractional algorithm in Section 4.1. In order to satisfy the integer constraints in reality, we design a rounding algorithm in Section 4.2 to obtain a near-optimal solution to the original problem with a bounded optimality gap. Finally, we use the characteristics of the rounding algorithm to obtain the re-selected model variables to meet the capacity constraints changed after the rounding scheme in Section 4.3.

4.1 Fractional Algorithm for Joint ConÔ¨Åguration Adaption and Model Selection

In order to overcome the challenge of the NP-hardness of mixed-integer programming problem, we Ô¨Årst relax the integer constraint (8e), obtaining the fractional optimization problem PR as follow:

Xh

i

PR : min

v √Å Acc√∞t√û √æ CEO√∞t√û √æ CCO√∞t√û √æ CItS ;

t2T

s.t. Constraint (8a) to (8d);

yj√∞t√û 2 ¬Ω0; Ej¬ä; 8t 2 T ; 8j 2 J :

To solve the above long-term fractional problem PR, a natu-

The weighted parameter v controls the tradeoff between ral online approach would be greedily adopting the best

resource cost and inference accuracy. Constraint (8a) decision for the relaxed problem in each independent time

ensures that the amount of requests processed on edge slot. However, the instance switching cost CItS temporally server do not exceed the total number of inference requests couples yj√∞t√û across the time span, this na¬®ƒ±ve approach per-

received in each time slot. Constraint (8b) is the workload haps deviates greatly from the global optimum for the long-

capacity constraint which enforces that for each DNN term cost.

model j, the number of inference requests processed by To decouple the term of switching cost without drastic

DNN model j does not exceed the provisioned processing shifts in the solution between time slot t and t √Ä 1, and thus

capacity yj√∞t√ûej. Constraint (8c) enforces that the average to obtain a good and provable competitive bound (through

edge-side latency of each application i does not exceed the the primal-dual approach in Section 5.2), we exploit the

performance threshold Li. Constraint (8d) is the non-nega- algorithmic technique of regularization in online learning

tive constraint for the decision variables. Finally, constraint [16]. Regularization is a way to stabilize the solution and

(8e) is the integer constraint for the number of launched allows the online algorithm to avoid blindly following the

DNN model instances.

best decision given the past data [34]. The basic idea of regu-

Solving the above optimization problem faces the follow- larization is to solve the relaxed problem PR w√Ç ith a smooth

ing challenges: 1) The long-term optimization problem P is a time-coupling problem that involves future system

c1o√û√Én√æv,exwfeunecmtiopnlotyo

substitute the intractable yj√∞t√û the widely adopted convex

√Ä yj√∞t √Ä relative

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5876

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

entropy function [16] as follow:

Algorithm 1. Fractional Algorithm for Joint ConÔ¨Ågura-

tion Adaption and Model Selection ‚Äî FAAS

D√∞yj√∞t√ûjjyj√∞t

√Ä

1√û√û

¬º

yj√∞t√û

ln

yj√∞t√û yj√∞t √Ä 1√û

√æ

yj√∞t

√Ä

1√û

√Ä

yj√∞t√û:

(9)

Input: I ; J ; K; E; L; a; c; d; e; f ; s; h; ; v Output: ex√∞t√û; ey√∞t√û

This smooth convex function is the sum of the relative

entropy

term

yj

√∞t√û

ln

yj √∞t√û yj √∞t√Ä1√û

and

a

linear

term

denoting

the

movement cost yj√∞t √Ä 1√û √Ä yj√∞t√û. To ensure that the frac-

tion is still valid when no instance of DNN model j is

deployed at time slot t √Ä 1 (i.e., yj√∞t √Ä 1√û ¬º 0), we add a

positive constant term  to both yj√∞t√û and yj√∞t √Ä 1√û in (9).

1: Initialization: ex√∞0√û ¬º 0; ey√∞0√û ¬º 0; 2: for each time slot t 2 T do 3: Observe A√∞t√û; b√∞t√û and ey√∞t √Ä 1√û;
4: Invoke the interior-point method to solve the regularized problem PRrt;
5: returnthe optimal fractional solution ex√∞t√û; ey√∞t√û;
6: end for

Moreover, we deÔ¨Åne an approximation weight factor

hj ¬º ln√∞1 √æ Ej√û and multiply the improved relative

entropy

function

with

1 hj

to

normalize

the

switching

cost

by regularization.

Let PRr represent the relaxed and regularized problem by

using the above enhanced regularizer D√∞yj√∞t√ûjjyj√∞t √Ä 1√û√û to approximate the time-coupling term ¬Ωyj√∞t√û √Ä yj√∞t √Ä 1√û¬ä√æ in

(5). Though PRr is still time-coupling, the Karush-Kuhn-

Tucker (KKT) optimality conditions [35] of the regularized

4.2 Rounding Algorithm for Resource Pr√Äovisionin√Åg The Algorithm 1 obtains a fractional solution ex√∞t√û; ey√∞t√û of problem PRrt, where the integer constraint (8e) is relaxed to
yj√∞t√û 2 ¬Ω0; Ej¬ä; 8t 2 T ; 8j 2 J . In order to satisfy the practical physical meaning of the variable yj√∞t√û, i.e., the number of model instances launched for DNN model j, we now design

problem in each time slot yield a lower bound on the perfor- a rounding algorithm that rounds the optimal fractional

mance of the online algorithm for solving a series of single- solution ey√∞t√û to an integer solution y√∞t√û. A straightforward

shot problems. So we temporally decouple PRr into a series solution is the independent randomized rounding scheme of single-shot convex programs PRrt, which can be solved in [37], whose basic idea is to round up each fractional ey√∞t√û to

each individual time slot t based the solution obtained from the previous time slot t √Ä 1. SpeciÔ¨Åcally, the decomposed subproblem PRrt for each time slot t 2 T can be denoted as
follow:

the nearest integer y√∞√àt√û ¬º dey√∞t√ûe w√âith the probability of ey√∞t√û √Ä bey√∞t√ûc, i.e., Pr y√∞t√û ¬º dey√∞t√ûe ¬º ey√∞t√û √Ä bey√∞t√ûc, and round down ey√∞t√û to the nearest integer y√∞t√û√à¬º bey√∞t√ûc with√â the probability of dey√∞t√ûe √Ä ey√∞t√û, i.e., Pr y√∞t√û ¬º bey√∞t√ûc

PRrt :

min

vX j2√ÅJAhscjjc√∞t√Ä√ûy√æj√∞tC√û E√æO

√∞t√û √æ CCO√∞t√û√æ

√Å 

ln

yj yj√∞t

√∞t√û √æ √Ä 1√û

 √æ



√Ä

yj

 √∞t√û ;

s.t. Constraint (8a) to (8d);

¬º dey√∞t√ûe √Ä ey√∞t√û. Although the above independent rounding scheme can
always generate a feasible integer solution, since the central cloud can cover all the unserved requests incurred by rounding down yej√∞t√û, directly applying this scheme may incur high edge operational cost (round up all fractional

yj√∞t√û 2 ¬Ω0; Ej¬ä; 8j 2 J :

yej√∞t√û) or cloud outsourcing cost (round down all fractional yej√∞t√û). To solve the above challenge, we develop a random-

We design FAAS - Fractional Algorithm for joint conÔ¨Åguration ized and dependent pairwise rounding scheme [38] which

Adaption and model Selection as shown in Algorithm 1. Our can exploit the inherent dependence of the variables yej√∞t√û. Algorithm 1 FAAS solves a standard convex problem PRrt in The basic idea is that, each rounded-down variable will be
each slot and outputs the fractional solution ex√∞t√û; ey√∞t√û. compensated by another rounded-up variable, ensuring

Therefore, the time complexity analysis of Algorithm 1 is that all requests received by the edge server could be fully

equivalent to the time complexity of solving the convex processed even after the rounding phase. Such a dependent

problem PRrt. Since the problem PRrt is a standard convex pairwise rounding scheme can more reasonably round vari-

optimization with linear constraints, it can be optimally ables to satisfy the edge server capacity constraint, reducing

solved in polynomial time by taking existing convex optimi- the cost of the expensive cloud outsourcing or launching an

zation techniques such as interior-point method [36]. And excessive amount of DNN instances at the edge.

the complexity analysis of interior-point method is given in Based on the above analysis, we design RARP - Rounding

Section 11.5 of reference [35].

Algorithm for Resource Provisioning which rounds the frac-

In our online scenario, future information includes the tional solution and is shown in Algorithm 2, it runs in each

Ô¨Çuctuating operational cost bj√∞t√û caused by real-time electricity price and inference request arrivals Ai√∞t√û. At the same time, the online algorithm also utilizes the solu-
tion of the previous time slot. Therefore, FAAS Ô¨Årst observes A√∞t√û; b√∞t√û and ey√∞t √Ä 1√û at each time slot t 2 T , where ey√∞t √Ä 1√û has been obtained when solving PRrt√Ä1 at
time slot t √Ä 1. T√Ähen, FA√ÅAS computes the optimal fractional solution ex√∞t√û; ey√∞t√û for the current time slot t.
Obviously, the optimal solution of the relaxed and regularized problem PRrt is a feasible solution of the relaxed

time slot and returns the integer result of the fractional solu-

tion sets

Jey√∞√ætt√û¬ºo√àbjtajiyenje√∞dt√û

b2yZF√âAaAndS.JF√Ätirs¬ºt,

√àwe jj

yeijn√∞ttr√ûo2duRc√æe√âtwtoodienndoetxe

the set of DNN models with integral yej√∞t√û and the set of

sDloNt,NsomothdaetlswweithhafvreacJtio√æt nSal

yej√∞t√û respectively for each time

J

√Ä t

¬º

J

in

each

time

slot.

For

each element j 2 J √Ät , we further introduce a probability

coefÔ¨Åcient pj and a weight coefÔ¨Åcient vj associated with it. Here we deÔ¨Åne pj ¬º yej√∞t√û √Ä byej√∞t√ûc and vj ¬º ej; 8j 2 J √Ät .

Next, the algorithm runs a series of rounding iterations. At

problem PR. Later, we will derive the competitive ratio each iteration, it randomly selects two elements j1 and j2

of FAAS in Section 5.2.

from J √Ät , and let the probability of these two elements

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5877

round to 0 or 1, which is decided by the coupled coefÔ¨Åcient

‚Äô1

and

‚Äô2.

Finally,

if

J

√Ä t

has

only

one

element

in

the

last

iter-

ation, we directly round it up to integer.

√à

√â

√à

√â

Pr yj√∞√Ät√û ¬º dyej√∞t√ûe ¬º√Åyej√∞t√û √Ä byej√∞t√ûc and Pr yj√∞t√û ¬º byej√∞t√ûc

¬º 1 √Ä yej√∞t√û √Ä byej√∞t√ûc . Based on this marginal distribution

property which has been proven in [38], we can further

derive:

Algorithm 2. Rounding Algorithm for Resource Provisioning ‚Äî RARP

Input: J ; e; ey√∞t√û

Output: y√∞t√û

1:

Let

J

√æ t

¬º

√à j

j

yej√∞t√û

2

√â Z;

J

√Ä t

¬º

√à j

j

yej√∞t√û

2

R√æ

√â ;

2:

for each edge DNN model j

2

J

√æ t

do

3: Set yj√∞t√û ¬º yej√∞t√û;

4: end for

5:

for each DNN model j

2

J

√Ä t

do

6: Let pj ¬º yei√∞t√û √Ä byej√∞t√ûc; vj ¬º ej;

7: end for

8:

while

jJ

√Ä t

j

>

1 do

9: 10: ‚Äô2 ¬º 11:

mDWRiaeninÔ¨Åt√àhdnpotejh1m‚Äô;evv1lpyjj21¬ºrs√∞oe1mbl√Äeaincbtp√àiljt1i2wt√û√Äy√âo;‚Äôpe1‚Äôj√æl12e‚Äô;m2vvjjse21enpttjs2 √âj,1;

j2

from

J

√Ät ;

12:

pj1

¬º

pj1

√æ

‚Äô1; pj2

¬º

pj2

√Ä

vj1 vj2

‚Äô1;

13:

With

the

probability

‚Äô1 ‚Äô1 √æ‚Äô2

set

14:

pj1

¬º

pj1

√Ä

‚Äô2; pj2

¬º

pj2

√æ

vj1 vj2

‚Äô2;

15: If pj1 2 f0; 1g, then set yj1 √∞t√û ¬º byej1 √∞t√ûc √æ pj1 ,

16:

J

√æ t

¬º

J

√æ t

[

fj1g;

J

√Ä t

¬º

J

√Ä t

n

fj1g;

17: If pj2 2 f0; 1g, then set yj2 √∞t√û ¬º byej2 √∞t√ûc √æ pj2 ,

18:

J

√æ t

¬º

J

√æ t

[

fj2g;

J

√Ä t

¬º

J

√Ä t

n

fj2g;

√à √â√Ä

√Å

E yj√∞t√û ¬º ye√Çj√∞t√û √Ä byej√∞t√ûc dyej√∞t√ûe√É

√æ√Ä 1 √Ä √∞yej√∞t√û √Ä √Åbyej√∞t√ûc√û byej√∞t√ûc

¬º ye√Çj√∞t√û √Ä byej√∞t√ûc √∞byej√∞t√ûc√É √æ 1√û

√æ 1 √Ä √∞yej√∞t√û √Ä byej√∞t√ûc√û byej√∞t√ûc ¬º yej√∞t√û:

The above equation shows that the expectation of each

rounded solution is exactly the value of the original frac-

tional solution. This property indicates that additional costs

with launching new DNN instances would not be incurred

after rounding the fractional solution.

Our Algorithm 2 RARP rounds the fractional solution

ey√∞t√û obtained by Algorithm 1 to the integer in each slot

through the iterations of the main loop. The main loop of

RARP

in

each

time

slot

needs

to

be

executed

at

most

jJ

√Ä t

j

times, and jJ √Ät j J. Thus, the time-complexity of Algo-

rithm 2 is O√∞J√û, which is independent of the scale of the

requests arrival. Later, we will discuss the optimality gap

between the rounded solution and the optimal fractional

solution in Section 5.3.

4.3 Fractional Solution Relocation
After performing the Algorithm 2 at each time slot t, the resource provision decision y√∞t√û produced by RARP together with the model selection decision ex√∞t√û may well not be a feasible solution of the original problem P. This means

19: end while

20:

if

jJ

√Ä t

j

¬º

1 then

21:

Set

yi√∞t√û

¬º

dyei√∞t√ûe

for

the

only

element

j

2

J

√Ä t

;

22: end if

that we further need to modify the fractional model selection decision ex√∞t√û to maintain the solution feasibility.
In order to obtain the model selection decision after rounding fractional solution ey√∞t√û, a naive approach is to bring the feasible rounded solution y√∞t√û back to the original

The proposed pairwise rounding scheme would not
aggressively launch new DNN instances or outsource
unserved requests to the central cloud, and this is achieved
by maintaining three desirable properties in the main loop
of each iteration.
1) Continuous Reduction Property. At least one of two selected variables yej1 √∞t√û and yej2 √∞t√û is rounded into integer. For example, if ‚Äô1 ¬º 1 √Ä pj1 and ‚Äô2 ¬º pj1 (line 10), then pj1 ¬º 1 if line 12 is executed or pj1 ¬º 0 if line 14 is executed. In both two cases, yej1 √∞t√û will be rounded to an integer.
2) Weight Conservation Property. After the main loop of
each iteration, the total weighted resource capacity of the
selected two elements (i.e., DNN instances) remains

problem P, and solve the degraded√Ä relocatio√Ån problem to obtain a complete feasible solution x√∞t√û; y√∞t√û . After bringing the rounded solution y√∞t√û, the edge operational cost and instance switching cost in the objective function of the original problem become constant. However, if we further consider the weight conservation property maintained by the rounding scheme in RARP, we could obtain a complete feasible solution more simply. SpeciÔ¨Åcally, the weight conservation property ensurPes that the total resource capacity of the rounded solPution j ejyj√∞t√û is equal to that of the fractional solution j ejyej√∞t√û. Therefore, we can solve the following simpliÔ¨Åed relocation problem without considering the cloud outsourcing cost to obtain x√∞t√û.

unchanged, in other words, the sum of yej1 √∞t√ûej1 √æ yej2 √∞t√ûej2 Ss√Çe‚Äôyteri2amtj√É1yye√∞isjtl1ia√ûnc√ær√æodlyni√Ç‚Äôc,syea1tjia√Ét2fee√∞nstjt1l√û.itn√æ√æFheoaee√Çrtjjye121jea‚Äô32xd√∞2ta√Édi√ûesmij√Ät2pieo¬ºeexlnjjee12,ayce‚Äôliuj1f1√Étc√∞leeltiodj√ûn2eu,ej¬ºd11w√æ1yeoejuiye1s√∞jta2stel√∞√ûosxteou√ûeje1rcjchu√æ2i.anteyvegTdje1h,c√∞itwo√Çs√ûyeseejtj1p2h√∞w.rtao√ûivpt√Ähe-

capacity reduction would not be incurred after rounding

the fractional solution.

3) Marginal Distribution Property. In each iteration of the

main loop, the probability of rounding up or down each ele-

ment

j

2

J

√Ä t

is

determined

by

the

fractional

part

yej√∞t√û √Ä

byej√∞t√ûc of the fractional solution yej√∞t√û. More speciÔ¨Åcally,

PRet :

min Acc√∞t√û XX

s.t.

xijk√∞t√û Ai√∞t√û;

X j2J X k2K

xijk√∞t√ûfjk yj√∞t√ûej;

Pi2I k2KP

Pj2J Pk2K xijk√∞t√ûdjk j2J k2K xijk√∞t√û

Li;

xijk√∞t√û ! 0;

8i; (10a) 8j; (10b)
8i; (10c) 8i; 8j; 8k:
(10d)

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5878

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

Fig. 3. An illustration of the basic idea of the performance analysis.
Since the above problem is a linear programming, therefore its optimal solution can be readily obtained in polynomial time. Our relocation scheme can work as a bridge to analyze the performance bound of the aforementioned na¬®ƒ±ve approach, as we will discuss later in Section 5. The rounded solution y√∞t√û obtained by RARP and the relocation solution obtained by solving the above problem together constitute the Ô¨Ånal solution of our online approach. In the next section, we will discuss the competitive ratio between the proposed online approach and the theoretical optimal solution in ofÔ¨Çine case.
5 PERFORMANCE ANALYSIS

techniques used in competitive analysis [40]. SpeciÔ¨Åcally, the inequality PoRpt ! DoRpt holds due to the celebrated Weak Duality in convex optimization theory [35], where DoRpt denotes the objective value of the dual problem achieved by the optimal solution. To connect the dual problem DR and the regularized problem PRr, we construct a feasible solution for DR mapped from the optimal fractional solution for PRrt. Based on such mapping, we can derive the competitive ratio r1 by applying Karush-Kuhn-Tucker (KKT) conditions [35] in Section 5.2. Finally, based on the three desirable properties maintained by the rounding scheme and the constraints in relocation problem PRe, we can further prove (11a) (11b) and characterize the competitive ratio r2 between the optimal fractional solution and the rounded solution after relocation in Section 5.3.
Theorem 1. PoRpt Popt since PR is a relaxed problem after relaxing the integer constraint in the original minimization problem P.

Proof. See Appendix B, available in the online supplemen-

tal material.

tu

In this section, we rigorously analyze the theoretical performance of the proposed online framework of EdgeAdaptor via competitive analysis. As a standard approach to quantifying the performance of online algorithms [39], the basic idea of the competitive analysis is to compare the performance of the online algorithm to the theoretical optimal solution in the ofÔ¨Çine case, where all the future information is given as prior knowledge. In particular, we prove that the proposed online algorithm has guaranteed performance, which is quantiÔ¨Åed by a worst-case parameterized competitive ratio.

5.1 The Basic Idea

As illustrated in Fig. 3, we introduce the dual problem of the relaxed problem DR to act as the bridge that connects the original problem and regularized problem. We formally prove that the objective value of the original problem evaluated with solutions produced by our proposed online approach, which is denoted as P , is upper-bounded by a parameterized constant multiple the ofÔ¨Çine optimum of the original problem. We will establish the following chain of inequalities:

√Ä

√Å

P x√∞t√û; y√∞t√û

r2 r1

Pe√Äex√∞t√û;

√Å ey√∞t√û

r2DeR

√Ä nj

√∞t√û;

ai√∞t√û;

bj√∞t√û;

g

i

√∞t√û;

j

√Å √∞t√û

r1r2PoRpt

r1r2Popt;

(11a) (11b) (11c) (11d) (11e)

5.2 Competitive Ratio of FAAS

We establish (11b) (11c) and derive the competitive ratio

r1 of FAAS in this subsection. SpeciÔ¨Åcally, we Ô¨Årst transform the relaxes problem equivalently to drive its Lagrange

problem. Then, we characterize the optimality conditions for a series of regularized problems PRrt. Finally, we con-
struct a feasible solution for DR mapped from the optimal primal and dual solutions for PRrt.
An Equivalent Problem Transformation. Due to the timecoupling switching cost CItS and the boxing constraints of decision variables yj√∞t√û, directly deriving the Lagrange dual
problem DR for the relaxed problem PR is not straightfor-
ward. In response, we Ô¨Årst introduce a set of auxiliary varia-

bles wj√∞t√û which satisfy wj√∞t√û ! yj√∞t√û √Ä yj√∞t √Ç√Ä 1√û; 8j 2 J ;

18yj√ût√∞√É2t√æ√Ä.TT1h√ût√Éeo√æ

replace the transformed is equivalent

time-coupling switching cost to the original

term CItS ¬º

yPj√∞t√û √Ä√Ç yj√∞t j sj yj√∞t√û

√Ä √Ä

expression since the

additional constraints wj√∞t√û ! 0; 8j 2 J ; 8t 2 T . For the box-

ing constraints yj√∞t√û 2 ¬Ω0; Ej¬ä, as suggested by the literature

[P16], we replace it by a setPofPknaPpsack cover (KC) constraints j02J yj0 √∞t√ûej0 √Ä yj√∞t√ûej ! i j k xijk√∞t√ûfjk √Ä Ejej; 8j 2 J ;

8t 2 T . With the above transformations, we rewrite the

relaxed problem in the following equivalent form.

X&

X

'

min

v √Å Acc√∞t√û √æ CEO√∞t√û √æ CCO√∞t√û √æ sjwj√∞t√û

t2T

j

where r ¬º r1r2 is the overall competitive ratio. Here Popt and s.t. wXj√∞tX √û ! yj√∞t√û √Ä yj√∞t √Ä 1√û; 8t; j;

PoRpt denote the objective value achieved by the optimal solu-

xijk√∞t√û √Ä Ai√∞t√û 0; 8t; i;

tion of the original problem P and the relaxed problem PR, respectively. And DeR denotes the objective value of PR‚Äôs

X j2J X k2K xijk√∞t√ûfjk yj√∞t√ûej; 8t; j;

dual problem DR, achieved by a feasible solution mapped from the optimal fractional solution and dual solution of the single-shot regularized problem PRrt. Finally, Pe and P denote the objective value of the original problem P achieved by the optimal fractional solution and the rounded solution after

X i2I X k2K

XX

xijk√∞t√ûdjk

xijk√∞t√ûLi; 8t; i;

X j2J kX 2K X

j2J k2K

X i j

k xijk√∞t√ûfjk √æ yj√∞t√ûej

relocation, respectively. We Ô¨Årst prove (11d) (11e) in Theorem 1. Next, the proof
of (11b) (11c) (11d) is based on the online primal-dual

√Ä j02J yj0 √∞t√ûej0 √Ä Ejej 0; 8t; j; xijk√∞t√û; yj√∞t√û; wj√∞t√û ! 0; 8t 2 T ; j 2 J :

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

(12a) (12b) (12c) (12d)
(12e) (12f)

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5879

Deriving the Lagrange Dual Problem. We use nj√∞t√û, ai√∞t√û, bj√∞t√û, gi√∞t√û, j√∞t√û to denote the corresponding dual variables for the constraints (12a) to (12e). Now we can derive the
Lagrange dual problem DR of the above transformed prob-
lem (and also equally the problem PR) as follows.

X&X

√Ä

√ÅX

'

DR : max

Ai√∞t√û ci √Ä ai√∞t√û √Ä j√∞t√ûEjej

t2T i2I

j2J

s.t. sj √Ä nj√∞t√û ! 0; 8t 2 T ; j 2 J ;

aijk √Ä ci √æ ai√∞t√û √æ bj√∞t√ûfjk

√æ gi√∞t√û√∞djk √Ä Li√û √æ j√∞t√ûfjk ! 0; 8t; i; j; k;

bj√∞t√û

√æ nj√∞t√û X

√Ä

nj

√∞t

√æ

1√û

√Ä !

bj√∞t√ûej

√Ä ej j0 j0 √∞t√û √Ä j√∞t√û ! 0; 8t; j;

(13a) (13b)

All the dual variables ! 0:

(13c)

Characterizing the Optimality of the Regularized Problem.

Recall that in Section 4.1, we regularize the relaxed problem

PR to a solvable √Äconvex pr√Åoblem PRr, so that the optimal fractional solution ex√∞t√û; ey√∞t√û obtained by Algorithm 1 FAAS

satisÔ¨Åes the Karush-Kuhn-Tucker (KKT) conditions, i.e., the

Ô¨Å√Äaersi√∞tt-√ûo;rbedje√∞rt√ûn; geeci√∞ets√ûs;aerjy√∞tc√û√Åontodidtieonnostefotrheopotpimtimalaitlys.oHluetrioenwteo

use the

dual problem of the regularized problem PRrt, correspond-

ing to constraints (12b) to (12e) respectively. To simplify the

presentation, we write some KKT conditions in the disjunc-

tive form, where a?b is equivalent to a; b ! 0 and ab ¬º 0.

Therefore, the KKT conditions of the optimal fractional

solution can be readily obtained as follows.

in which we let

nj√∞t√û

¬º

sj hj

ln

Ej √æ  yej√∞t √Ä 1√û √æ



;

ai√∞t√û

¬º

aei√∞t√û;

bj√∞t√û ¬º bej√∞t√û; gi√∞t√û ¬º egi√∞t√û; j√∞t√û ¬º ej√∞t√û:

Lemma

1.

The

mapping

√à P ex√∞t√û;

ey√∞t√û;

aei√∞t√û;

bej√∞t√û;

egi√∞t√û;

ej

√â √∞t√û

obtains a feasible solution of the relaxed dual problem DR.

Proof. See Appendix C, available in the online supplemen-

tal material.

tu

Due to this feasibility, we know that the objective value DeR of the relaxed dual problem DR is no larger than the optimal objective value DoRpt. Recall the inequality PoRpt ! DoRpt based on the Weak Duality in convex optimization theory, we can derive (11c) (11d). Meanwhile, through the KKT conditions, we can also derive the optimality gap between DeR and Pe. To further obtain the competitive ratio r1 of the proposed Algorithm 1 FAAS, we Ô¨Årst show that the total non-switching cost (i.e., the objective value excludes the switching cost) is upper bounded by the objective value DeR of the relaxed dual problem DR, as shown by the following Lemma 2.

Lemma 2. The sum of edge operational cost, cloud outsourcing cost and cumulative accuracy loss achieved by Algorithm 1 FAAS is no larger than the objective value of the relaxed dual problem DR achieved by the constructed feasible solution P, i.e.,
X √∞Acc√∞t√û √æ CEO√∞t√û √æ CCO√∞t√û√û DeR:
t2T



XX



aei√∞t√û? Ai√∞t√û √Ä 

Xj

Xk xeijk√∞t√û

; 8t; i 

ebj√∞t√û? yej√∞t√ûej √Ä X X

i

k

xeijk

√∞t√ûfjk 

; 8t; j

egeij√∞√∞tt√û√û??XXjj0

k√∞Li √Ä djk√ûxeijk√∞t√û ; 8t;

yej0 √∞t√ûej0 √æ Ejej √Ä yej√∞t√ûej

XX



i

√Ä i j k xeijk√∞t√ûfjk ; 8t; j

aijk √Ä ci √æ aei√∞t√û √æ bej√∞t√ûfjk √æ egj√∞t√û√∞djk √Ä Li√û

(14a) (14b) (14c)
(14d)

The instance switching cost is also bounded, as shown by

the following Lemma 3.

Lemma

3.

The

total

instance

switching

cost

P
t

CItS

of

the

frac-

ttihoannal¬Ωlnso√Äl1u√ætioEnm√àaaxc√Åh√æievEemddaxb¬ä ytimAelsg√âoorfitDehRm,

1 FAAS is no larger where Emax ¬º maxjEj

and d ¬º mint;j yej√∞t√û; yej√∞t√û > 0

X CItS
t2T

√Ä ln 1

√æ

Emax√Å 

√æ

!

Emax d

DeR:

√æ ej√∞t√û ¬º 0; 8t; i; j; k

(14e)

We give a detailed proof of Lemmas 2 and 3 in Appendix

bj√∞t√û

√æ

sj hj

ln

yej yej√∞t

√∞t√û √æ  √Ä 1√û √æ X



√Ä

bej

√∞t√ûej



√Ä ej

j0 ej0 √∞t√û √Ä ej√∞t√û ¬º 0; 8t; j

(14f)

D and Appendix E respectively, available in the online supplemental material.
Theorem 2. The objective value Pe of problem P through the optimal fractional solution achieved by our proposed Algorithm 1

FAAS is no larger than r1 times of the ofÔ¨Çine optimum Popt,

Constructing the Mapping. After characterizing the optimality conditions for a series of regularized problems PRrt, we now construct a mapping to jointly map PRrt‚Äôs optimal
primal and dual solutions to a feasible solution of the

where r1 is given by:





r1 ¬º ln

1 √æ Emax 

√æ Emax √æ 1: d

relaxed dual problem DR as follows:

Proof. Since the objective value of the original problem P is

√à P ex√∞t√û;

ey√∞t√û;

aei√∞t√û;

bej

√∞t√û;

egi

√∞t√û; √Ä

ej√∞t√û√â

√Å

¬º nj√∞t√û; ai√∞t√û; bj√∞t√û; gi√∞t√û; j√∞t√û ;

the sum of edge operational cost, cloud outsourcing,

instance switching cost and cumulative accuracy loss.

According to Lemmas 2 and 3, we can directly derive

Theorem 2.

tu

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5880

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

5.3 Integrality Gap of RARP and Relocation

In this subsection, we establish (11a) (11b) and study the

rounding gap incurred by Algorithm 2 RARP and reloca-

tion, in terms of competitive ratio r2 of the objective value P achieved by the Ô¨Ånal rounded solution to the objective value Pe achieved by the fractional solution. Recall the relationship between ey√∞t√û and y√∞t√û, which has been characterized by the

weight conservation property in Section 4.2. SpeciÔ¨Åcally,

after the main loop of each iteration, the total resource

capacity

of

the

two

selected

elements

j1;

j2

2

J

√Ä t

keeps

unchanged, i.e., yej1 √∞t√ûej1 √æ yej2 √∞t√ûej2 ¬º yj1 √∞t√ûej1 √æ yj2 √∞t√ûej2 .

Based on this equation, we know that after the execution of

RARP, for the integral elements j 2 J √æt , we have the deter-

ministic equation

X

X

yj√∞t√ûej ¬º

yej√∞t√ûej:

j2J

√æ t

j2J

√æ t

Then, we further take this connection as a bridge to

bound the terms in the objective function of the original

problem P. There is at most one element remaining in the

fractional

set

J

√Ä t

after

the

execution

of

RARP,

i.e.,

jJ

√Ät j

1.

SpeciÔ¨Åcally,

if

jJ √Ät j ¬º 1,

for

the

only

element

in

J

√Ä t

,

by

deÔ¨Åning kt ¬º PimPaxjj2PJ Ekjexeijjk√∞t√û , we have

X

j2J

√Ä t

yj√∞t√ûej XX

max
j2J
X

Ej

ej

¬º kt

xeijk√∞t√û

i jk

X kt yej√∞t√ûej:
j

Note that when there is no elementPin J √Ät , i.e., jJ √Ät j ¬º 0, the

above inequality still holds since

j2J

√Ä t

yj√∞t√ûej

¬º

0.

Com-

bining the above equation and inequality, we further have

X

X

X

yj√∞t√ûej ¬º

yj√∞t√ûej √æ

yj√∞t√ûej

j2J

jX 2J √æt

j2JX √Ät

yej√∞t√ûej √æ kt yej√∞t√ûej

jX 2J √æt

j2J
X

yej√∞t√ûej √æ kt yej√∞t√ûej

j2J

X

j2J

¬º √∞1 √æ kt√û yej√∞t√ûej:

j2J

By summing over the above inequality over all the time slots t 2 T and deÔ¨Åning k ¬º maxt2T kt, we Ô¨Ånally have

XX

X

X

yj√∞t√ûej

√∞1 √æ kt√û yej√∞t√ûej

t2T j2J

t2T

X X j2J

√∞1 √æ k√û

yej√∞t√ûej:

(15)

t2T j2J

X
t2T

X
j2J

yj√∞t√ûbj√∞t√û

¬º

X
t2T

X
j2J

yj√∞t√ûej

bj√∞t√û ej

max
t;j

bj√∞t√û ej

X
t2T

X
j2J

yj√∞t√ûej

max
t;j

bj√∞t√û ej

√∞1

√æ

k√û

X
t2T

X
j2J

yej

√∞t√ûbj

√∞t√û

ej bj√∞t√û

√∞1

√æ

k√û

max
t;j

bj√∞t√û ej

max
t;j

ej bj√∞t√û

X
t2T

X
j2J

yej√∞t√ûbj

√∞t√û

(16)

Upper Bound of the Instance Switching Cost. Based on the

inequality (15), we bound the instance switching cost as fol-

lows:

X

X

sj

√Ç yj

√∞t√û

√Ä

yj√∞t

√Ä

1√û√É√æ

t2T j2J

X
t2T

X
j2J

sjyj√∞t√û

¬º

X
t2T

X
j2J

ejyj√∞t√û

sj ej

max
j

sj ej

X
t2T

X
j2J

yj√∞t√ûej

√∞1

√æ

k√û

max
j

sj ej

X
t2T

X
j2J

yej√∞t√ûbj√∞t√û

ej bj√∞t√û

√∞1

√æ

k√û

max
t;j

ej bj√∞t√û

max
j

sj ej

X
t2T

X
j2J

yej

√∞t√ûbj√∞t√û:

(17)

Upper Bound of the Cumulative Accuracy Loss. Based on the constraint (10b) in relocation problem PRet and the inequal-
ity (15), the cumulative accuracy loss can be bounded as fol-

lows:

XXXX

aijkxijk√∞t√û

t ijk

max
i;j;k

aijk

X
t

X
i

X Xj

X Xk

xijk

√∞t√ûfjk

1 fjk

max aijk min fjk

i;j;k

j;k

t

yj√∞t√ûej
j

√∞1

√æ

k√û

max
i;j;k

aijk

min
j;k

fjk

X
t

X
j

yej

√∞t√ûbj√∞t√û

ej bj√∞t√û

√∞1

√æ

k√û

max
t;j

ej bj√∞t√û

max
i;j;k

aijk

min
j;k

fjk

X
t

X
j

yej√∞t√ûbj√∞t√û:

(18)

Theorem 3. The objective value P of problem P through the Ô¨Ånal rounded solution achieved by our Algorithm 2 RARP and relocation scheme is no larger than r2 times of Pe achieved by the optimal fractional solution, where r2 ¬º 1 √æ 2 √æ 3, and

1

¬º

√∞1

√æ

k√û max
t;j

bj√∞t√û max ej t;j

ej bj√∞t√û

;

2

¬º

√∞1

√æ

k√û max
t;j

ej max bj√∞t√û j

sj ej

;

3

¬º

√∞1

√æ

k√û

max
t;j

ej bj√∞t√û

max
i;j;k

aijk

min fjk:
j;k

Proof. Recall the weight conservation property in Section 4.2,

Upper Bound of the Edge Operational Cost. Based on the the total resource capacity of the rounded solution is no

inequality (15), we bound the edge operational cost as fol- smaller than that of the optimal fractional solution. There-

lows.

fore, when relocating the decision variables ex√∞t√û after
Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5881

rounding decision variables ey√∞t√û, we do not outsource additional requests to the central cloud, meaning that the

TABLE 2 Accuracy Loss aijk

cloud outsourcing cost of the Ô¨Ånal rounded solution is the

same with that of the optimal fractional solution. Combin-

ing the upper bounds of several other terms in the objec-

tive function (16) to (18), we can derive the competitive

ratio of the Ô¨Ånal rounded solution after the rounding

scheme and relocation.

tu

J YOLOv2

I K
240p 360p 540p 720p

people
0.287 0.238 0.195 0.187

car
0.329 0.283 0.242 0.235

bus
0.300 0.252 0.210 0.202

bike
0.281 0.231 0.188 0.180

dog
0.216 0.163 0.116 0.107

5.4 The Overall Competitive Ratio
Theorem 4. The objective value P of problem P achieved by the Ô¨Ånal rounded solution is no larger than r1r2 times of the ofÔ¨Çine optimum, where r1 and r2 are derived in Theorem 1 and Theorem 2, respectively. That is, our proposed online scheme for problem P achieves a competitive ratio of r1r2.
Proof. Based on the inequality (11a) to (11e) and the com-

SSD R-FCN

240p 360p 540p 720p
240p 360p 540p 720p

0.206 0.184 0.167 0.164
0.242 0.197 0.156 0.149

0.239 0.201 0.185 0.182
0.272 0.229 0.189 0.183

0.206 0.199 0.183 0.180
0.267 0.224 0.184 0.178

0.199 0.193 0.177 0.174
0.251 0.207 0.166 0.160

0.130 0.134 0.116 0.112
0.167 0.118 0.073 0.066

petitive ratio given in Theorem 2 and Theorem 3, we can

directly derive the overall competitive ratio of our pro-

posed online approach.

tu

The overall gap between our proposed online algorithms

and the ofÔ¨Çine optimum mainly comes from two parts: 1)

The long-term optimization problem is solved online in each

time slot without future information such as operational cost

bj√∞t√û and inference request arrivals Ai√∞t√û, which is reÔ¨Çected

in the competition ratio r1 of Algorithm 1. 2) Rounding the

fractional solution to the integer solution for practical physi-

cal meaning incurs additional edge operational cost (round

up yej√∞t√û) or cloud outsourcing cost (round down yej√∞t√û), which

is reÔ¨Çected in the competition ratio r2 of Algorithm 2. Intui-

tively, our online approach stabilizes the solution by avoid-

ing drastic shifts in the solution between adjacent time slots,

thus addressing the online scenario with the unknown future

information. Meanwhile, the online algorithm obtains a good

competitive bound as shown in Theorem 4.

For the overall competitive ratio given in Theorem 4, we

now discuss some insights as follows: 1) The Ô¨Ånal competi-

tive ratio r1r2 decreases with the increasing of the parameter

. By increasing  to be large enough, we can obtain the com-

petitive

ratio

r1

that

is

arbitrarily

close

to

Emax d

√æ

1,

but

at

the

same time with the increasing of the time complexity of the

regularized problem PRr. 2) Compared to the naive approach

that directly brings y√∞t√û back to the original problem P to

obtain the complete feasible solution, our relocation scheme

has lower time complexity but also reduced optimality. How-

ever, in a realistic edge-cloud system, the cloud outsourcing

cost is far more expensive than edge-based processing.

detection targets, we deÔ¨Åne Ô¨Åve types of inference requests are people, car, bus, bike, and dog respectively (I ¬º 5). For the convex problem PRrt at each time slot, we solve it with CVXPY [41] based on Python 3 by invoking the interiorpoint solver MOSEK [42].
Request Traces. Since edge intelligence is still in a very early stage, there is no publicly accessible inference request trace from the edge server. Moreover, for the protection of user data privacy, there is also no similar request trace with speciÔ¨Åc task type information. Therefore, in line with the recent work [27] on DNN model inference serving, we adopt workload trace of computing clusters to simulate the number of different types of inference requests arriving at the edge server. SpeciÔ¨Åcally, we randomly select Ô¨Åve different jobs from the 24-hour traces of Alibaba production cluster [43], and then expand these Ô¨Åve kinds of workload traces in different proportions to a reasonable range according to the actual situation of each type of inference request in the real world.
Accuracy and Delay. To obtain the accuracy and delay parameters of the object detection task at the edge server, we select three currently popular object detection models YOLOv2 [13], SSD [14], R-FCN [15]. SpeciÔ¨Åcally, for each DNN model, we calculate the parameters aijk based on the accuracy of different targets (i.e., people, car, etc.) and the parameters djk based on the FPS (frames per second) of images with different conÔ¨Ågurations (i.e., 240p, 360p, etc.) through the curve Ô¨Åtting method [23]. The service latency constraints Li of different object detection tasks are increased from 30ms to 150ms according to actual demands. SpeciÔ¨Åc parameters are listed in Tables 2 and 3.

6 PERFORMANCE EVALUATION
In this section, we evaluate the practical performance of the proposed online optimization framework through tracedriven simulations. The simulations are based on real-world request traces and inference accuracy and delay of object detection tasks.

Cost Parameters. We simulate the DNN instance running cost bj√∞t√û through the parameter amount of each DNN model and the electricity price with temporal diversities. As we mentioned in Section 3.3, there is the numerical relationship between service latency and price [28]. On the basis of the price of the serverless scheme (i.e., AWS Lambda [44]), we add the penalty incurred by the latency of data transmission

6.1 Experimental Setup

to the unit cloud outsourcing cost ci, and the value of the penalty depends on the service latency constraints of different

We simulate an edge server with three DNN models object detection tasks. SpeciÔ¨Åcally, every 30ms corresponds (J ¬º 3), corresponding to each model, the input images con- to 20% of the base price. Inspired by the experimental setup

tain different conÔ¨Åguration resolution of 240p, 360p, 540p, in [45] and [46], we normalize the unit switching cost sj by and 720p (K ¬º 4). According to different types of object using the mean of the operational cost bj√∞t√û. SpeciÔ¨Åcally, we
Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5882
DNN Model YOLOv2 SSD R-FCN

TABLE 3 Instance Latency djk

240p
11.0 ms 21.7 ms 76.9 ms

ConÔ¨Åguration

360p

540p

12.3 ms 24.6 ms 77.7 ms

16.9 ms 52.6 ms 85.0 ms

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023
720p 25.0 ms 80.4 ms 92.7 ms

√à√â set sj ¬º E bj√∞t√û , which corresponds to the operational cost to running one model instance in each time slot.
Capacity Constraints. The computing capability of different DNN model instances is difÔ¨Åcult to actually measure, so we refer to the model size of three DNN models and design the workload capacity ej of each DNN model instance based on this ratio in simulation experiment. The intuitive insight is that all containers with different DNN model have the same physical resources (e.g., CPU and memory). Therefore, the larger parameters amount of the loaded model in the container, the fewer requests processed per time slot. The degraded parameter ej represents the maximum number of requests processed by DNN model j in one time slot. As for Ej, the maximal number of available instances of DNN model j, we set it according to the peak-arrival of request traces at the edge server. Specially, we summary the DNN capacity conÔ¨Ågurations in Table 4.
Benchmarks. To demonstrate the efÔ¨Åcacy of the proposed online framework (FAAS+RARP), we compare it to the Online Lazy Switching Algorithm (OLSA) [47]. The basic idea of OLSA is to tolerate as much non-switching cost (i.e., the sum of edge operational cost, cloud outsourcing cost and cumulative accuracy loss) as possible until the non-switching cost signiÔ¨Åcantly exceeds the switching cost. OLSA is a stateof-the-art online algorithm with competitive analysis [48], which has been extensively applied to tackle problems with switching cost in recent literature [49], [50], [51]. However, the characteristic of OLSA is lazy switching, which makes it unable to cope well with the frequently changing request arrivals in the dynamic environment. Furthermore, to demonstrate the efÔ¨Åcacy of the rounding scheme RARP, we compare it to another two benchmarks: 1) EO-Greedy approach directly rounds up all the fractional solutions to process all received inference requests at the edge server. 2) CO-Greedy approach directly rounds down all the fractional solutions with the help of the central cloud to serve the inference requests which can not be covered by the edge server.
6.2 Evaluation Results
We use the competition ratio to measure the performance of the online algorithm, which is equal to the objective value achieved by the online algorithm divided by the ofÔ¨Çine

Fig. 4. Total cost versus T .
optimal value. In addition, we also use the following more intuitive quantitative indicators to measure the performance of our online algorithms: i) total cost, i.e., objective value in each time slot, ii) average total cost, i.e., total objective value over the entire time horizon averaged by T .
EfÔ¨Åciency of the Online Framework. We Ô¨Årst plot the total cost of the proposed online framework (FAAS+RARP) as well as the benchmark OLSA in each time slot as shown in Fig. 4, and we mark the average total cost of these two methods in 144 time slots. To further show the gap between our online algorithm and the ofÔ¨Çine optimal solution, we also plot the competitive ratio of the two online approaches in Fig. 5. From these two Ô¨Ågures, we observe that: 1) The proposed online approach (FAAS+RARP) has lower long-term total cost than OLSA, demonstrating the effectiveness of our proposed EdgeAdaptor. 2) The proposed online algorithm (FAAS+RARP) achieves a competition ratio of no more than 1.4 in different time horizons T , which is signiÔ¨Åcantly lower than that of OLSA, and achieves a maximum reduction of 30.8%. 3) As the number of total time slots varies, EdgeAdaptor has stable performance against uncertain inÔ¨Çuence caused by time-varying parameters (i.e., Ô¨Çuctuating workload and electricity price). The total cost of OLSA exhibits violent Ô¨Çuctuations in some time slots due to its lazy switching characteristic. The ‚Äúlaziness‚Äù of OLSA limits it to be unable to cope well with the frequently changing request arrivals in the dynamic environment compared to our proposed online approach.
EfÔ¨Åciency of the Rounding Scheme. For a more intuitive comparison of rounding scheme, we plot the average total cost of three different rounding approaches under varying numbers of total time slots in Fig. 6. It demonstrates that the average total cost of our proposed RARP outperforms those of the two benchmarks. However, the performance of RARP is not obvious compared with EO-Greedy. This is mainly because the number of DNN models is only 3 in our simulation setup, even if all the fractional solutions are rounded up, the

TABLE 4 DNN Model Instance ConÔ¨Åguration

DNN Model Workload Capacity ej Resource Capacity Ej

YOLOv2 SSD R-FCN

180

40

200

30

190

20

Fig. 5. Competitive ratio versus T .

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5883

Fig. 6. Average total cost versus T in rounding.

Fig. 8. Various cost versus resource capacity Ej.

additional instance running cost would not be too large. Actually, this gap will inevitably be further enlarged as the scale of edge servers increases. We also note that the average total cost gradually decreases with the increase of the total number of time slots. The reason for this phenomenon is that the decision variables are initialized to 0 in our simulation setup, thus a huge switching cost is generated in the Ô¨Årst time slot while algorithm running. In a reality system, we can preheat the system (e.g., implement some instances in advance) to avoid this terrible cold start phenomenon.
Effect of the Switching Cost. We next investigate the effect of the instance switching cost (i.e., sj in our formulation) on the average total cost of our online approach (FAAS+RARP) and OLSA, by multiplying the switching cost of each DNN instance with a various scaling ratio. The results are plotted in Fig. 7, which shows that as the switching cost increases, the average total cost of both our online approach and OLSA increase dramatically. The rationale is that, by increasing the scaling ratio of switching cost, the algorithms would pay more attention to minimize the total switching cost. However, since the switching cost term is time-coupling and involves future stochastic information, the optimality gap incurred by any online algorithm would increase as the switching cost sj grows. Nevertheless, we mark the ratio of the average total cost between two online approaches, which indicates that our proposed EdgeAdaptor can suppress these deteriorating increase compared to OLSA until the scaling ratio extends to 10.
Effect of the Edge Capacity. Recall that in Section 3.1, we use Ej to denote the resource capacity at the edge server, i.e., the maximal number of available instances of DNN models. Fig. 8 depicts the impact of Ej on the average total cost under different scaling ratios, especially the impact on edge operational cost, from which we observe that: 1) Increasing resource capacity of edge server can effectively reduce the proportion of expensive cloud outsourcing cost in the total system cost. 2) The average total cost of the

system is the smallest, when the scaling ratio is 0.5, i.e., the edge resource capacity is further limited. This is because that almost all containers on the edge server are running to provide inference services, so Ô¨Çuctuating workload will not generate switching cost (as shown by the green column in Fig. 8). Despite the high cost of cloud outsourcing, the rapid expansion of serverless technology makes it possible to reduce cold starts. 3) As the edge server resource capacity increases, the average total cost has a downward trend, however, this decline is quickly Ô¨Ålled by increased cumulative accuracy loss. Because compared with cloud outsourcing which does not consider the inference accuracy loss, increasing inference requests served at the edge server will inevitably lead to an increase on the cumulative accuracy loss.
Cost, Latency, and Accuracy. Fig. 9 shows the three-way tradeoff between resource cost, service latency, and inference accuracy. As the scaling ratio of Li increases (i.e., the tolerance for inference service latency increases), all three curves indicate that the total cost of the system decreases. And as the parameter v increases (i.e., the accuracy requirement of inference service increases), the total cost of the system increases between three curves. Although the plotted curves are in line with our intuitive perception, there is also an abnormal data point when v ¬º 5 and 0:5 √Ç Li. The reason may be that the algorithm tends to choose the cloud outsourcing policy under the Ô¨Çuctuating workload due to the dual effect of higher latency requirement and lower accuracy requirement, the expensive switching cost is avoided as a result. The convex relationship between the total cost and the inference service latency also brings inspiration for the deployment of realistic edge inference systems. In the range of 0.1 to 0.3, we can appropriately relax the latency requirement of inference service in exchange for a substantial reduction in system cost, while in the range of 0.5 to 3.0, we can choose a more stringent latency requirement without incurring much cost.

Fig. 7. Average total cost versus scaling ratio of sj.

Fig. 9. Cost versus Latency versus Accuracy.

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5884

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

Fig. 10. Distribution of execution time of our proposed online algorithms.
Algorithm Execution Time. Fig. 10 shows the distribution of execution time of our proposed online algorithms and OLSA. We observe that the empirical cumulative distribution function (CDF) curve of RARP is almost a vertical line, because its execution time in each slot is less than 1ms. The execution time of FAAS varies from 90ms to 200ms, and its average execution time over all 144 time slots is about 106ms. Our FAAS takes relatively much longer time to run than the rounding algorithm, because it involves a regularized convex problem to be solved. Our RARP takes negligible time to Ô¨Ånish, as it has a linear time complexity by design. Our online approach (FAAS+RARP) is extremely efÔ¨Åcient and scalable because the overall running times are much less than the commonly adopted length of 15 minutes of a time slot (recall that our algorithm runs once at each time slot), which indicates the real-time responses of our online algorithms to large-scale requests.
7 CONCLUSION
In this work, we propose an online optimization framework EdgeAdaptor for edge DNN inference serving at scale. It jointly optimizes the application conÔ¨Åguration adaption, DNN model selection and edge resource provisioning to judiciously navigate the three-way tradeoff between resource cost, inference accuracy and latency. Since the formulated optimization problem is NP-hard and involves future uncertain information, we carefully fuse the power of an online optimization technique and an approximate optimization method. SpeciÔ¨Åcally, with a regularization technique, we Ô¨Årst decompose the long-term problem into a series of one-shot fractional problems which can be readily solved. Then, applying a randomized dependent scheme, we round the fractional solutions to a near-optimal feasible solution of the original problem. Rigorous theoretical analysis and extensive trace-driven simulations demonstrate the efÔ¨Åcacy of our proposed framework.
REFERENCES
[1] J. Zhang and D. Tao, ‚ÄúEmpowering things with intelligence: A survey of the progress, challenges, and opportunities in artiÔ¨Åcial intelligence of things,‚Äù IEEE Internet Things J., vol. 8, no. 10, pp. 7789‚Äì7817, May 2021.
[2] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, ‚ÄúEdge intelligence: Paving the last mile of artiÔ¨Åcial intelligence with edge computing,‚Äù Proc. IEEE, vol. 107, no. 8, pp. 1738‚Äì1762, Aug. 2019.

[3] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, ‚ÄúEdge computing: Vision and challenges,‚Äù IEEE Internet Things J., vol. 3, no. 5, pp. 637‚Äì646, Oct. 2016.
[4] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for large-scale image recognition,‚Äù 2014, arXiv:1409.1556.
[5] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770‚Äì778.
[6] B. Reagen et al., ‚ÄúMinerva: Enabling low-power, highly-accurate deep neural network accelerators,‚Äù in Proc. IEEE/ACM 43rd Annu. Int. Symp. Comput. Archit., 2016, pp. 267‚Äì278.
[7] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, and J. Du, ‚ÄúOn-demand deep model compression for mobile devices: A usage-driven model selection framework,‚Äù in Proc. ACM Annu. Int. Conf. Mobile Syst. Appl. Serv., 2018, pp. 389‚Äì400.
[8] S. Han, H. Mao, and W. J. Dally, ‚ÄúDeep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding,‚Äù 2015, arXiv:1510.00149.
[9] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen, ‚ÄúDeepDecision: A mobile deep learning framework for edge video analytics,‚Äù in Proc. IEEE Conf. Comput. Commun., 2018, pp. 1421‚Äì1429.
[10] E. Li, Z. Zhou, and X. Chen, ‚ÄúEdge intelligence: On-demand deep learning model co-inference with device-edge synergy,‚Äù in Proc. ACM Workshop Mobile Edge Commun., 2018, pp. 31‚Äì36.
[11] J. Huang, C. Samplawski, D. Ganesan, B. Marlin, and H. Kwon, ‚ÄúCLIO: Enabling automatic compilation of deep learning pipelines across IoT and cloud,‚Äù in Proc. 26th Annu. Int. Conf. Mobile Comput. Netw., 2020, Art. no. 58.
[12] S. Laskaridis, S. I. Venieris, M. Almeida, I. Leontiadis, and N. D. Lane, ‚ÄúSPINN: Synergistic progressive inference of neural networks over device and cloud,‚Äù in Proc. 26th Annu. Int. Conf. Mobile Comput. Netw., 2020, Art. no. 37.
[13] J. Redmon and A. Farhadi, ‚ÄúYOLO9000: Better, faster, stronger,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 6517‚Äì6525.
[14] W. Liu et al., ‚ÄúSSD: Single shot multibox detector,‚Äù in Proc. Eur. Conf. Comput. Vis., 2016, pp. 21‚Äì37.
[15] J. Dai, Y. Li, K. He, and J. Sun, ‚ÄúR-FCN: Object detection via regionbased fully convolutional networks,‚Äù 2016, arXiv:1605.06409.
[16] N. Buchbinder, S. Chen, and J. Naor, ‚ÄúCompetitive analysis via regularization,‚Äù in Proc. 25th Annu. ACM-SIAM Symp. Discrete Algorithms, 2014, pp. 436‚Äì444.
[17] Y. Kang et al., ‚ÄúNeurosurgeon: Collaborative intelligence between the cloud and mobile edge,‚Äù in Proc. 22nd Int. Conf. Architect. Support Program. Lang. Operating Syst., 2017, pp. 615‚Äì629.
[18] J. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, and I. Stoica, ‚ÄúChameleon: Scalable adaptation of video analytics,‚Äù in Proc. Conf. ACM Special Int. Group Data Commun., 2018, pp. 253‚Äì266.
[19] P. Yang, F. Lyu, W. Wu, N. Zhang, L. Yu, and X. S. Shen, ‚ÄúEdge coordinated query conÔ¨Åguration for low-latency and accurate video analytics,‚Äù IEEE Trans. Ind. Informat., vol. 16, no. 7, pp. 4855‚Äì4864, Jul. 2020.
[20] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica, ‚ÄúClipper: A low-latency online prediction serving system,‚Äù in Proc. 14th USENIX Symp. Netw. Syst. Des. Implementation, 2017, pp. 613‚Äì627.
[21] F. Romero, Q. Li, N. J. Yadwadkar, and C. Kozyrakis, ‚ÄúINFaaS: Automated model-less inference serving,‚Äù in Proc. USENIX Annu. Tech. Conf., 2021, pp. 397‚Äì411.
[22] W. Wu, P. Yang, W. Zhang, C. Zhou, and X. Shen, ‚ÄúAccuracyguaranteed collaborative DNN inference in industrial IoT via deep reinforcement learning,‚Äù IEEE Trans. Ind. Informat., vol. 17, no. 7, pp. 4988‚Äì4998, Jul. 2021.
[23] C. Wang, S. Zhang, Y. Chen, Z. Qian, J. Wu, and M. Xiao, ‚ÄúJoint conÔ¨Åguration adaptation and bandwidth allocation for edgebased real-time video analytics,‚Äù in Proc. IEEE Conf. Comput. Commun., 2020, pp. 257‚Äì266.
[24] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look once: UniÔ¨Åed, real-time object detection,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 779‚Äì788.
[25] Z. Zhou, F. Liu, R. Zou, J. Liu, H. Xu, and H. Jin, ‚ÄúCarbon-aware online control of geo-distributed cloud services,‚Äù IEEE Trans. Parallel Distrib. Syst., vol. 27, no. 9, pp. 2506‚Äì2519, Sep. 2016.
[26] Z. Zhou, Q. Wu, and X. Chen, ‚ÄúOnline orchestration of cross-edge service function chaining for cost-efÔ¨Åcient edge computing,‚Äù IEEE J. Sel. Areas Commun., vol. 37, no. 8, pp. 1866‚Äì1880, Aug. 2019.

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

ZHAO ET AL.: EDGEADAPTOR: ONLINE CONFIGURATION ADAPTION, MODEL SELECTION AND RESOURCE PROVISIONING FOR EDGE

5885

[27] C. Zhang, M. Yu, W. Wang, and F. Yan, ‚ÄúMArk: Exploiting cloud services for cost-effective, SLO-aware machine learning inference serving,‚Äù in Proc. USENIX Annu. Tech. Conf., 2019, pp. 1049‚Äì1062.
[28] A. Singla, B. Chandrasekaran, P. B. Godfrey, and B. Maggs, ‚ÄúThe internet at the speed of light,‚Äù in Proc. 13th ACM Workshop Hot Topics Netw., 2014, pp. 1‚Äì7.
[29] Z. Liu, M. Lin, A. Wierman, S. Low, and L. L. H. Andrew, ‚ÄúGreening geographic load balancing,‚Äù in Proc. ACM SIGMETRICS Joint Int. Conf. Meas. Model. Comput. Syst., 2011, pp. 233‚Äì244.
[30] J. Dean and L. A. Barroso, ‚ÄúThe tail at scale,‚Äù Commun. ACM, vol. 56, no. 2, pp. 74‚Äì80, 2013.
[31] L. Suresh, M. Canini, S. Schmid, and A. Feldmann, ‚ÄúC3: Cutting tail latency in cloud data stores via adaptive replica selection,‚Äù in Proc. 12th USENIX Symp. Netw. Syst. Des. Implementation, 2015, pp. 513‚Äì527.
[32] A. Ali-Eldin, B. Wang, and P. Shenoy, ‚ÄúThe hidden cost of the edge: A performance comparison of edge and cloud latencies,‚Äù in Proc. Int. Conf. High Perform. Comput. Netw. Storage Anal., 2021, pp. 1‚Äì12.
[33] J. Csirik, ‚ÄúHeuristics for the 0-1 min-knapsack problem,‚Äù Acta Cybernetica, vol. 10, no. 1‚Äì2, pp. 15‚Äì20, 1991.
[34] A. Rakhlin, J. Abernethy, A. Agarwal, P. Bartlett, E. Hazan, and A. Tewari, ‚ÄúLecture notes on online learning draft,‚Äù 2009. [Online]. Available: http://www.mit.edurakhlin/papers/onlinelearning.pdf
[35] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.: Cambridge Univ. Press, 2004.
[36] Y. Nesterov and A. Nemirovskii, Interior-Point Polynomial Algorithms in Convex Programming. Philadelphia, PA, USA: SIAM, 1994.
[37] P. Raghavan and C. D. Tompson, ‚ÄúRandomized rounding: A technique for provably good algorithms and algorithmic proofs,‚Äù Combinatorica, vol. 7, no. 4, pp. 365‚Äì374, 1987.
[38] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan, ‚ÄúDependent rounding and its applications to approximation algorithms,‚Äù J. ACM, vol. 53, no. 3, pp. 324‚Äì360, 2006.
[39] M. Lin, Z. Liu, A. Wierman, and L. L. H. Andrew, ‚ÄúOnline algorithms for geographical load balancing,‚Äù in Proc. Int. Green Comput. Conf., 2012, pp. 1‚Äì10.
[40] N. Buchbinder et al., ‚ÄúThe design of competitive online algorithms via a primal‚Äìdual approach,‚Äù Found. Trends Theor. Comput. Sci., vol. 3, no. 2/3, pp. 93‚Äì263, 2009.
[41] S. Diamond and S. Boyd, ‚ÄúCVXPY: A Python-embedded modeling language for convex optimization,‚Äù J. Mach. Learn. Res., vol. 17, no. 83, pp. 1‚Äì5, 2016.
[42] Mosek Optimizer. Accessed: 2022. [Online]. Available: https:// www.mosek.com/
[43] Alibaba Production Cluster Trace Data. Accessed: 2022. [Online]. Available: https://github.com/alibaba/clusterdata
[44] AWS Lambda Price. Accessed: 2022. [Online]. Available: https:// aws.amazon.com/cn/lambda/pricing/
[45] M. Lin, A. Wierman, L. L. H. Andrew, and E. Thereska, ‚ÄúDynamic right-sizing for power-proportional data centers,‚Äù IEEE/ACM Trans. Netw., vol. 21, no. 5, pp. 1378‚Äì1391, Oct. 2013.
[46] Y. Zeng, Y. Huang, Z. Liu, and Y. Yang, ‚ÄúJoint online edge caching and load balancing for mobile data ofÔ¨Çoading in 5G networks,‚Äù in Proc. IEEE 39th Int. Conf. Distrib. Comput. Syst., 2019, pp. 923‚Äì933.
[47] B. Gao, Z. Zhou, F. Liu, and F. Xu, ‚ÄúWinning at the starting line: Joint network selection and service placement for mobile edge computing,‚Äù in Proc. IEEE Conf. Comput. Commun., 2019, pp. 1459‚Äì1467.
[48] L. Zhang, C. Wu, Z. Li, C. Guo, M. Chen, and F. C. M. Lau, ‚ÄúMoving big data to the cloud: An online cost-minimizing approach,‚Äù IEEE J. Sel. Areas Commun., vol. 31, no. 12, pp. 2710‚Äì2721, Dec. 2013.
[49] B. Gao, Z. Zhou, F. Liu, F. Xu, and B. Li, ‚ÄúAn online framework for joint network selection and service placement in mobile edge computing,‚Äù IEEE Trans. Mobile Comput., early access, Mar. 9, 2021, doi: 10.1109/TMC.2021.3064847.
[50] X. Qi, H. Xu, Z. Ma, and S. Chen, ‚ÄúJoint network selection and task ofÔ¨Çoading in mobile edge computing,‚Äù in Proc. IEEE/ACM 21st Int. Symp. Cluster Cloud Internet Comput., 2021, pp. 475‚Äì482.
[51] Q. Zhang, F. Liu, and C. Zeng, ‚ÄúOnline adaptive interferenceaware VNF deployment and migration for 5G network slice,‚Äù IEEE/ACM Trans. Netw., vol. 29, no. 5, pp. 2115‚Äì2128, Oct. 2021.

Kongyange Zhao received the BE degree from the South China University of Technology, China, in 2020. He is currently working toward the master‚Äôs degree with Sun Yat-sen University, China. His research interests include edge computing, edge intelligence, and serverless computing.
Zhi Zhou (Member, IEEE) received the BS, ME, and PhD degrees from the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China, in 2012, 2014, and 2017, respectively. He is currently an Associate Professor with the School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China. In 2016, he was a visiting scholar with the University of Go‚Ç¨ttingen. He was nominated for the 2019 China Computer Federation CCF Outstanding Doctoral Dissertation Award, the sole recipient of the 2018 ACM Wuhan & Hubei Computer Society Doctoral Dissertation Award, and a recipient of the Best Paper Award of IEEE UIC 2018. His research interests include edge computing, cloud computing, and distributed systems.
Xu Chen (Senior Member, IEEE) received the PhD degree in information engineering from the Chinese University of Hong Kong, in 2012. He is a full professor with Sun Yat-sen University, Guangzhou, China, and the vice director of National and Local Joint Engineering Laboratory of Digital Home Interactive Applications. He worked as a postdoctoral research associate with Arizona State University, Tempe, USA, from 2012 to 2014, and a Humboldt scholar fellow with the Institute of Computer Science, University of Goettingen, Germany from 2014 to 2016. He received the prestigious Humboldt research fellowship awarded by the Alexander von Humboldt Foundation of Germany, 2014 Hong Kong Young Scientist Runner-up Award, 2017 IEEE Communication Society Asia-PaciÔ¨Åc Outstanding Young Researcher Award, 2017 IEEE ComSoc Young Professional Best Paper Award, Honorable Mention Award of 2010 IEEE international conference on Intelligence and Security Informatics (ISI), Best Paper Runner-up Award of 2014 IEEE International Conference on Computer Communications (INFOCOM), and Best Paper Award of 2017 IEEE Intranational Conference on Communications (ICC). He is currently an area editor of the IEEE Open Journal of the Communications Society, an associate editor of the IEEE Transactions Wireless Communications, IEEE Internet of Things Journal and IEEE Journal on Selected Areas in Communications (JSAC) Series on Network Softwarization and Enablers.
Ruiting Zhou (Member, IEEE) received the PhD degree from the Department of Computer Science, University of Calgary, Canada, in 2018. She has been an associate professor with the School of Cyber Science and Engineering, Wuhan University since June 2018. Her research interests include cloud computing, machine learning, and mobile network optimization. She has published research papers in top-tier computer science conferences and journals, including IEEE INFOCOM, ACM MobiHoc, ICDCS, the IEEE/ACM Transactions on Networking, IEEE Journal on Selected Areas in Communications, IEEE Transactions on Mobile Computing. She also serves as a reviewer for journals and international conferences such as the IEEE Journal on Selected Areas in Communications, IEEE Transactions on Mobile Computing, IEEE Transactions on Cloud Computing, IEEE Transactions on Wireless Communications, and IEEE/ACM IWQoS.

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

5886

IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. 22, NO. 10, OCTOBER 2023

Xiaoxi Zhang (Member, IEEE) received the BE degree in electronics and information engineering from the Huazhong University of Science and Technology, in 2013, and the PhD degree in computer science from the University of Hong Kong, in 2017, advised by Prof. Chuan Wu and Prof. Francis C.M. Lau. She is an Associate Professor with the School of Computer Science and Engineering, Sun Yat-sen University. Before joining SYSU, she was a postdoctoral researcher with the Department of Electrical and Computer Engineering, Carnegie Mellon University, advised by Prof. Carlee Joe-Wong. Her research interests lie in the broad area of optimization and algorithm design for networked systems, including edge computing networks, and distributed machine learning systems.
Shuai Yu (Member, IEEE) received the BS degree from the Nanjing University of Post and Telecommunications, Nanjing, China, in 2009, the MS degree from the Beijing University of Post and Telecommunications, Beijing, China, in 2014, and the PhD degree from the University Pierre and Marie Curie (now Sorbonne Universite), Paris, France, in 2018. He is currently an associate professor with Sun Yat-sen University, Guangzhou, China. His research interests include edge computing, mobile computing, machine learning, and spaceair-ground integrated networks.

Di Wu (Senior Member, IEEE) received the BS degree from the University of Science and Technology of China, Hefei, China, in 2000, the MS degree from the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, in 2003, and the PhD degree in computer science and engineering from the Chinese University of Hong Kong, Hong Kong, in 2007. He was a postdoctoral researcher with the Department of Computer Science and Engineering, Polytechnic Institute of New York University, Brooklyn, NY, USA, from 2007 to 2009, advised by Prof. K. W. Ross. He is currently a professor and the associate dean with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China. He was the recipient of the IEEE INFOCOM 2009 Best Paper Award, IEEE Jack Neubauer Memorial Award, and etc. His research interests include edge/cloud computing, multimedia communication, Internet measurement, and network security.
" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.

Authorized licensed use limited to: KAUST. Downloaded on January 30,2024 at 12:32:58 UTC from IEEE Xplore. Restrictions apply.

