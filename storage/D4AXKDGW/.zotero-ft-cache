2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

A Deep Reinforcement Learning based Ofﬂoading Scheme in Ad-hoc Mobile Clouds
Duc Van Le∗ and Chen-Khong Tham† ∗Dept. of Computer Science, National University of Singapore †Dept. of Electrical and Computer Engineering, National University of Singapore
E-mails: anhduc.mta@gmail.com, eletck@nus.edu.sg

Abstract—In this paper, we consider the problem of making an optimal ofﬂoading decision for a mobile user in an ad-hoc mobile cloud in which the mobile user can ofﬂoad his computation tasks to nearby mobile cloudlets via a device-to-device (D2D) communication-enabled cellular network. We propose a deep reinforcement learning (DRL)-based ofﬂoading scheme which enables the user to make near-optimal ofﬂoading decisions by taking into account uncertainties of user’s and cloudlets’ movements and the cloudlets’ resource availabilities. We ﬁrst propose a Markov decision process (MDP)-based ofﬂoading problem formulation which considers the composite states of the user’s and cloudlets’ queue states and the distance states between the user and cloudlets as the system state space. The objective of the formulated MDP-based problem is to determine the optimal actions on how many tasks the user should process locally and how many tasks to ofﬂoad to each cloudlet at each observed system state such that the user’s utility obtained by task execution is maximized while minimizing the energy consumption, task processing delay, task loss probability and required payment. Then, we use a deep reinforcement learning scheme, called deep Q-network (DQN) to learn an efﬁcient solution for the proposed MDP-based ofﬂoading problem. Extensive simulations were performed to evaluate the performance of the proposed ofﬂoading scheme. The simulation results validate the effectiveness of the ofﬂoading policies obtained by the proposed scheme.
Index Terms—Ad-hoc Mobile Cloud; Computation Ofﬂoading; Deep Reinforcement Learning; Deep Q-Network
I. INTRODUCTION
An ad-hoc mobile cloud [1] has been recently introduced as a promising cloud architecture, in which a mobile device user (e.g., smart phones and tablets) can ofﬂoad its tasks to other nearby mobile devices, called mobile cloudlets, in an ad-hoc fashion. By ofﬂoading, the mobile user can save its energy and speed up processing-intensive mobile applications. In addition, since the user can utilize direct communication technologies such as Bluetooth and Wi-Fi Direct to communicate with the cloudlet, the ofﬂoading latency is lower compared to the case of accessing the traditional centralized resource clouds (e.g., Amazon EC2 and Microsoft Azure) through the backbone network via 3G/4G cellular networks or Wi-Fi access points.
However, making an optimal ofﬂoading decision in the adhoc mobile cloud has to face challenges due to uncertainties of the user and cloudlet movements and the cloudlets’ resource availabilities. For example, ofﬂoading may result in higher energy consumption compared to local processing since a large amount of energy may be consumed as a result of retransmissions when the task data is transmitted over a bad connection

arising from movements of the user and cloudlets. In addition, ofﬂoading may also result in a higher task processing delay arising from the communication delay. The user also has to make a certain payment for the use of the cloudlet’s resource and communication bandwidth. Furthermore, the challenge includes not only how to make an optimal “ofﬂoad or not” decision but also how to distribute the proper number of tasks to cloudlets with different movement and resource availability conditions.
In this paper, we focus on designing an ofﬂoading algorithm which can enable the user to make optimal ofﬂoading decisions, subject to the dynamics of the system in terms of user and cloudlet behaviors. More speciﬁcally, we formulate the ofﬂoading problem as a Markov decision process (MDP) which considers the composite states of the user’s and cloudlets’ queue states and the distance states between the user and cloudlets as a state space. The main objective is to ﬁnd the optimal actions on how many tasks the user should process locally and how many tasks to ofﬂoad to each cloudlet such that the user’s utility obtained by task execution is maximized while minimizing the energy consumption, task processing delay, required payment and task loss probability. A DRL algorithm, called a deep Q-network (DQN) [2] is used to learn the optimal ofﬂoading decision for the MDP-based ofﬂoading problem.
In the proposed DQN-based ofﬂoading algorithm, the user continuously learns online and improves its ofﬂoading decisions through a trial-and-error learning scheme. The state observation information and learning experience are combined in a nonlinear function approximator in the form of a neural network, which is iteratively trained and gives the user the best decision to take at each system state. The DQN algorithm is adopted in our study since it has achieved excellent performance in the high-dimensional decision-making problems which are similar to our formulated ofﬂoading decision problem. Furthermore, we evaluate the proposed ofﬂoading scheme in various simulations in which an ad-hoc mobile cloud environment is simulated. The simulation results show that the proposed scheme performs well in terms of the effectiveness of the ofﬂoading decisions.
In the remainder of this paper, we ﬁrst review related works in Section II. Section III describes the system model and problem statement. In Section IV, we describe the proposed DRLbased ofﬂoading algorithm. Simulation results and analysis

978-1-5386-5979-3/18A/u$t3ho1r.i0ze0d©lic2e0n1s8edIEuEseElimited to: KAUST. Downloaded on M7ay6207,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

are presented in Section V, followed by the conclusion in Section VI.
II. RELATED WORK
A. Computation Ofﬂoading in Ad-hoc Mobile Clouds
The problem of computation ofﬂoading in a general mobile cloud computing system has been extensively studied in the literature [1], [3]. From the perspective of the user, computation ofﬂoading can help to save energy and speed up the computation process. In general, an ofﬂoading algorithm can enable the user to make three types of ofﬂoading decisions which are local execution, full ofﬂoading and partial ofﬂoading [3].
Many studies [4], [5], [6] have proposed various ofﬂoading algorithms to enable the mobile user to make an ofﬂoading decision in the ad-hoc mobile cloud. The general objective is to make optimal ofﬂoading decisions such that a maximum number of tasks can be executed while minimizing the energy consumption, delay and other costs, such as communication cost caused by ofﬂoading.
For example, Zhang et al. [4] proposed an MDP-based ofﬂoading algorithm that helps the user decide whether to ofﬂoad the computation task of the same application phase in a multi-phase mobile application to all nearby cloudlets. Upon making an ofﬂoading decision, the user gets the result from the earliest completed mobile cloudlet. Moreover, the work in [5] considered a similar problem to ours, in which the user has a set of tasks which can be executed in parallel on the user and cloudlets. The user targets to distribute an optimal number of tasks to each cloudlet such that the task execution reward is maximized while the processing cost is minimized.
However, these studies differ from ours in that they assumed speciﬁc models for the user’s task execution demand, cloudlet’s resources and availability of cloudlets, which may not always reﬂect a realistic ad-hoc mobile cloud accurately. In this study, we propose a model-free deep reinforcement learning (DRL)-based ofﬂoading algorithm which does not require any assumption on the system model.
B. Reinforcement Learning Background
Reinforcement learning (RL) [7] is a mathematical framework for experience-driven autonomous learning through interaction. In the standard RL model, an autonomous learning agent interacts with an environment through a sequence of observations, actions and rewards. At each time step t, the agent ﬁrst observes a state st from its environment, then takes an action at and receives a scalar reward rt as a feedback. After doing the action at, the environment transits to a new state st+1. The process continues and the goal of the agent is to learn a policy (i.e., an action selection strategy) π that maximizes the expected reward over the long run. Formally, the RL can be described as a Markov decision process (MDP) in which the environment’s response about the next state st+1 depends only on the state st and taken action at. Moreover, the main focus of the RL is on learning without the knowledge of the underlying environment model. The most popular modelfree RL algorithm is Q-learning proposed by Watkins et al. [8]

Deep RL has emerged as a class of RL schemes which uses the deep learning [9] to enable reinforcement learning to scale to decision-making problems with high-dimensional state and action spaces [10]. In general, the deep RL is based on training neural networks to learn the value function Q(s, a) according to which the optimal policy is obtained.
The ﬁrst well-known success story of deep RL is the deep Q-network (DQN) algorithm which was developed by Mnih et al. [2] to play the Atari 2600 video games at superhuman level, directly from game screen images. Furthermore, Hasselt et al. [11] proposed a Double DQN algorithm which integrates the Double Q-learning to enhance the performance of the DQN algorithm.
In this paper, we apply the DQN algorithm for a mobile user to learn a policy which gives efﬁcient ofﬂoading decisions that the user should make at each system states.
III. SYSTEM MODEL
A. System Assumption
In this paper, we consider an ad-hoc mobile cloud in which a mobile user (thereafter referred to simply as ”user”) can ofﬂoad its computation tasks to N mobile cloudlets with available resources in terms of storage and computing capabilities. When the user has a number of computation tasks to be executed, it ﬁrst seeks for the cloudlets which are currently located in its D2D communication range, denoted by Rc. Then, the user may ofﬂoad a part of its tasks to the cloudlets. Upon receiving the task from the user, the cloudlets execute the tasks and send the results back to the user.
To represent the user’s computation task, we use a simpliﬁed model which deﬁnes a computation task with three parameters of J, B, O which denote the number of CPU cycles required to execute one task and the numbers of data transmission bits required to ofﬂoad the task and receive the result to/from the cloudlet, respectively. It is assumed that the user has a queue with a limited size to store its task which may arrive depending on its application execution demand. The tasks are stored and processed at the user’s queue according to a FCFS (FirstCome-First-Served) manner. Furthermore, let f u denote the CPU speed of the user.
When the user ofﬂoads the task to the cloudlet, it has to make a certain payment for the cloudlet’s resource usage. If we denote ηi as the price for the cloudlet i to execute a task, it is calculated as ηi = ςfic where ς > 0 is an execution payment constant and fic (i = 1, ..., N ) denotes the CPU speed of the cloudlet i. Each cloudlet stores the tasks from the user in a dedicated queue and executes them sequentially.
For communication between the user and cloudlets, we consider a Device-to-Device (D2D) communication-enabled cellular network [12], which includes cellular and D2D communication modes. More speciﬁcally, for ofﬂoading the tasks, the user ﬁrst starts the D2D mode using a direct link (e.g., Wi-Fi Direct or Bluetooth links) to transmit the task-related data to the cloudlet. During the data transmission time, if the D2D link between the user and the cloudlet is broken due to the user and cloudlet mobility, the communication mode

Authorized licensed use limited to: KAUST. Downloaded on M7ay6217,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

is automatically switched to the cellular mode (e.g., 3G/4G links) to transmit the rest of the data to the cloudlet.
Furthermore, to estimate the communication energy consumption in both D2D and cellular modes, we adopt the linear model which was empirically derived and validated in [13]. If we denote rm as the achievable data rate, the transmission power, denoted by Pm is calculated as follows [13]

Pm = αmrm + βm

(1)

where αm and βm are parameters whose values depend on the communication mode. Note that the subscript m ∈ {d2d, cel}, where d2d and cel represent D2D and cellular modes, respectively. In this study, we use values of αd2d = 283.17 mW/Mbps and βd2d = 132.86 mW for the D2D Wi-Fi network as derived in [13]. For the 3G cellular network, the value of βcel is 817.88 mW while the value of αcel is 868.98 mW/Mbps and 122.12 mW/Mbps for the up-link and down-link, respectively. Also, it is assumed that the D2D mode uses the out-of-band D2D communication with Wi-Fi Direct [12]. Let χcel denote the price of using the cellular bandwidth to transmit one data bit while the D2D link is regarded as free due to the usage of the unlicensed spectrum.
Time is logically divided into intervals of T , which are deﬁned as decision periods. At the beginning of a decision period, the user ﬁrst discovers the cloudlets in its D2D communication range and then makes an ofﬂoading decision how many tasks the user should process locally and how many tasks to ofﬂoad to each cloudlet based on the user’s observation on the number of tasks currently remaining in the user’s queue, the distance between the user and each cloudlet, and the queue status of the cloudlets.

IV. OBTAINING OFFLOADING POLICY THROUGH DEEP REINFORCEMENT LEARNING
A. MDP-based Ofﬂoading Problem Formulation
The problem of making an ofﬂoading decision for the user is formulated as a ﬁnite MDP which is deﬁned as a tuple M = (S, A, P, R), where S and A denote state and action spaces, P (s |s, a) indicates the transition probability from state s ∈ S to s ∈ S after tasking an action a ∈ A and R(s, a) denotes the immediate reward obtained by doing action a at state s. A policy, denoted by π is deﬁned as a mapping from a state s to an action a, i.e., π(s) = a. The main goal of the user to ﬁnd the optimal policy π∗ which maximizes the total amount of reward it can obtain over the long run.
1) State and Action Spaces: The state space S consisting of information about the user’s and cloudlets’ queue and the distance between the user and cloudlets is deﬁned as follows:

S = {s = (Qu, Qc, D)}

(2)

where Qu, Qc and D denote the user’s queue state, cloudlet’s
queue state and distance state, respectively. The user’s queue state Qu ∈ Qu is deﬁned as the number of
tasks currently in the user’s queue, where the state space Qu = {0, 1, . . . , |Qu|}. Note that |Qu| denotes the size of the user’s

queue. The cloudlet’s queue state Qc is a composite state of N cloudlet queue states, i.e., Qc = {(Qc1, Qc2, . . . , QcN )|Qci ∈ {0, 1, . . . , |Qci |}}, where |Qci | denotes the queue size of the cloudlet i and Qci is the number of tasks currently stored in the queue of cloudlet i.
The distance state D is a composite state of N cloudlet distance states i.e., D = {(D1, . . . , DN )|Di ∈ {1, 2, . . . , H}} where Di is the distance state of the cloudlet i and H is the number of possible distance states. More speciﬁcally, we adopt
a Markov chain model (MCM) [14] to describe the distance between the user and a cloudlet. A cloudlet i having distance di to the user is said to have a distance state h(h = 1, . . . , H) if (h − 1)W ≤ di < hW where W = Rc/(H − 1). The state H is used to represent the distance state of the cloudlet which is not in the user’s D2D range (i.e., di > Rc). Note that in this study, we include the distance information in the system
state since the distance state affects the residual link duration
between the user and cloudlet, as derived in [14]. The action space A is deﬁned as

A = {a = (a0, . . . , ai, . . . , aN ) ai ∈ {0, 1, . . . , amax}} (3)

where amax is the maximum number of tasks to be locally processed or ofﬂoaded to a cloudlet in each decision period. An action a ∈ A can be considered as the task distribution decision in which a0 is the number of tasks for local processing and ai (i = 1, . . . , N ) is the number of tasks to be ofﬂoaded to the cloudlet i. Let A(s) ⊆ A be the set of actions that can be taken at the state s. The set A(s) is determined such that the user can only ofﬂoad the tasks to cloudlets with an available queue space in its D2D communication range. In addition, the total number of tasks per action a must be equal to or less than the number of tasks currently remained in the user’s queue.
2) Immediate Reward: The main goal of the user is to make an optimal ofﬂoading action at each system state with the objective of maximizing the utility while minimizing the energy consumption, processing delay, required payment and and task’s loss probability. Therefore, we deﬁne the immediate reward function R(s, a) given an action a at state s as follows

R(s, a) = U (s, a) − C(s, a)

(4)

where U (s, a) and C(s, a) are immediate utility and cost

functions. For the utility, we adopt the logarithmic utility

function as
N

U (s, a) = ρ log(1 + ai)

(5)

i=0

where ρ is a utility constant. The immediate cost function C(s, a) is deﬁned as:

C(s, a) = ω1I(s, a)+ω2E(s, a)+ω3D(s, a)+ω4Γ(s, a) (6)

where I(s, a) , E(a, s), D(s, a) and Γ(s, a) are immediate required payment, energy consumption, delay and task loss probability, respectively.
The immediate payment I(s, a) is deﬁned as the total payment that the user has to make when taking an action a at

Authorized licensed use limited to: KAUST. Downloaded on M7ay6227,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

state s. If we denote τdi2d as the D2D link duration between the user and cloudlet i, the maximum amount of transmission data via the D2D link is Φi = τdi2drd2d. The value of I(s, a) is calculated as

i=N

I(s, a) = ai ηi + χcel max(0, aiB − Φi)

(7)

i=1

where max(0, aiB − Φi) function gives an amount of data transmitted via the cellular link after the D2D link is broken. Recall that ηi and χcel are prices for the use of cloudlet’s computation resources and cellular bandwidth, respectively.

The immediate energy consumption E(s, a) is deﬁned as the total energy that user has to consume given action a at state s. The value of E(s, a) includes total energy consumed by local execution and communication. That is

E(s, a) = a0J ξu + Ecom(s, a)

(8)

where ξu is the user’s local energy consumption per CPU cycle and Ecom(s, a) denotes the communication energy consumption for ofﬂoading the tasks and receiving the the results. The value of Ecom(s, a) is calculated as

N
Ecom(s, a) =
i=1

+ + min(Φi,aiB)Pd2d
rd2d

max(0,ai B −Φi )Pcuel rcel

ai OPcdel rcel

(9)

where min(Φi, aiB) is an amount of data transmitted via the D2D link, and Pcuel and Pcdel denote transmission powers on
cellular up-link and down-link, respectively. In (9), the ﬁrst

and second terms are the energy consumed for transmitting

the data via the D2D link and cellular up link, respectively,

while the ﬁnal term is the energy consumed on receiving the

results via the cellular down-link.

The immediate delay D(s, a) is deﬁned as a sum of average values of the waiting time at the user’s and cloudlet’s queues, the communication time of task ofﬂoading and the processing time by the user or cloudlets. That is

(tw + te + tcom)

D(s, a) =

N i=0

ai

(10)

where tw, te and tcom denote the total waiting time, total

execution time and total communication time, respectively.

They are calculated as



 

tw

= twu +



N i=1

(twu

+

) J Qic
fic

  

te

=

J a0 fu

tcom =

+

N J ai i=1 fic

( N min(Φi,aiB)

i=1

rd2d

+

) max(0,ai B −Φi )+Oai
rcel

(11)

where twu is the average waiting time of the task at the user’s queue. Moreover, the communication delay includes the data

transmission time for ofﬂoading the task and receiving the

result via a the D2D and cellular links.

We deﬁne Γ(s, a) as the probability that a task can be lost since it arrives when the user’s queue is already full. If we denote µtask as the average number of task arrivals during a

decision period of T , the value of Υ(s, a) is calculated as

max 0, µtask − Υavai

Γ(s, a) =

(12)

µtask

where Υavai = |Qu| − Qu +

N i=0

ai

denotes

the

available

space of the user’s queue after taking an action a. Recall that

Qu is the number of remaining tasks at state s.

B. DQN-based Ofﬂoading Decision Algorithm
To ﬁnd the solution to the above formulated MDP-based ofﬂoading problem, we propose an online learning scheme based on the model-free deep RL algorithm, called deep Qnetwork (DQN) [2]. In the DQN-based learning scheme, the user acts as an agent which interacts with the ad-hoc mobile cloud environment to select an ofﬂoading action at for state st at time-step t in a fashion that maximizes the future discounted reward over a long run. More speciﬁcally, a deep neural network, called a deep Q-network, is used to approximate the optimal action-value function [2]
∞
Q∗(s, a) = max E[rt + γkrt+k|st = s, at = a, π] (13)
π k=1
which is the maximum sum of reward rt discounted by γ after taking the action a at the state s at the time-step t in the ofﬂoading policy π. Note that E[.] denote the expectation function.
The Q-network can be considered as a neural network approximator with an approximate action-value function Q(s, a; θ) with weights θ. At each decision period, the user ﬁrst takes the state vector s = (Qu, Qc, D) as the input of the Q-network and obtains the Q-values Q(s, .) for all possible action a as outputs. Then, the user selects the action according to the ε-greedy method.
Furthermore, the Q-network is trained by iteratively adjusting the weights θ to minimize a sequence of the loss functions, where the loss function at time-step t is deﬁned as
Lt(θt) = E[ rt + γ max Q(st+1, a ; θt−1) − Q(st, at; θt) 2]
a
(14) In other words, given a transition < st, at, rt, st+1 >, the weights θ of the Q-network are updated in a way that minimizes the squared error loss between the current predicted Q-value of Q(st, at) and the target Q-value of rt + γ max Q(st+1, a ) .
a
Moreover, in the DQN algorithm, the experience replay technique is adopted as the training method to address the instability of the Q-network due to use of non-linear approximation function. More speciﬁcally, the user’s experiences, et =< st, at, rt, st+1 > are stored into a replay memory Ω = {et−Ψ, ..., et}, where Ψ is the replay memory capacity. At each time step, a random mini-batch of transitions from the replay memory is chosen to train the Q-network, instead of the most recent transition et.
The detailed DQN-based ofﬂoading decision algorithm is presented in Algorithm 1. The recursions in lines (2)-(4)

Authorized licensed use limited to: KAUST. Downloaded on M7ay6237,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

Algorithm 1 DQN-based Ofﬂoading Decision Algorithm
1: for t = 1, 2, . . . do 2: Observe current state st 3: Select action at according to ε-greedy policy 4: Ofﬂoad the tasks according to action at and observe
reward rt = R(st, at) and next state st+1 5: Store experience < st, at, rt, st+1 > into replay mem-
ory Ω 6: Randomly select a set of transitions < s, a, r, s >
from replay memory Ω 7: Train the Q-network based on selected transitions
using r + γ max Q(s , a ) − Q(s, a) 2 as loss function
a
8: end for

TABLE I SIMULATION SETTINGS

Parameters
Number of mobile cloudlets N User’s queue size |Qu|
Cloudlets’ queue size |Qci | Number of distance states H
# of CPU cycles per tasks J
Task’s data sizes B, O CPU speeds fu, fic
Constants (ς, χcel, ρ, ξu)
Weights (ω1, ω2, ω3, ω4) Data rates rd2d, rcel
D2D communication range Rc Decision period T

Values 4 10 tasks 4 tasks 4 [50, 100] Gcycles [0.5, 0.7], [0.05, 0.1] Gb [0.5, 2] Gcycles (10−8, 10−6, 104, 5mJ) (103, 10−5, 10−5, 103) (10, 5) Mbps 200 m 60 s

present the user’s action on making a ofﬂoading decision at the beginning of each decision period according to the Q-values which are estimated using the Q-network. In lines (5)-(7), the Q-network is trained using the experience replay method.
V. PERFORMANCE EVALUATION
A. Simulation Setup
We evaluate the performance of the proposed ofﬂoading algorithm in simulations which consider an ad-hoc mobile cloud consisting of N = 4 mobile cloudlets. Major simulation parameters are summarized in Table I.
To simulate the arrival of the user’s task, we use a Poisson process with a parameter λ, which is considered as the estimated value of the average number of arrived tasks µtask during a period T . Furthermore, we adopt the semi-Markov smooth mobility (SMS) model [15] to simulate the mobility of the user and cloudlets, in which the average movement speed of the user and cloudlets is set to 4 m/s. Moreover, we consider a network area of 500 × 500 m.
In the DQN-based learning algorithm, the discount factor is γ = 0.9 while the replay memory capacity Ψ is set to 104. The DQN learning algorithm is implemented using the Tensorﬂow APIs, in which a gradient descent optimization algorithm, called RMSProp is used as the optimization method for training the Q-network. Lastly, the behaviour action selection during the training is ε-greedy with ε which is annealed linearly from 1 to 0.1.
Five performance metrics are used, which are total utility, total energy consumption, total payment, average delay and task loss ratio. Note that the task loss ratio is calculated as a ratio of the number of lost tasks due to the user’s queue overﬂow to the total number of tasks generated during the simulation time.
B. Performance Analysis
We analyze the performance of the proposed algorithm in various simulation scenarios where the user’s task arrival rate λ varies from 1 to 5.
Fig. 1 shows the learning curves which present total reward obtained by the ofﬂoading policy learned in each training episode over variation of the task arrival rate. Note that an

episode includes 5000 iterations (i.e., the number of decision periods), in each of which the user selects an ofﬂoading action for each state according to the current policy learned by the proposed algorithm.
As shown in Fig. 1, the total reward obtained in each episode increases steadily when the learning time i.e., the number of episodes, increases from 1 to 200. Then, all learning curves become stable and no longer increase when the episode number is higher than 200. This result indicates that the proposed DQN-based learning algorithm converges after 200 learning episodes.
Figs. 2-6 show the quality of the learned ofﬂoading policy. More speciﬁcally, Fig. 2 exhibits the total utility that the user obtain during 300th episode over variation of the task arrival rate λ. The total utility greatly increases as the value of λ is increased from 1 to 3 since a higher number of tasks are executed by the user and cloudlets. Moreover, when the value of λ is higher than 3, the utility becomes slightly greater since the user and cloudlet nearly reach their maximum computation capabilities. They cannot execute more tasks although the number of arrived tasks is higher.
Furthermore, over the increase of the task arrival rate, the user consumes a higher amount of energy for local execution and communication and makes a higher payment, as shown in Figs. 3 and 4 since a higher number of tasks is processed.
Fig. 5 shows the average delay over the variation of the task arrival rate. When the task arrival rate increases, the delay also increases. The main reason is that the tasks’ waiting time at the user’s and cloudlets’ queues is longer when more tasks arrive. Given the system capability related to the user’s and cloudlets’ storage and computation resources, if more tasks are generated, a newly arrived task will have to wait at the queue for a longer time for all the previously stored tasks to be processed.
In addition, when the rate of task arrival increases, at a time instance, the probability that the queue is already full increases. Therefore, the number of lost tasks due to the lack of user’s queue space will be higher as shown in Fig. 6.

Authorized licensed use limited to: KAUST. Downloaded on M7ay6247,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

2018 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS): IECCO: Integrating Edge Computing, Caching, and Offloading in Next Generation Networks

Fig. 1. Learning curves.

Fig. 2. Total utility.

Fig. 3. Total energy consumption.

Fig. 4. Total required payment.

Fig. 5. Average delay.

Fig. 6. Task loss ratio.

VI. CONCLUSION
In this paper, we have proposed a deep reinforcement learning (DRL)-based ofﬂoading algorithm for the user to obtain the optimal ofﬂoading policy in an ad-hoc mobile cloud. The ofﬂoading problem is formulated as a Markov decision process (MDP) with the main objective of making an optimal ofﬂoading action decision at each system state such that the utility obtained by the task execution is maximized while minimizing the energy consumption, processing delay, required payment and task loss probability. An DRL scheme, namely deep Qnetwork (DQN) is adopted to learn a near-optimal ofﬂoading policy for the proposed MDP-based ofﬂoading problem. We evaluated the performance of the proposed ofﬂoading scheme in various simulations in which the user uses the proposed learning algorithm to make ofﬂoading decisions in realistic scenarios. The simulation results conﬁrm that the proposed schemes perform well in terms of various performance metrics.
ACKNOWLEDGMENT
This research/project is supported by the National Research Foundation, Prime Minister’s Ofﬁce, Singapore, under its Campus for Research Excellence and Technological Enterprise (CREATE) programme, and an MOE AcRF Tier 1 FRC Research Grant.
REFERENCES
[1] M. Chen, Y. Hao, Y. Li, C. F. Lai, and D. Wu, “On the computation ofﬂoading at ad hoc cloudlet: architecture and service modes,” IEEE Commun. Mag., vol. 53, no. 6, pp. 18–24, June 2015.

[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, “Human level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.
[3] P. Mach and Z. Becvar, “Mobile edge computing: A survey on architecture and computation ofﬂoading,” IEEE Commun. Surveys Tuts., vol. 19, no. 3, pp. 1628–1656, 2017.
[4] Y. Zhang, D. Niyato, and P. Wang, “Ofﬂoading in mobile cloudlet systems with intermittent connectivity,” IEEE Trans. Mobile Comput., vol. 14, no. 12, pp. 2516–2529, Dec 2015.
[5] T. Truong-Huu, C. K. Tham, and D. Niyato, “To ofﬂoad or to wait: An opportunistic ofﬂoading algorithm for parallel tasks in a mobile cloud,” in Proc. IEEE CloudCom, Dec 2014, pp. 182–189.
[6] D. V. Le and C.-K. Tham, “An optimization-based approach to ofﬂoading in ad-hoc mobile clouds,” in Proc. 2017 IEEE GLOBECOM, Dec. 2017.
[7] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning, 1st ed. Cambridge, MA, USA: MIT Press, 1998.
[8] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Mach. Learn., vol. 8, no. 3, pp. 279–292, 1992.
[9] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 512, pp. 436–444, 2015.
[10] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief survey of deep reinforcement learning,” Aug. 2017, arXiv:1708.05866.
[11] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with Double Q-learning,” in Proc. AAAI, Feb. 2016, pp. 2094–2100.
[12] A. Asadi, Q. Wang, and V. Mancuso, “A survey on device-to-device communication in cellular networks,” IEEE Commun. Surveys Tuts., vol. 16, no. 4, pp. 1801–1819, Fourth quarter 2014.
[13] J. Huang, F. Qian, A. Gerber, Z. M. Mao, S. Sen, and O. Spatscheck, “A close examination of performance and power characteristics of 4G LTE networks,” in Proc. the 10th MobiSys, 2012, pp. 225–238.
[14] S. Xu, K. L. Blackmore, and H. M. Jones, “An analysis framework for mobility metrics in mobile ad hoc networks,” EURASIP J. Wirel. Commun. Netw., vol. 2007, pp. 1–16, 2007.
[15] M. Zhao and W. Wang, “A uniﬁed mobility model for analysis and simulation of mobile wireless networks,” Wirel. Netw., vol. 15, no. 3, pp. 365–389, 2009.

Authorized licensed use limited to: KAUST. Downloaded on M7ay6257,2022 at 12:36:56 UTC from IEEE Xplore. Restrictions apply.

