Prediction
Fusion

Prediction Fusion


On-Sensor Inference
 Aggregator Inference

SplitNets: Designing Neural Architectures for Efﬁcient Distributed Computing on Head-Mounted Systems
Xin Dong1, Barbara De Salvo2, Meng Li2, Chiao Liu2, Zhongnan Qu3, H.T. Kung1 and Ziyun Li2 1Harvard University, 2Meta Reality Labs, 3ETH Zurich
xindong@g.harvard.edu

Multiple Cameras


On-Sensor Inference
 Aggregator Inference


Light-weight AR/VR processor


Multi-layer stacked camera with AI processing


Figure 1. AR/VR device with multiple intelligent AI cameras.

Abstract

We design deep neural networks (DNNs) and corresponding networks’ splittings to distribute DNNs’ workload to camera sensors and a centralized aggregator on head mounted devices to meet system performance targets in inference accuracy and latency under the given hardware resource constraints. To achieve an optimal balance among computation, communication, and performance, a splitaware neural architecture search framework, SplitNets, is introduced to conduct model designing, splitting, and communication reduction simultaneously. We further extend the framework to multi-view systems for learning to fuse inputs from multiple camera sensors with optimal performance and systemic efﬁciency. We validate SplitNets for singleview system on ImageNet as well as multi-view system on 3D classiﬁcation, and show that the SplitNets framework achieves state-of-the-art (SOTA) performance and system latency compared with existing approaches.

1. Introduction
Virtual Reality (VR) and Augmented Reality (AR) are becoming increasingly prevailing as one of the nextgeneration computing platforms [3]. Head mounted devices (HMD) for AR/VR feature multiple cameras to support various computer vision (CV) / machine learning (ML) powered human-computer interaction functions, such as object classiﬁcation [40, 61], hand-tracking [19] and SLAM [38].
Due to the recent advances of camera technologies, tiny

Figure 2. Distributed inference with sensors and an aggregator. In this work, we focus on single/multi-view classiﬁcation tasks.

multi-layer stacked cameras with AI computing capabil-

ity arise [32, 33], as depicted in Figure 1. Because of Multiple Cameras

the small form factor, these intelligent cameras have highly

constrained resources compared with general purpose mo-

bile or data center grade AI processors. However, each cam-

era is still capable of performing pre-processing Mduilrtie-lacytelry af-

ter

imagAReL/iVgahRtc-pwqroeucigeihssstoitr
ion,

signiﬁcantly

reducing

stacked camera
ewxitpheAnI psroivceessinrga
w

image data movement. In a modern HMD system, the

distributed intelligent stacked image sensors and a central

AR/VR processor (aggregator) form the hardware backbone

to realize complex CV/ML functions on device [3, 13, 14].

Within such systems, it’s natural to split the machine learning workload for an application between the sensors and the centralized computer (aggregator). As is shown in Figure 2, for an application that requires multiple layers of convolutional neural networks and fusion of multiple input sources, the early layers can be distributed to the onsensor processing. The feature fusion and rest of processing are on the aggregator. This way, overall system latency can be improved by leveraging direct parallel processing on sensors and reduced sensor-aggregator communication.

The success of distributed computing for DNNs between sensors and aggregator heavily relies on the network architecture to satisfy the application and hardware system constraints such as memory, communication bandwidth, latency target and etc.. Prior work [10, 27, 28, 39] searches network partitions (i.e., the splitting points) for existing models in either exhaustive or heuristic manners. Some work [4, 11, 36, 45, 46] manually injects a bottleneck mod-

12559

Advantages
All on sensors All on aggregator Distributed computing

↓ Comm. Cost
  

↓ Peak Sensor Mem.
  

↑ Parallelism
  

↑ Hardware Utilization
  

Privacy Preserving
  

Table 1. Comparison of different DNN computing ofﬂoad paradigms.

Components Sensor Comm.
Aggregator
Whole system

System factors to be considered
Computation capability Peak memory constraint Number of sensors (i.e., parallelism)
Communication bandwidth
Computation capability
Task performance (e.g., accuracy) Overall latency = on-sen. latency
+ comm. latency + on-agg. latency

Table 2. Factors need to be balanced when distributing DNN computation. The listed factors are interwoven with each other and have to be considered holistically.

ule into the model to reduce communication. However, these methods sometimes obtain naive splitting results (i.e., splitting after the last layer) [28] and lead to performance degradation [35].
The challenge of splitting DNNs comes from the complicated mutual-impact of many model and hardware factors. For example, existing (hand-crafted and searched) model architectures are designed without considering distributed computing over multiple computing modelities, and thus not suitable for splitting in the ﬁrst place. In addition, the position of the splitting point as well as the inserted compression module will simultaneously impact model performance, computation ofﬂoad, communication, and hardware utilization in different directions [37]. Heuristic and rulebased methods are limited in this context.
In this work, we adapt the neural architecture search (NAS) approach to automatically search split-aware model architectures targeting the distributed computing systems on AR/VR glasses. We propose the SplitNets framework that jointly optimizes the task performance and system efﬁciency subject to resource constraints of the mobile AR/VR system featuring smart sensors. We speciﬁcally answer the following two questions: 1. Can we jointly search for optimal network architectures
and their network splitting solution between sensors and the aggregator while satisfying resource constraints imposed by the underlining hardware?
2. Can we learn an optimal network architecture to compress and fuse features from multiple sensors to the aggregator and achieve SOTA performance and efﬁciency compared with conventional centralized models?

We design SplitNets, a split-aware NAS framework, for efﬁcient and ﬂexible searching of the splitting module in the network in the distributed computing context where the splitting module is able to split the network, compress features along the channel dimension for communication saving as well as view fusion for multi-view tasks.
We introduce a series of techniques for module initialization and sampling to stabilize the training with information bottlenecks and mitigate introduced accuracy degradation. We further extend SplitNets to support searching of splitting modules with view fusion for multi-view tasks. To our best knowledge, it is the ﬁrst framework supporting the position searching of information compression / fusion in a multi-input neural network. Overall, our contributions are summarized as follows: • We propose SplitNets, a split-aware NAS framework, for
efﬁcient and ﬂexible position searching of splitting modules for single / multi-view task. • We introduce splitting modules for single- and multiview tasks which can achieve model splitting, feature compression, as well as view fusion simultaneously. To search the optimal position of the splitting module, we propose to use separate supernets for sensors and the aggregator respectively, and stitch them together to form the split-aware model of interest, using the shared splitting module as the joint point. In addition, we combine the compression- / recovery- / fusion- based splitting module design with custom weights initialization, and a novel candidate networks sampling strategy to mitigate the accuracy drop due to model partition. • We evaluate SplitNets with single-view classiﬁcation and multi-view 3D classiﬁcation. Empirical observations validate the importance of joint model and splitting position search to improve both task and system performance. Our results show that optimized network architectures and model partitions discovered by SplitNets signiﬁcantly outperform existing solutions and ﬁt the distributed computing system well on AR/VR glasses.
2. Background and Related Work
We consider a system that consists of three kinds of components, V sensors, one aggregator, and communication interfaces between any of sensors and the aggregator. For simplicity, we assume that sensors are homogeneous.
A DNN is a composition of layers / computation which can be distributed to sensors and aggregator. Compared

12560

with on-sensor computing (All-on-sensor) and conventional mobile computing (All-on-aggregator), distributed computing has the ﬂexibility to achieve a better balance between computation and communication, and enable high utilization of hardware, as summarized in Table 1.
A challenge is to determine a splitting point in the DNN. The on-sensor (on-sen.) part fsen, which is composed of all layers of the DNN before the splitting point, is executed on sensors’ processors. The feature z generated by each sensor will be uploaded to the aggregator. The on-aggregator (onagg.) part fagg, which consists of all remaining layers of the DNN after the splitting point, receives z and ﬁnishes its computation on the computationally more capable aggregator processor.
The problem of ﬁnding the optimal splitting point can be summarized as follows,

min Tsen(fsen, x) + Tcomm(z) + Tagg(fagg, z)
fsen ,fagg

where z = fsen(x)

(1)

s.t. L(fagg ◦ fsen; Dval) ≤ Loss target

PeakMem(fsen, x) ≤ Memsen,

where T (·, ·) is latency measurement function and L is the loss function, e.g., cross entropy. PeakMem(·, ·) measures the peak memory consumption of the on-sensor part, and Memsen is the memory size of a sensor processor.
To ﬁnd the optimal splitting point, one has to take several factors into consideration simultaneously as summarized in Table 2. Existing solutions only consider a fraction of these factors via heuristic or exhaustive approaches [10, 27, 28, 39]. DNNs typically have tens or even hundreds of layers. Therefore, exhaustive searching with retraining is costly. Some literature proposes injecting a hand-crafted compression module to reduce the feature communication [4, 11, 36, 45, 46]. However, manually designing and inserting the compression module not only lead to high engineering effort and training cost, but also result in sub-optimal system performance and accuracy degradation.
In addition, previous methods all consider to split existing models like VGG [47] and ResNet [22]. However, those models may not be suitable for splitting. For example, Kang et al. [28] proﬁles seven models and ﬁnds that their best splitting points typically fall in the ﬁrst or the last layers. Furthermore, injecting compression module to existing models usually leads to signiﬁcant accuracy drop [36].
Other work proposes dedicated lossy (or lossless) compression techniques to reduce intermediate activation and save communication, including compressive sensing [59], pruning [26] and quantization [6, 8, 29]. SplitNets are orthogonal to, and can beneﬁt from these prior approaches.

3. Split-Aware NAS
We adapt NAS to solve the constrained optimization in Equation (1). The goal is to minimize the systemic latency Tsen + Tcomm + Tagg, and improve task performance while satisfying the hardware constraints.
We search the whole model fsen ◦ fagg and the position of splitting point, denoted as Split-Aware- NAS (or SA-NAS). This way, all layers before (or after) the splitting point will compose fsen or fagg, respectively. In addition, we also consider a splitting module which is inserted at the position of splitting point. The feature compression in the splitting module can reduce communication cost by reducing the channel size c of feature tensor z ∈ Rc×s×s, but could affect training stability and result in loss of task performance. We propose several approaches to mitigate the issue. The splitting module evolves along with fsen and fagg during SA-NAS as elaborated in Section 3.1.3. Eventually, the searched network must have exactly one splitting module.
Preliminary: Two-stage NAS Previous NAS approaches that leverage evolutionary search [42, 43, 50, 58] or reinforcement learning [52, 62–64] require excessive training due to the large number of models to be trained in a single experiment. Recent advancement of NAS decouples the model training and architecture search into two separate stages [5, 7, 18, 55, 60] that signiﬁcantly reduces the training cost. In a two-stage NAS framework, the model training stage optimizes a supernet and its all sub-networks with weight-sharing. In the subsequent architecture optimization stage, sub-networks are searched based on system constraints to yield the best task and system performance trade-off. Due to the above advantages, we construct our SA-NAS based on two-stage NAS framework.
3.1. Building and Training Supernet (Stage 1)
Backbone of Supernet We build the supernet’s search space following FBNet-V3 [9] which uses the MobileNetv2 block (denoted as MB, a.k.a., inverted residual block) [44] as meta architecture. Check Appendix C for more details. We augment the supernet with SA-NAS speciﬁc search space (Figure 3) for our experiments as discussed in the following two sections.
3.1.1 SA-NAS for Single-View SplitNets
Now we elaborate how to search SplitNets for a single-view task (e.g., ImageNet classiﬁcation), on searching the position of a splitting module, as shown in Figure 3.
The splitting module for single-view tasks consists of two parts, Conv-Reduce and Conv-Recover, which will be discussed in Section 3.1.3 for their architectures and initialization strategy. The splitting module could be inserted

12561

SplitNets

The i-th Sensor Model Phase

Dynamic reduced 
 #channels: 


Dynamic #blocks after: 
 K = [0, 1, ...]

Dynamic #blocks before: M = [1, 2, ...]

= [8, ...]

...

MB-K

MB-(K+1)
 stride=2

...

MB-M

Gate : g_i = [0,1]

... ... Sen.


Sen.


Sen.


Phase 1

Phase i

Phase N

Dynamic resolutions: [192, 224, 256]

... ... Agg.

Phase 1

Agg.
 Phase i

Agg.
 Phase N

The i-th Aggrgator Model Phase

Searchable inverted residual block (MB)

Dynamic expansion Dynamic output #channels: 


ratio: [4, 8, ...]

[64, 72, ...]

etc.

Shared Gate: g_i = [0,1]

...

MB-K

MB-(K+1)
 stride=2

...

MB-M

Search space from SplitNets

Search space from standard NAS

Conv-Reduce

Conv-Recover (single-view) or View-Fuse (multi-view)

Figure 3. Architecture sampling space in training SA-NAS. Both on-sen. and on-agg. networks consist of multiple phases with each containing a split module, ﬂexible blocks before stride, a stride-2 block, and ﬂexible blocks after stride. The splitting module can be a reduce operator (on-sen., green) or a recover / fuse operator (onagg., purple). SA-NAS also includes standard NAS search space (blue) such as input resolution, channel widths, and etc.

On-Sen. Supernet 1-S
shared
On-Agg. Supernet 1-A

2-S
shared
2-A

3-S
shared
3-A

4-S
shared
4-A

Sampled network-1

1-S

2-S

3-A

4-A

Sampled

network-2 1-S

2-S

3-S

4-A

Figure 4. SA-NAS for single-view SplitNets. At each training step, several sub-networks are sampled from the search space. A sub-network is speciﬁed by a set of choices including on-sen. blocks, splitting position, on-agg. blocks and blocks’ conﬁgurations. For single-view, weight-sharing between on-sen. and onagg. blocks are enabled since they are learning similar features.

after any of inverted residual block (MB) in a supernet theoretically. However, instead of equipping every MB with a candidate splitting module, we divide a supernet into several phases and insert one splitting module for each phase to reduce the number of splitting positions to be optimized.
Suppose a supernet has N stride-2 MB blocks, We divide the whole model into N phases where each phase has one stride-2 MB block. Within a phase, MB blocks before the stride-2 block have the same spatial size and similar channel sizes. We then insert one Conv-Reduce (green trapezium, Figure 3) layer φ : Rc×s×s → Rd×s×s in each phase for the on-sensor supernet, associated with a gate variable gi ∈ {0, 1} indicating whether this layer is selected. A corresponding Conv-Recover (purple trapezium, Figure 3) layer φ : Rd×s×s → Rc×s×s will be inserted into the on-

... ... ... ...
softmax FC

On-Sen. Supernet 1-S

2-S

3-S

4-S

On-Agg. Supernet 1-A

2-A

3-A

4-A

Sampled

1-S

2-S

network-1

V

3-A

4-A

1-S

2-S

Sampled

1-S

2-S

network-2

V

3-S

4-A

1-S

2-S

3-S

Figure 5. SA-NAS for multi-view SplitNets. A convolution layer (Conv-Reduce, purple trapezium) to reduce input’s channel size on the sensor side. The reduced input from V views will be concatenated together (View-Fuse, green trapezium) on the aggregator. In this example, 3-A and 3-S are learning features for local view and mixed views respectively, thus their weights cannot be shared.

aggregator supernet. Conv-Recover layer shares the same gate variable with Conv-Reduce, meaning that they are always selected, or not, at the same time. Using ConvRecover allows weight-sharing of a block between different sampled sub-networks speciﬁed by different Conv-Reduce selections during training.
The number of MB blocks before (variable M ) and after (variable K) the splitting module can be adjusted freely and searched through SA-NAS. This provides us with full search space of the splitting point at ﬁne granularity, as well as a small amount of candidate networks to train. In particular, on-sen. and on-agg. blocks at the same depth learn similar features and can share weights for single-view problems.
In sub-network sampling, once a splitting module (i.e., gi = 1) is selected, we take blocks before splitting from the on-sensor supernet and blocks after splitting from the onaggregator supernet, and stitch them together using splitting module as the joint point to form a sampled network as illustrated in Figure 4.
Eventually, we need exactly one splitting module. Therefore, gate vector g = (g1, . . . , gM ) is restricted to a onehot vector in the resource-constrained architecture search stage. However, during supernet training, we are free to insert more splitting modules to help reduce the maximum performance loss (see Appendix D).

3.1.2 SA-NAS for Multi-View SplitNets
With V sensors in the system, V images are captured from different perspectives. Each sensor processes its captured image with the on-sen. model. Afterwards, a fusion module on the aggregator will fuse the compressed V features from sensors and the on-agg. model further transforms the fused features to the ﬁnal result.
Similar to searching the position of splitting module (i.e., Conv-Reduce and Conv-Recover) for single-view tasks, we

12562

Initialization Method
Kaiming Fan-In or -Out [21] Xavier [17]
Ours

Weights’ Variance

or 2
k2 ·cin

2 k2 cout

2

k2 ·(cin +cout )/2

k2

√2 cin

·cout

Average
No Arithmetic Geometric

Table 3. Comparison of different initialization approaches. Equations in this table assume the activation function as ReLU.

now use SA-NAS to search the position of splitting module (i.e., Conv-Reduce and View-Fuse) for multi-view tasks. Compared with single-view case, we keep the Conv-Reduce layers in the on-sen. supernet, but replace the Conv-Recover layers in the on-agg. supernet with View-Fuse layers to aggregate features across multiple views as illustrated in Figure 5.
Different from single-view case, no weight sharing can be performed between the on-sen. and on-agg. supernets because on-agg. network learns to process fused representation which contains mixed information from V views, while on-sen. supernet processes only local information. In other words, blocks on-sen. and on-agg. have radically different functionalities, and it is sub-optimal to enforce a shared block to play these two roles simultaneously.
3.1.3 Architecture of Splitting Module
Single-View: Reduce and Recover Inspired by [4, 11, 36, 45, 46], we adopt a straight-forward implementation for the splitting module in SA-NAS. As illustrated in Figure 3, we use two convolution layers to compress features: φ : Rc×s×s → Rd×s×s and recover features: φ : Rd×s×s → Rc×s×s on channel dimension, where d can be searched by SA-NAS. In SA-NAS, φ and φ are always selected simultaneously.
Multi-View: Reduce and Fuse Prior work has proposed intricate fusion architecture like recurrent and graph neural networks targeting different applications [16, 20, 34, 49].
Similar to the single-view, multi-view splitting module uses a convolution layer φ : Rc×s×s → Rd×s×s for communication saving, and then fuses these reduced features from V views together with the View-Fuse layer. Without loss of generality, we use a highly simpliﬁed fusion operation - concatenation (See Appendix B). View-Fuse will concatenate V views together along the channel dimension concat V × Rd×s×s → RV ·d×s×s.
Despite of the extreme simplicity of fusion architecture, we empirically show that searching the splitting (i.e., fusion) position jointly with network architecture using SANAS achieves considerable accuracy improvement, compared with SOTA approaches with dedicated fusion layers (see Section 4).
Split-aware Weights Initialization As is also observed by [36], adding compression / fusion modules into supernet

training process degrades the training stability and yields suboptimal training performance. We ﬁnd that the issue is caused by the gradient of the compression module. In practice, the gradients of the compression layer are >10× bigger than regular layers, starting from initialization.
Take Kaiming initialization as an example [21]. Its FanIn mode is to ensure that the output of each layer has zero mean and unit variance , so the output magnitude will not explode in the forward pass. Similarly, the backward pass of a convolution layer is also a convolution but with the transposed weight matrix WT , and applying the same idea will lead to Fan-Out mode .
Either Fan In or Fan Out works well for standard neural architectures because most of layers’ input channel size cin and output channel size cout are similar (see Table 3). However, for Conv-Reduce and Conv-Recover, cin and cout are very different, thus Kaiming and Xavier cannot reconcile both forward and backward at the same time.
In order to mitigate this conﬂict, we use a new initialization st√rategy by replacing cin or cout with their geometic average cin · cout. Using geometric average allows us to ﬁnd a better compromise between forward and backward passes and signiﬁcantly improve training stability and ﬁnal accuracy as shown in Appendix F.
3.2. Resource-constrained searching (Stage 2)
After the supernet training stage, all candidate networks are fully optimized. The next step is to search the optimal network which satisﬁes system constraints. Since two-stage NAS decouples the training and searching, the system constraints are fully conﬁgurable. In addition, one does not have to redo the costly supernet training when the system constraints change.
Since the searching space of supernet is extremely huge, evolution algorithm is adopted to accelerate the searching process [5, 7, 18, 55, 60]. Speciﬁcally, we mutate and crossover pairs of current generation’s networks to generate their children networks. All children networks are evaluated and selected according to their hardware and task performance. The selected children networks will be used to populate candidate networks for the next generation. After several generations, optimal networks can be found.
We consider two kinds of hardware constraints simultaneously. 1) Soft constraints, such as overall latency under the same task accuracy: Children networks with topk smallest latency’s will be selected. 2) Hard constraints, such as peak memory usage: Regardless of accuracy and performance, only children networks that satisfy all hard constraints can be selected to the next generation.
System Hardware Modeling Given a sampled network, its hardware performance is modeled through a hardware simulator. In this work, we use a hardware simulator customized for a realistic HMD system [40, 61] with smart sen-

12563

Top-1 Acc (%) Overall Latency (ms)
Top1 Acc (%)

Figure 6. Hardware model for the target system. On-sensor compute uses a less advanced technology node (16nm) with limited on-chip memory. Smart sensors are connected to an aggregator using a shared 1.2 Gb/s bus. Aggregator uses advanced process (7nm) and has faster compute with larger memory.

79

Neurosurg. All-on-Sen.

77

All-on-Agg. Head-Distill

SplitNets

75

73

4

Sen. Comm. Agg.

79 77

2

75

71 Overa1l0l L0atency (ms1)01 00

Mo2d0el Index 40 73

Figure 7. Left: Comparison of SplitNets with prior approaches using different network architectures on ImageNet from Table 4. Right: Visualization of SplitNets searched models’ accuracy (dashed line and the right y-axis) and overall latency breakdown (the stacking bar plot and the left y-axis). Each searched model’s overall latency is represented by a vertical stacked bar. From bottom to top of each bar, the length’s of three colors represent Tsen, Tcomm, Tagg respectively.

sors (see Figure 6). The on-sensor processor is equipped with a 16nm neural processing unit (NPU) with peak performance of Compsen = 125 GOP/s and peak memory of Memsen = 2 MB. The aggregator processor is modeled with a powerful 7nm NPU with peak performance of Compagg = 1.25 TOP/s and sufﬁcient on-chip memory. The communication between the sensors and the aggregator is modeled with a high-performance shared bus for HMD with peak bandwidth of BWcomm = 1.2 Gbs. The high speed bus will be shared between multiple sensors in practice. More implementation details are summarized in the appendix.

4. Results
We validate that SplitNets can satisfy all system hardware constraints and ﬁnd an optimized model with competitive accuracy and the best system performance compared with SOTA models crafted by hands or searched by standard NAS methods. Speciﬁcally, we evaluate our SplitNets for single-view system on ImageNet classiﬁcation (Section 4.1) and SplitNets for multi-view system on 3D classiﬁcation (Section 4.2).

4.1. Single-view Task: ImageNet
For the distributed computing with a single-view system, we use the public large-scale single-view classiﬁcation dataset, ImageNet, to train and test the method and evaluate performance. Results are summarized in Table 4.
SplitNets training / searching conﬁgurations In supernet training, we sample 5 kinds of networks (see Appendix D) and optimize for 360 epochs with batch size 4096 using the standard training receipt from [54]. In resource-constrained searching, we sample 512 candidate networks with 20 generations using evolution algorithm [9, 54, 55].
Comparing SplitNets against SOTA Methods we compare SplitNets against several existing models which are handcrafted or searched by existing NAS methods. The compared models include MobileNet-v2 [44], MNASNet [52], EfﬁcientNet [53], ResNet [22], DenseNet [25], Inception-v3 [51], and RegNet [41], where the ﬁrst three architectures are speciﬁcally designed for mobile on-device computing. To make the evaluation more realistic to realtime applications [32], we assume each of the four sensors is allocated 25% bandwidth of the shared bus. We also assume that all models’ weights and activation are quantized to 8-bit without accuracy degradation. (And literature has shown this assumption is reasonable. [12, 15, 30, 56]) If models have larger bitwidth, SplitNets will beneﬁt more because the communication saving from feature compression is more pronounced.
We compare SA-NAS approach with four baseline model split methods: • “All-on-sen.”: All computation happens on the sensor.
The communication overhead is negligible but the peak memory often exceeds sensor’s memory capacity. In our experiments, we discover that all existing models, even lightweight ones, require >5MBs peak memory, which are too large for on-sensor deployment. • “All-on-agg.”: Transmit raw image to aggregator and perform all computation on aggregator. Communication becomes the bottleneck, and the system is not scalable with increasing number of sensors. • “Neurosurgeon [28]”: a heuristic method which proﬁles each layer and exhaustively searches every possible splitting position to ﬁnd the optimal partition. Neurosurgeon performs the model split for many networks and ﬁnds that the optimal split often falls in the beginning or the last layer, which degenerates to “All-on-agg.” or “All-onsen.”. We apply “Neurosurgeon [28]” on recent efﬁcient architectures and observe similar results. This validates the necessity of joint designing network and the split. • “Head Distill [23]”: a method that ﬁrst manually determines the position of splitting point and then replaces the head part with a smaller networks through knowledge distillation. This approach is able to signiﬁcantly

12564

HW Constraints

Method

Backbone

Top-1

Memsen ≤ 2MB
Compsen (16nm) = 125 GOP/s
Compagg (7nm) = 1.25 TOP/s
BWcomm = 37.5 MB/s
∗: For all models, We assume weights
and activation are quantized to 8-bit without loss of accuracy.

All-on-sen. All-on-agg. Neurosurgeon Head Distill SplitNets

MobileNet-v2 MNASNet-1.0§ EfﬁcientNet-B0§
ResNet-152 RegNetX-3.2GF§
MobileNet-v2 MNASNet-1.0§ EfﬁcientNet-B0§
ResNet-152 RegNetX-3.2GF§
MobileNet-v2 MNASNet-1.0§ EfﬁcientNet-B0§
ResNet-152 RegNetX-3.2GF§
ResNet-152† DenseNet-169† Inception-v3†
A§ B§ C§ D§ E§ F§

71.88 73.46 77.13 78.31 78.36 71.88 73.46 77.13 78.31 78.36 71.88 73.46 77.13 78.31 78.36 75.13↓ 72.03↓ 75.78↓ 73.15 73.45 76.98 77.42 78.56 78.91

#Params
3.51M 4.38M 5.30M 60.2M 15.3M
0 0 0 0 0 0 82.0K 0 0 0 12.8K 12.8K 12.8K 93.4K 0.12M 92.8K 208K 211K 98.0K

On-sensor #OPs Peak

301M 314M 386M 11.5G 3.18G
0 0 0 0 0 0 103 M 0 0 0 125M 125M 223M 63.7M 70.5M 98.6M 137 M 214 M 194 M

5.90MB 5.59MB 7.70MB 120 MB 29.3MB
0 0 0 0 0 0 1.29MB 0 0 0 0.82MB 0.82MB 1.45MB 0.68MB 1.00MB 1.14MB 1.52MB 1.54MB 1.76MB

Latency
2.46ms 2.52ms 3.09ms 92.1ms 25.4ms
0 0 0 0 0 0 0.83ms 0 0 0 1.00ms 1.00ms 1.78ms 0.51ms 0.56ms 0.79ms 1.10ms 1.71ms 1.55ms

Comm. Size Latency

0
0
0
0
0 3 · 2242 3 · 2242 3 · 2242 3 · 2242 3 · 2242 3 · 2242 80 · 142 3 · 2242 3 · 2242 3 · 2242 12 · 292 12 · 292 12 · 382 18 · 122 18 · 122 18 · 162 18 · 162 18 · 182 32 · 362

0 0 0 0 0 4.01ms 4.01ms 4.01ms 4.01ms 4.01ms 4.01ms 0.84ms 4.01ms 4.01ms 4.01ms 0.27ms 0.27ms 0.46ms 0.07ms 0.07ms 0.13ms 0.13ms 0.16ms 1.11ms

On-aggregator #Params #OPs Latency

0 0 0 0 0 3.51M 4.38M 5.3M 60.2M 15.3M 3.51M 4.30M 5.3M 60.2M 15.3M 60.8M 14.9M 24.1M 6.73M 7.43M 7.63M 7.85M 7.85M 8.13M

0 0 0 0 0 301M 314M 386M 11.5G 3.18G 301M 212M 386M 11.5G 3.18G 11.8G 3.27G 5.03G 372M 402M 727M 831M 1050M 1.18G

0 0 0 0 0 0.24ms 0.25ms 0.31ms 9.21ms 2.54ms 0.24ms 0.17ms 0.31ms 9.21ms 2.54ms 9.45ms 2.61ms 4.02ms 0.30ms 0.32ms 0.58ms 0.66ms 0.84ms 0.94ms

Overall Latency
2.46ms 2.52ms 3.09ms 92.1ms 25.4ms 4.25ms 4.26ms 4.32ms 13.2ms 6.60ms 4.25ms 1.82ms 4.32ms 13.2ms 6.60ms 10.72ms 3.88ms 6.26ms 0.88ms 0.95ms 1.49ms 1.88ms 2.71ms 3.59ms

Table 4. Overall latency comparison on single-sensor system for ImageNet classiﬁcation task. Green (or red) numbers mean the on-sensor memory consumption is less (or more) than a sensor’s memory constraint. §: The backbone model is from NAS methods. †: The model,
splitting point and accuracy are directly from “Head Distill” [36].

Method
MVCNN-su MVCNN-su MVCNN-su MVCNN-su MVCNN-new MV-LSTM SeqViews Auto-MVCNN Auto-MVCNN
SplitNets SplitNets SplitNets

Backbone
VGG-M MobileNet-v3 MNASNet-0.5 EfﬁcientNet-B0
VGG-11 ResNet-18 VGG-19 AM-c24 AM-c36
A B C

#Params
90.5M 2.54M 2.22M 5.29M 132 M 11.2M 144M 2.10M 4.70M
1.39M 2.44M 2.32M

#OPs
34.6G 661M 1.24G 4.62G 90.0G 21.9G 235 G 3.20G 6.90G
513M 759M 1.83G

Position

Fusion Feature size

Last Conv Last Conv Last Conv Last Conv Last Conv Last Conv Penultimate FC Last Conv Last Conv

512 · 132 576 · 72 1280 · 72 1280 · 72 512 · 72 512 · 72
4096 · 1
-
-

6-th (tot. 8) block 8-th (tot. 13) block 8-th (tot. 13) block

16 · 62 6 · 122 6 · 142

Arch.
View-Pool View-Pool View-Pool View-Pool View-Pool Bi-LSTM Bi-RNN Searched Searched
Concat. Concat. Concat.

Top-1
81.1 86.0 91.4 92.1 88.7 89.1 89.3 90.5 91.0
92.3 93.0 93.8

Table 5. Performance comparison on multi-view 3D classiﬁcation for MVCNN-su [48], MVCNN-new [49], MV-LSTM [34], SeqViews [20], Auto-MVCNN [31], and SplitNets. We assume that all models are not pre-trained on ImageNet.

94

Top1 Acc (%)

92

90

SplitNets

Other Methods

S0e.6n./Agg0..8Ratio1.0

Figure 8. We compare achieved accuracy as a function of the ratio, #OPs on one sen. / (#OPs on one sen. + #OPs on the agg.). Since prior methods conduct fusion around the ﬁnal layer, their ratios are ∼1. In contrast, SplitNets are able to explore various positions and pick the best one.

reduce the communication cost compared with “All-onagg.” and satisfy the hardware constraints. However, “Head Distill [23]” introduces 1.5%-3.6% accuracy drop even for parameter-redundant architectures like ResNet152, DenseNet-169, and Inception-v3. Also, exhaustively searching the splitting point and hyper-parameters of the on-sen. network in [23] also requires redoing the costly training, which is inefﬁcient for deployment.
In the bottom of Table 4, we report a series of networks searched by SA-NAS. Under different accuracy targets, SplitNets consistently outperforms prior methods (“Neurosurgeon” [28] and “Head Distill” [23]) on overall latency while satisfying all hardware constraints. In addition, SplitNets are able to balance the workload of sensor and ag-

gregator and make trade-off among communication, accuracy and computation when system hardware conﬁguration changes (See Appendix H).
4.2. Multi-view Task: 3D Classiﬁcation
We also evaluate SplitNets on a multi-view system with ModelNet 3D Warehouse classiﬁcation dataset that consists of 3D CAD models from 40 categories. CAD models in this benchmark come from 3D Warehouse [1]. Each 3D object is captured from 12 different views by 12 sensors, where each view is a 2D image of size 3 × 224 × 224.
Searching Fusion Module’s Position Beneﬁts Accuracy In Table 5, we compare the task performance of SplitNets models against existing models with various back-

12565

HW Constraints

Split

Backbone

Top-1

VGG-11

88.7

ResNet-18

89.1

Sensor × 12 All-on-agg. MobileNet-v3 86.0

Aggregator × 1

MNASNet-0.5 † 91.4

EfﬁcientNet-B0 † 92.1

BWcomm =

VGG-11

88.7

12.5 MB/s

ResNet-18

89.1

Split at fusion MobileNet-v3 86.0

∗: The others

MNASNet-0.5§ 91.4

are same as

EfﬁcientNet-B0§ 92.1

as Table 4.

A§

92.3

SplitNets

B§

93.0

C§

93.8

#Params
0 0 0 0 0 9.22M 11.2M 0.93M 0.94M 4.01M 0.14M 0.17M 0.18M

On-each-sensor #OPs Peak

0 0 0 0 0 7.49G 1.81G 54.9M 103M 385M 41.8M 58.7M 147M

0 0 0 0 0 12.4MB 12.8MB 1.38MB 1.54MB 6.42MB 0.06MB 0.08MB 0.11MB

Latency
0 0 0 0 0 59.9ms 14.5ms 0.44ms 0.83ms 3.08ms 0.33ms 0.47ms 1.17ms

Comm. Size Latency

3 · 2242 3 · 2242 3 · 2242 3 · 2242 3 · 2242 512 · 72 512 · 72 576 · 72 1280 · 72 1280 · 72 16 · 62 6 · 122 6 · 142

12.0ms 12.0ms 12.0ms 12.0ms 12.0ms 2.00ms 2.00ms 2.26ms 5.03ms 5.03ms 0.05ms 0.07ms 0.09ms

On-aggregator #Params #OPs Latency

132 M 11.7M 2.55M 2.22M 5.29M 123M 0.51M 1.62M 1.28M 1.28M 1.25M 2.27M 2.15M

90.0G 21.8G 661M 1.24G 4.62G 123M 0.51M 1.62M 1.28M 1.28M 11.4M 55.4M 73.3M

73.2ms 17.7ms 0.58ms 1.01ms 3.76ms 0.10ms 0.40ns 1.29ns 1.03ns 1.03ns 0.01ms 0.06ms 0.07ms

Overall Latency
84.9ms 29.7ms 12.6ms 13.0ms 15.8ms 62.1ms 16.5ms 2.71ms 5.83ms 8.03ms 0.39ms 0.59ms 1.34ms

Table 6. Overall latency comparison on a multi-sensor system with a 3D classiﬁcation task. Since 12 sensors are assumed in this task,
each sensor is allocated 12.5MB/s bandwidth. Green (or red) numbers mean the on-sensor memory consumption is less (or more) than a sensor’s memory constraint. §: The backbone model is from NAS methods.

2
3 1

... ... ... ... ... ...

1

192 16

2 64 6

3 24 3

12

12

12

Stride-1 Conv
Stride-2 Conv
Conv-reduce
Concat.
* M N: Channel size is reduced from M to N.

* The length of each vertical lines presents output channel size.


Figure 9. Left: Accuracy (stars and the right y-axis) and overall latency breakdown (the stacking bar plot and the left y-axis) of searched networks by SplitNets. Each searched model’s overall latency is represented by a vertical stacked bar. From bottom to top of each bar, the length’s of three colors represent Tsen, Tcomm, Tagg respectively. Right: Visualization of three searched networks by SplitNets. The length of vertical lines represents the output channel size of a convolution layer. Dash means that the stride size is two. The green and purple trapezoids are Conv-Reduce and View-Fuse. Among the three networks, the ﬁrst and third networks conduct fusion either too ‘late’ or too ‘early’, leading sub-optimal task and system performance.

bones [22, 24, 47, 52, 53] with dedicated fusion modules [31]. We assume all models are trained from scratch. Prior methods do not have the ﬂexibility in adjusting the position of fusion module, and typically fuse features after the last convolution. SplitNets are able to explore all possible fusion positions (Figure 9, Right) and learn to pick the optimal split. Despite of the simplicity of fusion used, SplitNets signiﬁcantly outperforms prior methods with fewer parameters and less computation.
Superior Overall Latency with SOTA Accuracy We further demonstrate the beneﬁts of SplitNets for distributed computing on a 12-sensor system in Table 6. Compared to single-view system, multi-view system has heavier communication burden because multiple sensors have to share a single serial communication bus. As a result, the communication advantage of SplitNets is more pronounced. We visualize networks found by SplitNets in Figure 9. The position of fusion module keeps changing given different latency and accuracy targets. Interestingly, the best accuracy is not strictly correlated with the largest model which further validates the necessity of searching the best model

instead of increasing model size.
5. Conclusion and Discussion
In summary, we introduce SplitNets with SA-NAS for efﬁcient partition and inference of ML models on-device with distributed smart sensors. The proposed SA-NAS approach enables end-to-end model search with ﬂexible positioning of a splitting module for single-view and multi-view problems. The resource-constrained searching successfully identiﬁes model architectures that reduce overall system latency, satisfy hardware constraints, while maintaining high accuracy. Empirical results on ImageNet (single-view) and 3D Classiﬁcation (multi-view) show that our approach can discover SOTA network architectures and model splitting solutions for distributed computing systems on AR/VR glasses.
Limitations and future work. The hardware model we adopt leverages an analytical model combing different hardware modalities, which are calibrated separately. Full system simulation with cycle accurate hardware models could be deployed for more precise latency evaluation.

12566

References
[1] 3d warehouse. https://3dwarehouse.sketchup. com/. 7, 12
[2] Oculus quest 2: Our most advanced new all-in-one vr headset. 14
[3] Micheal Ambrash. Creating the future: Augmented reality, the next human-machine interface. pages 1–4, 2021. 1
[4] Juliano S Assine, Eduardo Valle, et al. Single-training collaborative object detectors adaptive to bandwidth and computation. arXiv preprint arXiv:2105.00591, 2021. 1, 3, 5
[5] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and specialize it for efﬁcient deployment. In International Conference on Learning Representations, 2020. 3, 5, 12
[6] Hyomin Choi and Ivan V Bajic´. Deep feature compression for collaborative object detection. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 3743– 3747. IEEE, 2018. 3
[7] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12239–12248, 2021. 3, 5
[8] Robert A Cohen, Hyomin Choi, and Ivan V Bajic´. Lightweight compression of neural network feature tensors for collaborative intelligence. In 2020 IEEE International Conference on Multimedia and Expo (ICME), pages 1–6. IEEE, 2020. 3
[9] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, and Joseph E. Gonzalez. Fbnetv3: Joint architecture-recipe search using neural acquisition function. CoRR, abs/2006.02049, 2020. 3, 6, 12
[10] Amir Erfan Eshratifar, Mohammad Saeed Abrishami, and Massoud Pedram. Jointdnn: An efﬁcient training and inference engine for intelligent mobile cloud computing services. IEEE Transactions on Mobile Computing, 2019. 1, 3
[11] Amir Erfan Eshratifar, Amirhossein Esmaili, and Massoud Pedram. Bottlenet: A deep learning architecture for intelligent mobile cloud computing services. In 2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED), pages 1–6. IEEE, 2019. 1, 3, 5
[12] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. ICLR, 2020. 6
[13] Abrash et al. Creating the future: Augmented reality, the next human-machine interface. IEEE IEDM, 2021. 1
[14] Pinkham et al. Near-sensor dist. dnn processing for ar/vr. IEEE ESTCS, 2021. 1
[15] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Re´mi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. ICLR, 2021. 6
[16] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. Gvcnn: Group-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 264–272, 2018. 5, 12
[17] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-

culty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. 5, 14 [18] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path oneshot neural architecture search with uniform sampling. In European Conference on Computer Vision, pages 544–560. Springer, 2020. 3, 5 [19] Shangchen Han, Beibei Liu, Randi Cabezas, Christopher D Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, et al. Megatrack: monochrome egocentric articulated hand-tracking for virtual reality. ACM Transactions on Graphics (TOG), 39(4):87–1, 2020. 1 [20] Zhizhong Han, Mingyang Shang, Zhenbao Liu, Chi-Man Vong, Yu-Shen Liu, Matthias Zwicker, Junwei Han, and CL Philip Chen. Seqviews2seqlabels: Learning 3d global features via aggregating sequential views by rnn with attention. IEEE Transactions on Image Processing, 28(2):658– 672, 2018. 5, 7 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026– 1034, 2015. 5, 14 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 3, 6, 8, 14 [23] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 6, 7, 12 [24] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314–1324, 2019. 8 [25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017. 6 [26] Mikolaj Jankowski, Deniz Gu¨ndu¨z, and Krystian Mikolajczyk. Joint device-edge inference over wireless links with pruning. In 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), pages 1–5. IEEE, 2020. 3 [27] Hyuk-Jin Jeong, InChang Jeong, Hyeon-Jae Lee, and SooMook Moon. Computation ofﬂoading for machine learning web apps in the edge server environment. In 2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS), pages 1492–1499. IEEE, 2018. 1, 3 [28] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia Tang. Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge. In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems. ACM, 2017. 1, 2, 3, 6, 7 [29] Guangli Li, Lei Liu, Xueying Wang, Xiao Dong, Peng Zhao,

12567

and Xiaobing Feng. Auto-tuning neural network quantization framework for collaborative inference between the cloud and edge. In International Conference on Artiﬁcial Neural Networks, pages 402–411. Springer, 2018. 3 [30] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-oftwo quantization: An efﬁcient non-uniform discretization for neural networks. ICLR, 2019. 6 [31] Zhaoqun Li, Hongren Wang, and Jinxing Li. Auto-mvcnn: Neural architecture search for multi-view 3d shape recognition. arXiv preprint arXiv:2012.05493, 2020. 7, 8 [32] Chiao Liu, Andrew Berkovich, Qing Chao, Song Chen, Ziyun Li, Hans Reyserhove, Syed Shakib Sarwar, and TsungHsun Tsai. Intelligent vision sensors for ar/vr. In Imaging Systems and Applications, pages ITu5G–1. Optical Society of America, 2020. 1, 6 [33] Chiao Liu, Andrew Berkovich, Song Chen, Hans Reyserhove, Syed Shakib Sarwar, and Tsung-Hsun Tsai. Intelligent vision systems–bringing human-machine interface to ar/vr. In 2019 IEEE International Electron Devices Meeting (IEDM), pages 10–5. IEEE, 2019. 1 [34] Chao Ma, Yulan Guo, Jungang Yang, and Wei An. Learning multi-view representation with lstm for 3-d shape recognition and retrieval. IEEE Transactions on Multimedia, 21(5):1169– 1182, 2018. 5, 7 [35] Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato, and Sameer Singh. Distilled split deep neural networks for edge-assisted real-time systems. In Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges, pages 21–26, 2019. 2 [36] Yoshitomo Matsubara, Davide Callegaro, Sabur Baidya, Marco Levorato, and Sameer Singh. Head network distillation: Splitting distilled deep neural networks for resource-constrained edge computing systems. IEEE Access, 8:212177–212193, 2020. 1, 3, 5, 7 [37] Yoshitomo Matsubara, Marco Levorato, and Francesco Restuccia. Split computing and early exiting for deep learning applications: Survey and research challenges. arXiv preprint arXiv:2103.04505, 2021. 2 [38] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE transactions on robotics, 31(5):1147–1163, 2015. 1 [39] Daniele Jahier Pagliari, Roberta Chiaro, Enrico Macii, and Massimo Poncino. Crime: Input-dependent collaborative inference for recurrent neural networks. IEEE Transactions on Computers, 2020. 1, 3 [40] Guofang Qin and Guoliang Qin. Virtual reality video image classiﬁcation based on texture features. Complexity, 2021, 2021. 1, 5 [41] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dolla´r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428–10436, 2020. 6 [42] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiﬁer architecture search. In Proceedings of the aaai conference on artiﬁcial intelligence, volume 33, pages 4780–4789, 2019. 3 [43] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Sax-

ena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classiﬁers. In International Conference on Machine Learning, pages 2902–2911. PMLR, 2017. 3 [44] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4510–4520. Computer Vision Foundation / IEEE Computer Society, 2018. 3, 6 [45] Marion Sbai, Muhamad Risqi U Saputra, Niki Trigoni, and Andrew Markham. Cut, distil and encode (cde): Split cloudedge deep inference. In 2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), pages 1–9. IEEE, 2021. 1, 3, 5 [46] Jiawei Shao and Jun Zhang. Bottlenet++: An end-to-end approach for feature compression in device-edge co-inference systems. In 2020 IEEE International Conference on Communications Workshops (ICC Workshops), pages 1–6. IEEE, 2020. 1, 3, 5 [47] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 3, 8 [48] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945–953, 2015. 7 [49] Jong-Chyi Su, Matheus Gadelha, Rui Wang, and Subhransu Maji. A deeper look at 3d shape classiﬁers. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0–0, 2018. 5, 7 [50] Masanori Suganuma, Mete Ozay, and Takayuki Okatani. Exploiting the potential of standard convolutional autoencoders for image restoration by evolutionary search. In International Conference on Machine Learning, pages 4771–4780. PMLR, 2018. 3 [51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826, 2016. 6 [52] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 2820–2828. Computer Vision Foundation / IEEE, 2019. 3, 6, 8 [53] Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6105–6114. PMLR, 2019. 6, 8 [54] Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra.

12568

Attentivenas: Improving neural architecture search via attentive sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 6, 12 [55] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efﬁcient natural language processing. In Annual Conference of the Association for Computational Linguistics, 2020. 3, 5, 6 [56] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 6 [57] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912–1920, 2015. 12 [58] Lingxi Xie and Alan Yuille. Genetic cnn. In Proceedings of the IEEE international conference on computer vision, pages 1379–1388, 2017. 3 [59] Shuochao Yao, Jinyang Li, Dongxin Liu, Tianshi Wang, Shengzhong Liu, Huajie Shao, and Tarek Abdelzaher. Deep compressive ofﬂoading: Speeding up neural network inference by trading edge computation for network latency. In Proceedings of the 18th Conference on Embedded Networked Sensor Systems, pages 476–488, 2020. 3 [60] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. In European Conference on Computer Vision, pages 702–717. Springer, 2020. 3, 5, 12 [61] Lim Jia Zheng, James Mountstephens, and Jason Teo. Fourclass emotion classiﬁcation in virtual reality using pupillometry. Journal of Big Data, 7(1):1–9, 2020. 1, 5 [62] Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural network architecture generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2423–2432, 2018. 3 [63] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. [64] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697–8710, 2018. 3
12569

