2022 32nd International Conference on Field-Programmable Logic and Applications (FPL)

SAMO: Optimised Mapping of Convolutional Neural Networks to Streaming Architectures

Alexander Montgomerie-Corcoran∗, Zhewen Yu∗ and Christos-Savvas Bouganis Imperial College London, UK
{alexander.montgomerie-corcoran15, zhewen.yu18, christos-savvas.bouganis}@imperial.ac.uk

Abstract—Signiﬁcant effort has been placed on the development of toolﬂows that map Convolutional Neural Network (CNN) models to Field Programmable Gate Arrays (FPGAs) with the aim of automating the production of high performance designs for a diverse set of applications. However, within these toolﬂows, the problem of ﬁnding an optimal mapping is often overlooked, with the expectation that the end user will tune their generated hardware for their desired platform. This is particularly prominent within Streaming Architecture toolﬂows, where there is a large design space to be explored. In this work, we establish the framework SAMO: a Streaming Architecture Mapping Optimiser. SAMO exploits the structure of CNN models and the common features that exist in Streaming Architectures, and casts the mapping optimisation problem under a uniﬁed methodology. Furthermore, SAMO explicitly explores the reconﬁgurability property of FPGAs, allowing the methodology to overcome mapping limitations imposed by certain toolﬂows under resource-constrained scenarios, as well as improve on the achievable throughput. Three optimisation methods - BruteForce, Simulated Annealing and Rule-Based - have been developed in order to generate valid, high performance designs for a range of target platforms and CNN models. Results show that SAMO-optimised designs can achieve 4x-20x better performance compared to existing hand-tuned designs. The SAMO framework is open-source: https://github.com/AlexMontgomerie/samo.
Index Terms—streaming architecture, neural network accelerator, optimisation
I. INTRODUCTION
The success of deep learning models, primarily in the form of CNNs, has fuelled research into custom hardware accelerators tuned for speciﬁc models. A popular type of platform for these accelerators are FPGAs, as their versatility and range of sizes suit a variety of applications. Many toolﬂows have been developed that reduce the time to develop such FPGA-based systems, and the architectures generated by these toolﬂows generally come under two categories: Systolic Array and Streaming (dataﬂow) architectures [1].
Systolic Array architectures have the ability to execute nearly any CNN model by mapping all the convolution layers to a time-shared matrix multiplication engine. Due to this ﬂexibility, Systolic Array architectures are often served as compute kernels in general-purpose neural network accelerators that are not tailored to speciﬁc CNN models [2], [3].
Streaming Architectures, on the contrary, tailor their hardware towards the computation and memory workload of a speciﬁc CNN model, with the promise of greater performance. Instead of time-sharing processing elements between layers,
∗equal contribution

each layer is mapped to a custom computation kernel tailored to the characteristics of the layer, where the overall CNN computation is achieved by pipelining these kernels. As such, Streaming Architectures have higher throughput and energy efﬁciency compared to equivalent Systolic Array architectures [4]. However, the design process for a Streaming Architecture accelerator is often more time consuming and tedious, as the customisability brings forward a large design space to be explored. Early work on Streaming Architecture accelerators explored the design space manually, frequently resulting in many sub-optimal designs [5], [6]. Several works have proposed algorithms that allow automatic design space exploration [7]–[9], which are closely coupled to the speciﬁc accelerator framework and do not generalise to other toolﬂows.
This paper introduces the SAMO framework, which addresses the optimisation stage of mapping CNN models to Streaming Architectures in a uniﬁed manner. Key innovations of the tool are the introduced abstract representation for capturing the characteristics of the accelerator’s building blocks (both performance and resource requirements) and the Hardware Description Graph (HD-Graph), a data structure that allows the optimisation of the CNN mapping to a target FPGA device. Furthermore, the proposed framework makes explicit use of the reconﬁguration feature of FPGA devices, and introduces a partitioning methodology that allows CNN mapping toolﬂows to achieve high throughput designs, as well as to produce valid designs even in a resource constraint setting.
II. BACKGROUND
Among existing Streaming Architecture toolﬂows, fpgaConvNet [10], FINN [11] and HLS4ML [6] are good representatives of that set due to their popularity within the research community, as well as their wide variation in the accelerator design space that they deﬁne [1]. These three toolﬂows are referred to as backends throughout the rest of this paper.
The core computation unit in FINN is the Matrix-Vector Threshold Unit (MVTU) which contains a conﬁgurable number of Processing Elements (PE) and SIMD lanes for parallel Matrix-Vector operations [4], [12]. The number of PE and SIMD lanes for each MVTU are reﬁned by iteratively allocating extra resources to the slowest MVTU in the whole accelerator [11]. However, the authors of FINN also state this algorithm to be sub-optimal and can often be outperformed

1946-1488/22/$31.00 ©2022 IEEE

418

DOI 10.1109/FPL57034.2022.00069

by hand-tuning1. FINN has also recently been extended into multi-FPGA execution where Integer Linear Programming is used to balance the workload between multiple FPGAs [13]. However the design mapped in each FPGA is still manually tuned.
HLS4ML sacriﬁces the conﬁgurability of the accelerator to obtain simple and low-latency designs [6], [14]. Therefore, it deﬁnes a relatively small design space compared with other toolﬂows and supports two hardware generation modes: resource and latency. For the latency mode, the design architecture is completely unrolled, and the HLS compiler is given the task of optimising the latency for a given initiation interval target. In the case of the resource mode, the hardware is only partially unrolled, and the performance of the design is manually tuned through the reuse-factor parameter, which deﬁnes how often resources are re-used.
fpgaConvNet [10] focuses on the conﬁgurability of the accelerator and explores several degrees of parallelism in the network including coarse-grained folding and ﬁne-grained folding. The toolﬂow further supports bitstream reconﬁguration and partial reloading of weights in order to overcome limitations imposed due to resource constraints. Therefore, fpgaConvNet deﬁnes the largest design space amongst the three considered toolﬂows. In terms of the optimisation algorithm, fpgaConvNet proposed a Simulated Annealing optimiser [9] to explore its design space, however this has been tailored to this speciﬁc toolﬂow.
To summarise, the above toolﬂows provide conﬁgurable hardware building blocks and deﬁne a large design space to expose a trade-off between performance and resources. However, a uniﬁed way of exploring the space across different toolﬂows is still missing. SAMO aims to ﬁll this research gap by capitalising on the common characteristics of the CNN models and streaming architectures.
III. GENERALISED OPTIMISATION PROBLEM
This section outlines the generalised optimisation problem of tailoring a parameterised Streaming Architecture to a target CNN and FPGA pair. The aim is to efﬁciently utilise the available resources on the platform to optimise for either a throughput or latency objective, for a given network. For simplicity, this section only discusses sequential CNN models, even though SAMO can support residual networks as well.
A. Hardware Description Graph
In order to generalise the optimisation of the CNN mapping to a target FPGA device, SAMO proposes a data structure called the Hardware Description Graph (HD-Graph), which provides an abstraction of both the CNN model topology and the hardware building block implementation. The deﬁnition of the HD-Graph is as follows:
Assume the given CNN model is represented as a Directed Acyclic Graph with L layers as M = {l1, . . . , lL}, where li is the ith layer within the CNN model. This graph has edges EM
1https://github.com/Xilinx/ﬁnn/blob/main/src/ﬁnn/transformation/ fpgadataﬂow/set folding.py

between the layers. For sequential networks, the edges are only between adjacent nodes, so EM = {(l1, l2), . . . , (lL−1, lL)}.
SAMO uses a parser (elaborated on in Section IV-A) to translate each layer of the CNN model to a computation node or set of computation nodes in the HD-Graph, where each computation node corresponds to a parameterised hardware building block implemented in the backend. After the translation, the HD-Graph is described as H, which contains N computation nodes, H = {n1, . . . , nN } where ni is the ith node within it. As the CNN models are sequential, so is the HD-Graph, and the edges of H are such that EH = {(n1, n2), . . . , (nN−1, nN )}.

B. Partitioning

SAMO introduces a partitioning methodology to gen-

erate high throughput designs by reconﬁguring and time-

multiplexing hardware building blocks on the single FPGA

device. The partitioning methodology is described in terms of

cuts of the HD-Graph H, which transform it into multiple

sub-graphs P where each sub-graph constitutes an FPGA

conﬁguration containing a subset of nodes from the HD-Graph.

The positions of where the cuts take place are represented

by the optimisation variable C = {e1, . . . , e|P |−1}, and |P | denotes the number of sub-graphs. As such, the computation

nodes from which the sub-graphs P are constructed are

deﬁned in Eq. (1).

⎧ ⎪⎨{nei+1, . . . , nei+1 } ∀ ei ∈ C

|C| > 1

P

=

⎪⎩{{nn11

, ,

. .

. .

. .

, ,

nei nN

}, }

{nei+1 , . . . , nN }

|C| = 1 |C| = 0

(1)

This leads to the properties that the partitions are disjoint ( P = ∅) and complete ( P = H).

C. Variables
In the HD-graph, each computation node corresponds to a parameterised hardware building block whose implementation is backend-speciﬁc. To capture the possible parametrisation of the current landscape of Streaming Architecture frameworks, SAMO deﬁnes three associated variables for each node ni: input channel folding (sIi ), output channel folding (sOi ) and kernel folding (ki). These variables can be vectorised across all nodes (sI , sO, k).
The input and output channel folding variables describe the degree of parallelism of the channel dimension of the featuremap entering and exiting a node respectively. The kernel folding variable describes the parallelism within a node.

D. Objective
With all the optimisation variables deﬁned, the optimisation problem is now outlined. SAMO supports two optimisation objectives, which are latency minimisation and throughput maximisation. Both objectives depend on the variables V = {C, sI , sO, k}, which denote the HD-Graph conﬁguration.

419

Fig. 1: Overview of the proposed SAMO framework

As the hardware building blocks are pipelined and the CNN
models are assumed to be sequential, the latency of each subgraph is dictated by its slowest node2, deﬁned in Eq. (2).

T (Pi) = max{t(nj|sIj , sOj , kj) : nj ∈ Pi}

(2)

t(nj|sIj , sOj , kj) and T (Pi) denotes the latency estimate for the node nj and the partition Pi respectively. When
considering the whole HD-graph, the objective for minimising

latency of a design is described in Eq. (3), where tconf is the reconﬁguration time of the device.

O(V ) =

T (Pi) + |C| · tconf

(3)

Pi ∈P

Conversely, the objective of maximising throughput for a

given FPGA and network pair is described in Eq. (4), where

B is the batch size for the inputs to be run.

O(V ) = − B ·

B Pi∈P T (Pi) + |C| · tconf

(4)

The above two optimisation objectives can be used to

construct the following optimisation problem described in

Eq. (5), where O represents the objective.

min O(V ), V = {C, sI , sO, k}

(5)

V

E. Constraints

In order to generate a valid and synthesisable accelerator design, there are certain constraints imposed on the optimisation problem. In this subsection, we deﬁne the constraints that are observed across all backends, although speciﬁc backends are not necessarily constrained by all.
Eq. (6) deﬁnes the resource constraint that each partition must ﬁt within the FPGA on-chip resource budget.

R(Pi) =

r(nj |sIj , sOj , kj )

nj ∈Pi

(6)

R(Pi) ≤ Rplatform, ∀ Pi ∈ P

r(nj|sIj , sOj , kj) and R(Pi) denote the resource utilisation per node and per partition respectively. The resource types
considered include DSP, BRAM, LUT and FF. Apart from on-chip resources, the memory bandwidth for a
given partition is also bounded, as described in Eq. (7).

B(Pi)

=

DI (Pi) + DO(Pi) T (Pi)

≤

Bplatf orm,

∀

Pi

∈

P

(7)

2Pipeline depth is ignored as it has a negligible effect on latency.

where DI (Pi) and DO(Pi) are the folded dimensions of the feature-map in and out of the partition respectively, and Bplatform is the bandwidth upper bound of the given platform.
For backends which do not support padding, the channel
folding variables must be factors of the channel dimension of
the feature-map that the node is operating on, which is referred to as the channel factor constraint. This is described in Eq. (8), where cI (ni) and cO(ni) are the input and output channel dimensions of the feature-map of the ith node respectively.

cI (ni) mod sIi = 0 ∀ ni ∈ H cO(ni) mod sOi = 0 ∀ ni ∈ H

(8)

Furthermore, there are certain types of layers, such as Max
Pooling or ReLU layers, where the output channel dimension
depends on the input. Therefore, for the computation nodes corresponding to these type of layers, H ⊂ H, they must have matching input channel folding (sIi ) and output channel folding (sOi ), which is referred to as intra folding matching, as described in Eq. (9),.

sIi = sOi ∀ ni ∈ H

(9)

The constraint of matching folding factors may also exist between nodes in order to ensure that all the data lines are connected, which is referred to as inter folding matching, as described in Eq. (10).

sOi = sIi+1 ∀ ni ∈ H

(10)

IV. BACKEND INTEGRATION & OPTIMISATION SCHEMES

So far, SAMO has deﬁned the constrained optimisation problem in a uniﬁed way. In this section, we elaborate on how SAMO interacts with backends, and subsequently solve the optimisation problem. An overview of our framework is given in Figure 1, which highlights the key components of SAMO including the parser, optimiser and exporter.

A. Parser: customised IR to HD-graph
The existing backends take the CNN model as an input and transform it into their own customised Intermediate Representation (customised IR), a graph that contains the backendspeciﬁc information to describe the characteristics of the corresponding hardware building blocks. Therefore, the parser of SAMO is responsible for further abstracting and unifying the customised IR into the generalised HD-graph.
At the node level, the tunable design parameters in the customised IR are mapped to their respective optimisation

420

variables in the HD-Graph (Table I). This mapping is not necessarily one-to-one. For example, the reuse-factor in HLS4ML is mapped to the product of input channel folding, output channel folding and kernel folding.

Optimsation Variable
Input Channel Folding Kernel Folding Output Channel Folding

Backend Design Parameter

fpgaConvNet FINN HLS4ML

Coarse-In Fine

SIMD Reuse-Factor

Coarse-Out PE

TABLE I: Relationship between backend-speciﬁc design parameters and HD-graph optimisation variables.

The parser exposes the resource and latency models to the HD-Graph, if these models have been provided by the backend. For backends which do not contain these models, such as HLS4ML, analytical latency and DSP predictions are provided.

B. Optimiser: Brute-Force
SAMO provides three optimisation solvers to exploit the trade-off between the optimisation evaluation time and the performance of the generated design. Among them, the BruteForce optimiser enumerates all possible values of optimisation variables. Any design point that violates the constraints of the HD-Graph is discarded. The rest of design points are evaluated on the objective function and the optimal one is then identiﬁed. The advantage of the Brute-Force optimiser is that it guarantees the identiﬁcation of the optimal design point, at the cost of a lengthy optimisation time.

C. Optimiser: Simulated Annealing
Simulated Annealing [15] is a well-known stochastic optimisation algorithm. Algorithm 1 outlines its implementation. At ﬁrst, all the optimisation variables, V = {C, sI , sO, k} are initialised to a resource-minimal state (Vinit), where the computation inside each node is in sequential order and the HD-graph is split into as many partitions as possible.

Algorithm 1 Simulated Annealing Optimisation Algorithm

1: K = Kstart, V = Vinit

initialisation

2: while K > Kmin do

3:

Vprev = V

store previous design

4: V = random transformation on V

5: if constraints satisﬁed then

6:

if ψ(V, Vprev, K) < x ∼ U (0, 1) then

7:

V = Vprev

reject new design

8: K = λ · K

reduce temperature

It then enters the main optimisation loop where it performs a random change to the optimisation variables in each iteration and evaluates the decision function (Eq. (11)). If the output is below the stochastic decision threshold x, which is a random variable sampled from a uniform distribution, or any constraint

has been violated, then the current design is discarded and the previous design is kept.

ψ(V, Vprev, K) = exp

min

0, O(Vprev) − O(V ) K

(11)

The algorithm requires two hyper-parameters: K and λ. K is temperature, which starts from Kstart and decays by the cooling rate λ, for each iteration until reaching Kmin.

D. Optimiser: Rule-Based
The Rule-Based optimiser starts at a resource-minimal state and solves the optimisation problem using a deterministic method.

Algorithm 2 Ruled-based Optimisation Algorithm

1: procedure OPTIMISE PARTITION(P )

2: repeat

3:

j = argmax t(nj|sIj , sOj , kj), nj ∈ P slowest j

4:

Δ = {δsI , δsO, δk} , Δ > 0

folding increment

5:

rnew = r(nj |sIj + δsI , sOj + δsO, kj + δk)

6:

rprev = r(nj |sIj , sOj , kj )

predict resource

7:

min
Δ

rnew

−

rprev

smallest resource change

8:

sIj = sIj + δsI , sOj = sOj + δsO, kj = kj + δk

9: until no more resources or fully parallel

As outlined in Algorithm 2, the Rule-based optimiser deals with each partition independently. For each partition, the optimiser identiﬁes the slowest node and reduces its latency by updating the value of the optimisation variable with the increment Δ, which causes the smallest change in resources. The above step repeats until the latency of the slowest node cannot be reduced further, either due to reaching a resource constraint, or that the slowest node is fully unrolled. The optimiser then incrementally merges pairs of partitions. This is done by applying heuristics which identify partition merges which are more likely to create an optimal design point.
These heuristics are based on the following characteristics of the partition:
• is memory-bound • the slowest node is already fully unrolled • latency is smaller than reconﬁguration time
Once all the identiﬁed partitions cannot be merged further, the optimiser terminates.
E. Exporter: customised IR to HD-graph
The optimised HD-graph is transformed back to the custom IR belonging to the respective backend, and the design parameters of the hardware building blocks are conﬁgured with the values of the corresponding optimisation variables. This optimised IR can then be used to synthesise and generate bitstreams for the target platform.

421

Backend fpgaConvNet
FINN

Network/ Precision
LeNet/w16a16 CNV/w16a16
MPCNN/w4a4 CNV/w1a1
MobileNetV1/w4a4

Partitions
init. lat. thr.
1 13 1 68
1 23 1 11 1 5 13

Latency (ms/batch)

init. lat.

thr.

16.0 2.0 283.9 289.0 421.7 1304.7

10.0 289.0 513.8

84.5 0.3 475.3

432.1 73.7 11347.0

Throughput (img/s)

init. lat.

thr.

62.5 500.0 901.6 3.5 2.4 196.2

100 11.8 592.4

3.5 3472.2 3472.2

1.9 2.1

22.6

Resource (%)
init. lat. thr.
50.8 50.5 83.5 120.5 64.5 82.0
48.1 44.4 47.4 18.7 33.0 33.0 187.1 63.8 82.8

TABLE II: Comparison of optimisation results for the Rule-Based optimiser against unoptimised designs (init.) targeting both throughput (thr.) and latency (lat.) objectives for a ZedBoard device. The batch size for the throughput objective is 256. Resource is the average utilisation across DSP, BRAM and LUT. Designs with resources in red violate constraints.

V. EVALUATION
The proposed SAMO framework is evaluated in terms of its ability to identify high-performance designs. Outlined in Table III, the CNN models used in the evaluation are sourced from the design examples provided by the backends. In terms of target platforms, ZedBoard, ZC706 and U250 are selected to evaluate the framework over resource proﬁles that span from embedded systems to high-end server-graded FPGA device.

Task

Network No. Conv No. Dense Params

Jet Tagging

3-layer

0

4

4K

Hand Gestures MPCNN

3

2

70K

MNIST

TFC LeNet

0 2

4

59K

2

430K

CIFAR-10

CNV

6

3

1.543M

VGG11

8

ImageNet MobileNetV1 27

ResNet50

52

3

132.854M

1

4.209M

1

25.371M

TABLE III: Model zoo for evaluation.

A. Choice of Optimisers
In subsection IV-B to IV-D, we demonstrated the implementation of three optimiser solvers. Among them, the BruteForce optimiser exhaustively explores the design space. Apart from the 3-layer network, such an exhaustive exploration is intractable, with some tests estimated to take centuries to complete.
Beyond the Brute-Force optimiser, SAMO also provides the Simulated-Annealing and Rule-Based optimisers to traverse the design space and deal with its large cardinality. Fig. 2 demonstrates the performance of these two optimisers by reporting the optimisation time and achieved performance.
For Simulated Annealing, due to its stochasticity, 50 independent runs are launched with different random seeds. The annealing temperature K is initialised as 1000 and reduced by 2% every iteration until reaching the minimum temperature Kmin=1. It is then left to run at minimum temperature and for the same time budget as the Rule-Based optimiser. The distribution across 50 runs of Simulated Annealing is calculated and compared with the Rule-Based result, which is deterministic.

For CNV, all 50 runs of the Simulated Annealing optimiser give exactly the same latency as the Rule-Based optimiser and converge quicker, suggesting that Simulated Annealing is able to ﬁnd optimal designs much faster. However this is not the case when it comes to the optimisation of a wider and deeper3 network, MobileNetV1, where the Simulated Annealing runs did not converge. Considering each run of the optimiser takes about 33 minutes to complete on an Intel i7-9700 CPU, the stochasticity of the annealing algorithm becomes a major drawback for reproducible high performance designs.
To summarise, the results suggest that the Brute-Force, Simulated Annealing and Rule-Based Optimiser should be used for small, medium, and large networks respectively. The Brute-Force optimiser guarantees identiﬁcation of an optimal design point, but its lengthy search time makes it only feasible for small networks. Both the Simulated Annealing optimiser and the Rule-Based optimiser efﬁciently explore the design space at the risk of being stuck at a local minimum. However, the Rule-Based optimiser performs better whilst handling larger networks, as the randomness of Simulated Annealing makes it sub-optimal when the same time budget is considered.
B. Discussion on Partitioning
As described in Section III-D, SAMO supports both latency and throughput objectives for optimisation. By introducing partitioning into the design space, these two objectives can lead to very different hardware outcomes. This section evaluates the difference in throughput and latency driven designs for both high and low-end devices.
Table II demonstrates the capability of SAMO to identify the design points for a resource-constrained device. Both throughput and latency objectives are compared to an unoptimised design whose optimisation variables are all set to 1.
The ﬁrst beneﬁt of introducing partitioning is the ability to overcome resource constraints. For CNV for fpgaConvNet, and MPCNN and MobileNetV1 for FINN, the unoptimised designs exceed the available resources for the ZedBoard. By splitting the HD-Graph into multiple sub-graphs, SAMO is able to overcome this constraint for both latency and throughput optimised designs. This is at the cost of increased latency needed for reconﬁguration. And in all cases where the unoptimised design ﬁts, SAMO is able to ﬁnd an improved design.
3“wide” and “deep” are used here to describe the number of channels and the number of layers.

422

(a) CNV (w1a1)

(b) MobileNetV1 (w4a4)

Fig. 2: Comparison between the Simulated Annealing and Rule-Based optimisers. Networks are mapped to a U250 using the FINN backend with a latency objective.

Partitioning also enables much greater throughput to be achieved compared to being constrained to a single partition. In Table II, it can be seen that for LeNet for fpgaConvNet, the throughput for the throughput-driven design is nearly double that of the latency-driven one. This is in part due to the ability to amortise the cost of reconﬁguration through large batch sizes. Here the throughput-driven design spends signiﬁcantly more time executing hardware than reconﬁguring. It is also observed that throughput-driven designs lead to more efﬁcient use of the hardware per partition, with much higher resource utilisation compared to latency-driven designs.

the design parameters of the hardware building blocks are manually tuned. The results highlight the power of SAMO’s automated design space exploration, as it can achieve the same or higher performance compared to a hand-crafted method, demonstrating improvements of 4x-20x in performance across different backends.

Backend
HLS4ML4 fpgaConvNet
[9]
FINN5

Platform U250
Zedboard
U250

Network/Precision
3-layer/w16a16
LeNet/w16a16 MPCNN/w16a16
CNV/w1a1 MobileNetV1/w4a4
ResNet-50/w1a2

Latency (us) baseline SAMO

0.001 0.001

7917.0 2000.0 3919.0 180.0

163.8 567.9 4515.8

41.0 567.9 3081.3

TABLE IV: Comparison with baseline designs. Latency is predicted by the backend performance models.

Fig. 3: Comparison of Throughput and Latency using different batch sizes for a VGG11 network targeting a U250
device using the fpgaConvNet backend. p indicates the number of partitions.
Exploring throughput-driven designs further, different batch sizes are used in Fig. 3 to show achievable throughput for a U250 device deploying VGG11 using the fpgaConvNet backend. This ﬁgure highlights that partitioning is not only a mechanism for satisfying resource constraints, but also a way of further improving throughput, since as the batch size is increased, more partitions are used to increase throughput.
C. Comparison with Existing Designs
Table IV compares designs generated by SAMO with example designs provided by the authors of each backend, where

VI. CONCLUSION & FUTURE WORK
This paper presents the SAMO framework, an open-source Streaming Architecture Mapping Optimiser, which serves as a powerful tool for CNN Accelerator designers. The framework has been integrated with popular open-source Streaming Architectures in order to prove its ability in achieving high performance designs across a range of CNN networks and FPGA platforms. The potential of different optimisers are demonstrated, with considerable gains in performance observed. This framework can be seen as a launchpad for further research into CNN-FPGA co-design, with potential for use in the exploration of Neural Architecture Search (NAS), as well as exploring improved optimisation methods.
ACKNOWLEDGEMENT
For the purpose of open access, the author(s) has applied a Creative Commons Attribution (CC BY) license to any Accepted Manuscript version arising.
4https://github.com/fastmachinelearning/hls4ml 5https://github.com/Xilinx/ﬁnn-examples

423

REFERENCES
[1] S. I. Venieris, A. Kouris, and C.-S. Bouganis, “Toolﬂows for mapping convolutional neural networks on fpgas: A survey and future directions,” ACM Comput. Surv., vol. 51, no. 3, 2018.
[2] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-datacenter performance analysis of a tensor processing unit,” in Proceedings of the 44th annual international symposium on computer architecture, 2017.
[3] D. A. Vink, A. Rajagopal, S. I. Venieris, and C.-S. Bouganis, “Caffe barista: Brewing caffe with fpgas in the training loop,” in 2020 30th International Conference on Field-Programmable Logic and Applications (FPL). IEEE, 2020.
[4] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre, and K. Vissers, “Finn: A framework for fast, scalable binarized neural network inference,” in Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 2017.
[5] R. DiCecco, G. Lacey, J. Vasiljevic, P. Chow, G. Taylor, and S. Areibi, “Caffeinated fpgas: Fpga framework for convolutional neural networks,” in 2016 International Conference on Field-Programmable Technology (FPT). IEEE, 2016.
[6] J. Duarte, S. Han, P. Harris, S. Jindariani, E. Kreinar, B. Kreis, J. Ngadiuba, M. Pierini, R. Rivera, N. Tran, and Z. Wu, “Fast inference of deep neural networks in FPGAs for particle physics,” Journal of Instrumentation, vol. 13, no. 07, Jul. 2018.
[7] H. Li, X. Fan, L. Jiao, W. Cao, X. Zhou, and L. Wang, “A high performance fpga-based accelerator for large-scale convolutional neural networks,” in 2016 26th International Conference on Field Programmable Logic and Applications (FPL). IEEE, 2016.
[8] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, “Dnnbuilder: an automated tool for building high-performance dnn hardware accelerators for fpgas,” in 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 2018.
[9] S. I. Venieris and C.-S. Bouganis, “fpgaconvnet: A framework for mapping convolutional neural networks on fpgas,” in 2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), 2016.
[10] ——, “fpgaconvnet: Mapping regular and irregular convolutional neural networks on fpgas,” IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 2, 2019.
[11] M. Blott, T. B. Preußer, N. J. Fraser, G. Gambardella, K. O’brien, Y. Umuroglu, M. Leeser, and K. Vissers, “Finn-r: An end-to-end deeplearning framework for fast exploration of quantized neural networks,” ACM Transactions on Reconﬁgurable Technology and Systems (TRETS), vol. 11, no. 3, 2018.
[12] J. Faraone, G. Gambardella, D. Boland, N. Fraser, M. Blott, and P. H. Leong, “Customizing low-precision deep neural networks for fpgas,” in 2018 28th International Conference on Field Programmable Logic and Applications (FPL). IEEE, 2018.
[13] T. Alonso, L. Petrica, M. Ruiz, J. Petri-Koenig, Y. Umuroglu, I. Stamelos, E. Koromilas, M. Blott, and K. Vissers, “Elastic-df: Scaling performance of dnn inference in fpga clouds through automatic partitioning,” ACM Trans. Reconﬁgurable Technol. Syst., vol. 15, no. 2, dec 2021.
[14] T. Aarrestad, V. Loncar, N. Ghielmetti, M. Pierini, S. Summers, J. Ngadiuba, C. Petersson, H. Linander, Y. Iiyama, G. Di Guglielmo et al., “Fast convolutional neural networks on fpgas with hls4ml,” arXiv preprint, 2021.
[15] C. R. Reeves, Ed., Modern Heuristic Techniques for Combinatorial Problems. USA: John Wiley & Sons, Inc., 1993.
424

