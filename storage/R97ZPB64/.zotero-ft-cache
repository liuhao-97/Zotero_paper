ESAI: Efﬁcient Split Artiﬁcial Intelligence via Early Exiting Using Neural Architecture Search
Behnam Zeinali∗, Di Zhuang†, and J. Morris Chang‡ Department of Electrical Engineering University of South Florida Tampa, Florida 33620
Email: ∗behnamz@usf.edu, †dizhuang@usf.edu, ‡chang5@usf.edu

arXiv:2106.12549v1 [cs.DC] 21 Jun 2021

Abstract—Recently, deep neural networks have been outperforming conventional machine learning algorithms in many computer vision-related tasks. However, it is not computationally acceptable to implement these models on mobile and IoT devices and the majority of devices are harnessing the cloud computing methodology in which outstanding deep learning models are responsible for analyzing the data on the server. This can bring the communication cost for the devices and make the whole system useless in those times where the communication is not available. In this paper, a new framework for deploying on IoT devices has been proposed which can take advantage of both the cloud and the on-device models by extracting the meta-information from each sample’s classiﬁcation result and evaluating the classiﬁcation’s performance for the necessity of sending the sample to the server. Experimental results show that only 40 percent of the test data should be sent to the server using this technique and the overall accuracy of the framework is 92 percent which improves the accuracy of both client and server models.
Impact Statement—In this article, a novel method of actionable intelligence implementation on local mobile devices has been proposed. The proposed framework intelligently decides and controls if the sample should be operated on the local or the server model using the meta-information. The performance of the embedded AI unit is improved by proposing the novel method of neural architecture search technique using the knowledge distillation idea. Also, the efﬁciency and ﬂexibility of the embedded AI unit have been enhanced by introducing the earlier exit on the client model. Experimental results show the efﬁciency and effectiveness of the proposed framework. The framework is also implemented on real devices.
Index Terms—Internet of Things; Embedded Deep Learning; Split Artiﬁcial Intelligence; NAS; Skin
I. INTRODUCTION
With the advancement of deep learning, deep neural networks have been utilized in many real-world applications, such as botnet detection, [1] community detection [1]–[3], active authentication [4] as well as facial recognition systems [5], [6]. Information technology such as Cloud computing, wireless communication, and the Internet of Things have also progressed and as a result, mobile and IoT devices are ubiquitous these days. The progress that has been occurred in different ﬁelds of mobile technologies recently has made mobile devices more practical and desirable. An important one of such technologies is enabling deep learning models running on mobile and IoT devices.

However, deploying Deep Learning applications on mobile and IoT devices is not an easy task to do so. The majority of practical deep learning models are heavy and powerconsuming and consequently, it is not possible to implement them on mobile devices. On the other hand, using a lighter model with less power consumption can reduce the accuracy of the model drastically. Considering the trade-off between resource usage efﬁciency and prediction accuracy is vital for such an application. Cloud computing technology can be operated to tackle this problem. For example, Google Cloud AutoML [7] has provided Machine Learning as a Service (MLaS) system that is providing powerful servers for machine learning applications. Using IoT devices, data should be sent to these servers and after performing machine learning tasks, results will be shown to the users. Even though this approach provides better accuracy and less computational energy consumption for mobile devices, it brings the communication cost that needs to be considered. It also requires permanent and appropriate communication resources. Nonetheless, there are some circumstances that communication resources are limited and even not available. For example in some remote or rural areas satellite communication may not be available.
Consequently, it is very important to design a framework to enable such applications that take advantage of both the client and the server-side. The framework should be a client-server system where the client is a mobile device and the server is cloud computing. For each input sample, The framework intelligently makes a decision to classify it either using the client or server model. By doing so, it will be able to receive the best advantages from the server-provided model and the client model in a very efﬁcient manner with as maximum accuracy as possible.
There are some challenges in designing such a system. An accurate, compact, and efﬁcient DNN model needs to be deployed on the client-side to have an acceptable accuracy with the least power consumption in any scenarios that satellite network is not available. During the network availability, the framework has to be intelligent enough to choose suitable samples and send them to the cloud network to get better accuracy with lower communication costs. Moreover, since saving energy is very important in mobile devices, the framework should exit the sample in the earlier stage when the same result can be achieved with fewer layers which can reduce the

computational energy consumption. The model on the client needs to be compressed enough
to be deployable on mobile devices. It should also have appropriate accuracy in such a scenario that satellite resources are not available. A few pieces of research have been done so far to provide compressed and accurate models. Knowledge distillation techniques [8]–[10]consider a heavy and complex model with acceptable accuracy as a teacher and try to teach or convey the information from this model to a simpler model in terms of layers and size. The simple model which known as the student model has the lass size but about the same accuracy. In this technique, the framework is not able to take advantage of the model on the server which can be a model with very high accuracy without any computational limitation. Split-DNN architecture [11]–[14] is also used to have a lighter model on the client. In this technique, a complex DNN model is divided into two sections so that the lighter one is used on the client and the heavier one is used on the server. However, these models are highly dependant on the availability of satellite resources and are not able to work when communication is impeded.
In this paper, we propose an intelligent and efﬁcient AI framework using multi-exit DNN. This framework is based on a client-server system and intelligently makes a decision about each input data to whether it needs to be sent to the server or can be classiﬁed using the client model. This decisionmaking is based on the amount of the required precision and the energy state of the device and the trade between the two of them needs to be optimized by the framework. About the data that does not need to be sent to the server, the framework decides to exit it from the ﬁrst exit if it is possible to use less computational energy. In order to achieve a light model with acceptable accuracy for the client model, we have used NAS morphism [15] algorithm to ﬁnd an appropriate network. By applying the knowledge distillation technique [8], a new searching strategy has been proposed to get a light model with acceptable accuracy. Then, the obtained model has been converted to a multi-exit model. A decision unit for each exit has been proposed separately using meta-information which is responsible to make a decision about each data sample in each exit for sending it to the next stage or the server or showing the classiﬁcation result to the user.
To evaluate the proposed method, in the experimental results section, ﬁrst, we evaluate the efﬁcacy of applying the knowledge distillation technique [8] to the morphism-based neural architecture search algorithm [15]. Then we show that this model provides better meta information for training the decision unit comparing to some of the state-of-the-art models which have been introduced so far for implementing on IoT devices. In the end, we evaluate the accuracy and efﬁciency of the multi-exit framework in saving computational power.
Skin lesion 2019 data set [16]–[18] has been used to evaluate our proposed method. The data set contains 25331 images from eight diseases. The data set has been divided into train, validation, and test set which contains 80, 10, and 10 percent of the data set.

To summarize, the proposed method contributions are:
• Designing a novel and intelligent Split-AI architecture to get efﬁcient and accurate deep learning services from the mobile/IoT based applications from both client and server models
• Enhancing the design of morphism-based NAS by introducing Knowledge Distillation techniques into its searching strategy to get a mobile-deployable light model
• Introducing a multi-exit DNN architecture to enable ﬂexible and efﬁcient tuning trade-off between the resource usage efﬁciency and prediction accuracy
In the section II, we will talk about some preliminary knowledge about NAS algorithms and knowledge distillation techniques. Then In the section III, we introduce the proposed method in more detail step by step. The experimental result will be shown in the section IV . In the section V, we will take a look at some efforts the have been done so far by researchers to implement deep neural networks on mobile devices.
II. PRELIMINARIES
In this section, we introduce a basic understanding of neural architecture search (NAS) and knowledge distillation techniques (KD).
A. Neural Architecture Search and morphism-based NAS
Human-made neural network architectures have been progressed tremendously since the Alexnet network was introduced in 2012 [19]. Variety of neural networks with different architectures have been designed by researchers such as VGG19 [20], InceptionV3 [21], Xception [22], DenseNet [23], InceptionResNetV2 [24], and ResNet-152 [25] while each of them has different combination of convectional layers, fully connected layers, and concatenation and skip layers. These models are general models that have been designed by some experts that have years of experience in the ﬁeld of machine learning and feature engineering. Even though these kinds of designing are time-consuming and tedious to do so, it requires knowledge to design a different architecture for different data set and a target.
To address this problem, a new technique called Neural Architecture Search [26] has been introduced to overcome these mentioned drawbacks. It tries to search and ﬁnd a good architecture for a speciﬁc target in an automatic manner which can lead to ﬁnding a model with better results comparing to man-made models in the classiﬁcation tasks. For doing so, two important deﬁnitions should be clariﬁed for running such an algorithm. First, what is the search space, where and how the algorithm is going to connect different architecture together reasonably, and second, how it will be able to evaluate each new architecture and ﬁnd the optimum one with the best results? Different techniques for addressing these two strategies have been introduced recently. Liu et al [27] have introduced cell-based NAS algorithms in which the number of different architecture cells is connected together to deﬁne a search space in every try and an evolutionary algorithm has been used to evaluate different architectures. Liu et al [28]

Fig. 1: The proposed framework

used the same strategy for search space but deﬁned a bilevel optimization system to evaluate each architecture.
Neural Architecture Search method based on the morphism algorithm has been introduced by Elsken et al [15]. In their work, for deﬁning the search space, they have introduced a morphism based method in which at ﬁrst, the algorithm deﬁnes an initial architecture, then it tries to extend this architecture by randomly adding different layers such as fully connected, convolution, skip, and concatenation layers to its layers. The layers are being selected in a random fashion. For addressing the second deﬁnition, the algorithm uses a hill-climbing method to evaluate each new architecture. hill climbing is a greedy algorithm that compares each new model to the previous one and replaces it if the new one shows better performance.
B. Knowledge Distillation
The majority of man-made models, as well as models that have been obtained by NAS algorithms, are cumbersome which makes them efﬁciently unreasonable to be implemented on IoT devices due to their power and storage consumption. Most of the energy in IoT devices is consumed by memory references and reducing the memory size of the model can lead to less storage and consequently less power consumption. Different techniques of model compression and acceleration have been proposed to overcome these problems. Knowledge distillation [8] and model compression [29] techniques can be used to reduce the model size while maintaining the same accuracy. Attentions to knowledge distillation have been increased rapidly in recent years due to its simplicity and effectiveness [30]. This technique trains a single small model by using transferred knowledge from a cumbersome model and a model with less model size but approximately the same accuracy can be obtained. In order to do so, instead of training the smaller model using the hard labels of the data set, it is being trained using the softened labels of each data sample using the output of Softmax calculation of that sample from the cumbersome model. In this way, more information has

been transferred to the smaller model without transferring the noise and unnecessary data. The amount of data that have been transferred to the smaller model can be controlled by the temperature value (T) in the following equation:

Si =

exp (zi/T ) j exp (zj/T )

(1)

A bigger value for T means that more information is being transferred to the smaller model. The smaller model does not need to worry about overﬁtting since the cumbersome model has already dealt with that problem.

III. METHODOLOGY
A. Overview
For a client-server Split-AI Architecture that wants to take advantage of both the model on the client (mobile and IoT devices) and the model on the server(a complex model with high accuracy), it is of vital importance to make an accurate decision about whether a sample should be sent to the server or be classiﬁed using the client model. For a client model with 80 percent accuracy, in the ideal scenario, 20 percent of samples (falsely classiﬁed samples) should be determined and sent to the server. The model on the server has no limitation in terms of storage and computational cost and can be a model with better performance. By sending data in an intelligent manner, not only the framework can take advantage of the client model as much as possible, but also it can use the communication cost only for those samples that need to be classiﬁed by a better model on the server. In this manner, overall accuracy using both client and server models can be improved while using fewer communication costs. The decision unit tries to ﬁnd falsely classiﬁed samples after classiﬁcation using the meta-information. It learns how to make a decision about test samples by being trained using the labeled meta information that has been obtained from applying the validation data set to the client model. To reduce the computational power, another exit can be extracted from the client model and use a similar

idea to prevent data to get through the whole layers of the model. Therefore, not only the model on the client side needs to be small to have less power consumption, but also it has to be accurate enough even in the earlier exit to produce appropriate information for training the decision unit. To get an appropriate client model, our proposed method improves the NAS morphism algorithm in terms of accuracy by applying the knowledge distillation technique to its searching strategy. In section III-B, we introduce our Split-AI framework in more detail. Then in the section III-C, the proposed method for getting the client model will be presented. In section III-D, a multi-exit idea will be explained.
B. Split-AI framework
The proposed Split-AI framework has three different parts. A client model is a light model with acceptable accuracy. A decision unit that decides about each sample after being classiﬁed and a communication unit that is responsible for sending the data to the server, getting the results, and show them to the user. Figure 1 shows the proposed framework. The Decision unit plays an important role in the framework. It is responsible to identify samples that have been classiﬁed incorrectly by the client model and send them to the communication section for being classiﬁed by the server. It is a light machine learning model that is trained by using meta-information.
To do so, after classifying some samples with the machine learning model, meta-information has been extracted from the result and since we have the label of that sample, the Decision Unit is being trained about what decision it should make about that sample. For testing, those samples that have more uncertainty are separated and sent to the server. Certainty means that it is highly probable that the machine learning model has classiﬁed those samples correctly. Based on the uncertainty, a parameter name decision unit’s sensitivity is deﬁned which is a number between 0 and 1. In our work, the server model plays an expert role. Indeed, the decision unit will show the classiﬁcation result to the user only when the certainty of its decision exceeds its value of sensitivity, otherwise, it will send the sample to the communication stage to be sent to the server.
A similar idea has been used in active learning [31]. Labeling the data by an expert is one of the most important and tedious work in the ﬁeld of machine learning. When you have a new data set with unlabeled data, you need to label them based on the class that each sample belongs to. In the case of the medical data set, for labeling data, hiring an expert is vital which makes it expensive and important. When the data set is really large, it is not logically and ﬁnancially acceptable to send the whole samples to the experts to be labeled. One of the ideas here is determining and sending some samples to the expert which can lead to better results. Here, samples with higher uncertainty are determined and sent to the expert using a similar idea.
1) Meta Information: Different types of meta-information can be extracted from the machine learning model output to identify the uncertainty of the results. In our work, we have

extracted maximum probability, least conﬁdence, entropy, and standard deviation as meta information. The equations for all of them can be deﬁned as below respectively:

M P = max(Pi)

(2)

LC = (Ps(0) − Ps(1))

(3)

Entropy = Pi ∗ log Pi

(4)

i

std =

(Pi − µ)2 N

(5)

where Ps = sort(Pi) , Pi is the probability of the client output for each class i, µ is the mean value of Pi and N is the number of classes. Imagine a classiﬁer with three classes and sampleA and sampleB have been applied to this classiﬁer and we want to decide which one of them has more uncertainty and can be a better candidate to be sent to the server instead of on-device classiﬁcation. Table I shows the extracted meta information from two example samples. For example, the least conﬁdence of sampleA after being classiﬁed is 0.9 − 0.09 = 0.81 while sampleB is 0.5 − 0.3 = 0.2 which means sampleB has higher uncertainty and it is more prone to be sent to the server. By computing other values (entropy, maximum probability, and standard deviation which are the last three rows of the table I), sampleB is an appropriate sample to be sent to the server.
2) Decision Unit: In the Split-AI framework, the decision unit is responsible for identifying samples with high uncertainty by extracting meta-information from the classiﬁcation results. It can be a machine learning model that has learned true classiﬁed and false classiﬁed samples based on metainformation. In fact, extracted meta information from each sample can be considered as features and whether the sample has been classiﬁed truly can be used as a label to train such a binary classiﬁer. We extracted meta information from validation set data and used them to train such a classiﬁer and evaluate a decision unit using the test set data. Figure 2 shows the distribution of meta-information after applying all the validation set data to the client model. It can be easily seen that meta-information has a good separability feature based on true and false classiﬁed samples. For a machine learning classiﬁer, two different models have been trained. One model is the deep neural network model with 4 hidden layers and another is an XGBoost model [32] using a gradient boosting algorithm. Our experimental result shows that the deep learning model performs a little bit better on the test set.

C. The Client Model
Designing a neural network for IoT devices has absorbed quite a little attention during recent years. Researchers are trying to come up with new models that have a low storage and inference time and higher accuracy. The research on this topic can be divided into two main methodologies. In the ﬁrst method, a light model with a good accuracy has been designed to solve the classiﬁcation, segmentation, and object detection

(a) First Exit Meta Data Distribution

(b) Second Exit Meta Data Distribution

Fig. 2: Distribution of different meta information extraction from validation set after being classiﬁed

TABLE I: Example of meta information. probability of each sample for different classes and their related meta information using equation 2-5

class1 class2 class3 probability least conﬁdence entropy standard deviation

sampleA 0.9 0.09 0.01 0.9 0.81 -0.35 0.12

sampleB 0.2 0.5 0.3 0.5 0.2 -1.02 0.4

problem. SqueezNet [33], MobileNet [34], MobileNetV2 [35], EfﬁcientNetB0 and B1 [36], MNasNet [37] are the architectures that have been introduced recently. In the second method, Researchers are trying to reduce the model size of big architectures while maintaining their accuracy. Techniques like compression [38] and distillation [30] are being used in recent years to take advantage of famous architectures.
In our proposed algorithm, we have harnessed both methods simultaneously. In this manner, unlike the other compact DNN models which an expert has designed a light network for IoT devices, the model can be obtained automatically. Moreover, In the knowledge distillation technique, the student model needs to follow the structure of the cumbersome model to get the best performance, but in the proposed method, a simpler model does not need to follow any speciﬁc structure and reaching the good accuracy with acceptable model size is the ﬁnal goal. Moreover, since the performance of the decision unit is highly dependent on the extracted meta information from the client model, it is really important to have a client model with an acceptable accuracy.
To apply the proposed method, ﬁrst, the NAS morphism algorithm has been applied to the train data set to ﬁnd the architecture and train it using the softened labels. In the morphism algorithm, the algorithm is using validation loss to evaluate each step. In the proposed method, during the searching process, instead of training the model with hard

labels, softened labels from a cumbersome model have been used. The softened loss also has been used to evaluate each iteration. Since hill climbing is taking a random step in each iteration to ﬁnd the maximum point in the search space, by stronger evaluation of each random step, the algorithm is more likely able to ﬁnd the better way to reach the top of the hill. In this manner, by transferring the information from a cumbersome model and evaluate each search iteration based on the softened loss, the algorithm is more able to ﬁnd the better architectures and layers to reach the cumbersome model’s accuracy. To avoid getting a big model, the iteration of the hill-climbing greedy algorithm is set to a small value.
For producing the soft labels, using the exact method in [39], some imageNet [40] pre-trained state-of-the-art neural network architecture such as InceptionResNet [24], Xception [22], and DensNet [23] have been trained by using the transfer learning method [41]. Among them, the InceptionResNet model has been used to produce the softened label since it outperforms others by the accuracy of 86.5%. The temperature value(T ) ﬁrst is set to 5, but the accuracy and model size did not change a lot comparing to the hard label search. So, we considered T = 20 for obtaining more softened labels. After ﬁnding the architecture, hard labels are used to train the model from scratch. The obtained model is converted to a two-exit model by adding an exit layer to the middle of the model’s architecture.
D. Multi-exit architecture
The proposed framework can be extended to three stages. The client model can be converted to a two-exit neural network and a similar idea for the last exit can be applied to it. The output logits of earlier exit have been calculated after applying the model to the validation set data set and after extracting the meta information, a separate deep neural network model has been trained as a decision unit for this stage. Figure 2 (a) shows the meta information extracted from the ﬁrst exit. Based on meta information, this time, true classiﬁed samples from the earlier exit can be determined by the decision unit

and show the classiﬁcation result to the users. By doing so, a sample does not need to be calculated through all the layers of the neural network in the client model on IoT devices which is computationally efﬁcient. Consequently, the inference time can be reduced and more energy can be saved.
IV. EXPERIMENTAL RESULT
Machine learning models are very appropriate to recognize hard diseases such as breast cancer recognition [42], skin lesion [43]. These models are providing the opportunity for the majority of people to observe their well-being in an easy, accessible manner. Consequently, a variety of healthcare frameworks have been implemented on mobile devices [44]. Consequently, we have used the international Skin Imaging Collaboration Challenge 2019 (ISIC 2019) [17], [45], [46] to evaluate our proposed framework. The total number of images contain train and test images is 33,569 images. nevertheless, only the labels of the training data are available. In this paper, only the labeled images have been harnessed to evaluate the proposed methods. The number of labeled images is 25,331 images of eight different skin diseases which are basal cell carcinoma, benign keratosis, vascular lesion, melanoma, squamous cell carcinoma, melanocytic nevus, actinic keratosis, and dermatoﬁbroma. The number of images for each classes are 3,323, 2,624, 253, 4,522, 628, 12,875, 876, 239 respectively. We randomly split 80%, 10%, and 10% as training data, validation data, and testing data respectively.

TABLE II: KD-NAS and other state-of-the-art architectures.

Architecture is EfﬁcientNet-B0 [36] EfﬁcientNet-B1 [36]
MobileNet [34] MobileNetV2 [35] InceptionResNet [24]
NAS [15] KD-NAS

Accuracy 81.5 83 80.5 76 85.8 80 82

Number of Parameter 4,059,812 6,585,480 3,237,064 2,268,232 54,877,872 1,090,312 4,969,032

TABLE III: KD-NAS and other state-of-the-art architectures decision unit performanc.

Architecture EfﬁcientNet-B0 [36] EfﬁcientNet-B1 [36]
MobileNet [34] MobileNetV2 [35]
KD-NAS

Accuracy 81 82 83 75 84

AUC 0.79 0.78 0.80 0.74 0.83

Sensitivity 0.91 0.94 0.95 0.8 0.95

Speciﬁcity 0.32 0.19 0.26 0.51 0.32

paper. All other models are state-of-the-art models that have been proposed by researchers for deploying on mobile and IoT devices. Among them EfﬁcientNet-B0 and EfﬁcientNetB1 [36] have been obtained using the neural architecture search technique and MobileNet [34] and MobileNetV2 [35] are human-designed models. Experimental results show that even though the NAS and Specially KD-NAS are trained from scratch, they have good accuracy and an acceptable number of parameters to be deployed on the IoT devices.

A. Experimental Considerations
For experimental evaluation, Python has been used. For the deep learning approach, Keras which is a platform that is running on top of the TensorFlow has been used [47]. All the obtained models can be converted to the TensorFlow lite format to implement on real devices. A single GPU (Nvidia GeForce GTX 1080 Ti with 11 GB GDDR5X memory) has been used for all the evaluations.
B. Evaluating the performance of KD-NAS
Results of applying the knowledge distillation technique to the search algorithm of the NAS morphism show that the proposed method is able to ﬁnd an appropriate model for implementing on mobile devices compared to the state-of-theart light models. Table II shows the experimental results. The NAS model is the model that has been obtained by applying the method in [15] to the data set. The KD-NAS model has been obtained by applying the knowledge distillation technique during the search time and using the softened loss to evaluate each iteration.
The iteration number has been set to a small number to avoid expanding and obtaining a big model. After ﬁnishing the searching procedure and ﬁnding the model, the model was trained from scratch. For training other models on the table, the imageNet [40] pre-trained architectures have been used and models have been trained using the transfer learning method. The InceptionResNet [24] model is a complex model with higher accuracy and it has been used as a server model in this

C. Evaluate the performance of Split-AI (Decision Unit)
For evaluating the performance of our proposed SplitAI framework, we have focused on the performance of the decision unit. To do so, after training the model using the train data set(the result of the table II), the output logits of the client model have been calculated by applying the model to the validation data set and meta information have been extracted from logits as features for training the decision unit model. The decision unit is a binary classiﬁer model which classiﬁes the test sample based on whether sending it to the server or not. Four trained models from table II (EfﬁcientNetB0, EfﬁcientNet-B0, MobileNet, MobileNetV2), as well as the KD-NAS model (proposed model), have been used to extract meta information, and a separate decision unit classiﬁer has been trained using each model’s meta information for comparison. Figure 3 shows the ROC curves of the different decision unit models for different client models on table II. Experimental results show that the decision unit classiﬁer of the proposed client model outperforms the decision unit classiﬁer of the state-of-the-art models in terms of accuracy and AUC(area under the curve) number. Table III shows the performance of the decision unit classiﬁers of different client models.
For evaluating how the decision unit idea can improve the overall accuracy, we also investigated the inﬂuence of the decision unit on the overall accuracy in Figure 4 by evaluating only the second(normal) exit’s performance based on different decision unit’s sensitivity numbers. As it can be seen, when

Fig. 3: The ROC Curves of Decision Units

Fig. 4: The performance of ESAI system with only second decision unit, x-axis is the sensitivity of the second decision unit

the sensitivity is the lowest (0), it means that all the samples are being classiﬁed by the local model and the accuracy is around 82% and when the sensitivity is highest(1), all samples are being classiﬁed by the server model and the accuracy is around 86% . For the sensitivity between 0.5 and 0.9, the overall accuracy is better than the client and server models and 92% accuracy can be reached by only sending around 40% of the test samples to the server which shows the effectiveness of the decision unit. For calculating the overall accuracy this equation has been used:

acc = TN + SP ST

(6)

Where TN is the true negative number of the second exit, SP is the number of samples that have been sent to the server and

have been classiﬁed correctly and ST is the total number of test samples. For the server model, the image-net pre-trained

inceptionResnet [24] model in table II has been used. Any

models with better performances can be replaced by this model

to get better accuracy in the infrastructure implementation,

notwithstanding.

D. Evaluating the Performance of Multi-exit Architecture

To implement the multi-exit architecture, the output proba-

bility of the ﬁrst exit decision unit has been calculated. If the

certainty of the classiﬁcation task is higher than the decision

unit’s sensitivity, the sample will be classiﬁed at the current

exit, otherwise, it will be sent to the next exit. The same

procedure will be applied to the last exit. For those samples

that have been chosen to be sent to the server, the classiﬁcation

result of the server model has been calculated for each of them.

The total accuracy has been calculated based on the following

equation:

acc

=

TN 1

+

TN 2 ST

+

SP

(7)

where TN1 is the true negative number of the ﬁrst exit, TN2 is the true negative number of the second exit, SP is the number of samples that have been sent to the server, and have

Fig. 5: The accuracy of ESAI system for different sensitivities, x-axis is the values of sensitivity for the ﬁrst decision unit and y-axis is the values of sensitivity for the second decision unit.
been classiﬁed correctly and ST is the total number of test samples. In this way, the whole accuracy of the framework has been calculated regardless of in which stage the samples are going to be classiﬁed.
Figure 5 and Figure 6 show the accuracy of the whole Split-AI framework and the number of samples that have been classiﬁed on the local model and its earlier exit based on a different amount of sensitivities of each decision unit. According to Figure 5, the model accuracy is changing from 63.5% to 92%. If all the samples have been classiﬁed in the ﬁrst exit, the accuracy is 63.5%. If they have been classiﬁed

TABLE IV: Inference time of the ESAI framework on real devices in Millie Second.

Device Samsung-s10(Android)
Iphone-11(Ios)

First Exit 38 34

Second Exit 74 68

using only the second exit, the accuracy is 82%, and using the server model for all samples, the accuracy would be 85%. When the accuracy of the model is around 90%, 60% of the test data has been classiﬁed in the client model and only 40% of them have been sent to the server. From the samples that have been classiﬁed on the client, 18% of them have been classiﬁed in the ﬁrst exit. This demonstrates the efﬁcacy of our proposed method which can bring more communication (sending fewer data to the server) and computation (less inference time by exiting from the ﬁrst exit) energy saving for the mobile and IoT devices, while the overall accuracy has been improved.
E. Evaluating the effectiveness and efﬁciency of our proposed system
We implemented the whole framework on real devices. For doing so, after training the networks, we have three networks from the beginning to the dividing node (Figure 7), from the node to the ﬁrst exit (Figure 8), and from the node to the last exit (Figure 9). We also have two decision unit networks. After converting all the ﬁve networks to the TensorFlow lite, the whole framework has been implemented on real devices. Samsung-s10 devise and iPhone-11have been used to implement the framework on them as an Android and IOS platforms respectively. Table IV shows the inference time of the framework on each mobile platform. According to the table, the inference time of the second exit is almost twice the inference time of the ﬁrst exit for both mobile platforms. It is also possible to tune the sensitivity of each decision unit to regulate the amount of data that should be sent to the last exit and to the server based on device battery status and communication network availability. The framework can use both the client and the server model in normal conditions. If the battery power is not really high or the satellite is impeded, the framework can use only the client model, and if the battery power is really low, the framework can just use the ﬁrst exit for the classiﬁcation.
V. RELATED WORK
A. Compact Deep Neural Networks
Compact Deep Neural Networks (Compact DNNs) approaches have been proposed to enable the development and deployment of real-world AI applications (e.g., mobile healthcare, smart home, wearable technologies) on mobile and IoT devices. Most of such compact DNNs approaches build efﬁcient DNNs by creating efﬁcient building blocks for removing the redundancy of the current DNNs designs.
For instance, by downsampling the data using 1 × 1 convolution ﬁlters, SqueezeNet [33] obtains AlexNet [19] level

of accuracy with 50x fewer parameters and less than 0.5MB model size. MobileNet [34], [35], [48] signiﬁcantly reduces computation complexity without accuracy loss, compared with traditional DNN models, by suggesting a useful building block, “inverted residual block” into its design of DNNs. By using customized architecture, that only has one forth operations of VGG16 [49], YOLO [50] becomes one of the state-of-the-art, real-time object detection systems. Recently, efﬁcientNet [36] has been proposed for execution on mobile and IoT devices, that uniformly scales each dimension (e.g., width, depth, and resolution) of DNN models with a ﬁxed set of scaling coefﬁcients.
Although the compact DNNs could dramatically reduce the computation complexity, such “hand-craft” approaches require substantial design insight to optimized the compact DNNs’ performance. And the overall performance of the compact DNN model still would not be as good as the more advanced models deployed on the server-side, which could also be the ensemble/fusion of several well-trained DNN models.
B. Compressed Deep Neural Networks
DNN model compression [8]–[10], [51]–[55] is an alternative, automatic approach that does not require certain prior design principles to design efﬁcient DNNs running on mobile and IoT devices. For instance, data quantization [53] has been proposed to reduce the number of bits to represent each weight value of DNN models. Network pruning [56] has been proposed to trim the network connections within DNNs that have less inﬂuence on the inference accuracy. Knowledge distillation [8]–[10] has been proposed to compress a model by teaching a simpliﬁed student DNN model, step by step, exactly what to do using a complex pre-trained teacher DNN model and then deploy the student DNN model on the mobile devices. Compressed DNNs provide more ﬂexibility in designing efﬁcient DNNs running on mobile and IoT devices. However, solely deploying compressed DNN models on mobile and IoT devices cannot take advantage of the more advanced models deployed on the server-side.
C. Split Deep Neural Networks
Split-DNN architectures [11]–[14] have been proposed to split a DNN into the client-side component and the server-side component, that maintains the lightweight feature extraction and data prepossessing DNN functionalities on the mobile or IoT devices, and pushes the execution of complex DNN models to much more powerful servers. For instance, Osia et al. [14] proposes a hybrid architecture where a DNN model, that has previously been trained and ﬁne-tuned on the cloud, would be split into two smaller neural networks: a feature extraction network that runs on the mobile or IoT devices, and a classiﬁcation network that runs on the cloud system, and both neural networks on the local device and the cloud system would collaborate on running the original complex DNN model. Matsubara et al. [57] propose a KD-based SplitDNN framework to reduce the communication cost between the client and the server. However, such approaches usually

(a) First exit classiﬁed samples

(b) Local model classiﬁed samples

Fig. 6: Number of classiﬁed samples from the ﬁrst exit and the on-device model based on different values of sensitivity

cannot fully rely on the client-side model, thus unable to work if the communication is impeded. However, none of such approaches consider the communication bottleneck between the client and the server. Also, the existing approaches cannot adjust the split-AI work assignments between the client and the serve depending on the device’s condition (e.g., storage size, power consumption, and communication bandwidth).
D. Multi-exit Deep Neural Networks
Multi-exit Deep Neural Networks [58]–[62] are proposed to design DNNs with additional exits, where an appropriate exit would be chosen at the testing phase in terms of certain criteria (e.g., accuracy, efﬁciency, and power consumption). For instance, BranchyNet [58] designs their early exiting structures of DNNS by augmenting existing standard DNN architectures such as LeNet, AlexNet, and ResNet. MultiScale [59] DenseNet is a customizable multi-exit architecture inspired by the idea that the network structure of earlier exits could serve as the feature extractors for the later exits. DNNet [60] is a doubly nested network where each neuron represents a single sub-model that all aim to solve the same task (i.e., image classiﬁcation). Compared with the existing multi-exit DNN approaches, our approach introduces network morphism-based Neural Architecture Search (NAS) together with Knowledge Distillation techniques into the design of multi-exit split-DNN.
Our work: Our proposed split-AI architecture takes advantage of not only the compact and efﬁcient DNNs on the clientside but also the highly accurate DNNs on the server-side, thus, outperforms just applying the compact DNNs. For instance, we designed a novel split-AI architecture to enable efﬁcient and accurate deep learning services from both the mobile/IoT devices and the server. We introduced Knowledge Distillation techniques into the searching strategy of morphism-based NAS

to produce “lighter” DNN models. And we proposed to utilize multi-exit DNN architecture to enable ﬂexible and efﬁcient trade-off tuning between the resource usage efﬁciency and the prediction accuracy.
VI. CONCLUSION
In this paper, a novel and effective framework for the mobile healthcare system has been proposed. This framework is able to get beneﬁts of both models on the cloud server and the client model intelligently. To do so, a framework contains a decision unit model that is responsible to make a decision about each sample to whether sending it to the server or not. After classifying the model by the client model, the decision unit tries to intelligently evaluate the exactitude of the classiﬁcation task and send the samples with high classiﬁcation uncertainty. Early exiting classiﬁcation has been produced by following a similar idea in which a decision unit makes the decision on the classiﬁed samples with a lower uncertainty in the earlier exit to reduce the computational power. To obtain an accurate and acceptable model, the knowledge distillation technique has been applied to the morphism-based neural architecture search methodology in a speciﬁc way and then deﬁne an early exit before the last convolutional layers of the model. In our future work, we will try to enhance the framework by introducing better decision unit techniques and more powerful meta information. Also, we will try to Improve the model parameters based on the last decision on the sample in an intelligent manner.
ACKNOWLEDGMENTS
Effort sponsored in part by United States Special Operations Command (USSOCOM), under Partnership Intermediary Agreement No. H92222-15-3-0001-01. The U.S. Government

is authorized to reproduce and distribute reprints for Govern- [18] N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza,

ment purposes notwithstanding any copyright notation thereon.
1

D. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti et al., “Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic),” arXiv preprint

arXiv:1902.03368, 2019.

REFERENCES

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in NIPS, 2012.

[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

[1] D. Zhuang, M. Chang, and M. Li, “Dynamo: Dynamic community

large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.

detection by incrementally maximizing modularity,” IEEE Transactions [21] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-

on Knowledge and Data Engineering, vol. PP, pp. 1–1, 11 2019.

thinking the inception architecture for computer vision,” arXiv preprint

[2] D. Zhuang and J. M. Chang, “Peerhunter: Detecting peer-to-peer botnets

arXiv:1512.00567, 2015.

through community behavior analysis,” in 2017 IEEE Conference on [22] F. Chollet, “Xception: Deep learning with depthwise separable convo-

Dependable and Secure Computing. IEEE, 2017, pp. 493–500.

lutions,” arXiv preprint arXiv:1610.02357, 2016.

[3] D. Zhuang and J. Chang, “Enhanced peerhunter: Detecting peer-to-peer [23] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely

botnets through network-ﬂow level community behavior analysis,” IEEE Transactions on Information Forensics and Security, vol. PP, 02 2018.

connected convolutional networks,” arXiv preprint arXiv:1608.06993, 2016.

[4] P.-Y. Wu, C.-C. Fang, J. M. Chang, and S.-Y. Kung, “Cost-effective [24] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4,

kernel ridge regression implementation for keystroke-based active authentication system,” IEEE transactions on cybernetics, vol. 47, no. 11,

inception-resnet and the impact of residual connections on learning,” arXiv preprint arXiv:1602.07261, 2016.

[5] [6] [7]

pp. 3916–3927, 2016.

[25]

H. Nguyen, D. Zhuang, P.-Y. Wu, and M. Chang, “Autogan-based dimension reduction for privacy preservation,” Neurocomputing, vol.

[26]

384, pp. 94–103, 2020.

D. Zhuang, S. Wang, and J. M. Chang, “Fripal: Face recognition in privacy abstraction layer,” in 2017 IEEE Conference on Dependable

[27]

and Secure Computing. IEEE, 2017, pp. 441–448.

“Google cloud automl,” https://cloud.google.com/automl/,accessed:2018-11-09.

K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv preprint arXiv:1512.03385, 2015. B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” arXiv preprint arXiv:1707.07012, 2017. C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy, “Progressive neural architecture search,” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 19–34.

[8] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural [28] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture

network,” arXiv preprint arXiv:1503.02531, 2015.

search,” arXiv preprint arXiv:1806.09055, 2018.

[9] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in [29] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing

Advances in neural information processing systems, 2014, pp. 2654–

deep neural networks with pruning, trained quantization and huffman

2662.

coding,” arXiv preprint arXiv:1510.00149, 2015.

[10] A. Polino, R. Pascanu, and D. Alistarh, “Model compression via [30] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A

distillation and quantization,” arXiv preprint arXiv:1802.05668, 2018.

survey,” 2020.

[11] H.-J. Jeong, I. Jeong, H.-J. Lee, and S.-M. Moon, “Computation ofﬂoad- [31] S. Tong, Active learning: theory and applications. Stanford University

ing for machine learning web apps in the edge server environment,” in

USA, 2001, vol. 1.

2018 IEEE 38th International Conference on Distributed Computing [32] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”

Systems (ICDCS). IEEE, 2018, pp. 1492–1499.

in Proceedings of the 22nd acm sigkdd international conference on

[12] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and

knowledge discovery and data mining, 2016, pp. 785–794.

L. Tang, “Neurosurgeon: Collaborative intelligence between the cloud [33] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,

and mobile edge,” ACM SIGARCH Computer Architecture News, vol. 45,

and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer

no. 1, pp. 615–629, 2017.

parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,

[13] N. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, L. Jiao, L. Qen-

2016.

dro, and F. Kawsar, “Deepx: A software accelerator for low-power [34] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,

deep learning inference on mobile devices,” in 2016 15th ACM/IEEE

T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-

International Conference on Information Processing in Sensor Networks

lutional neural networks for mobile vision applications,” arXiv preprint

(IPSN). IEEE, 2016, pp. 1–12.

arXiv:1704.04861, 2017.

[14] S. A. Osia, A. S. Shamsabadi, S. Sajadmanesh, A. Taheri, K. Katevas, [35] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,

H. R. Rabiee, N. D. Lane, and H. Haddadi, “A hybrid deep learning

“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings

architecture for privacy-preserving mobile analytics,” IEEE Internet of

of the IEEE conference on computer vision and pattern recognition,

Things Journal, 2020.

2018, pp. 4510–4520.

[15] T. Elsken, J.-H. Metzen, and F. Hutter, “Simple and efﬁcient ar- [36] M. Tan and Q. V. Le, “Efﬁcientnet: Rethinking model scaling for

chitecture search for convolutional neural networks,” arXiv preprint

convolutional neural networks,” arXiv preprint arXiv:1905.11946, 2019.

arXiv:1711.04528, 2017.

[37] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,

[16] D. Gutman, N. C. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra,

and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for

and A. Halpern, “Skin lesion analysis toward melanoma detection: A

mobile,” in Proceedings of the IEEE Conference on Computer Vision

challenge at the international symposium on biomedical imaging (isbi)

and Pattern Recognition, 2019, pp. 2820–2828.

2016, hosted by the international skin imaging collaboration (isic),” [38] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model

arXiv preprint arXiv:1605.01397, 2016.

compression and acceleration for deep neural networks,” arXiv preprint

[17] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti,

arXiv:1710.09282, 2017.

S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler et al., “Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by

[39] H. Zanddizari, N. Nguyen, B. Zeinali, and J. M. Chang, “A new preprocessing approach to improve the performance of cnn-based skin lesion classiﬁcation,” Medical & Biological Engineering & Computing,

the international skin imaging collaboration (isic),” in 2018 IEEE 15th

vol. 59, no. 5, pp. 1123–1131, 2021.

International Symposium on Biomedical Imaging (ISBI 2018). IEEE, [40] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:

2018, pp. 168–172.

A large-scale hierarchical image database,” in 2009 IEEE conference on

computer vision and pattern recognition. Ieee, 2009, pp. 248–255.

[41] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans-

1The views and conclusions contained herein are those of the authors

actions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–

and should not be interpreted as necessarily representing the ofﬁcial policies

1359, 2009.

or endorsements, either expressed or implied, of the United States Special [42] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova,

Operations Command.

H. Ashraﬁan, T. Back, M. Chesus, G. C. Corrado, A. Darzi et al.,

“International evaluation of an ai system for breast cancer screening,” Nature, vol. 577, no. 7788, pp. 89–94, 2020. [43] F. Perez, S. Avila, and E. Valle, “Solo or ensemble? choosing a cnn architecture for melanoma classiﬁcation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0–0. [44] B. Farahani, F. Firouzi, and K. Chakrabarty, “Healthcare iot,” Intelligent Internet of Things, p. 515. [45] P. Tschandl, C. Rosendahl, and H. Kittler, “The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions,” Scientiﬁc data, vol. 5, p. 180161, 2018. [46] M. Combalia, N. C. Codella, V. Rotemberg, B. Helba, V. Vilaplana, O. Reiter, C. Carrera, A. Barreiro, A. C. Halpern, S. Puig et al., “Bcn20000: Dermoscopic lesions in the wild,” arXiv preprint arXiv:1908.02288, 2019. [47] F. Chollet et al., “Keras,” 2015, https://keras.io, [accessed April. 1, 2020]. [48] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., “Searching for mobilenetv3,” in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1314–1324. [49] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014. [50] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Uniﬁed, real-time object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779– 788. [51] L. N. Huynh, Y. Lee, and R. K. Balan, “Deepmon: Mobile gpubased deep learning framework for continuous vision applications,” in Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services, 2017, pp. 82–95. [52] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally, “Eie: efﬁcient inference engine on compressed deep neural network,” ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 243–254, 2016. [53] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,” arXiv preprint arXiv:1510.00149, 2015. [54] S. Liu, Y. Lin, Z. Zhou, K. Nan, H. Liu, and J. Du, “On-demand deep model compression for mobile devices: A usage-driven model selection framework.(2018),” 2018. [55] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “Deepthings: Distributed adaptive deep learning inference on resource-constrained iot edge clusters,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 11, pp. 2348–2359, 2018. [56] J.-H. Luo, J. Wu, and W. Lin, “Thinet: A ﬁlter level pruning method for deep neural network compression,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 5058–5066. [57] Y. Matsubara, S. Baidya, D. Callegaro, M. Levorato, and S. Singh, “Distilled split deep neural networks for edge-assisted real-time systems,” in Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges, 2019, pp. 21–26. [58] S. Teerapittayanon, B. McDanel, and H.-T. Kung, “Branchynet: Fast inference via early exiting from deep neural networks,” in 2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 2464–2469. [59] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger, “Multi-scale dense networks for resource efﬁcient image classiﬁcation,” arXiv preprint arXiv:1703.09844, 2017. [60] J. Kim, S. Hong, Y. Choi, and J. Kim, “Doubly nested network for resource-efﬁcient inference,” arXiv preprint arXiv:1806.07568, 2018. [61] C. Zhang, M. Ren, and R. Urtasun, “Graph hypernetworks for neural architecture search,” arXiv preprint arXiv:1810.05749, 2018. [62] M. Phuong and C. H. Lampert, “Distillation-based training for multiexit architectures,” in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1355–1364.

node. Figure 8 shows the architecture from dividing node to the earlier exit. Figure 9 shows the rest of the network from the divided node to the normal output.

VII. APPENDIX
In this section, the client network has been depicted. After applying the proposed NAS method to the data set. An earlier exit has been applied manually to the network. Figure 7 shows the initial section of the network from input to the divided

Fig. 7: Initial section of the client model obtained using proposed NAS algorithm.

Fig. 8: First Exit of the client model with manual design.

Fig. 9: Second Exit of the client model obtained using proposed NAS algorithm.

