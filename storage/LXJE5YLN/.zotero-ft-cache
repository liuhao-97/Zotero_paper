A Thesis Submitted for the Degree of PhD at the University of Warwick Permanent WRAP URL: http://wrap.warwick.ac.uk/152664 Copyright and reuse: This thesis is made available online and is protected by original copyright. Please scroll down to view the document itself. Please refer to the repository record for this item for information to help you to cite it. Our policy information is available from the repository home page. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk
warwick.ac.uk/lib-publications

Modelling and Characterisation of Distributed Hardware Acceleration
by
Ryan A. Cooke
Thesis
Submitted to the University of Warwick for the degree of
Doctor of Philosophy
School of Engineering
July 2020

Contents

List of Tables

vi

List of Figures

vii

Acknowledgments

ix

Declarations

x

Abstract

xi

Abbreviations

xii

Chapter 1 Introduction

1

1.1 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.3 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.4 Thesis Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.5 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

Chapter 2 Background and Literature Review

8

2.1 Computing Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.1.1 Central Processing Units . . . . . . . . . . . . . . . . . . . . 9

2.1.2 Field Programmable Gate Arrays . . . . . . . . . . . . . . . . 11

2.1.3 FPGA Accelerator Design . . . . . . . . . . . . . . . . . . . . 16

2.1.4 Graphics Processing Units . . . . . . . . . . . . . . . . . . . . 21

2.1.5 Application Specific Integrated Circuits (ASICs) . . . . . . . 23

2.1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.2 Accelerator Integration . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.2.1 PCIe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.2.2 Network interface . . . . . . . . . . . . . . . . . . . . . . . . . 26

i

2.2.3 Tightly coupled SoC . . . . . . . . . . . . . . . . . . . . . . . 27 2.2.4 Interface overheads . . . . . . . . . . . . . . . . . . . . . . . . 28 2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.4 Networked computing systems . . . . . . . . . . . . . . . . . . . . . . 29 2.4.1 Edge Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.4.2 Datacentre Networks . . . . . . . . . . . . . . . . . . . . . . . 30 2.4.3 In-network computing . . . . . . . . . . . . . . . . . . . . . . 32 2.4.4 Network elements . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.4.5 FPGAs for network applications . . . . . . . . . . . . . . . . 34 2.4.6 Cloud Computing . . . . . . . . . . . . . . . . . . . . . . . . 37 2.4.7 FPGAs in the cloud . . . . . . . . . . . . . . . . . . . . . . . 38 2.5 Mathematical Modelling . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.5.1 Sensor network modelling . . . . . . . . . . . . . . . . . . . . 42 2.5.2 Distributed Stream Processing Models . . . . . . . . . . . . . 42 2.5.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

Chapter 3 Modelling distributed computing with heterogeneous hard-

ware

45

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

3.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.3.1 Edge/Fog Computing . . . . . . . . . . . . . . . . . . . . . . 49

3.3.2 Hardware acceleration . . . . . . . . . . . . . . . . . . . . . . 49

3.4 Scenario and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

3.4.1 Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

3.4.2 Bandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50

3.4.3 Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.4.4 Financial Cost . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.5 Proposed Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.5.1 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

3.5.2 Implementations . . . . . . . . . . . . . . . . . . . . . . . . . 54

3.5.3 Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

3.5.4 Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

3.5.5 Sources and Data . . . . . . . . . . . . . . . . . . . . . . . . . 55

3.5.6 Allocation Variables . . . . . . . . . . . . . . . . . . . . . . . 56

3.5.7 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

3.6 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

ii

3.6.1 End-to-End Latency . . . . . . . . . . . . . . . . . . . . . . . 58 3.6.2 Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3.6.3 Data-rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 3.6.4 Energy Consumption . . . . . . . . . . . . . . . . . . . . . . . 63 3.6.5 Financial Cost . . . . . . . . . . . . . . . . . . . . . . . . . . 64 3.6.6 Combined Evaluation Metrics . . . . . . . . . . . . . . . . . . 64 3.7 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3.7.1 Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3.7.2 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.7.3 Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.7.4 Centralised Software . . . . . . . . . . . . . . . . . . . . . . . 68 3.7.5 In-network software . . . . . . . . . . . . . . . . . . . . . . . 69 3.7.6 Centralised Hardware . . . . . . . . . . . . . . . . . . . . . . 69 3.7.7 In-network hardware . . . . . . . . . . . . . . . . . . . . . . . 69 3.7.8 Optimal Placement . . . . . . . . . . . . . . . . . . . . . . . . 70 3.7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3.7.10 Event Driven Simulation . . . . . . . . . . . . . . . . . . . . . 71 3.8 Further Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.8.1 Relative Computing Capability . . . . . . . . . . . . . . . . . 74 3.8.2 Task Data Reduction . . . . . . . . . . . . . . . . . . . . . . 77 3.8.3 Network Structure . . . . . . . . . . . . . . . . . . . . . . . . 78 3.8.4 Hardware Acceleration . . . . . . . . . . . . . . . . . . . . . . 80 3.9 Generating In-Network Task and Hardware Placement with Heterogeneous Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 3.9.1 Objective function formulation . . . . . . . . . . . . . . . . . 80 3.9.2 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 3.9.3 Evaluation with Synthetic Networks . . . . . . . . . . . . . . 84 3.9.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Chapter 4 Quantifying the Latency Overheads of FPGA Accelerators 92 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 4.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 4.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.4.1 Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.4.2 Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
iii

4.4.3 Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
4.5.1 Median Latency . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.5.2 FPGA Latency Breakdown . . . . . . . . . . . . . . . . . . . 100 4.5.3 Latency Distributions . . . . . . . . . . . . . . . . . . . . . . 101 4.5.4 Tail Latencies . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.5.5 Packet Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4.5.6 Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 4.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.6.1 PCIe Accelerators . . . . . . . . . . . . . . . . . . . . . . . . 104 4.6.2 Network-attached Accelerators . . . . . . . . . . . . . . . . . 105 4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106

Chapter 5 Near-Edge FPGA Acceleration for the Internet of Things107 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 5.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 5.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 5.4 Design and Experiments . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.4.1 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.4.2 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . 111 5.4.3 Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 5.5.1 Isolated Edge Node Measurements . . . . . . . . . . . . . . . 118 5.5.2 Impact of Multiple Edge Devices . . . . . . . . . . . . . . . . 121 5.5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 5.6 Accelerator Location . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 5.6.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 5.6.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

Chapter 6 Conclusions and Future Work

132

6.1 Summary of Contributions . . . . . . . . . . . . . . . . . . . . . . . . 132

6.1.1 Mathematical representation of in-network and near edge com-

puting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

6.1.2 Optimising hardware and task placement . . . . . . . . . . . 133

6.1.3 Quantifying costs associated with FPGA accelerators . . . . 133

6.1.4 Demonstration of in-network FPGA acceleration . . . . . . . 133

6.2 Future Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

iv

6.2.1 Practically validating the model . . . . . . . . . . . . . . . . 134 6.2.2 Improving optimisation runtime . . . . . . . . . . . . . . . . 134 6.2.3 Developing generalised in-network FPGA infrastructure . . . 134 6.2.4 Combination of model and FPGA infrastructure . . . . . . . 135 6.2.5 Defining accelerator communication protocols . . . . . . . . . 135 6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
v

List of Tables
3.1 Summary of symbols used in formulation. . . . . . . . . . . . . . . . 57 3.2 Case study task values . . . . . . . . . . . . . . . . . . . . . . . . . . 67 3.3 Case study platform values . . . . . . . . . . . . . . . . . . . . . . . 68 3.4 Case study SW results . . . . . . . . . . . . . . . . . . . . . . . . . . 68 3.5 Case study HW results . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.6 Performance metrics for MILP optimisation of model . . . . . . . . . 71 3.7 Different placement policies used in simulations . . . . . . . . . . . . 75 3.8 Summary of task parameter values. . . . . . . . . . . . . . . . . . . . 82 3.9 Summary of available platforms. . . . . . . . . . . . . . . . . . . . . 83 4.1 Latency results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 4.2 Network-attached FPGA delays . . . . . . . . . . . . . . . . . . . . . 100 5.1 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5.2 Network traversal times . . . . . . . . . . . . . . . . . . . . . . . . . 125
vi

List of Figures
2.1 Representative modern FPGA architecture . . . . . . . . . . . . . . 11 2.2 Xilinx 7 Series CLB arrangement . . . . . . . . . . . . . . . . . . . . 12 2.3 Xilinx DSP48E1 architecture . . . . . . . . . . . . . . . . . . . . . . 12 2.4 Partial reconfiguration example . . . . . . . . . . . . . . . . . . . . . 16 2.5 Example network infrastructure . . . . . . . . . . . . . . . . . . . . . 29 2.6 Example datacentre network . . . . . . . . . . . . . . . . . . . . . . 31
3.1 Example networked system . . . . . . . . . . . . . . . . . . . . . . . 46 3.2 Network node abstraction . . . . . . . . . . . . . . . . . . . . . . . . 52 3.3 Difference between software and hardware nodes . . . . . . . . . . . 60 3.4 Case study network structure . . . . . . . . . . . . . . . . . . . . . . 66 3.5 Model vs simulation results . . . . . . . . . . . . . . . . . . . . . . . 72 3.6 Compute capability results . . . . . . . . . . . . . . . . . . . . . . . 76 3.7 Reduction factor results . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.8 Network fanout results . . . . . . . . . . . . . . . . . . . . . . . . . . 79 3.9 Naive vs. model placement . . . . . . . . . . . . . . . . . . . . . . . 84 3.10 Configurations generated by optimization . . . . . . . . . . . . . . . 85 3.11 Optimal cost with latency constraint . . . . . . . . . . . . . . . . . . 86 3.12 Sythnthetic networks optimised for cost with latency constraint . . . 86 3.13 Synthetic networks optimised for energy and latency with cost con-
straint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 3.14 Optimal vs pushed for varying task centralisation . . . . . . . . . . . 89 3.15 Optimal latency and energy vs pushed down . . . . . . . . . . . . . . 90 3.16 Bandwidth consumtion for varying centralisation of tasks . . . . . . 90
4.1 Outline of the experimental setup . . . . . . . . . . . . . . . . . . . . 95 4.2 Accelerator configurations . . . . . . . . . . . . . . . . . . . . . . . . 97 4.3 Tail latency results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 4.4 CDF of latencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
vii

4.5 Packet size differences . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4.6 Throughput results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.7 Packet rate results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 5.1 Experimental testbed . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 5.2 Zynq accelerated edge node . . . . . . . . . . . . . . . . . . . . . . . 113 5.3 FPGA network switch accelerator . . . . . . . . . . . . . . . . . . . . 116 5.4 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5.5 Multiple edge nodes offloading to cloudlet . . . . . . . . . . . . . . . 122 5.6 Multiple edge nodes offloading to networked FPGA . . . . . . . . . . 122 5.7 Analysed network structure . . . . . . . . . . . . . . . . . . . . . . . 124 5.8 Estimation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 5.9 Network-attacehd latency redcution . . . . . . . . . . . . . . . . . . 128 5.10 Effects of 200ms base computation time . . . . . . . . . . . . . . . . 129 5.11 Effect of 100ms base computation time . . . . . . . . . . . . . . . . . 130
viii

Acknowledgments
I would like to thank my supervisor Suhaib Fahmy for giving me this opportunity. His guidance has been invaluable, and I am extremely grateful for all of the time he spent helping me.
I would also like to thank my friends Lenos and Alex, also part of the Connected Systems group, who at this point are still completing their PhDs. Their advice and company is much appreciated, and helped get through the day to day in the lab.
Lastly, I thank my partner Eleanor, whom I met for the first time shortly after beginning the PhD. Her patience, love, and kindness have kept me motivated and I wouldn’t have been able to complete this thesis without her.
ix

Declarations
This thesis is submitted to the University of Warwick for the degree of Doctor of Philosophy. The work contained in this thesis comprises my own work. This thesis has not previously been submitted for a degree at another university.
Parts of this thesis have been published by the author: 1. Ryan A. Cooke, Suhaib A. Fahmy, In-network online data analytics with FP-
GAs, in Proceedings of the International Conference on Field Programmable Logic and Applications (FPL), 2017 [1]. 2. Ryan A. Cooke, Suhaib A. Fahmy, A model for distributed in-network and nearedge computing with heterogeneous hardware, in Future Generation Computer Systems (FGCS), vol. 105, 2020 [2]. 3. Ryan A. Cooke, Suhaib A. Fahmy, Quantifying the latency benefits of nearedge and in-network FPGA acceleration, in Proceedings of the International Workshop on Edge Systems, Analytics and Networking (EdgeSys), 2020 [3]. 4. Ryan A. Cooke, Suhaib A. Fahmy, Characterizing latency overheads in the deployment of FPGA accelerators, in Proceedings of the International Conference on Field Programmable Logic and Applications (FPL), 2020 [4]. 5. Ryan A. Cooke, Suhaib A. Fahmy, Exploring Hardware Accelerator Offload for the Internet of Things, submitted to: it - Information Technology
x

Abstract
Hardware acceleration has become more commonly utilised in networked computing systems. The growing complexity of applications mean that traditional CPU architectures can no longer meet stringent latency constraints. Alternative computing architectures such as GPUs and FPGAs are increasingly available, along with simpler, more software-like development flows. The work presented in this thesis characterises the overheads associated with these accelerator architectures. A holistic view encompassing both computation and communication latency must be considered. Experimental results obtained through this work show that networkattached accelerators scale better than server-hosted deployments, and that host ingestion overheads are comparable to network traversal times in some cases. Along with the choice of processing platforms, it is becoming more important to consider how workloads are partitioned and where in the network tasks are being performed. Manual allocation and evaluation of tasks to network nodes does not scale with network and workload complexity. A mathematical formulation of this problem is presented within this thesis that takes into account all relevant performance metrics. Unlike other works, this model takes into account growing hardware heterogeneity and workload complexity, and is generalisable to a range of scenarios. This model can be used in an optimisation that generates lower cost results with latency performance close to theoretical maximums compared to naive placement approaches. With the mathematical formulation and experimental results that characterise hardware accelerator overheads, the work presented in this thesis can be used to make informed design decisions about both where to allocate tasks and deploy accelerators in the network, and the associated costs.
xi

Abbreviations
API Application Programming Interface AR Augmented Reality ASIC Application Specific Integrated Circuit AXI Advanced eXtensible Interface BRAM Block Random Access Memory CLB Configurable Logic Block CNN Convolutional Neural Network CPU Central Processing Unit DMA Direct Memory DRAM Dynamic Random Access Memory DSP Digital Signal Processing DNN Deep Neural Network FIFO First In First Out FFT Fast Fourier Transform FPGA Field Programmable Gate Array GPU Graphics Processing Unit
xii

HDL Hardware Description Language HLS High Level Synthesis
IC Integrated Circuit IoT Internet of Things ISP Internet Service Provider
IP Internet Protocol KNN K-Nearest Neighbour LAN Local Area Network
LTE Long Term Evolution LUT Look Up Table MAC Media Access Control MILP Mixed Integer Linear Programming NFV Network Function Virtualisation NIC Network Interface Card
NN Neural Network NoC Network on a Chip NPU Network Processing Unit OFDM Orthogonal Frequency Division Multiplexing PCIe Peripheral Component Interconnect Express
PL Programmable Logic PLL Phase Locked Loop
xiii

PR Partial Reconfiguration PS Processing System RAM Random Access Memory SDN Software Defined Networking SoC System on a Chip SRAM Static Random Access Memory SVM Support Vector Machine TCP Transmission Control Protocol ToR Top of Rack TPU Tensor Processing Unit UDP User Datagram Protocol VM Virtual Machine WAN Wide Area Network
xiv

Chapter 1
Introduction
Computational offloading is a term used to describe a scenario where one computing system transmits data to another, where computation is carried out, and the result transmitted back. The total computation latency of a task, the power consumed by the hardware, and the processing throughput can all be improved using this approach, when the target platform is more capable.
It can be used to describe a range of scenarios. Early implementations of this technique involved central processing units (CPUs) offloading to a co-processor such as separate floating point units, which were optimised for floating point arithmetic [5]. Computation latency could be drastically reduced, and in some cases the main processor would be free to carry out other tasks while waiting for the result. As processing tasks became more complex with the growth of applications such as graphics rendering, co-processors could begin to take the form of separate chips on the same board, or as chips hosted on a separate expansion board, connected through external interfaces such as peripheral component interconnect (PCI) [6]. Modern system-on-chip (SoC) architectures comprise multiple different hardware cores to accelerate particular tasks such as encryption, digital signal processing, or graphics rendering [7].
The growth of the Internet and networking technologies presented opportunities for computational offload to distinct machines over a network [8; 9; 10; 11; 12; 13]. As network bandwidth improved, it became more viable to transmit significant amounts of data to a remote computing resource. The platform carrying out the computation no longer had to be restricted to the same location as the data source. The arrival of data centre cloud computing allowed for computing resources to be centralised and shared across many clients [14]. There are numerous benefits to this approach. Computing resources can be scaled with much less friction and
1

hardware can be modified to enhance processing capability without disturbing the data source. This is particularly valuable in applications where the data source is in a hard-to-reach location.
Transmitting data to another processing platform has an associated communication cost, however [15]. The improvement to performance, whatever the metric may be, offsets this cost. Recently, there has been a surge of interest in low-latency, ‘real-time’ applications, driven by factors such as the internet of things (IoT) and a growing demand for responsive, complex web applications [15; 16; 17]. Industrial IoT systems rely on networked sensors and data acquisition systems, and have strict latency requirements on closed-loop control. Mobile applications requiring complex image processing such as augmented reality (AR) have to meet latency targets to provide an acceptable user experience [16]. Smart vehicles must process data and communicate with other vehicles in real-time to satisfy safety constraints [18].
This presents a problem – processing may be too complex to perform at the data source in the required time interval, so must be offloaded. While transmitting data to the cloud can reduce the computation latency, the communication penalty may be too great, and thus the total latency would still violate the required constraints. This has led to the paradigm of ‘edge computing’, where computation is offloaded to resources closer to the data source. This could mean improved hardware at the data source itself - but this results in difficult management of hardware and increased cost. Alternatively, smaller data centres or servers could be placed on local networks with the data source, removing the need for data to be transmitted over the Internet. Processing could even be moved into the networking elements that facilitate the transfer of data between machines, resulting in even less communication time. This is known as ‘in-network’ computing [19]. The resultant scenario is one where there are a range of processing elements in different locations, with different capabilities and constraints, that all offer a target for computational offload. There is now an opportunity for improved processing times, with lower communication costs than when using computation entirely at the data source, or entirely in the cloud. A new set of key challenges must now be addressed to best make use of these systems.
One of these challenges is where in the network to offload tasks. An application may comprise many tasks, each with different requirements and dependencies on other tasks, and must be allocated across a set of heterogeneous offload platforms with varying computational properties. This is a complex decision, where the profile of the application and the available resources must be accounted for. As these networked systems increase in scale, this becomes even more challenging. Systems
2

can become large enough that manual evaluation of task placement can become impossible. Design decisions must be made to balance opposing performance metrics, and to make the best use of finite resources.
Another challenge is ensuring that the in-network or edge computing resources can provide adequate computational performance. Despite being closer to the data sources, this hardware must still reduce computation latency enough to justify offloading to them. Hardware must be able to be shared across multiple client devices, and be able to perform complex processing quickly. In the case of innetwork computing, networking elements are being extended to perform additional processing, so hardware that causes minimal disruption of the usual networking functions must therefore be used. The integration between the network flow and the computations datapaths can also influence latency significantly.
Hardware acceleration is necessary in order to fulfil these requirements. Devices called field programmable gate arrays (FPGAs) are particularly well suited to this. They comprise of an array of simple logic blocks and other computational resources, linked through a configurable interconnect, which can be used to implement accelerator architectures optimised to perform a particular task, providing a computational benefit compared to standard CPU architectures. Being reconfigurable means they can support a range of applications and be dynamically configurable at runtime. They are particularly useful for in-network computing as some network elements such as switches, base stations, and routers already utilise them to carry out networking tasks, and are good at packet processing. FPGAs offer a greater range of deployment and integration capabilities compared to other hardware accelerators such as GPUs [20], and are a key enabler in meeting performance requirements at the network edge as applications scale. In Chapter 2, the differences between accelerator platforms are discussed in more detail.
1.1 Motivations
Computing is becoming increasingly connected. Networked computing systems are becoming more complex, larger in scale, and operate under greater performance constraints. Various factors, such as the growth of the IoT, commercial cloud computing and developments in hardware have been significant drivers of this.
A key challenge in effective use of networked systems for computational offload of tasks is the partitioning and placement of sub-tasks amongst computational nodes. Where computation is carried out is becoming more important. Ad-hoc placement of tasks may have been viable for smaller systems, but as the number
3

of variables grows, finding solutions that meet required constraints is not viable. Not only are there systems with more connected devices, the devices within the system are becoming more capable. Network elements that were historically passive and simply passed data between nodes, can now be tightly coupled with compute, increasing the number of possible placement solutions. Additionally, edge nodes such as sensors and other data sources which typically just produced data, are also becoming more capable due to the greater availability of cheap microcontrollers and single board computers, such as the Raspberry Pi family of devices [21]. These platforms can handle data acquisition, networking, and computation, and have a small enough form factor that they can be deployed in a range of scenarios. Increased hardware heterogeneity, and the growing market for hardware accelerators such as FPGAs add even more variables to networked system deployment. Taking this into consideration, it becomes clear that a holistic view accounting for modern trends and advancements, must be used when making decisions regarding hardware deployment and task placement. There are such an abundance of options and implications that any ad-hoc or naive decision making is likely to lead to sub-optimal performance.
As more heterogeneous computing platforms become available for lower costs, and existing platforms improve their capabilities, the decision on what combinations of hardware to use becomes more important. FPGA acceleration provides an opportunity to increase processing capability at selected nodes, and unlike other accelerator platforms, allow for a greater flexibility in how they are integrated into the network. Unlike alternative accelerator platforms such as GPUs, FPGAs need no CPU host, and can be connected directly to the network [22].
A greater understanding of the implications of using this approach is needed. The costs and benefits of utilising these accelerators must be quantified, and generalisations for types of applications made. Understanding of the trade-offs between computation and communication is vital to making informed decisions regarding accelerator deployment.
1.2 Objectives
The objectives of this research are as follows:
1. To develop a methodology for exploring and evaluating heterogeneous networked systems used to offload complex applications, and to quantify key performance metrics.
4

2. To quantify the costs associated with the deployment of reconfigurable accelerators in alternative offload configurations.
3. To characterize the performance of emerging application types with different compute placement strategies, taking into account the variation in computing capability and connectivity of heterogeneous platforms.
1.3 Contributions
The main contributions of this work are a set of tools, measurements, and investigations that can be used to evaluate the placement of offloaded tasks across a network of heterogeneous computing and networking elements. These can be summarised as follows:
1. A mathematical formulation of this scenario, which allows for the detailed description of a heterogeneous network, tasks, and accelerator hardware. The model, unlike other related works, is generalised to account for heterogeneous hardware accelerators, complex task structures, and is easily extensible to model a range of scenarios, capturing all the important metrics of interest.
2. Derived from this model, generalised insights into how computation should be offloaded across networked devices depending on various application and network characteristics.
3. This model can be used within a mixed integer linear programming optimisation to generate hardware and task allocations to meet various performance objectives typically used when considering computational offload.
4. Experiments used to determine inherent latency limits in the use of FPGA hardware accelerators in various deployment scenarios in a networked context.
5. The use of an FPGA network switch extended to perform additional computation in an edge computing case study, and show the resulting performance benefits against other competing strategies.
1.4 Thesis Roadmap
Chapter 2 is a comprehensive background and literature review. It contains background and comparisons of different computing architectures, how they are integrated into networked systems, and where they can be deployed. How this deployment has been modelled in other works is also discussed.
5

Chapter 3 details a mathematical model that can be used to describe and evaluate the deployment of hardware acceleration and the offload of computing tasks within a network of connected devices. It also demonstrates the usage of this model to generate optimal task and hardware placement for a given scenario and set of constraints. The optimisation is evaluated against naive placement strategies using a representative case study as well as synthetically derived scenarios.
Chapter 4 focuses on experiments designed and carried out to determine latency characteristics in the deployment of FPGA accelerators. Both hosted PCIe and server-less in-network deployments are characterised.
Chapter 5 is a case study comparing in-network FPGA acceleration with other offload strategies for a complex image processing application. The in-network approach uses an augmented FPGA network switch, and is compared to edge node and cloudlet offloaded computation. This chapter details the design of the experimental testbed, results, and general insights that can be derived from the experiments. Results show that the host ingestion latency is comparable to the network traversal time.
Chapter 6 discusses further work and conclusions drawn from the work presented in this thesis.
1.5 Publications
Work presented in this thesis has featured in the following publications.
1. Ryan A. Cooke, Suhaib A. Fahmy, In-network online data analytics with FPGAs, in Proceedings of the International Conference on Field Programmable Logic and Applications (FPL), 2017 [1].
2. Ryan A. Cooke, Suhaib A. Fahmy, A model for distributed in-network and nearedge computing with heterogeneous hardware, in Future Generation Computer Systems (FGCS), vol. 105, 2020 [2].
3. Ryan A. Cooke, Suhaib A. Fahmy, Quantifying the latency benefits of nearedge and in-network FPGA acceleration, in Proceedings of the International Workshop on Edge Systems, Analytics and Networking (EdgeSys), 2020 [3].
4. Ryan A. Cooke, Suhaib A. Fahmy, Characterizing latency overheads in the deployment of FPGA accelerators, in Proceedings of the International Conference on Field Programmable Logic and Applications (FPL), 2020 [4].
6

5. Ryan A. Cooke, Suhaib A. Fahmy, Exploring Hardware Accelerator Offload for the Internet of Things, submitted to: it - Information Technology.
7

Chapter 2
Background and Literature Review
When considering modern networked computing systems, there are two primary considerations: the computing resource used to complete the processing, and where they are located in the network. As discussed in Chapter 1, these systems are increasingly heterogeneous, utilizing a variety of hardware architectures to perform the required computation. Each of these platforms have varying capabilities and trade-offs. Additionally, there are a growing number of network nodes capable of hosting these hardware platforms, increasing the deployment possibilities.
This means that there are a greater variety of deployment options when considering distributed hardware acceleration. It becomes more difficult to evaluate task placement, and to manually design these systems. Existing mathematical models don’t take these recent trends into account, or are designed for runtime task allocation in an environment with heterogeneous hardware architectures.
How these architectures are integrated to distributed, networked processing systems is also a significant challenge, with implications on performance. There are now a greater variety of integration techniques, with little experimental evaluation.
This chapter first examines the variety of processing architectures available in modern systems. It then examines the various integration possibilities, and relationship with the supporting network infrastructure, highlighting the need for experimental comparisons between approaches. Finally, it contains a study of other modelling efforts, and how they aren’t sufficient for modelling heterogeneous, distributed systems given modern advances.
8

2.1 Computing Platforms
In this thesis, the term computing platform is used to define a hardware architecture capable of carrying out a computation. Historically, this processing would be carried out on a general-purpose processor, but as workloads have increased in scale and complexity, alternative accelerator platforms have seen a surge of interest.
2.1.1 Central Processing Units
General-purpose central processing units (CPUs) are fundamental components of computing systems. While there is a significant variety of specifications and implementations, the general execution model is the same across most devices. The task to be carried out is expressed through a program, a set of instructions composed from the processor’s instruction set – the set of fundamental operations that a CPU can perform. The program is stored in memory, and instructions are executed sequentially.
Processing Model
At the start of each cycle, the next instruction to be executed is fetched from program memory, pointed to by a special purpose register called the program counter (PC). The instruction typically comprises an opcode denoting the operation to be performed, and register and memory location references to the operands. After being fetched, it is decoded into signals that control the execution unit. The decoded instruction triggers a series of actions that execute the operation. These actions will vary depending on the operation required, but usually involve loading operands into registers, fast memory local to the processor. Arithmetic and logical operations are carried out between these registers.
Application Development
CPUs are designed for flexibility, and must be able to implement a wide range of applications. Users can define their programs at different levels of abstraction. At the lowest level, programs can be written in the assembly language for the target processor instruction set. Programs are written directly in terms of CPU instructions by the user. For most applications, this approach is impractical and tedious, and is not realistic to write anything more than simple code at this level. The resultant program is also only able to be executed by an architecture that uses the same instruction set. Most programs are expressed in higher level languages that allow
9

for architecture independent, complex functions to be expressed more succinctly than at the instruction level. For some languages, such as C or C++, the program is compiled from this higher level expression down to machine instructions that are directly executed by the processor. Other languages are compiled down to hardware independent bytecode, which runs on an intermediate virtual machine, which has hardware-specific implementations. This way the same executable can be executed on any hardware, given that it has a virtual machine implementation that can translate the bytecode to native processor instructions.
Types of CPU
Due to the general-purpose nature of CPU-based computation, there are many variants of the basic architecture that are used for difference application domains.
Server class processors such as the Intel Xeon family are designed to handle larger, more complex workloads, and many tasks at the same time. In comparison to other processors, they usually have many cores on the same silicon die. Each core is capable of independent concurrent operation, and is often also multi-threaded, which allows for the core to be rapidly switched between contexts. This effectively allows multiple software processes to be run concurrently on the same, time-shared physical core. These devices are expensive, and compared to processors used in other contexts, have high power consumption and heat dissipation.
The next class of processors are for use in general-purpose, desktop machines, such as the Intel Core series. Compared to server-class processors, these devices have fewer cores and are cheaper.
Processors not designed for use in a desktop or server environment can be classed as embedded CPUs. They are used in application specific deployments, often controlling or interfacing with external actuators or sensors. In comparison to the other classes, they are lower-power and cheaper. Single-board computers such as the Raspberry Pi are often used to control embedded applications, and utilise 32-bit multi-core embedded processors capable of running Linux. Micro-controllers integrate a processor, memory, and other peripherals onto the same die and are generally even more low power and low capability. Small 8-bit or 16-bit processors may be used in devices with severe power constraints such as remote sensor nodes.
In all cases, when offloading to or from processors, data must traverse a network interface and standard bus into processor memory space.
10

2.1.2 Field Programmable Gate Arrays
Field programmable gate arrays (FPGAs) are silicon integrated circuits comprising an array of configurable logic resources. There are several primary vendors of these devices, the most notable being Xilinx and Intel. While these vendors hold a majority of the market, other vendors exist, such as Lattice, who specialise in small low-power solutions, and Microsemi, who offer specialized products such as radiation-hardened devices. Architecture
Figure 2.1: Representative modern FPGA architecture [23] Modern FPGA architectures, regardless of vendor, are based around a generic design. Figure 2.1 shows a high-level overview of a Xilinx FPGA architecture. Users can express a digital circuit in a high-level abstraction, which goes through a flow of vendor tools to convert the circuit to be implemented using these logic resources on the FPGA fabric. User logic is implemented via an array of configurable logic blocks (CLBs) [24], which are connected to other CLBs through a switching matrix. Each CLB comprises several slices, where each slice typically contains a look-up table (LUT) and
11

Figure 2.2: Xilinx 7 Series CLB arrangement [24]
Figure 2.3: Xilinx DSP48E1 architecture [25] storage elements like flip-flops or small amounts of RAM. For example Xilinx 7 series FPGAs use CLBs with 2 slices, as in Figure 2.2, with each slice having four 6-input LUTs, 16 flip flops and 256-bits of distributed RAM. The CLB LUTs can be configured to implement a large variety of logic functions, and chained together to implement even more complex operations. In addition to CLBs, most modern devices also have hardened blocks of logic optimised for digital signal processing, such as the DSP48 [25] blocks built into Xilinx FPGAs (Figure 2.3). DSP slices can be used to implement complex arithmetic functions with fewer resources.
12

Hardened Block RAM (BRAM) is also usually present in modern FPGA architectures. This provides storage within the fabric, reducing the need for costly off-chip memory transfers. IO-blocks (IOB) are configurable blocks that contain the appropriate signal conditioning to allow for signals to be brought into or out of the FPGA. Any signal on any pin must go through an IOB. IOBs are arranged into banks - for example, Xilinx 7 series banks contain 50 IOBs. high-speed transceivers are also commonly implemented, to allow the FPGA to interface with high-speed signals such as those used in PCIe and Ethernet. Implementing this functionality using CLBs would consume significant resources, or not meet required performance standards.
All resources are connected through a routing network that routes signals throughout the FPGA. Dedicated interconnect is used to route clock signals. Clocks can be introduced to the FPGA through IO pins. Global clock lines can be used to route the clock to multiple elements throughout the device. These lines ensure minimal skew in comparison to the general-purpose routing resources. Many FPGAs include built-in PLLs or DLLs to synthesise signals of a configurable frequency.
Application Development
The user can express their design at different levels of abstraction. As designs and FPGA devices are complex, it would not be viable to specify the design at the individual gate and wire level. The lowest level of abstraction typically used by designers is the register transfer level (RTL). This is a paradigm where digital circuits are modelled as combinational transformations on data as they move between registers. Designers write this RTL description in a hardware description language (HDL), the most popular being ‘Verilog’ and ‘Very high-speed Integrated Circuit Description Language’ (VHDL). All mainstream vendor tools accept these languages. Once a design has been described with a HDL at the RTL level, it is converted to a low-level description by vendor tools in a process called synthesis. The synthesised circuit is then mapped to the target FPGA resources by the vendor place-and-route tool, creating a device specific configuration that implements the desired circuit. This configuration is then encoded into a bitstream that can be loaded into the FPGA configuration memory.
Designing at the RTL level requires knowledge of HDL, digital logic design, and some knowledge of the target FPGA architecture. These present one of the main barriers to entry for developing for FPGAs, leading to the development of language and design flows that let users work at an even higher levels of abstraction, an approach commonly referred to as high-level Synthesis (HLS). Here, designers
13

describe the design at an algorithmic level using languages such as C++. Both Xilinx and Intel offer HLS tools for their platforms.
Vivado HLS is the commercial Xilinx HLS tool, where users express accelerator designs using C or C++ [26]. The user writes functions in these languages that are compiled into HDL descriptions. Features such as loop unrolling are available which allow for the user to control to what extent loop constructs are implemented spatially in parallel in the resulting FPGA design. This can increase the performance of the design with the penalty of it using more resources. Python-based HLS has also been demonstrated with the open source Migen framework [27]. The framework also includes a set of free IP cores written in python that can be instantiated into user designs. Testbenches can also be written in python.
The open computing language (OpenCL) framework is also utilised for FPGA HLS. This framework allows for cross-platform code to be written in general-purpose languages such as C++, which can then be targeted to heterogenous platforms such as GPUs and FPGAs. Intel offer this as part of their FPGA SDK [28], and Xilinx as part of their SDAccell platform[29]. The benefit of this approach is that it allows for side-by-side development of the base software application that will run on the host, tightly coupled, or soft-core processor and the hardware accelerator. These tools hide details regarding the software and hardware interfacing from the user. They represent a growing trend of heterogeneous computing system designs. Various academic works look to extend OpenCL FPGA integration for use with other languages. The work in [30] enables the coupling of Java programs with FPGA accelerators within th OpenCL ecosystem. This is useful for large-scale analytics applications as many of the frameworks used for this, such as Hadoop, utilise Java frontends. Another work [31] presents a similar strategy for accelerating python code with FPGAs using OpenCL.
In addition to using general-purpose languages for HLS, many works have examined using domain specific languages (DSL) to generate FPGA designs. These are languages designed to either simplify or enhance the development of applications for a specific domain. An early work in this area [32] mapped a network processing DSL called CLICK to an FPGA. The DSL was compiled to a HDL module that could then be synthesised with vendor tools. Network processing languages such as P4 have also had HLS integrations [33; 34]. Hipacc [35] is a DSL than can be used to create FPGA accelerators for image processing. It utilises OpenCL, and can also be used to target other platforms such as GPUs. The framework also allows for more image processing kernels to be added to the database of supported operations. Another image processing DSL, Halide, is adapted to generate FPGA accelerators
14

in [36]. An end-to-end flow is presented, and an evaluation the generated accelerators performed around 4× faster than CPU implementations. The framework presented in [37] allows for SQL queries to be compiled to FPGA accelerator blocks to be loaded into PR regions at runtime. SQL is a DSL used to interact with relational databases. A generic backend named FROST, capable of automatically adapting multiple different DSLs to HLS is demonstrated in [38]. It can extract an abstract representation of the functionality described by the DSL and then generate a C++ equivalent which can then be used with Xilinx HLS and SDAccel tools. The tool also exposes the same optimisations offered by Vivado HLS such as loop unrolling to the user.
One of the key barriers to the utilisation of FPGAs within networked computing systems is the considerable design effort required. This includes both datapath and system-level design. Advances to high-level development flows such as HLS simplifies accelerator designs that use standard interfaces, greatly reducing friction when deploying FPGAs as accelerators in networked systems.
Types of FPGA
FPGAs can vary in terms of the amount of available resources, hardened blocks, and IO. In general, more expensive devices will have a greater number of CLBs, DSP slices, and more available BRAM, as well as higher bandwidth I/O. Devices such as the Xilinx 7 or Ultrascale series are designed for high-speed networking or accelerating datacentre workloads, so have high-speed transceivers and large amounts of on-chip memory available.
FPGAs designed for smaller scale and even embedded applications also exist. The lattice ECP5 is a cheaper, mid-range device that’s designed for automotive or embedded applications. The Lattice ICE40 is a very low density FPGA with few resources used for ultra-low-power embedded applications running on limited power supplies.
Many modern FPGAs can utilize partial-reconfiguration (PR). This feature allows for reconfiguration of selected regions of the FPGA at runtime, without having to reconfigure the whole device [39]. Hardware modules must be partitioned into partially reconfigurable regions, and then these regions mapped to the FPGA fabric through floorplanning. The vendor tool generates partial bitstreams that implement these modules with resources in the reconfigurable regions. These partial bitstreams can then be loaded at runtime to reconfigure these regions without affecting the rest of the design (Figure 2.4). PR has several benefits over full reconfiguration: resources can be time-shared, reducing the overall resources required, designs are
15

Figure 2.4: Partial reconfiguration allows for selected regions of the FPGA to be reconfigured using partial bitstreams [39]
more flexible and can be modified at runtime, and the same FPGA can be used to independently serve multiple applications in isolation.
2.1.3 FPGA Accelerator Design
FPGAs can provide improvements to application performance through the deployment of computing logic optimised for a given task. This is in contrast to CPUs, which execute software on a general-purpose architecture. The task, algorithm or process is implemented using logical resources on the FPGA. This may be the entire algorithm, or only computationally intensive functions that act as a bottleneck when performed in software. Custom FPGA implementations achieve improvements over software through several avenues that will be discussed in this section.
Static Accelerators
Static accelerators are architectures that are designed to implement a single function. While these application specific architectures may be reconfigured through changing register values, there is no dynamic, device level reconfiguration. To change the core functionality of the accelerator, the entire FPGA must be reconfigured.
One of the key benefits of FPGA accelerators is the ability to implement architectures that exploit temporal parallelism. A sequence of operations can be ‘pipelined’,where each stage of the pipeline can operate concurrently – resulting in an improvement to throughput. Another of the key features of FPGA acceleration architectures is the exploitation of spatial parallelism. A function that may take many iterations of a loop in software can be ‘unrolled’, and multiple iterations of that loop can be implemented with independent FPGA logic, allowing them to execute in parallel. This property is exploited in many data mining and machine learning
16

algorithms, which often involve performing large quantities of the same operations on a large data set. Utilising a tiled design, of the same operations repeated spatially across the FPGA fabric, is commonplace. The SVM accelerator in [40] demonstrates an example of this approach. This design uses a set of tiles that operate on different parts of the dataset in parallel. Within each of these tiles is another tile structure. The outputs of these tiles are aggregated by additional logic that is in sequence with the tiles. This architecture outperformed a software implementation of the same algorithm by 2–3 orders of magnitude as a result. Decision tree algorithms also commonly use FPGA accelerators, and benefit from spatial parallelism. The architecture in [41] again uses a tiled design, where each comparison in the tree is implemented in parallel. An implementation of a random forest classifier takes this further in [42], implementing multiple trees operating on the same data, in parallel. The design had a greater performance per watt compared to CPUs and GPUs. The paper additionally highlights an issue with spatially parallel designs the finite resources in the fabric can limit performance gains, and the size of the algorithm that can be implemented. To extend the forest classifier beyond a given size for the available hardware, the authors had to use multiple FPGAs, while still needing only one unit for the GPU implementation.
These decision tree works use static tiles and tree structures. This means that to implement a different tree, the FPGA must be reconfigured. The work in [43] proposes a structure that uses generic, dynamically configurable tiles that use configuration data stored in on-chip RAM. The tree is converted into a rules table and written to RAM to reconfigure the accelerator without having to reconfigure the entire device. Utilisation of an array of flexible tiles is a technique often employed with convolutional neural network (CNN) accelerators. CNN inference requires many 3D convolutions, which include large sets of multiplications. The number of operations required means that unrolling the application in its entirety would be impossible due to resource limitations, therefore a set of processing elements that can be reconfigured dynamically is typically used, operating on chunks of data sequentially. The CNN accelerator in [44] uses an array of processing elements that are configured by a soft-core processor on the FPGA. Weights and data are fetched from off-chip memory. This accelerator improved processing time by 7× compared to a CPU implementation, and had a 24× reduced power consumption. Data transfer from off-chip memory is a bottleneck for these systems, so on-chip BRAM is used for temporary buffering. This reduces the number of costly off-chip memory transfers, and allows for processing to be offset from data transfers temporally. Another CNN accelerator demonstrated in [45] attempts to optimise these on-chip buffers to take
17

data re-use into account, greatly reducing the number of required off-chip transfers. They also develop a 3 dimensional array of processing elements as opposed to a traditional 2 dimensional one, allowing for greater re-use of processing resources.
In all, FPGAs can be used to accelerate a large variety of intensive tasks, reducing the computation latency in comparison to CPU architectures.
Dynamic Partial reconfiguration
Dynamically reconfigurable accelerators are desirable in a number of cases and can be achieved through partial reconfiguration (PR), a feature available on most modern FPGAs that allows for reconfiguration of only selected regions of the FPGA fabric.
Most PR accelerator designs use an architecture comprising a static shell that contains communication and control logic, and reconfigurable regions that can be loaded with different accelerator logic at runtime using PR. This allows the device to continue running and operating within a larger system while the accelerator is being modified.
One scenario where this technique is useful is when designing an accelerator for adaptive systems. These systems must alter accelerator properties at runtime to respond to external stimulus. In [46], an FPGA accelerator is deployed as part of a wireless sensor network application that can track points of interest. The application is too computationally intensive to use microcontrollers, and a static FPGA design would be unsuitable as full device reconfiguration would take too long, as well as sacrifice communication with the network. Different filters are swapped in to the reconfigurable region at runtime depending on the targets characteristics such as velocity, noise and priority. Deploying all filter datapaths on the device at once would not be possible due to limited resources. A PR-based accelerator is similarly deployed in [37], for database query processing. Acceleration of these queries is essential for large databases to ensure throughput targets are met. Implementing all possible query pipelines on the FPGA at once was not possible as it would have consumed too many resources. Reconfiguring the entire accelerator for each query would be too slow. This design therefore loads the appropriate query processing pipeline as they arrive using PR. Compared to a software solution, the FPGA accelerator achieved between 1.4× and 6.15× speedup depending on the query, with greater relative performance gains for more arithmetically complex queries.
An adaptive K-nearest neighbour (KNN) accelerator is developed in [47], where PR is used to modify the design based on user parameters. This allows for faster reconfiguration times for latency-sensitive workloads, and again allows the
18

accelerator to be integrated into a communication infrastructure mpre easily. The design provided a 68× speedup over a CPU implementation, and using PR instead of full-device reconfiguration resulted in around 5x faster reconfiguration. The user parameters for KNN applications are frequently changed, with users often experimenting and comparing results for different parameter values, making this faster reconfiguration time beneficial for this application. PR has been used in cognitive radio applications [48] to swap in different functions depending on the conditions such as the signal-to-noise ratio of the transmission channel, or available power. For example, a function performing the same function but with a lower power consumption can be loaded on to the FPGA if a more efficient implementation can be used. An OFDM cognitive radio is implemented in [49] to support different OFDM standards. This design uses PR only for modules that require significant changes to support a different standard, and parametrised modules for those requiring less change, and the additional resource overhead caused by the parametrisation is below a threshold. This was shown to give performance benefits compared to the traditional approach of making using PR modules for the entire pipeline, or a monolithic module containing the entire design.
PR-based accelerators are also used to time-multiplex FPGA resources, overcoming spatial constraints. The entire application may not fit within the FPGA at once, so PR is used to swap in different tasks at runtime. An example of such an accelerator can be found in [50]. The bio-informatics application examined benefits from comparing the outputs of different classifier algorithms. Using partial reconfiguration, classifiers can be loaded sequentially, saving FPGA resources, and can be loaded 8× faster than complete device reconfiguration. Similarly, this timemultiplexing technique can be seen in [51], for a KNN application. The accuracy of the algorithm can be improved by comparing the results of an ensemble of classifier configurations. Resources can be saved by loading classifiers with different parameters sequentially. This method has the benefit of making the design more scalable, and compatible with a greater number of devices with different resources available. For large FPGAs with an abundance of resources, more classifiers can be instantiated in parallel, while for smaller devices, more time-multiplexing can be utilised. Time-multiplexing using PR is exploited for a NN inference application in [52]. The results of each layer are dependent on the results of the layer before, so each layer is loaded sequentially using PR, saving resource consumption. This makes the design more compatible with smaller devices. Similarly, the video broadcasting decoder accelerator is demonstrated in [53], which again takes advantage of time-multiplexing using PR to save resources. Again, this is a highly sequential algorithm where the
19

results of a given part are highly dependent on the previous parts. Dynamically reconfigurable accelerator devices are beneficial for applications
that must be flexible at runtime. Additionally, dynamic reconfiguration allows for time-sharing of resources and isolation of tasks, which are key enablers of device virtualisation. These properties are particularly advantageous in networked computing systems where resources are shared across multiple tasks, and process constantly changing workloads.
Overlays
Related to this design paradigm is the concept of overlays. This technique involves implementing an intermediate, coarse-grained architecture on an FPGA, quite often an array of generic processing elements, which can then be programmed without reconfiguring the FPGA. Designers express their requirements using a higher level of abstraction, and a custom compiler transforms and maps this design to an architecture pre-loaded onto the FPGA. This gives added flexibility in some scenarios, especially when fast reconfiguration is needed. It also has the added benefits of reducing the difficulty in designing an accelerator, and reduces the time taken to generate a design as no synthesis or place and route is needed.
An example of an overlay architecture can be found in [54]. This design resulted in a 140× speedup and an improved area-time product compared to a softcore running on an FPGA. One issue with overlays is that they consume more area and can limit throughput and operating frequency compared to fine-grained designs optimised for a specific application. Some overlay architectures have attempted to solve this by designing overlays more closely around the underlying FPGA architecture [55; 56]. These works utilise functional units built around the DSP48E1 primitives found on modern Xilinx devices. As a result, there were large improvements to resource consumption, and improved throughput and reconfiguration time.
Overlay architectures can be targeted to specific domains in order to alleviate their performance penalty compared to hard logic implementations. Optimising the overlay for specific domains sacrifices flexibility for increased performance. The ‘DLA’ architecture presented in [57] is targeted at neural network inference acceleration. Using this overlay, the authors were able to achieve a throughput of 900fps for GoogleNet. One of the factors enabling this high throughput was the fast reconfiguration times offered by overlay architectures – layers’ filters could be quickly loaded in ahead before they were needed. The overlay in [58] was similarly targeted specifically at NN inference, but expanded the overlay and supporting instruction set to be able to support a greater range of NN architectures, and allow for a more
20

fine-grained control of operator deployment - resulting in more efficient utilisation of overlay resources. Their platform showed significant improvements in performance per watt compared to both GPUs and FPGA implementations demonstrated in other NN inference works. They also demonstrated improvements over a GPU for a license plate detection applications, achieving 3× greater throughput. An overlay for accelerating DSP workloads is demonstrated in [59]. Fast reconfiguration is beneficial for workloads with variable parameters, or for devices that don’t have the resources to allow for the entire application to be mapped onto the FPGA at once. It was evaluated using a 255 tap FIR filter implementation, where the accelerator implemented with the overlay resulted in 10× speedup compared to a CPU, compared to 16× speedup for a full custom design. However for larger data sizes, when reconfiguration time is taken into account, the overlay architecture begins to outperform the hard logic implementation.
While overlays can be used to implement dynamically reconfigurable accelerators, performance limitations exist as the architecture cannot be completely optimised for a given application, and the overlay architecture can limit what logic can be implemented, making it unsuitable for some applications. However they can simplify application development, enable fast reconfiguration, and abstracts away system design details.
2.1.4 Graphics Processing Units
Graphics processing units (GPUs) are another reprogrammable hardware accelerator platform, historically used to accelerate graphics rendering. Compared to a CPU, a GPU comprises large numbers of simpler compute units operating in parallel. Complex functions are decomposed and distributed amongst these cores, leading to reductions in processing time. This processing architecture can be used to accelerate non-graphics rendering applications. In [60] a GPU was used to accelerate quantum chemistry calculations, and resulted in a speedup of 3.8× compared to a CPU. GPUs have also been demonstrated to improve performance in neural network applications such as in [61] where a 2-11× speedup was achieved for a recurrent neural network.
Processing model
GPUs comprise of a vast array of processing elements or cores. Each core has limited functionality compared to a CPU core, but is comparatively highly optimised to perform floating point arithmetic operations. Extraneous logic isn’t implemented, which means a much higher density of processing cores can be implemented on the
21

same die. Instead of computation being carried out sequentially, it is decomposed and distributed to a larger number of cores in parallel, increasing throughput and reducing computation time for tasks with large data sets. Cores are also often gathered into processing groups, that all only execute the same instruction. This removes the need for the CPU fetch, decode, execute cycle.
Application Development
Applications are usually written for GPUs using standard software-based programming languages such as Python or C++, with an explicitly parallel GPU framework that abstracts low-level detail away from the user. CUDA is a framework developed by Nvidia for use with its GPU architectures. As previously mentioned, OpenCL is another framework that can be used. OpenCL allows for users to express applications independently of the target architecture.
Types of GPU
GPUs are often connected to a host processor through the PCIe bus. Datacentre class GPUs have high densities of processing elements, and are hosted on boards with large amounts of GDDR5 memory.
Embedded GPUs are lower density and lower power solutions, often integrated onto SoCs used in mobile phones or single board computers.
Comparison to FPGAs
There are several works comparing GPU to FPGA acceleration. Both were tested with a random forest classifier application in [42]. This study determined that for this application, the FPGA achieved a higher performance per watt, however the GPU produced the best performance per dollar. While the FPGAs were more power efficient, it was also noted that the FPGAs ran into resource comsumption issues for larger forest sizes, resulting in the need for additional FPGA boards. The GPU did not, however had a degrade in performance instead. A RNN application was evaluated with a Zynq SoC and Nvidia Tegra GPU development board in [62], platforms marketed towards embedded applications. The Zynq outperformed the GPU significantly in terms of performance per watt. This observation was mirrored in a K-means clustering application [63], where the FPGA accelerator was significantly more power efficient and energy efficient than the GPU.
An extensive review of FPGA and GPU comparison studies was carried out in [20], which looked at the relative performance of GPUs, FPGAs and CPUs for
22

a variety of applications such as matrix multiplication, FFTs, and encryption/decryption. It was found that for algebraic operations and complex simulations that involved complex mathematics with floating point numbers, GPUs generally provided greater speedup. FPGAs were better at combinational logic applications like encryption, and for fixed-point or integer signal processing. FPGAs had a lower power consumption in most surveyed applications.
As both platforms excel at different tasks, some efforts have been made to produce heterogeneous systems utilising both. This technique has been used for a medical application requiring very high frame rate image processing [64]. An FPGA acting alone did not have the resources available for very large parallel operations and large buffers required for the application, and a lone GPU ran into issues with data flow bottlenecks. A combined solution led to double the frame-rate of a lone GPU and reduced latency. A hybrid GPU-FPGA architecture was used for a SVM image classifier application in [65], where the FPGA performs feature extraction and the GPU the classification. The FPGA provided low latency processing of the image feed as it could interface with the camera easier. The GPU allowed for highly parallel computation that made up the classification phase.
Overall, FPGAs have been demonstrated to have greater power efficiency than GPUs. Another main difference is that FPGAs allow for more integration possibilities. GPUs rely on a PCIe connection to a CPU host, while FPGAs can be connected directly to the network as standalone devices.
2.1.5 Application Specific Integrated Circuits (ASICs)
Application specific integrated circuits are devices designed and fabricated to carry out a specific group of tasks. Unlike FPGAs, the architecture is fixed at manufacture, and unlike CPUs or GPUs, they offer limited programmability. The benefit of this rigid architecture is it can be completely optimised to perform these tasks. While FPGAs require flexible interconnect which limits the achievable clock rate, ASICs do not. There is also no redundant logic or unused resources.
The work in [66] compared a recurrant neural network application implemented on an FPGA and a 14nm ASIC. They estimate that for this purpose, the FPGA was only 7× less efficient than the ASIC. Furthermore, they identified that the number of available DSP blocks was a limiting factor in the FPGAs performance compared to the ASIC, and that this gap will close as new devices offer these resources in greater densities. In another work, the authors make a similar comparison with binarized neural networks [67], and reach the same conclusion, with the FPGA performing around 8× less efficient than the ASIC implementation.
23

There have been a number of commercial ASICs developed to accelerate machine learning applications. Google have recently released a programmable ASIC for neural network inference called a Tensor processing Unit (TPU), which allows fast execution of tensor models [68]. This architecture was shown to achieve up to 30× speedup and 80× energy efficiency for DNN inference compared to CPU and GPU architectures tested [69]. Intel’s Myriad X vision processing unit (VPU) is an ASIC optimised for computer vision applications, and contains a dedicated neural network processor [70]. There are also versions of these devices designed for operation at the network edge. These platforms are relatively new, and there are very few works comparing their performance to FPGAs.
2.1.6 Summary
In recent years there has been a significant increase in the number of available processing platforms and architectures. These architectures have vastly different computational abilities and execution models. When modelling distributed hardware acceleration, the heterogeneity of modern platforms must be considered.
The variety of processing platforms available introduces further variables to consider when deciding how to distribute processing. Unlike other works, the modelling work presented within this thesis takes this into account. There is also little work comparing the performance of these platforms at a systems level, taking into account both the computation on the platform, and communication to it. Both of these overheads must be considered when evaluating distributed systems.
2.2 Accelerator Integration
Data transfer to and from the device is often a system bottleneck, and provides complex challenges, hence requiring significant engineering effort.
2.2.1 PCIe
FPGA accelerators have often acted as slave devices to host processors through PCIe, similarly to other peripherals such as NICs or GPUs. Utilising PCIe means minimal disruption to the rest of the infrastructure, and provided a scalable high throughput interface already commonly used in datacentre and workstation environments. The host CPU would control the flow of data to the accelerator, which had no direct connection to the rest of the network.
Implementing a PCIe connection between the accelerator and host requires
24

the user to design logic around a PCIe controller IP on the FPGA, as well as write the software drivers that allow the host to control and communicate with the accelerator, requiring knowledge of PCIe and driver development. Commercial integration frameworks such as Xilinx Xillybus [71] and IBMs CAPI [72] are built on top of PCIe, and abstract this detail from the user. However these solutions had several limitations, mainly being restricted to particular hardware. This led to the development of many open source frameworks. One of the first open frameworks that provided a generalised hardware interface based on the Xilinx PCIe endpoint IP and associated software API was Riffa [73]. Building upon this, other frameworks such as EPEE increased device throughput and allowed for additional features such as user defined interrupts [74]. Dyract [75] was a framework based on the open source Riffa framework that additionally provided the capability for partial reconfiguration over PCIe. Revisions to Riffa [76] introduced support for PCIe Gen3 and more FPGA platforms, and Jetstream [77] provided partial reconfiguration over Gen3 PCIe and multi-FPGA support.
The high throughput of PCIe lends itself towards batch processing applications with large volumes of historical data and large transfer sizes. FPGA accelerators using PCIe have been demonstrated to increase performance relative to CPUs in numerous works. A Virtex-5 based PCIe accelerator was used to accelerate neural network image classification for the Baidu search engine [78]. The accelerator provided a speedup of around 13× compared to a software implementation on their test data sets. These results are when using PCIe Gen2×4, so if upgraded to more the higher bandwidth PCIe Gen3, an even larger improvement would likely be seen. A KNN accelerator was demonstrated in [79], using PCIe Gen2×4 to connect to a CPU host. The accelerator provided a 148× speedup compared to a software implementation. A GPU outperformed the FPGA in terms of application speedup, but provided a 3× smaller performance per joule compared to the FPGA. Additionally, the FPGA was only using PCIe Gen2×4, while the GPU was using Gen3×16, which would account for some of the additional speedup of the GPU. PCIe-based FPGA accelerators have also been used to accelerate relational database queries [80]. The FPGA delivered a 10.7× and 6.7× speedup compared to the host CPU, for compressed data. When data was uncompressed, there was a smaller relative speedup due to the reduced effective bandwidth of data to the FPGA.
PCIe-based accelerators have an associated communication cost. The work in this thesis quantifies these overheads in Chapter 4.
25

2.2.2 Network interface
Alternatively, FPGAs can communicate with servers via a network interface, typically Ethernet. This allows FPGAs to connect directly to the network without the need for a CPU host. The FPGA can receive and process data without the latency and complexity of a software network stack and non-deterministic CPU behaviour. These characteristics lend themselves well to stream processing applications, where data must be processed immediately as it arrives, and latency is of greater importance than throughput.
One method of networked FPGA accelerator deployment is commonly referred to as a ‘bump-in-the-wire’, where an FPGA is placed between a servers NIC and the network, so that data goes through the FPGA before reaching the server. One example of this approach is demonstrated in [81]. FPGAs can be used as local compute accelerators, used for on the wire processing of packets from the network, or as a set of shared pooled resources between servers. Networking applications such as line-rate encryption/decryption were accelerated, providing performance benefits over software implementations. Acceleration of the Bing search engine was also implemented, which involved multiple remote FPGAs working together across the network.
FPGAs have alternatively been integrated directly into the NIC, with a PCIe connection to the host server and Ethernet connection to the wider network. Again, this allows the accelerator to operate on data from the network independently of the host, and removes the need for sperate NIC and FPGA accelerator boards. Microsoft have adopted this approach to integrate FPGA acceleration into their Azure cloud platform [82]. FPGA accelerator NICs have also been demonstrated to improve performance per watt in applications such as key-value stores, such as in LaKe [83]. Here, memory caches are implemented on the FPGA BRAM and external DRAM connected directly to the FPGA, which doubles as a NIC for all non-application traffic. Upon reception of a query, the packet is only forwarded to the host server if the requested data is in neither cache. This allows for drastic improvements to latency and power consumption compared to software implementations. Outlier filtering for data mining applications was implemented on a 10GbE FPGA-based NIC in [84]. The FPGA NIC filters incoming data and only forwards packets of interest to the host, drastically increasing the effective throughput. The system was capable of filtering the incoming data stream at 95.8% of 10GbE line-rate.
Utilising high-speed network interfaces with FPGA accelerators also allows for multi-FPGA clusters to be implemented. This technique is used to implement a mapreduce accelerator in [85]. Each FPGA is connected to a host server through
26

PCIe, but connected to other FPGAs in the system through Gigabit Ethernet. The host mainly controls and configures the FPGA, and to retrieve input data from the host memory. Key-value pairs are passed between FPGAs through the network interface. The FPGA-acclerated system was 15× to 20× faster than the software implementation depending on application parameters. The main bottleneck was identified as the communication from host to FPGA through PCIe. Similarly, a multi-FPGA accelerator for deep learning mapreduce applications is presented in [86]. Instead of using a cluster of host-FPGA pairs, it uses the Xilinx Zynq boards, where the FPGA and CPU are tightly coupled, removing the PCIe bottleneck. They reported 8× to 12.6× speedup for a CNN application with this system compared to a cluster of CPU nodes, as well as significant power consumption reduction.
2.2.3 Tightly coupled SoC
More recently, platforms that feature an FPGA tightly coupled to a CPU in the same package have emerged. Examples include the Zynq family from Xilinx, which couples an FPGA with an ARM core, and some of the Xeon family from Intel. The FPGA can communicate with a processor through high-speed on-chip interconnect, instead of external interfaces like PCIe or Ethernet. Typically these devices use the processor for control and configuration, and the programmable logic for compute intensive processing. While a traditional FPGA may utilise a soft-core processor implemented within the FPGA fabric, this is resource inefficient in comparison. The integrated processor has often led to the reprogrammable SoC devices being targeted towards embedded and edge of network applications.
The tightly coupled processor simplifies software controlled partial reconfiguration of the programmable logic part of the device. This allows for reconfiguration of only specified regions of the fabric, as opposed complete device reconfiguration, enabling time multiplexing of the programmable logic, increased flexibility to modify logic at runtime, and sharing resources across multiple applications. There is an area of research focused on accelerator management with tightly coupled SoCs. ZyCap [87] is a framework that utilises a custom reconfiguration controller and software interface developed to allow for more efficient partial reconfiguration on the Zynq platform. When evaluated it had 3× higher throughput than the Xilinx ICAP alternative. The MiCap framework [88], uses a controller that is designed for smaller bitstreams, and allows for the configuration to be read back. It is targeted at dynamic circuit specialisation applications, where circuit parameters are implemented into the design as constants rather than regular inputs, allowing the design to be specialised around those constants. The work on MiCap is extended in [89].
27

These tightly coupled architectures are suitable for software applications with a few complex tasks. A PR-based cognitive radio is presented in [90], where the control and configuration is in software in the PS, while the more computationally intensive baseband processing is implemented within the PL. Additionally, the tight coupling with a CPU without the need for an external host makes them effective for embedded applications. In [91], a Zynq is used to implement a NN application to monitor gas sensors to determine the composition of the gas being monitored. The accelerator achieved comparable performance to hosted FPGA accelerators for the same application. A Zynq-based accelerator for a road sign detection application is demonstrated in [92], where the PL is used to interface with the sensor and filter data before it is classified in software in the PS.
2.2.4 Interface overheads
Factors such as I/O, network, and CPU stress have been demonstrated to have significant impact on the magnitude and variability of packet delays [93; 94; 95]. The various contributors of packet latencies in datacentre environments were examined in detail in [96]. The PCIe interface has also been shown to be a significant contributor to latency [97]. It was demonstrated in [93] that virtualisation using Linux Vserver typically added a small delay to packet round trip times, while Xen virtualisation added 3 to 4 times greater latency.
There has been little to no work on the interface overheads for FPGA accelerators in particular. As these overheads can be significant, the lack of this information can make it difficult to evaluate the value of accelerator platforms at a system level.
2.3 Summary
FPGAs have can be integrated into distributed computing systems through various interfaces. Despite this, there has been little work investigating the overheads of these interfaces, especially the emerging network-attached method.
While individual works have presented performance studies focussing on the designs they present, there have been no generalised examinations that can be used to aid in modelling or design decisions. Chapters 4 and 5 of this thesis detail experimental studies on these overheads, and demonstrate their effect on an application at the systems level.
28

Figure 2.5: Example of network infrastructure supporting computational offload to remote resources.
2.4 Networked computing systems
In networked computing systems, processing is moved from a data source to another more capable network node for processing, to reduce computation time, power consumption, or increase throughput, and the result is sent back. An additional benefit is that the original platform is free to perform other tasks while it waits for the result. The benefits gained must make up for the communication penalty incurred through offloading the data. Depending on the nature of the application, this network structure will vary. An example of such as network structure can be seen in Figure 2.5.
2.4.1 Edge Networks
Endpoint nodes sit at the very edge of the network, and typically act as data sources. Examples of such nodes are IoT sensors, mobile phones, networked cameras, or autonomous vehicles. They offer some limited processing capability, but need to offload data to more capable hardware to meet power or response-time constraints. Typically processing is offloaded to the cloud or a remote datacentre, requiring devices to transmit data across the Internet to reach the target platform. Endpoint nodes could utilise various communication Interfaces. Nodes in remote, hard-toaccess locations, or mobile phones, utilise cellular communication such as long-term evolution (LTE) to gain access to a larger scale network. Data is transmitted through the device antenna to the nearest eNodeB or base station [98]. This node facilitates the transmission through the core network to the service provider’s datacentre, and then over the Internet. Other endpoint devices may utilise Wifi, in which case data
29

is transmitted to a local access point, which is then connected to the wired local area network. Data will traverse a series of Ethernet switches and aggregation nodes, and gain Internet access through a router. Applications that require high communication reliability, or are not in difficult to access, static locations may utilise wired Ethernet for the first hop, then follow the same path.
2.4.2 Datacentre Networks
Inside the datacentre itself is another complex network, an example structure can be seen in Figure 2.6. A datacentre is a location with a large collection of large-scale networked servers and other machines [99]. These servers are made up of racks of commodity machines, network resources, storage, power distribution, and cooling. A standard rack is 42U high, which means that 42 1U rack mount servers or other devices can be housed. Blade mounts allow for multiple servers to be hosted within the same 1U enclosure. As a result, server racks may contain a high density of machines, which all need to communicate with other machines in the datacentre. Each rack has a top-of-rack (ToR) switch which facilitates communication between servers in the same rack, and between the rack and the wider datacentre network [100]. External switches allow for racks within the same cluster to communicate with each other, and aggregation switches connect clusters to routers that allow for intercluster communication. In the particular topology shown in Figure 2.6, data must go through at least one switch to go from one server to another in the same rack, and at least 3 switches to reach a server in another rack. Quality-of-service policies are implemented on switches and host NICs to prioritise traffic flows under high network load.
While datacentres are typically large centralised environments, smaller scale datacentres are increasingly being deployed to the network edge, closer to the clients they serve. Being closer geographically reduces the latency. These edge datacentres are similar in structure to their larger counterparts, just at a smaller scale. For example, telco central offices traditionally used for telephone switching are being re-purposed as datacentres [101], that are in geographical locations close end users. Micro-datacentres have also been proposed – these are even smaller scale, comprising only 10s of servers in few racks, placed in extreme proximity to premises. This means that platforms requiring offload could potentially have a lower latency option. This approach can be seen in mobile edge computing, where processing data on a mobile device would consume too much power, and doing so in the cloud would lead to high latency [8; 9; 10]. Edge datacentres have been demonstrated in video processing and augmented reality applications [11; 12; 13] where latency is an
30

Figure 2.6: Example of datacentre network topology [100] important consideration. Applications such as face recognition [11; 12] and video surviellance [17] have also been demonstrated on these platforms with significant latency improvements over traditional cloud offloading.
31

2.4.3 In-network computing
Another emerging method of computing offload is utilising elements traditionally used for moving data, such as switches, gateways, routers, or NICs to perform extra computing on top of their standard networking functions. These elements are discussed in Section 2.4.4. This technique is often termed ‘in-network’ computing. Cisco utilise this approach in an edge processing context in what they call ‘fog computing’, where endpoints at the network edge utilise local network elements to assist with computation [102]. In-network computing can also be utilised within datacentre networks, implemented within the dense switching network. There are several benefits to this approach. Firstly, for edge processing applications it provides a computing resource extremely close to the endpoint. The trip from an endpoint to a centralised datacentre over the Internet, or even to a geographically closer edge datacentre or cloudlet is much larger in comparison. For applications with very tight latency constraints, such as closed-loop control of industrial equipment, this is vital. The same holds true for datacentre applications — processing can be carried out within a few hops, and potentially avoid having to be processed on another server, circumventing the latency and jitter of the server’s network stack. Keeping processing local using in-network processing can also make applications more resilient to the variable latency caused by dense bursty traffic generated elsewhere in the datacentre. Another benefit of in-network processing is that it can reduce the load on the network, as data is terminated or reduced once processing is complete.
Devices such as network switches and gateways are extended to perform additional data processing as well as their network functions. This technique has been demonstrated to result in a reduction in data and execution latency in map reduce applications [103]. A key-value store implemented on an FPGA based NIC and network switch outperformed a server based implementation [22]. In-network computation using programmable network switches for a consensus protocol was demonstrated in [104].
The result of the computation is typically smaller than the data used in it - for example a neural network may return a class label, or a filtering application removes extraneous data. This can reduce the network load, and the resulting impact on other applications. Additionally, it makes use of elements already utilised within the network, reducing the need for adding extra servers or components, which can reduce space overheads, costs and idle power consumption. In-network computing is an emerging area, but is becoming more readily available due to advancements in programmable switches, smartNICs and the utilisation of FPGAs within networking contexts.
32

2.4.4 Network elements
Data is moved from machine to machine through a network, passing between other machines and dedicated network elements. Most modern systems primarily utilise the Ethernet networks [105]. In these networks, data is transmitted in bitstreams across either twisted pair or fiber-optic cables between switches. Ethernet networks are switch-based, where machines only communicate directly with the closest switch, and not with other machines. Each device in the Ethernet network has a MAC address, which is used to direct data to the appropriate machine.
Data enters and leaves a device through a network interface, controlled through software running on the device. Embedded devices usually have a network controller integrated circuit (IC) on board to allow for the physical connection to the network through cable or WiFi, and handling of the physical layer protocol. Servers usually use a network interface card (NIC), which are peripheral component interconnect express (PCIe) expansion cards that contain the network controller IC and supporting circuitry and buffers. Upon reception of a packet at the network interface, it is written to a ring buffer in the host via direct memory access (DMA) transfers, and an interrupt is triggered. The operating system then moves the packet into an input queue, and then processed in software by the CPU. This processing involves examining and extracting data from the various headers for each protocol layer in sequence. Once processed the packet is accessible by applications through the sockets interface. A server may host several NICs with varying numbers of ports. Modern NICs may also integrate internet protocol (IP) and transmission control protocol (TCP) stacks on the card before it reaches the host machine. Offloading this processing away from the host and onto the NIC can have significant throughput and latency benefits.
Switches are fundamental network elements that facilitate many-to-many communication between machines. They forward frames from input ports towards the appropriate output port to reach the target machine on the local network. A switch will have input and output arbitration and buffering logic either side of the main switch fabric. Most switch fabrics are implemented using a crossbar design, where each input has a potential connection to each output, but only one input or output can be connected to another at one time [106]. The links through the crossbar matrix are bufferless, and buffers are placed either at the input or output. Unmanaged switches offer mostly simple movement of frames between ports, although can sometimes offer additional features such as basic prioritisation of selected traffic and diagnostic abilities. They offer limited reconfigurability, and are designed to just be deployed out of the box. Managed switches on the other hand
33

can be reconfigured during deployment, and offer a more comprehensive feature set. These can include additional security, traffic control and quality-of-service capabilities. Smaller, unmanaged switches are more likely to be found closer to the network edge, while more capable managed switches are more likely to be used to aggregate larger groups of links from smaller switches. Routers perform a similar function, but usually work with the layer 3 protocol (for example IP), and are used to connect multiple networks, to each other.
2.4.5 FPGAs for network applications
FPGAs have seen extensive use in networking applications. The growth of softwaredefined networking, and the need for flexibility in modern networking devices mean that traditional ASIC implementations are no longer viable. On the other hand, software implementations of these functions running on CPU-based hardware cannot meet the strict throughput and latency requirements, especially considering the growth of 40Gb and 100Gb Ethernet in datacentre environments. Specialised network processing units (NPU) are available as a compromise between these options, but still pose a restriction on what functionality can be implemented. FPGAs can provide the both high performance and sufficient flexibility that are needed in modern networking contexts, in a range of applications.
NetFPGA is a platform designed for networking related FPGA research [107]. It is a development board hosting a Xilinx FPGA, memory, PCIe, and 1Gb Ethernet interface. Further revisions of this board host bigger FPGAs, more memory and more Ethernet interfaces [108]. The NetFPGA SUME [109] board is a further revision which has four 10Gb Ethernet interfaces and a Virtex 7 FPGA. The boards come with a library of IP cores to ease development of networking applications.
Packet parsing and classification
A fundamental component of modern networking appliances is packet parsing and classification. This involves identifying and extracting information from fields within packet headers, often nested within headers of other protocols, and classifying packets into flows based on this information. This can help with enforcing security, routing flows to the appropriate network application and meeting quality-of-service constraints. FPGAs are an attractive platform for this function due to the ability to ingest and process packets at high data rates, while retaining the flexibility to be reconfigured to support new protocols or classification policies. Packet parsing at 400Gb/s is demonstrated in [110]. The design implements a pipeline of generic
34

parsing elements that are configured through microcode by the stages previous to it. A high-level language is also provided to be able to express header formats, which then automatically configure the FPGA parsing logic. The parser presented in [111] aims to reduce the area consumption and latency for high throughput parsers. Embedded applications may have less resources available and require a smaller footprint parser, and applications like high frequency trading require extremely low latency. This architecture allows the user to control the number of pipeline stages to suit their application, and allows for packets to share data words, making better use of the wide bus widths that must be used to achieve high throughput. One of the challenges faced by packet processors implemented on FPGAs is that the clock rate is slow relative to the data rates required, so large bus widths are required, consuming large amounts of resources. The design in [112] utilizes an FPGA with a hardened ‘network on a chip’ interconnect, that provides a high-speed interface across the FPGA. This allows for lower bus widths and area consumption. Packet classification typically sorts packets into ‘flows’ based on the contents extracted from the packet headers. Typically this is done through table lookups that match header content to class labels. The memory footprint of the classification ruleset is identified as a key bottleneck in [113], where the authors develop an algorithm that reduces the required memory by breaking down the ruleset into smaller rulesets that can be cross referenced. The properties of the ruleset are exploited to reduce the memory footprint in [114]. An architecture that allows for classification at 40Gb/s data rate is presented in [115]. Memory efficiency is sacrificed in this design in exchange for throughput. This has the added benefit of not relying on the format of the ruleset, making it more applicable to a range of applications.
Network Function Virtualisation
Network function virtualisation involves the replacement of traditionally specialised hardware middleboxes that performed network functions with commodity, generalpurpose platforms. Functions can be dynamically allocated to these platforms in order to respond to changing requirements, without having to replace or alter hardware. FPGAs have been highlighted as attractive platforms for NFV as software implementations of these functions have performance limitations, while ASICs do not have the required flexibility. Intrusion detection systems have been implemented on FPGAs in [116] and [117], demonstrating improvements in throughput compared to software implementations. Firewalls have been shown in [118] and [119], using the packet classification techniques discussed above. Deep packet inspection has seen a recent surge in interest, with FPGA implementations being developed in several
35

works such as [120] and [121]. There has also been work into developing general platforms for FPGA-based NFV, and integration into NFV infrastructure. The architecture in [122] implements a static region consisting of protocol independent switching units, and a partially reconfigurable region that can be loaded with custom network functions. The work in [123] details a framework for the deployment of FPGA networking functions alongside software VNFs within a heterogenous system.
Switching
Recently FPGAs have seen increased interest as platforms for network switching. Devices are being manufactured with more transceivers, memory, and logic resources, and the ability to reconfigure the design to account for new functionality is becoming more favourable. This is decreasing the gap between FPGA and ASIC switching solutions. The feasibility of high-speed network switching using FPGAs is investigated in [124], for up to 50Gb/s per port. A 16×16 156Gb/s aggregate switch is implemented on a Virtex-6 in [125], making use of the hard RAM blocks available on the FPGA. These hardened memory resources can be run a higher clock rate than the general-purpose fabric, and in this design are used to perform a majority of the switching. The authors evaluate the design to have comparable performance to ASIC switches of similar scale. The architecture in [126] utilises a hardened network on a chip to achieve a 16×16 switch with over 900Gb/s aggregate throughput. They use a 64×64 mesh NoC which provides high-speed links across the FPGA, allowing for faster transport than the general-purpose fabric. This technique consumes less area, power and memory resources than the SRAM design. A similar NoC based design is demonstrated in [127]. A 16×16 900Gb/s aggregate throughput is achieved in [128] without the use of a NoC, and is demonstrated to be more resilient to difficult traffic patterns. It does this by removing the crossbar switch structure used in the other FPGA switches discussed, and instead uses a pipelined algorithm to process incoming packets and efficiently organise them at output queues.
Software-defined Networking
While these works focus on the implementation of the switching fabric, other works related to FPGA switches focus on their integration into SDN infrastructure. SDN allows for the control plane of switches to be decoupled from the data plane. The data plane of ASIC devices only allows for configuration between a set of parameters and functions defined at manufacture, while FPGAs allow for a reconfigurable data plane, allowing even more flexibility. An SDN switch compatible with Openflow, an
36

open SDN standard, is demonstrated in [129]. SDN data planes are often configured using high-level, platform and protocol
independent languages. Thus there is a broad set of work focused on allowing these languages to target FPGA platforms. P4 in particular has seen significant interest. P4FPGA [33] extends the standard P4 compiler to generate verilog code that can the be used to generate FPGA logic. To evaluate the compiler, L2/L3 forwarding, Paxos, and a financial trading protocol were implemented. An implementation that reduces the resource overhead on the FPGA is demonstrated in [34]. A full workflow, from the compilation from the P4 program to the bitstream generation, to testing on the hardware is presented in [130]. A P4 to FPGA workflow is used to implement the Paxos consensus protocol in [131]. This is an application typically deployed on standard Servers, but here it is implemented in a programmable data plane within network elements, reducing the number of network transfers and reducing latency significantly.
All of this work demonstrates that FPGAs are well-suited to packet processing, independent of a CPU host, and are present within networking infrastructure. These platforms can be extended to perform additional computation. This provides new opportunities to distribute processing into the network, but increases the complexity of decided where to place tasks.
2.4.6 Cloud Computing
Cloud computing is usually used to describe the flexible, ‘on-demand’ provisioning of computing and storage resources from a datacentre to clients over the internet. This can be a private cloud, where the datacentre is owned by and serves clients only from the same business or enterprise, or a public cloud, where the datacentre resources are not owned by the same party as the client. Public clouds operate under a service-oriented payment model, where customers pay as they use resources. The most common service model is ‘infrastructure-as-a-service’ (IaaS). Physical hardware is virtualised, which means that multiple guest operating systems can run on the same host. A hypervisor is a software layer running on the host that enables this. A guest OS is often run as a virtual machine (VM), and all VMs on a host share the same processor, memory and I/O. The Cloud service provider allows users to elastically create VMs in the remote datacentre.
The cloud model provides numerous benefits for users. It gives users access to a range of compute resources without them having to purchase or manage any of the physical hardware. Deployments can be also be scaled up or down dynamically depending on requirements. For applications where data is generated at resource
37

constrained nodes such as IoT sensors or mobile phones, the cloud provides more capable resources that could be used to offload processing.
There are several primary public cloud providers. Amazon provide a cloud platform called Amazon Web Services (AWS). AWS has a large range of services available for many use cases. The main compute service offered is their elastic compute cloud (EC2), which allows users to generate VMs that can be accessed remotely [132]. Capability can be scaled dynamically, with resources such as CPU and memory being flexible. AWS offers a storage service called S3, which allows for persistent storage in the cloud. Their F1 service provisions VMs with up to 8 attached FPGAs, connected over PCIe [133]. Microsoft have a competing platform called Azure [134], with similar functions to AWS. Google also have a cloud platform called Google Cloud, again providing many of the same basic features [135].
2.4.7 FPGAs in the cloud
In recent years, there has been a surge of interest in deploying FPGAs in the cloud, as a provision-able, shareable resource. The goal is to allow users to remotely access FPGA resources on demand to accelerate compute intensive applications. This growth has been driven by better PCIe-based hosting and the availability of more accessible, software-like development flows.
Commercial cloud FPGA platforms
At present, FPGAs have been integrated into several public commercial cloud platforms. Amazon’s AWS platform allows users to directly access an FPGA and implement their own logic, as an extension of their EC2 elastic compute service [133]. EC2 allows users to rent virtualised compute and memory resources. Users can rent an F1 variant of one of these instances, which attaches up to 8 FPGAs. The FPGAs are attached via PCIe to host servers, and are attached to each other in a ring topology using dedicated interconnect, for high-speed inter-FPGA transfer. Developers can design accelerators using the traditional workflow or higher level languages. Amazon provide an interface IP core that user logic must integrate with, and the resulting design is submitted to the AWS servers as an Amazon FPGA image (AFI). This design goes through several stages of checks and can then be deployed to the FPGA instance. The F1 service only allows for the provisioning of entire FPGAs, and these are required to be attached to a CPU instance. There is no virtualisation of the FPGA resources. FPGAs are treated as slave accelerator devices rather than standalone devices, and have no direct connection to the network.
38

Baidu smart cloud has FPGAs computing instances available in public beta [136]. Each instance has exclusive access to a monolithic FPGA. As part of the service they offer a range of IP cores and software components to simplify development. Similarly to Amazon F1, Baidu provide a static shell and software APIs to handle interfacing between the host and the FPGA, meaning the user only has to develop the computing logic for their application. Huawei cloud has an FPGA cloud server platform [137] which also has up to 8 FPGAs attached to a compute instance over PCIe, with a high-speed network connection between FPGAs.
Microsoft’s competing cloud platform Azure also offers FPGA-acclerated instances. In contrast to AWS, which provisions the user an entire FPGA to implement their own designs, Azure offers no direct access to FPGA resources. Based on the work presented in project brainwave [138], Azure gives users the option to deploy pre-trained DNNs to FPGA accelerators, for a specified set of supported model architectures. Users only provide the DNN model parameters rather than any hardware design, and load it onto Microsoft’s fabric of FPGA resources within their datacentre using a high-level software API. Another FPGA related cloud service provided by Microsoft is Azure Accelerated Networking. This gives users the option of offloading the network stack to an FPGA smart NIC on the host, allowing the VM direct access to the network interface and bypassing the hypervisor virtual switch implemented in software on the host. This results in greatly reduced latency and jitter [139].
Google cloud offers tensor processing unit (TPU) instances to accelerate DNN inference, but no FPGA platforms. IBM have a cloud platform called Supervessel [140] designed for development and educational use, which utilises FPGA accelerators connected through PCIe to the hosts. Biadu utilise FPGA accelerators to speed up web search [78].
Academic works
The current commercial cloud offerings are ultimately limited, and FPGAs are offered either as a monolithic resource, are not directly available to users, or are restricted to specific sets of applications. Academic research has been focused on overcoming these limitations, and making FPGAs an accessible, virtualised resource in the cloud. Research addresses several key challenges in this area, mainly efficient sharing of resources, and integration of FPGAs into existing cloud management infrastructure.
39

Resource sharing
Allowing FPGAs to be shared across multiple applications and users in isolation is one of the main objectives of research surrounding FPGAs in the cloud. Offering FPGAs as monolithic resources in the cloud is ultimately not resource efficient, and if a user design takes up less than the total resources available, the extra space is essentially wasted.
Most commonly implemented is an architecture comprising a static ‘shell’ made up of control and interface logic, with application logic being implemented in reconfigurable regions at runtime. An example can be seen in [141], which partitions the reconfigurable region into virtual FPGAs (vFPGAs) that can host accelerators. The shell implements PCIe connectivity, and manages the arbitration of access to communication and memory resources between the vFPGAs. The hypervisor running on the host manages scheduling and controls the loading of accelerators into the vFPGAs. This design was primarily developed with streaming applications in mind that make limited use of on board memory. Another shell for virtualised PCIe-based accelerators is presented as part of an end to end FPGA cloud platform in [142]. Again, the shared PCIe logic and reconfiguration controller is implemented within the shell, with partially reconfigurable regions used for application logic. This architecture also implements scheduling logic within the shell, which implements policies described in software from the host. Security and error detection logic is also implemented in the shell. A similar architecture can be seen in [143]. This design adds a local soft-core processor to allow low latency management of FPGA resources, and faster access to memory shared with the accelerators. Additionally, this architecture includes on board memory virtualisation, and allows for logic and memory resources to be dynamically allocated to applications depending on the characteristics of the workload. The paper evaluated the design with multiple applications commonly found in the datacentre and demonstrated that sharing the FPGA across multiple applications provided performance benefits compared to monolithically hosting each application in series, due to reduced reconfiguration time.
These approaches treat the FPGA as a coprocessor to a CPU. This presents several issues with cloud deployments. For one, the number of FPGAs available is dependent on the number of hosts and the amount of PCIe interconnect presented. Furthermore, any host failures render all attached FPGAs unusable. Finally, data must travel through the host to reach the accelerator, leading to latency penalties. This has lead to several works arguing in favour of deploying FPGAs as independent resources, connected directly to the the datacentre network. The shells for these accelerators implements a high-speed network interface instead of PCIe. The
40

network attached accelerator in [144] has resource management logic in the shell that receives commands over the network from centralised management software. This work is extended in [145] in the context of multi-FPGA fabrics. In this case, a master server can co-ordinate the distribution of tasks to multiple network attached accelerators. Feniks [146] again allows for multiple slots of user logic, but allows for management over the network or via a PCIe host connection.
This thesis attempts to quantify the overheads of these approaches.
Integration with cloud management
Along with allowing FPGAs to be shared by multiple users at the FPGA level, the other key challenge is integrating them within existing cloud management infrastructure. This means allowing VM instances to interact with the accelerators, or allowing FPGAs to be managed as traditional software resources are. One example of this is demonstrated in [147], where an FPGA accelerator is integrated with the popular Xen hypervisor. The accelerator is connected to a host with PCIe, and the extra virtualisation layer adds minimal access overhead. The platform presented in [142] is integrated with the Openstack cloud management framework. Openstack is in fact a common target for FPGA cloud integration. Several works have presented standalone, network attached accelerators that can be managed through Openstack. One of the first was [148], where standard Openstack commands can be used to set up or tear down accelerators. Similar approaches are detailed in [144] and [145]. The frameworks present FPGAs as generic cloud resources, and allow for the control of multiple independent devices. Galapagos is a full stack framework for integrating FPGAs in the cloud [149], which automates and simplifies the deployment of multi-FPGA clusters. Under this framework, FPGAs are directly connected ot the datacentre network as standalone devices. The framework manages the physical setup of device and provisioning from a pool of shared resources.
Summary
FPGAs are utilised within the network infrastructure for various networking functions. These platforms could be extended to perform additional computation, in addition to these base tasks. This increases the number of locations within a network that tasks can be allocated to, making manual placement significantly more complex.
FPGAs are also present within the cloud ecosystem. The technologies developed to facilitate this allow for the sharing of FPGA resources and integration
41

within a networked environment. There has been no work examining the overheads of the virtualisation methods used to achieve this. These overheads are examined in part in chapter 4 of this thesis.
2.5 Mathematical Modelling
In the previous sections it is noted that the increased availability and heterogeneity of hardware accelerators, and the growing number of ways they can be distributed throughout a network make manual evaluation and determination of performant deployments of tasks and hardware impractical. Mathematical representation of this scenario is therefore important.
Fundamentally, this is a graph embedding problem - a set of connected tasks must be allocated to a graph of networked nodes. This sections details existing modelling solutions, and why a new method is required.
2.5.1 Sensor network modelling
Models that deal with in-network processing go back to early research on data aggregation in wireless sensor networks. These networks were often meshes of sensor nodes with very limited computational resources, for example simple microcontroller based systems such as Mica motes [150]. Computations were simple, mostly consisting of basic aggregation functions such as averages and maximums. The models’ goals were to allocate these functions to sensor nodes to minimise energy consumption, thus maximising the lifetime of the sensor nodes with limited power supplies and in difficult to access locations. Due to the low power-consumption hardware and simple compute functions, along with energy hungry wireless communication between nodes, computation was generally assumed to be cheaper in terms of energy than communication. This meant that these models for the most part push computation towards the leaves to a substantial degree in order to reduce communication costs as much as possible. These models include TAG [151], directed diffusion [152], EADAT [153], and MERIG [154]. These either assume uniform sensor nodes or do not account for the computation time at each node, rather focusing on reducing network load and minimising energy consumption.
2.5.2 Distributed Stream Processing Models
The allocation of streaming tasks to networked processing nodes has been explored in a variety of existing work. Applications are represented as a graph of tasks
42

with edges representing dependencies, while networks are represented as a graph of compute nodes with edges representing links.
Earlier models such as Aurora/Medusa [155] focused on load balancing in task placement, primarily for the allocation of tasks to multiple servers in a datacentre environment. However, network costs are not modelled, making them unsuitable for scenarios that consider larger scale networks where communication and network costs are more significant. Work on more network-aware placement [156; 157; 158; 159; 160] was tailored towards networks of machines that are more widely distributed, and include network utilization and latency in their formulation. These models are all focused on placing operators to optimise specific objectives, for example bandwidth utilisation, meaning that they aren’t generalisable when wanting to model a range of different performance metrics. Since these online optimisations are run dynamically, the models are significantly simplified to minimise their impact on the application. These models consider homogeneous processor platforms and do not support alternative hardware platforms with different computational models and metrics.
Recently, more generalised placement models have emerged [161; 162; 163]. These focus on creating a general representation of the operator placement problem, developing formulations based on integer linear programming instead of focused heuristics. They are still limited as they assume a fully connected cluster of machines, and their models of computing resources and tasks are coarse grained. The work in this thesis is concerned with a scenario where hardware acceleration may be utilised at certain computing nodes, using a different computational model to that of a processor, which is considered solely in these models.
2.5.3 Summary
Most of the existing work regarding task placement within a network of connected compute resources are focused on dynamic runtime allocation. The time taken to generate a solution is therefore a major constraint. A static allocation model does not have runtime execution constraints, so can be designed with more complexity, including more variables and taking into account more detail. Existing works also assume fixed hardware at each node.
As demonstrated in this chapter, hardware acceleration is more commonplace, and can be deployed at many locations within the network. A different modelling approach is needed that not only allows for the allocation of tasks, but the allocation of additional accelerator hardware at existing network nodes to facilitate this. This model could be used statically, pre-deployment as part of a design phase,
43

and allow for more detailed modelling of both tasks and hardware. 44

Chapter 3
Modelling distributed computing with heterogeneous hardware
3.1 Introduction
Distributed data processing applications involve the processing and combination of data from distributed sources to extract value, and are increasing in importance. Emerging applications such as connected autonomous vehicles rely on complex machine learning models being applied to data captured at the edge, while also involving collaboration with other vehicles. Further example applications include factory automation [164], smart grid monitoring [165], and video surveillance and tracking [166]. Such applications present a challenge to existing computational approaches that consider only the cloud and the very edge of the network. Computationally-intensive algorithms must now be applied to streams of data, and latency must be minimised. In these applications, data sources transmit streams of data through a network to be processed remotely, with a focus on continuous processing, and potentially involvement in a feedback loop, as opposed to other applications that involve large-scale storage and delayed processing. Latency, the time taken to extract relevant information from the data streams, and throughput, the rate at which these streams can be processed, are key performance metrics for such applications.
Centralised cloud computing is often utilised in these scenarios, since the data sources do not typically have adequate computing resources to perform complex computations. Applications also rely on the fusion of data from multiple sources,
45

Datacenter

Cloud (Servers, Servers+accelerators)

In-Network (Cloudlets, Smart switches, FPGA accelerators)

Source Source Source Source

Edge (Microcontrollers, Accelerators)

Figure 3.1: An example of the type of networked system that the proposed model targets. Shaded nodes can perform computation.

so centralised processing is useful. The cloud also offers benefits in scalability and cost, and has been shown to provide benefits in applications such as smart grid processing [165; 167] and urban traffic management [168].
However, many emerging streaming applications have strict latency constraints, and moving data to the cloud incurs substantial delay. Furthermore, while the data generated by sources can be small, a high number of sources means that, in aggregate, the volume of data to be transmitted is high. For example, in 2011, the Los Angeles smart grid required 2TB of streamed data from 1.4 million consumers to be processed per day [165]. Some applications, such as those dealing with video data, must also contend with high-bandwidth data requirements.
These limitations have led to an increased interest in ‘edge’ or ‘fog’ computing, a loosely-defined paradigm where processing is done either at or close to the data sources. This could mean at the source, such as on a sensor node with additional processing resources [169]. It can also encompass performing processing within the network infrastructure, such as in smart gateways [170], or in network switches or routers. Cisco offer a framework that allows application code to be run on spare computing resources in some network elements, and Ethernet switches from Juniper allow application compute to be closely coupled with the switching fabric.
Edge computing can also include the concept of ‘cloudlets’, which are dedicated computing server resources placed a few hops away from the data sources. These can vary in scale, from a single box placed on a factory floor to a small scale datacenter comprising multiple networked machines. While the data sources
46

themselves may not have the required computing capabilities, these resources can support complex applications and are accessible at shorter latencies than a remote cloud [171].
In complex applications, it is likely that some processing, such as filtering and pre-processing can be performed at the edge, greatly reducing the volume of transmitted data, and additional processing and fusion of data can be carried out in the cloud. The benefits of this approach are that latency sensitive parts of the application can be done locally, while more computationally-intensive operations that may require more processing power or additional data can be done centrally. Stream processing applications are well suited to being partitioned and distributed across multiple machines, as is common in stream processing frameworks such as Apache Storm and IBM Infosphere Streams. Additionally, cloud service providers such as Microsoft Azure have edge analytics platforms that allow processing to be split between the cloud and the edge.
Edge and in-network computing is an emerging area. Cloudlets have been utilised for image processing applications [11; 12] and augmented reality [13]. Platforms such as Google’s Edge Tensor Processing Unit demonstrate that there is a trend towards moving complex computation closer to the data source. In-network computing has seen application for network functions, machine learning [103], and high data rate processing [22].
In order to explore the implications of distributing application computation across a network of heterogeneous compute platforms, a suitable model is needed. This would allow for the evaluation of different deployment strategies using metrics such as throughput and end-to-end latency. Existing models that deal with placement of processing on distributed nodes do not consider hardware resources, varied connectivity, and application features together.
To this end, this chapter will propose a generalised formulation that can represent applications and target networks with heterogeneous computing resources. It supports reasoning about in-network and near-edge processing scenarios that are emerging including both general processor-based machines and hardware accelerator systems.
Figure 3.1 summarises the application scenario of interest, giving an example of the type of networked system that the proposed model targets. Edge nodes such as sensors and microcontrollers transmit data through a network towards centralised computing resources. In a traditional cloud computing setup, only the central resources perform computation (shaded). In edge computing, the edge nodes are capable of performing some computation (shaded). In-network computing al-
47

lows some tasks to be performed in the network as data traverses it, using smart switches (shaded).
3.2 Contributions
The key contributions of this chapter are:
• A model for evaluating different in-network computing approaches is developed, encompassing:
– Multiple levels of network structure, unlike existing models that focus on clusters of machines.
– Hardware heterogeneity including accelerator platforms, and the resulting differences in computing and networking.
– Realistic representation of performance metrics, alongside energy and financial cost.
• The model is used to examine a case-study scenario and draw general lessons about in-network computing on different platforms using a set of synthetic applications.
3.3 Related Work
The allocation of streaming tasks to networked processing nodes has been explored in a variety of existing work. Applications are represented as a graph of tasks with edges representing dependencies, while networks are represented as a graph of compute nodes with edges representing links. Those works are discussed in Chapter 2.5.
In contrast to these works, the model proposed in this thesis is not focused on finding an optimal allocation of tasks to a given set of resources at runtime. Instead, it is to be used to investigate the implications of placing computing resources at different locations in a network and to understand the benefits and costs of doing so. Since the model is not concerned with dynamic optimisation of operator placement within a time constraint, the model can include more fine grained detail for tasks and hardware, accounting for hardware acceleration, heterogeneous resources required by tasks, the financial cost of adding additional compute capability to network nodes, and energy consumption. The model also considers the networked system as a whole, from the sensor nodes to the datacenter, instead of focusing on a cluster
48

of computational servers. The focus of this work is not limited to optimisation, but rather an analysis of different distributed computing paradigms in the context of streaming applications. The model can still be used for an optimisation however.
3.3.1 Edge/Fog Computing
In response to increasing demand for low latency in distributed streaming applications, efforts have been made to move computation closer to the data source, or the ‘edge’ of the network. Where processing occurs varies, and it is rare that the application is entirely pushed to the edge. Typically operations such pre-processing and filtering take place at the edge, with aggregation and decision making centralised. This approach has been applied to domains such as smart grid, radio access networks, and urban traffic processing [172; 173; 174]. The model developed in this chapter is capable of representing this scenario.
In-network computing is another emerging paradigm in which traditionally centralised computation is distributed throughout the networking infrastructure. As the capability of this hardware improves, this method in which networking elements are used for both moving data as well computing, is becoming more viable. Extending such capabilities to broader applications requires the ability to analyse applications composed of multiple dependent tasks and determining how to allocate these to capable nodes. The proposed model allows this to be explored in a manner not possible using existing distributed computing models.
3.3.2 Hardware acceleration
A primary motivation for this work is the increasing complexity of applications, growing volumes of data, and more widespread availability of alternative hardware such as GPUs and FPGAs that can boost the performance of these applications. As discussed in Chapter 2, recent work has demonstrated the use of hardware acceleration for a variety of algorithms relevant to networked systems. To reflect the trend towards heterogeneity, the model proposed in this thesis encompasses the idea of distinct hardware platforms with different computational characteristics. This further differentiates this work from others that consider only traditional processor based compute architectures.
49

3.4 Scenario and Metrics
The scenario of interest comprises a set of distributed data sources producing continuous streams of data, connected through a network comprised of intermediate nodes (for example gateways, routers, or cluster heads) to a central data sink, such as a datacenter. These data sources could be cameras, streams of documents, environmental/industrial sensors, or similar. An application consisting of a set of tasks and their dependencies processes these streams to make a decision or extract value. These tasks operate on the different streams of data, and some combine information from multiple (possibly processed) streams. Individual tasks affect the data volume through a reduction factor that determines the ratio of input data to output data, which reflects the properties of many stream processing tasks. An example of such an application is a smart surveillance system that monitors video streams from many cameras to detect specific events. Video streams can come from a mix of fixed cameras and mobile platforms, with different resolutions, frame-rates, and interfaces, requiring different amounts of processing. The application uses processed information to adapt how the cameras are deployed and positioned.
In order to evaluate alternative allocations of resources and tasks, following key metrics of interest are considered, with some explanation of how they are impacted below. The comprehensive formulation of these metrics is provided in Section 3.6.
3.4.1 Latency
Latency is important when data is time-sensitive. Fast detection of an event may have safety or security implications, or in some applications, there could be realtime constraints. In this case-study, transmitting all video streams to the cloud introduces large communication delays and competition for resources in the cloud can add further latency. Performing computation closer to the cameras, whether at the cameras or in network switches can reduce these communication delays, and distributing the tasks to different network nodes reduces the delays from sharing centralised resources. Even with less powerful hardware, latency can improve as a result of this stream processing parallelisation.
3.4.2 Bandwidth
Processing sensor data often reduces the size of data, outputting filtered or aggregated data, or simple class labels. Hence, if this processing is performed nearer to the data source, bandwidth consumption further up the network can be reduced sig-
50

nificantly. There may also be scenarios where early processing can determine that a particular stream of data is useless, and hence further transmission can be avoided. In this example, some cameras may use low resolutions or frame rates, and hence be less costly in terms of bandwidth, while others might require significantly higher bandwidth, which would be more efficiently processed nearer to the cameras. It is clear once again that this decision depends on the specific application and tasks.
3.4.3 Energy
Energy remains a key concern as cloud computing continues to grow; the power consumption of datacenter servers and the network infrastructure required to support them is significant. One approach vendors have taken to try and address this is to introduce heterogeneous computing resources, such as FPGAs, to help accelerate more complex applications while consuming less energy. However, these resources add some energy cost to the datacenter, in the hope that this will be offset by significantly increased computational capacity. There is similarly an energy cost for adding accelerators in the network infrastructure but this is likely less than the cost of full server nodes, and leads to a reduced load on the datacenters as they then only deal with processed data. However, it is clear that energy consumption is heavily dependent on where such resources are placed. It is also possible that energy constraints at source nodes can impact what can be done there. In this example, battery-powered drones carrying cameras may have constrained power, so performing more computing there may not be viable.
3.4.4 Financial Cost
Adding computing capabilities to all data sources is expensive, especially where the tasks to be performed are computationally expensive, possibly requiring dedicated hardware. In this example, the cameras would have to be smart cameras with dedicated processing resources attached, and this is likely to increase cost significantly. While centralising all computation is likely to be the cheapest solution in terms of hardware, placing some computation in the network can come close to that cost, while offering significant benefits in the other metrics.
3.5 Proposed Model
The proposed model defines a network topology, task/operator graph, and hardware platforms. Tasks and hardware platforms can be allocated to network nodes, and
51

. . .
Figure 3.2: Nodes in the network graph can represent a single device or a cluster of networked devices.
values for the previously mentioned performance metrics can be calculated. The network communication topology is assumed to be pre-determined, though not the hardware at the nodes or the task allocation. The model is flexible enough to be used in a range of situations.
The logical topology of the network is represented as a graph, GN = (N, EN ), where N is the set of network nodes, with bidirectional communication across the set of edges between them, EN . Application data travels through these nodes and edges towards a central sink. A node can represent either a single machine in the network, such as a gateway, switch or, server, or a ‘tier’ or ‘level’ of the network infrastructure. In this case a node represents multiple machines but the connectivity between them is not modelled at the higher level in the graph (see Figure 3.2). Using this representation allows the network topology to be represented in a tree structure as suits the application models considered.
To represent the application, a directed acyclic graph (DAG) is used to define the relationships between tasks, GT = (T, ET ), where T is the set of tasks and ET defines the dependencies between them. GT is a tree structure with a global task at the root, with other nodes representing sub-tasks, such as aggregations and preprocessing. This task model is based on the stream processing model, where data is processed per sample as it arrives. DAGs are commonly used to model stream processing flows in other works, which are acyclic in nature as data is processed as it arrives with minimal storage.
Each task t ∈ T can be assigned to a node through an implementation. Implementations are pieces of software or hardware logic that can perform the required task. This allows for selection between software implementations and hardware ar-
52

chitectures that may have different benefits and drawbacks. This is in contrast to previous work which typically do not allow for the possibility of alternative implementations of a task. Implementations and tasks are treated as black boxes that take inputs and produce outputs, and have already been benchmarked to determine an estimate of processing time and energy consumption on a reference platform with no other tasks running. A set of platforms that can be assigned to nodes, P , to execute the tasks can have varying computational models and available resources.

3.5.1 Tasks

T = {t1, t2, t3...tT } is the set of application tasks to be allocated to nodes in the network. Individual tasks represent functions to be carried out on a data stream. Together tasks represent the operations performed on each data stream, and specify how they are combined and manipulated to extract value. In this model, data is consumed by a task and transformed, with the result passed to the parent task. Task dependency is captured in the DAG, with each task unable to begin until all of its child tasks have been completed on a given instance of data—tasks with multiple children are typically aggregation operations. Each task t ∈ T is defined by t = (ft, Mt, Ct, at).
• The set Ct ⊂ T contains the prerequisite tasks for t that must be completed before task t can begin—its child tasks;

• at ∈ T is the parent task of t, which cannot begin until t has finished.

• ft is the reduction factor, where 0 < ft ≤ 1. This parameter represents the amount that a task will reduce the volume of data it operates on;

• Mt ⊂ M is the set of implementations that can implement the functionality of t;

• The data into (operated on by) a task t, denoted δt, is the sum of the data

out from all sub tasks, δt = (

|Ct| i=0

di

);

• The data output from a task, dt, to be processed by the task’s parent task, is given by dt = ftδt.

This representation of tasks supports different types of operations, for example, a filtering tasks that reduces a data stream, or aggregation tasks that merge multiple streams. Traditionally, aggregation tasks that process several data streams would have to be centralised but in this model they can be placed at intermediate nodes that has access to the requisite streams.

53

3.5.2 Implementations
M = {m1, m2, m3...mM } is the set of all implementations, which are the pieces of software or hardware that implement the functionality of a task. Implementations can represent different software algorithms or hardware accelerator architectures that give the same functionality but have different computational delays or hardware requirements. Each task t ∈ T has a set of implementations Mt, and each m ∈ M is defined by m = (tm, τm, Rm, hm)
• tm ∈ T is the task that is implemented by m;
• the set Rm = {rm1, rm2, rm3...rRm} contains the amount of each resource needed to be able to host the implementation, such as memory, FPGA accelerator slots, etc; These are integer values representing the quantity of each resource available.
• τm is the time taken for this implementation to complete the task it implements per unit of data, compared to a reference processor;
• hm = {0, 1} signals whether the implementation is software or hardware. A value of 0 is software, 1 is hardware.
3.5.3 Platforms
Platforms represent the systems in a network node that can carry out tasks. It can be defined that P = {p1, p2, p3...pP } are the set of platforms that could be assigned to node n ∈ N . Each platform p ∈ P is defined by p = (ep, cp, wp, Rp, hp), where:
• ep is the execution speed of the platform relative to a reference processor—this represents different processors having different computing capabilities;
• cp is the monetary cost of the platform;
• wp is the power consumption of the platform;
• Rp = {rp1, rp2, rp3...rpR} is the set of resources available on the platform, such as memory, FPGA accelerator slots, etc. Resources are required by implementations;
• hp = {0, 1} indicates whether the platform runs software or hardware versions of tasks. A value of 0 means the platform is a processor that executes software, and a value of 1 means the platform is a hardware accelerator that executes application-specific logic. This is used to ensure correct allocation of software and hardware implementations.
54

Unlike existing work, this model makes the distinction between platforms that execute software code and hardware acceleration platforms such as FPGAs as they have different computational delay models, discussed in Section 3.6.1. Hardware acceleration platforms incur no latency penalty when multiple tasks are present on the same node, whereas software platforms do, as a result of contention for computing resources.
3.5.4 Network
N = {n1, n2, n3...nN } is a set of the network nodes, for example sensors, gateways, and routers, or servers. Each n ∈ N is defined by n = (an, Cn, Pn, bn), where:
• an ∈ N is the parent node of n linking it to towards the central data sink;
• Cn ∈ N is a set of child nodes of n linking it to towards the source(s);
• Pn ⊂ P is the set of platforms that can be assigned to node n. For example, a large datacenter class processor that must be housed in a server rack cannot be placed on a drone;
• bn is the outgoing interface between the node n and its parent node, and represents the data-rate in terms of data per unit time.
3.5.5 Sources and Data
S = {s1, s2, s3...sS} is the set of data sources. Data is modelled as continuous streams, as this work is interested in applications that process and merge continuous streams of data. A data source could represent a sensor, database, video, or other source that injects a stream of data into the network. Each s ∈ S is defined by s = (ns, ts, ds, es).
• ns ∈ N is the parent node of the source, the node where the data stream enters the network;
• ts ∈ T is the task to be performed on data being produced by the source;
• ds is the amount of data in one instance from this source per period es;
• es is the period between subsequent units of data of size ds entering the network.
The model assumes a constant periodic stream of data from the source, such as a periodic sensor reading, frame of a video, or set of captured tweets for example.
55

There are some systems that do not fit this model – for example where sensors may only send out data if there is some change detected. This case can still be represented in the proposed model, as the sensor is still continually capturing data as a source and the detection component can be modelled as a filtering task that reduces it.

3.5.6 Allocation Variables
Boolean variables represent the allocations of tasks and hardware to network nodes. xnm = {0, 1} represents the allocation of an implementation m ∈ M to node n ∈ N . Similarly, ynp = {0, 1} represents the allocation of platform p ∈ P to node n ∈ N . znmp = {0, 1} represents the allocation of platform p ∈ P , and task m ∈ M to a node n ∈ N , using a set of constraints.
A summary of the symbols used in the model is presented in Table 3.1.

3.5.7 Constraints
Constraints are used to ensure correct allocation of tasks, platforms, and nodes.

Allocate tasks only once

|N | |Mt|

∀t ∈ T,

(xij) == 1

i=0 j=0

(3.1)

One platform per node

|P |
∀n ∈ N, (yni) == 1
i=0

(3.2)

Resource availability

Allocations cannot exceed the available resources for the platform assigned to a

node:

|T | |Mi|

|P |

∀n ∈ N, ∀e ∈ R,

(xnj rje) ≤ (ynkrke)

i=0 j=0

k=0

(3.3)

Additional constraints
The model allows for additional constraints to be added in order to better model a specific system or set of requirements. Constraints can be added to give certain

56

Symbol Meaning

xnm ynp znmp unm1m2p τmax g

allocation of implementation m to node n allocation of platform p to node n allocation of m and p to n allocation of m1, m2, and p to n maximum path delay throughput

Kt ⊂ T Kn ⊂ N Ds ⊂ N vnp Ph ⊂ P Ps ⊂ P H Ht ⊂ H OHt I ⊂M φmpt q L⊂T SKn

set of tasks lower than t in task sub-tree with t at the root set of nodes lower than n in network sub-tree with n at the root set of nodes on path from s to root node 1 if p ∈ Pn, 0 otherwise set of all platforms that run hardware implementations set of all platforms that run software implementations set of all paths from leaves to root in task graph set of tasks on path from leaf task t to root Set of all other tasks not on path Ht Set of all software implementations Time to complete task implementation on node Data-rate of streams / tasks Set of tasks with no child tasks Set of all sources that lie beneath node n

Table 3.1: Summary of symbols used in formulation.

tasks deadlines, constrain bandwidths, restrict specific nodes to certain platforms, and more.

3.6 Performance Metrics
As previously mentioned, there are five main metrics of interest in this analysis. Latency, throughput, data rates and energy consumption, and financial cost. This section presents the formulation of these metrics, and discuss how the formulation allows each to be evaluated.

57

3.6.1 End-to-End Latency
The end-to-end latency is the total time between an instance of data entering the network and its root task being completed. For example, this could be the time between a sensor reading or image being taken and a fault or anomaly being detected. This value is of interest in time-sensitive applications such as those concerned with safety or closed-loop control, such as for industrial equipment, or coordinated control. The model incorporates several assumptions and behaviours that are relevant for this metric:
• Sources s ∈ S produce continuous streams of data of an amount ds, every period of time es. For the purpose of the formulation, the model considers a ‘snapshot’ of the network at any instance of time, and say that data is entering the system at this instant from all sources, of an amount ds. The equation formed gives the latency of the data instances entered at the beginning of this ‘snapshot’;
• Only one software implementation can run at a time on a node. Software runs as soon as possible;
• Hardware implementations of tasks operate independently from one another so can operate in parallel;
• A task cannot begin until all of its child tasks have been completed;
• Tasks start as soon as all of the data required is available, as soon as possible, and once completed send the result to the next task as soon as possible;
• Communication and computation happen independently and can be parallel to each other;
• There is no communication time between tasks on the same node.
As tasks can only begin once their child tasks are complete, the root task of the graph G(T, ET ) can only start once all paths to it are complete. The end-to-end latency is therefore equal to the longest path delay of the task graph, including network and computation delay.

Computation Delay The time to complete one task on the node it is allocated to can be represented:

φmpt = τmepδt 58

(3.4)

For a task t, implemented with m on platform p. The time to complete the task is the product of the time taken to complete the task per unit of data for that implementation, the amount of data, and the processing speed of the hardware platform. To find the end-to-end latency, the values of φmpt for each path in the task tree are summed, and the maximum value determined. This is because the total time taken to complete the tree is dependant on the slowest path.
In the case of software implementations, nodes are assumed to carry out one task at a time. So in the cases of multiple tasks being assigned to the same node, in the worst case scenario, a data instance must wait for all other tasks not in the path to finish before beginning the next task. Note that this applies even if a node supports concurrent software tasks, since it is assumed that multiple software tasks suffer degraded performance in proportion to the parallelism applied. Unlike some other works, which are only concerned with preventing the allocated tasks exceeding a measure of available resources on a platform, running multiple software tasks at once on the same node in the model presented in this thesis affects computational delay. For hardware implementations, no such assumption is made, as they can operate in parallel as separate streams since they are spatially partitioned, and so it is sufficient to only sum the path of interest, though available hardware resources are factored in, as discussed later. This distinction between software and hardware implementations of tasks better represents the growing trend of using alternative computing platforms to accelerate computation, compared with previous work that only accounts for software running on processors. Figure 3.3 shows this difference in scheduling for software and hardware nodes. On software nodes, tasks are performed in series in the worst case, and on hardware nodes, tasks can be performed concurrently. In this example, this means that tasks C and D can be performed in parallel to tasks A and B. Task E is dependent on tasks B, C and D, so must happen once they are completed. The added concurrency of hardware accelerator nodes helps reduce task execution latency when multiple tasks are assigned to a node.
In order to represent this behaviour, a set of new allocation variables is introduced: u. Each one of these unm1m2p = {0, 1} represents the allocation of two implementations m1 and m2 to node n, assigned platform p.
The set of tasks on the path from a leaf node on the task graph t to the root of the task graph is Ht ⊂ T . Let the set H contain all of the task path sets (Ht ∈ H). The set OHt is declared, containing all other tasks not on the path Ht. The set I ⊂ M is defined as the set of all software implementations. The computation time for a path Ht, τHtc in the task tree is given by:
59

Software node

E

AB C

D

E

B

C

D

Hardware node

AB

E

C

A

D

time

Figure 3.3: The difference in how a set of tasks allocated to a single node are scheduled on software and hardware accelerator nodes.

|N | |Ht| |Mj | |P |

|Oi| |Im|

τHtc =

(ziklφklj + ( (uikqlφqlm)

i=0 j=0 k=0 l=0

m=0 q=0

|Iaj |
− (uikrlφqlm))
r=0

(3.5)

The znmp term in this equation is the sum of delays on all paths of the task tree. The unm1m2p terms represent the extra delays on the path caused by having multiple tasks not on the same path allocated to the same node in software. The

computation times of any other tasks allocated to the same node as any task in the

path are added. The subtraction is present to ensure that this computation time is

only added once for each set of tasks in a path allocated to the same node.

Communication Delay

A simple communication model is used where tasks send data to the parent task as soon as it is ready. There is no communication cost between tasks, only between nodes. Communication and computation can occur simultaneously and independently. If a node receives data for a task not assigned to it, it forwards this data immediately to the next node.
Data is transferred from one node to another when a task’s parent task is allocated to another node. Similarly to the computational delay, total communication delay τHtm between tasks in each path in the task tree Ht ∈ H can be expressed:

τHtm

=

|Ht| i=0

|N

|

|Kj
(

| |Mi|
((

xkl

di

)

j=0 k=0 l=0 bk

−

|Mai

|
(

xkm

di

))

m=0 bk

(3.6)

60

If an implementation is allocated to a node, the communication time to the next task is added. If the parent task is allocated to the same node, this time is then subtracted.
The delay from the data source s on path Ht to the node that performs the first task on it, τHts, is given by:

τHts

=

|Ds

|
(

ds

)

i=0 bi

−

|Ki| j=0

|Mtl

|
(

xitl

dsl

k=0 bi

)

(3.7)

Where tl is the leaf in the task path. The total communication delay in a path τHtk is thus:

τHtk = τHts + τHtm

(3.8)

The proposed model can be extended to incorporate different communication delays for software and hardware tasks as would be the case for network-attached hardware accelerators that can process packets with lower latency. The computation and communication latencies are likely to vary in reality. This model considers the worst case latency where a node processes all other tasks first and transmits the results last.

Total Delay The total latency for a path, τHt, is equal to:

τHt = τHtk + τHtc

(3.9)

The largest of these values is the total latency, τmax. Although this section has discussed a scenario where only a single task graph
is present, the model allows the possibility of multiple independent task graphs representing separate applications. Using the same method and equations, a τmax can be formulated for other task graphs.

3.6.2 Throughput
The throughput of the system is the rate at which results are output, and dependent on the node with the longest processing time in the network. A continuous variable g can be introduced to represent the maximum delay processing stage. For software implementations, where only one task can run on a node at any time, this can be

61

expressed:

|T | |Ps| |Mi|

∀n ∈ N, g ≥

(znkj φkji)

i=0 j=0 k=0

(3.10)

Where Ps is the set of all platforms that run software implementations. This equation assigns the value of the largest total computation times for all tasks on a node to the value of g. For platforms that run hardware implementations, Ph:

|Ph| |Mt|

∀t ∈ T, g ≥

(znjiφkji)

i=0 j=0

(3.11)

Here, only the value of the longest computation time task is assigned, as processing is carried out in parallel.
The throughput, v, can then be expressed:

v = 1/max(g)

(3.12)

3.6.3 Data-rate

Data-rate can be very significant in scenarios involving information sources with dense data and for large networks and applications. Poor utilization can also lead to additional communication delays.
The data-rate of a data stream at a source s, qs is given by:

qs

=

ds es

(3.13)

The data-rate of a task t, denoted qt, is given by:

|Ct|
qt = ft (qi)
i=0

(3.14)

Where the output data-rate is the sum of the output data-rates of all child tasks, reduced by the reduction factor of the task ft.
For leaf tasks tl where |Ct| = 0, it is given by:

qtl = ftqs

(3.15)

The total data-rate at the output of a network node is the sum of the data-

62

rates of all streams passing through it.

|Kn| |T | |Mj |

|Cj | |Ml|

qnc = ( ( (xikqj) −

(xim qj ))

i=0 j=0 k=0

l=0 m=0

(3.16)

If a task is allocated to any node beneath that node in the network tree, the datarate consumption is added, and if the parent task is allocated to any of these nodes, it is then subtracted.
The data not yet processed by any tasks must also be taken into account. If SKn ⊂ Kn is the set of all sources that lie beneath n in the network graph, and L ⊂ T is the set of all tasks where |Ct| = 0:

|Kn| |L| |SKn |

|Mj |

qnl = ( ( qk − ( (xilqsj ))

i=0 j=0 k=0

l=0

(3.17)

Where qst is the data-rate of the source that leaf task t operates on. The total data-rate at a node n ∈ N is given by:

qn = qnc + qnl

(3.18)

This is the sum of the data-rate of all streams passing through a node, and the data-rate of the stream from the data source to that node, if applicable. This gives the data-rate at each link between nodes.

3.6.4 Energy Consumption
The energy consumption of the network can be relevant for a variety of applications. In an application that deploys remote nodes with limited power sources for example, such as a wireless sensor network, energy usage can be a significant constraint. Most related works do not consider computational energy costs. The energy used at a node n ∈ N depends on the power consumption wp of the platform p ∈ P at that node, and the times taken τm to complete the implementations mt ∈ M of tasks t ∈ T allocated to the node. Just as when formulating an equation for the end-toend latency, taking a ‘snapshot’ of the network, the energy consumed by the network per data instance is given by:

|N | |T | |Mj | |P |
(ziklφklj wl)
i=0 j=0 k=0 l=0

(3.19)

63

This equation adds the product of the computation time and the power consumption of the platform, if a combination of platform of task is allocated to a node.

3.6.5 Financial Cost

The simplest metric is the financial cost of the solution. An equation can be formed that represents the total cost of the system, based on the platforms selected at all of the nodes. The total cost of the solution is given by:

|N | |P |

cmax =

(yij cj )

i=0 j=0

(3.20)

This is a simple equation where the cost of a platform is added to the total cost, if a platform is allocated to a node.
Financial cost is a concern as it is ultimately one of the key drivers in the decision of where to place computing capability, and will always be one of the largest barriers to achieving the best possible placement. The model considers the added cost of the computing platforms required to implement the in-network computation.

3.6.6 Combined Evaluation Metrics
This section has presented formulations for the 5 important performance metrics relevant for evaluating heterogeneous distributed systems. The goal was to keep these distinct as the proposed model is designed to be flexible enough to use for different scenarios and purposes, where the relative importance of these five metrics will vary depending on the application. Users of the model are able to build more complex metrics based on the requirements of their analysis, combining whichever of these five is relevant to their evaluation, and suitably weighting the different components.
This model to be used in the design and evaluation of alternative structures for deploying heterogeneous applications. In such scenarios, a constraint-driven approach is more sensible than a combined metric, and the model supports such evaluations. For example, a required financial budget or latency target can be set and other metrics evaluated for different designs. If used to compare designs, the primary metric of importance can be evaluated, with constraints placed on the other metrics, such as the best latency for a fixed financial cost and energy budget.
The flexibility in this model in determining general lessons around the placement of tasks and hardware resources is demonstrated in the evaluation in Section 3.8.

64

3.7 Case Study
This section investigates the implications of different placement strategies in a distributed object detection and tracking system. While the formulation presented in Section 3.5 can be used to create an optimal placement of computing resources and tasks for a given application and network, it might be argued that such a bespoke design would not be highly practical, since a more uniform approach to deploying computing resources is generally required, and the variability of applications might make a static allocation less ideal. Hence, this section evaluates strategies for a representative application to learn general lessons about the placement of computing resources in such networks. This study considers a network of cameras, some fixed and some mobile, such as drones, tasked with surveying an area to detect human presence. The images collected by each camera are processed through a sequence of tasks including the histogram of oriented gradients (HOG) and an SVM classifier to detect objects of interest, and a tracking algorithm is applied that relies on the fusion of data from multiple cameras.
3.7.1 Network
A network structure was chosen that is generally representative of that seen in an application such as this. The outermost layer represents the very edge of the network, comprising the cameras themselves (layer A). The next layer represents an access or gateway layer, that connects the cameras to the larger network (layer B). Each gateway and the connected sensors represent different areas that are to be monitored – for example rooms or neighbourhoods. Cameras connect to this layer through interfaces such as 100 Mb Ethernet or 802.11 wireless LAN. A transfer time of 10 ns per bit of data was selected for this layer. The next layer is a routing layer that connects the local network to the wider network, with higher speed and bandwidth interfaces such as 10G Ethernet (layer C). Here a latency of 0.1 ns per bit of data is modelled. Finally, there is the cloud layer, which houses the remote computing resources. To reach this layer data must travel through the internet, for which a communication time of 1 ns per bit is assumed, based on round trip times to AWS EC2 instances measured in [175]. These communication times are estimates and ignore frame/packet overheads, and many other delays, but are there to model variation in transfer time between different layers.
The topology used in this case study is shown in Figure 3.4. It includes a mix of nodes with high and low fanout, and nodes at all of the layers discussed above. Links appear unidirectional as it is assumed data must flow through these
65

layers in order to reach the cloud/datacenter. It is important to note that the layer B/layer C nodes do not represent individual machines, but rather layers of the network hierarchy, comprising multiple machines. Communication within these nodes is neglected in this case study.
Cloud Datacenter
Layer C: Router Layer B: Gateway/Switch Layer A: Sensor
Figure 3.4: Network structure used in this case study.
3.7.2 Tasks
The HOG algorithm used in this case study has been previously implemented on a variety of computing platforms [176; 177; 178]. For the sake of this case study the algorithm is broken down into 3 tasks: gradient computation, normalisation, and classification. While there are more tasks that form this algorithm, these 3 take a majority of the computation time and have a significant effect on data size. Estimates are obtained from [176] for the reduction factor of each task. The tracking algorithm uses these HOG features and a KLT tracker [179], relying on fusion of data from multiple cameras. Therefore this task must be placed elsewhere in the network, at a location that can access all necessary cameras.
In this case study, each camera has a set of tasks, gradient comp → histogram → classification, associated with them, and then each area of cameras has a tracking task that processes the result from multiple camera chains.
3.7.3 Platforms
Computation time for each task on each platform can be estimated based on previous work. Though these are estimates, and different implementations may have varying optimisations, the relative computation times are the important factor for this case
66

Platform

Grad,Hist Normalisaton Classification Tracking

Cortex A9 Intel i7 Intel Xeon Xilinx Zynq Xilinx Virtex 6

2,000 40 2.6 260 1.3

3,200 60 4.0 400 2.1

1,900 35 2.3 240 1.2

2,000 40 2.6 260 1.3

Reduction factor

0.77

0.004

0.16

0.16

Table 3.2: Computation times in milliseconds for each task on different platforms, from work referenced above.

study. If computation is placed at a camera node, an embedded platform is assumed. An embedded Arm Cortex A9 is used in [176] to implement the HOG algorithm, so this case study uses the computation times presented there.
If computing is placed at the access or routing layers, it can be assumed a more powerful CPU is available. The work in [178] implements the algorithm on an Intel Core i7 processor. Finally, the cloud layer would use server-class processors, such as the Intel Xeon platform used to implement the algorithm in [177]. The work in [176] presents an FPGA design that gives a speed-up of around 7× on a Xilinx Zynq platform that could be embedded at the camera. An FPGA accelerator implemented on a larger Xilinx Virtex-6 FPGA was reported in [177], and the case study assumes this is the FPGA platform available at other layers. The relative performance on these platforms is used to estimate the computation time of the tracker task. Table 3.2 summarises time taken for each task on each platform per frame.
The costs of each platform are also relevant. This case study considers the extra costs associated with adding computing resources to different layers of the network. A Cloud/datacenter node is present regardless of other node placements. Table 3.3 summarises approximate costs and power consumption for each of the platforms in arbitrary currency and energy units based on costs determined from OEM suppliers, and manufacturer power estimation utilities.
The FPGA resource utilization estimates in the previously cited works at the top of Section 3.7.3 suggest that both FPGA platforms can implement 3 full pipelines of the algorithm pipeline each, so 12 tasks. It is assumed CPU-based platforms have no limit to the number of tasks that can be running, though, as discussed in the formulation, there is a latency penalty for sharing resources. The case study focuses on latency, throughput, energy consumption, and financial cost
67

Platform

Cost Power Consumption

Arm Cortex A9 10

1

Intel Core i7

300

5

Intel Xeon

2000

100

Xilinx Zynq

250

5

Xilinx Virtex-6 1000

10

Table 3.3: Financial cost and power estimates for each platform.

Placement Latency Throughput Cost Energy

Centralised Layer C Layer B Layer A

1.95s 1.97s 1.93s 7.16s

3.43 frames/s 0.88 frames/s 0.94 frames/s 0.14 frames/s

2000 3200 4100 2300

30.03 23.00 23.00 241.2

Table 3.4: Performance metrics for different placement strategies using software platforms.

as the metrics of interest. The model is used to build the above scenario and evaluate different compu-
tation placement strategies. Results are presented in Table 3.4 for software platforms and Table 3.5 for hardware platforms. The output is the latency, throughput, financial cost, and total energy consumption of the entire system. Bandwidth results are not shown in this table as they are calculated per node in this model.
3.7.4 Centralised Software
A typical approach to such an application would be to centralise all tasks, performing them in software, transmitting all data to the cloud or datacenter. In this case study, this gives a latency of around 1.95 s, and a throughput of 3.4 frames per second for each camera. Note that this is in the worst case, where all camera streams compete for CPU resources. The large communication latency coupled with the large amount of data being transmitted undermines the extra computing power provided by the cloud. Energy consumption was also joint highest with this approach, as the Centralised hardware has the highest power consumption.

68

3.7.5 In-network software
An alternative approach is to push processing tasks down into the network. One possibility is placing the gradient computation, normalisation, and classification tasks on the camera nodes (layer A), and placing the tracking tasks at the appropriate layer B nodes as they require information from a set of cameras. This results in a latency of around 7.16 s and a poor throughput of 0.14 frames per second, unsuitable for real time applications. The energy consumption seems high, but this value is the energy consumption of the entire system - the consumption at each individual node is much lower. While there is communication latency, and fewer tasks competing for the same resources, the computing capability of these edge nodes is so low that the latency and throughput are much worse than the centralised placements.
Distributing tasks within the intermediate network infrastructure offers improved latency relative to placing tasks in layer A, but has minimal impact when compared to centralised placement. In this scenario, the reduced communication latency is offset by the increased computation latency. Layer B and layer C approaches introduce additional costs of 2100 and 1200 currency units respectively. The centralised solution also has 3.65× higher throughput than these approaches. This is because of its increased computing capability relative to these other nodes, meaning that there is less computation latency. Energy consumption is less than centralised software, due to the lower power consumption of the hardware. This energy consumption is also spread across a greater number of nodes, meaning each node consumes less energy.
3.7.6 Centralised Hardware
Utilising FPGA acceleration at the server node reduces the latency to 1.68 s, and increases throughput to 133 frames per second, as a result of reductions in computation latency. While the FPGA should in theory provide a greater performance boost than this, the time taken for data to travel to the cloud limits the improvement that can be achieved for the application. The energy cost of running these tasks in hardware is also much lower than in software. The FPGA accelerator has a lower power consumption, as well as lower computation time.
3.7.7 In-network hardware
Adding FPGA accelerators to layer C reduces latency to 0.84 s, and increases throughput to 133 frames per second due to the performance of the FPGA accelerators dramatically reducing computation latency. Placing FPGAs in layer B further im-
69

Placement Latency Throughput Cost Energy

Centralised

1.68s 133 frames/s 13000

1.56

Layer C

0.844s 133 frames/s 14000

1.56

Layer B

0.8s 133 frames/s 16000

1.56

Layer A

0.94s 1.10 frames/s 11600

30.6

Table 3.5: Performance metrics for different placement strategies using hardware platforms.

proves latency to 0.83 s. These placements give improvements over the centralised FPGA approach due to the reduction in communication latency. There is little difference in latency between placing tasks predominately in layers B or C, as the fast link between these layers means that there is minimal communication delay. The disadvantage of the in-network FPGA approach is the additional cost, with the layer B and C methods costing 16000 and 14000 currency units respectively. Moving all tasks in hardware to the layer A camera nodes offers improvements over the software equivalent due to the increased computing capability. It also improves over centralised approaches due to the reduced communication latency. However the higher computation latency relative to layers B and C means that there is a higher overall latency, and worse throughput. While the total energy consumption for the layer A approach looks high, it is spread across a greater number of nodes. Each layer A node actually has a power consumption of approximately 0.956. The same processing hardware is implemented on the FPGAs in layers B and C, as well as when centralised. This results in the throughput being equal in all circumstances, despite the higher communication latency.
3.7.8 Optimal Placement
The model outlined in this thesis can be used with a Mixed Integer Linear Program (MILP) solver to generate a specific task and hardware placement strategy to optimise any of the performance metrics detailed in Section 3.6. To do this, the Python PULP front end was used to interface to the CBC solver. In this case, the system is optimised for latency, as in this example, energy and throughput are directly related to latency. First generated is the optimal latency placement, then ran the optimisation again with a latency constraint 5% higher than this value, but optimising for cost. This forces the solver to generate the cheapest placement that achieves a latency within 5% of the optimal value. As a result, the model generated a placement with the metrics shown in Table 3.6. This is presented for completeness;
70

it may be argued that customising a network for a specific application is unlikely to be a common requirement. Hence, this section has focused primarily on the general lessons learnt in terms of placement strategies for hardware in the network.

Placement Latency Throughput Cost Energy

Optimised

0.87s 133 frames/s 9000

1.56

Table 3.6: Performance metrics for MILP optimisation of model

3.7.9 Summary
Improvements can be made to streaming application latency by pushing tasks into the network in either software or hardware. This also offers improvement in energy consumption at each individual node, important when there may be a limited power budget. There is a balance between the communication latency to reach higher capability nodes, and the benefits to computation latency that they provide. Placing tasks at the very edge of the network minimises communication latency but is limited by poor computational capability. The cloud offers the highest computing capability but there is a communication latency bottleneck. The downside of using in-network task placement is the additional financial cost of the extra hardware. However, with the price/performance ratio for embedded devices scaling significantly faster than for server class CPUs, this should improve over time.
3.7.10 Event Driven Simulation
As part of this thesis, a discrete event simulator was written in Python using the SimPy library, to test the validity of results produced by the model. Data sources emit periodic packets of data into the network with the same topology and task structure. The tasks are allocated to the relevant nodes, and are executed at the nodes in a first-in first-out fashion, with priority given to the oldest data packets.
Differences should be expected in the reported latencies from the model and simulator primarily due to the more detailed task and communication scheduling in the simulator. The simulation processes individual packets as opposed to the considering abstract streams in the model. The data sources in the simulator emit packets with fixed periods, sources are unsynchronised, whereas the model implicitly assumes synchronisation. The simulation also takes into account a small switching delay at nodes, representing the transfer of data form received packets to the

71

Latency(s)

Total Energy consumption

8 model
simulation
6

model simulation

4

2

0 Cent.

Layer C Layer B Layer A Cent. Configuration

Layer C Layer B Layer A Configuration

(i) Software platforms

(ii) Hardware platforms (a) Latency

250 model
simulation 200

model simulation

150

100

50

0 Cent.

Layer C Layer B Layer A Cent. Configuration

Layer C Layer B Layer A Configuration

(i) Software platforms

(ii) Hardware platforms

(b) Total Energy Consumption

120

model simulation

100

80

60

40

20

0 Cent.

Layer C Layer B Layer A Cent. Configuration

Layer C Layer B Layer A Configuration

(i) Software platforms

(ii) Hardware platforms

(c) Throughput - A higher value indicates superior performance

Throughput (result/s)

Figure 3.5: Difference between values calculated through the formulated model and a discrete event simulator for the same configurations and parameter values.

72

computing platform. Various network related parameters are not included in the simulation, as these are not influenced by the allocation of tasks and platforms.
Simulations of the above scenario were run for 20,000 packets entering the network from each source. The sources were fixed to the same period, but set out-ofsync with each other, to a degree determined from a uniformly distributed random variable. Figure 3.5 shows the deviation between the metrics predicted by the model and those measured in the simulation. Financial cost is not shown, as there will be no difference between the simulation and model, and data-rate is not shown as it is calculated for each individual node, not the system as a whole.
It can be seen that if considering only software platforms, the difference between the model and simulator is close to 6%, and in hardware 7%. These differences stem from the data sources being out of sync, and the switching delays introduced at each node, not represented in the model. The ratio between computation time and network switching delay impacts this error, and hence in the case of hardware, where computation time is reduced, the overhead is more significant. However, these deviations are still well within tolerable levels.
Additional Model Validation
The discrete event simulation detailed above provides only limited validation of the model. While it models the flow of individual packets as opposed to the abstracted streams used in the model, the results cannot be cited as external validation of results generated by the model. This presents a limitation of the model presented in this chapter. The model can be used to evaluate the performance of various placement strategies relative to each other, verified by the simulator. This is still useful within the context of a design space exploration, eliminating the clearly sub-performant solutions.
Further work must be carried out to verify that the model generates performance metrics that match real-world implementations. To carry out this work, a substantial test-bed would be required, as well as the increased maturity of various technologies such as network attached computation. This makes practical validation at scale difficult. In addition, existing modelling tools, as outlined in the literature review, aren’t designed to model the same scenario, making validation using other models challenging.
As a result, it is suggested that this validation forms an important piece of further work.
73

3.8 Further Analysis
While determining a fixed optimal solution for a given application and network topology is possible by using an MILP solver as discussed, this sections considers synthetic scenarios in an attempt to draw general lessons about distributed, accelerated in-network computing. It explores how application and network properties influence the decision on where to place computing resources for this range of scenarios. Since latency is the primary metric of interest, it is the focus of this section.
For this analysis, a Python script is used to pseudo-randomly generate application task graphs. These are in a tree structure with a maximum depth of 4 tasks, reflect a realistic partitioning granularity rather than a very fine-grained structure that would skew further towards in-network computing. The analysis uses a fixed network structure, with a similar layer A/B/C hierarchy and interface specification as used in the case study in Section 3.7, however with 8 layer C nodes, each serving 2 layer B nodes, each of which serves 5 layer A nodes.
Several constraints are placed on the task generation. The tree is built up leaf tasks, with a random variable determining whether each task is connected to a new task or joins one already existing in the tree. Tasks can only join other tasks whose leaf tasks originate from nodes that share the same layer B parent. The script generates 100 random task trees in this manner, and the same 100 trees are used to evaluate each placement strategy, summarised in Table 3.7. For the purposes of this analysis, it is assumed that there are no restrictions on the number of tasks that can be allocated to a node, and all tasks are to be executed in ’software’ - meaning that in this model there is a latency penalty dependant on the number of tasks allocated to the same node.
This section is focused on the latency metric, as latency reduction is one of the main motivations behind in-network and edge computing. The case study in Section 3.7 showed that latency and throughput are closely related for these types of streaming applications.
3.8.1 Relative Computing Capability
A key factor that determines where to place tasks is the relative computing capability that can be accessed at different layers of the network. In general, the closer to the centre a node is, the greater the computing capability, since the cost is amortised across more streams. The resources at the edge of the network are more likely to be limited due to space, energy, or cost constraints, while nodes further up in the hierarchy will have access to better hardware. However using better resources further
74

Strategy

Explanation

Centralised Pushed
Intermediate Edge/Central Edge/Network

All tasks allocated to root node All tasks pushed down toward the leaf nodes as much as possible All tasks pushed down as far as possible, but not to leaf nodes
Leaf tasks placed at leaf nodes, others placed centrally Leaf tasks placed at leaf nodes, others pushed down as far as possible but not to leaf nodes

Table 3.7: Different placement policies used in the simulations presented in this section.

up the network entails a communication latency penalty, which must be overcome by improved computation latency. For this comparison, tasks are set to have a reduction factor of 50% and equal latency on the same platform. Figure 3.6 shows how different placement strategies impact latency, for different relative computing capabilities.
In the unlikely case where computing capability is equal across all layers (i.e. a Centralised:B/C:A computing capability ratio of 1:1:1), pushing all tasks as close to the data sources as possible yields the lowest latency as there is minimal communication delay, and no benefit to placing tasks higher up. This may be the case if the network is a commodity cluster of homogenous machines. Computation time is also improved since the tasks are distributed across many nodes resulting in less contention than for a centralised placement.
If the compute capability at the data sources is significantly smaller (50× in this case), while the rest of the network offers equivalent computing capability (a ratio of 1:50:50), pushing tasks down to intermediate nodes offers the best latency. In this case, the slight reduction in communication latency gained through placing tasks at the data sources is outweighed by the computation latency penalty. Placing them any closer to the central node adds further communication latency with no additional benefit, and causes contention due to more tasks being allocated to fewer nodes.
The more likely case is that resources at the central node are more capable than intermediate nodes, which offer greater capability than the edge. In the case of the central node being 5× more capable than the intermediate nodes (a computing capability ratio of 1:50:250), pushing tasks as low as possible into the intermedi-

75

Latency relative to Centralised placement

Centralised Pushed Intermediate Edge/Central Edge/Network

1.4

1.4

1.2

1.2

1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0 Task tree
(a) 1:1:1 compute capability ratio.

0 Task tree
(b) 1:50:50 compute capability ratio.

7.5

15

Latency relative to Central Placement

12.5 6
10 4.5
7.5 3
5
1.5 2.5

0 Task tree
(c) 1:50:250 compute capability ratio.

0 Task tree
(d) 1:50:500 compute capability ratio.

Figure 3.6: Latency comparison for different Layer A:B:C computing capability ratios.

ate nodes still outperforms the centralised solution, as tasks are distributed to a larger number of nodes, reducing computation latency. Increasing the difference in

76

Centralised Pushed Intermediate Edge/Central Edge/Network

2

2

2.5

Latency relative to Centralised placement

2

1.5

1.5

1.5

1

1

1

0.5

0.5

0.5

0 Task tree
(a) Signiﬁcant reduction at edge tasks.

0 Task tree
(b) Moderate reduction at edge tasks.

0 Task tree
(c) No reduction at edge tasks.

Figure 3.7: Latency comparison for different edge task data reduction factors.

computing power to 10× (1:50:500) causes the central solution to become dominant. Hence, it can be seen that a key requirement for in-network computing to be
feasible is that suitably capable computing resources be employed for executing tasks in the network. The more capable the edge nodes are in comparison to the root node, the greater the benefits of placing tasks further towards the edge.
3.8.2 Task Data Reduction
The time taken to transmit data further up the network is tied to the amount of data being transmitted. Tasks can reduce data by varying degrees, and this impacts the balance between computation and communication latency. For this experiment, reduction factors of tasks are modified to observe the impact on latency. It uses the same network topology as in the previous experiment, and the same method of generating task trees. A 1:50:500 relative computing capability configuration is used, as discussed in Section 3.8.1. This is to model the difference in compute resources at the different levels of the network.
Figure 3.7 shows how different placement strategies impact latency, for different task reduction factors. If data is dramatically reduced by tasks close to the edge of the task tree, placing tasks as close as possible to the data source is more
77

likely to provide a latency improvement as the communication cost for every other transfer between nodes is reduced. It can be seen that intermediate placement reduces latency by 5× compared to a centralised allocation in such a scenario. Placing all tasks at the edge results in 30% worse latency, despite the reduced communication latency, due to the low computing capability of these nodes. Placing only the leaf task at the edge and the rest either in the network or at the central node also provides a significant reduction in latency in this scenario.
If data is not significantly reduced in the task tree, or only at tasks higher up in the tree, placing tasks towards the central sink is preferred, especially if those resources are more capable. A centralised placement provides the best latency in a majority of cases, although only by a slight margin. For some task trees, the in-network approach is superior. This result is impacted by the relative computing capability of layers. For scenarios where the central node is much more capable than the rest of the network, the instinct is to place tasks there. However, if data is reduced significantly at the leaf tasks then placing tasks in the network can reduce communication latency significantly.
It can be seen that, generally, the closer to the edge tasks that data is reduced, the greater the benefits of placing tasks closer to the edge of the network.
3.8.3 Network Structure
The structure of the network determines to what extent tasks can be distributed and parallelised and how much they must compete for resources. Related to this is the structure of the application task graph; having tasks that require data from multiple sources closer to the root of the tree means that tasks cannot be pushed down into the network to a layer with more computing nodes. To investigate this factor, this section considers different network structures and their impact on latency, as shown in Figure 3.8. The tasks were generated with the same method as before, and network nodes had the same computing capability as in Section 3.8.2. All tasks were set to a fixed reduction factor of 0.5.
Firstly, consider a network with low fanout, where layer B nodes each have 2 layer A nodes attached. While this means that there was more available resources towards the edge of the network, in many cases pushing tasks into the network results in almost 2× the latency of a centralised solution. Tasks that require data from more than one source must be pushed further up the network, adding additional communication latency. Additionally, as there are few layer C nodes, these nodes are over-utilised. Increasing the number of layer C nodes, or the computing capability of these nodes would offer performance benefits in this scenario.
78

Centralised Pushed Intermediate Edge/Central Edge/Network

6

2

2

Latency relative to Centralised placement

5

1.5

1.5

4

3

1

1

2

0.5

0.5

1

0 Task tree
(a) Layer A fanout of 2.

0 Task tree
(b) Layer A fanout of 5.

0 Task tree
(c) Layer A fanout of 20.

Figure 3.8: Latency comparison for different network fanout factors.

Raising the fanout of the layer B nodes to 5 instead of 2 increases the benefits of pushing tasks into the network. As more sources share the same paths towards the central node, there is a higher chance that a task that works with data from multiple sources can be placed closer to the edge. Increasing the number of nodes at layer C in this case again slightly decreases latency, as tasks that do have to be placed there have access to more resources.
Further increasing the fanout of the layer B nodes to 20 starts to increase latency again, up to around 0.45× the centralised placement. Increasing it to 40 increases the latency to around 0.7× the central placement.
A larger fanout at layer A (the edge layer), up to a point means that there is a greater benefit of pushing tasks down towards the network edge, as there are more opportunities to place tasks that require data from multiple child tasks closer to the edge. However if the fanout is too great, resource competition starts to reduce the benefits of this approach.
It can be seen that there exists a trade-off between having multiple sources connected to the same path of nodes, and creating too much resource contention by having too many tasks assigned to the same intermediate nodes.

79

3.8.4 Hardware Acceleration
There are several key points that can be taken away from this investigation. Innetwork computing is more effective in situations where the edge and intermediate nodes are comparable in capability to the central node. While this is unlikely with traditional software processing platforms, it makes the case for trying to integrate hardware accelerators such as FPGAs into the network, as they can provide processing times closer to the more powerful processors found in a datacenter environment. It can also be seen that in-network computing provides more benefits in applications where data is more greatly reduced in tasks closer to the edge of the task tree. These tasks can often be large filtering or pre-processing tasks, and in order to place them close the the edge of the network, more capable hardware is required. This again makes the case for hardware acceleration. Finally, high-fanout network topologies benefit more from in-network computing as there are more opportunities for data fusion between tasks. The ability of hardware acceleration architectures to process streams of data in parallel is well suited to these scenarios, suffering less of a latency penalty due to resource contention.
3.9 Generating In-Network Task and Hardware Placement with Heterogeneous Hardware
As systems scale, manual placement of tasks and the selection of appropriate hardware becomes more cumbersome and potentially impractical. Additionally, naive placement strategies may result in sub-optimal performance. This means that a model able to determine optimal placement has significant value. An optimisation is also useful during the infrastructure planning phase when decisions need to be made about which systems to deploy.
This section outlines how the model can be used to generate task and hardware placement, for different objectives. It compares the placements generated by the model with various naive placement strategies for an example scenario, and then evaluate the generated placements with synthetically generated network topologies.
3.9.1 Objective function formulation
Section 3.6, outlines equations to calculate the bandwidth, energy consumption, end-to-end latency, financial cost, and throughput for a given network.
Each of these equations could be used as a singular objective function to be used with a mixed integer linear programming (MILP) solver. The solver will
80

attempt to minimise the value of the function, given the constraints outlined in Section 3.5. The experiments in this section use PULP, a Python front-end for the CBC MILP solver.
There is the possibility of multiple optimal solutions existing for any one objective function. The Python implementation of the model allows for the definition of an order of preference among the metrics. For example, the order (Latency, Cost) would provide the cheapest version of the lowest-latency solution. This is achieved by running the solver with the latency objective function, then adding a constraint to the linear program to force the latency to be lower than or equal to the newly found optimal value within 5%, and running the solver again with the cost objective function. This forces the solver to generate the cheapest placement that achieves a latency within 5% of the optimal value. For these experiments the value of 5% is used, but any threshold could be used depending on the requirements of the application. An issue with this approach is that it is inefficient, as the solver must be run multiple times. It also doesn’t allow for mixtures of these performance metrics to be targeted. Further work could be done to construct more complex objective functions that combine multiple metrics.
Users could alternatively select one of the five available objective functions, and add constraints on the other metrics. This technique may be enough for a ‘design’ phase scenario as users will probably have a set budget and performance constraints that have to be met. For example, the cheapest solution that satisfies a given latency and energy constraint can be generated, or the lowest-latency solution for a given energy and financial budget could be found.
3.9.2 Case Study
To demonstrate optimisation of placement strategies with the model, it is applied to an example scenario. The goals are:
1. Demonstrating that computation in the network is feasible and offers advantages over centralised cloud, and fully-distributed edge computation.
2. Quantifying the improvements provided by the optimisation compared to the more naive approaches.
For this case study, complete formulation and optimisation took an average of 5 minutes. While compute time does increase with the scale of the network and task graphs, the initial use of this model is offline when planning the deployment of accelerators, and so this is an acceptable delay in light of the model’s flexibility.
81

In most scenarios, it is likely that a more constrained search is sufficient due to existing hardware placement and other constraints. The model is also useful in scenarios where after planning hardware placement, new tasks are to be allocated to those existing resources. These additional constraints will cause the problem to be further reduced in the pre-solve stage and to a decrease in the total time to solve.

Example Scenario
The scenario examined is based on the smart surveillance application detailed in the Section 3.7, using the same HOG algorithm and task structure. Using the values for task execution time for the various platforms detailed, more generalised estimates can be extracted. Consider a network of cameras, some fixed and some on mobile platforms such as drones, tasked with surveying an area to detect human presence. Each camera source is capable of hosting a computing platform and can send data to a central data sink hosting a more powerful computing platform. This study uses an arbitrary network topology that includes several possible structures—nodes with high fanout, long and short paths, data sources at different depths—as depicted in Figure 3.10.
Parameter values for each task are shown in Table 3.8. It is assumed there is one software implementation of each task, and one hardware implementation. Each hardware implementation of a task consumes one ‘slot’ on a hardware platform. The experiments also assume infinite tasks can be allocated to software platforms.

Task

τsw

τhw

ft

Gradient Computation Normalisation Real AdaBoost

0.16 6.33 650.00

0.0496 1.9500 201.5000

0.770 0.004 0.160

Table 3.8: Summary of task parameter values.

Available platforms are detailed in Table 3.9, with their relative performance to each other. The ‘Processor’ platform represents a server-class processor and can only be hosted at the root node. The FPGA platforms have no speed as the same logic deployed on different FPGAs is likely to have roughly equal compute times, if the logic is the same, and implementation the same. The same holds true for power consumption - given the same logic implemented on the different sizes of FPGAs, the differential power consumption for that task should be similar. The values in the table, derived from [176] are designed to be representative, and can be replaced

82

with real measured values for specific applications.

Platform

Cost Speed Power Slots

Microcontroller

30

Processor

500

Very small FPGA (fpgaVS) 35

Small FPGA (fpgaS)

70

Medium FPGA (fpgaM)

85

Large FPGA (fpgaL)

100

1

0.1

–

8

1

–

–

0.4

1

–

0.4

3

–

0.4

6

–

0.4

9

Table 3.9: Summary of available platforms.

Performance Compared to Naive Placement
The generality of the proposed model means direct comparison against other published work is difficult. Hence, this section analyses how it performs against the more traditional centralised cloud and distributed edge approaches.
The naive placement options considered are:
• Allocation of all tasks to a central server node, housing the ‘Processor’ platform, similar to a cloud deployment (Cent);
• All tasks ‘pushed down’ as far as possible towards the data sources—what is often referred to as the ‘edge’, performed on microcontrollers (PushM);
• All tasks ‘pushed down’ as far as possible towards the data sources, performed on FPGAs (PushF);
The solutions generated by the model to minimise latency using only microcontrollers (OptM), solutions generated by the model to minimise latency using any available platforms (OptF), and solutions generated by the model to minimise the time between results using only microcontrollers (OptT), can be compared;
Figure 3.9 shows the results for each of the metrics under these conditions, normalised against the centralised approach. Energy, and latency are improved, as is ‘throughput’ (throughput is plotted as the time between results, so lower is better).
The model finds solutions that improve upon the fully-distributed ‘edge’ (PushM and PushF) approach significantly in terms of cost, while also offering improvements in the other metrics, with the ability to optimise for a preferred metric. Figure 3.10 shows two of the generated configurations.
83

Cost

2.5

Throughput

Latency

Energy
2

Relative Performance

1.5

1

0.5

0 Cent PushM PushF OptM OptT OptF
Figure 3.9: Naive vs. model placement. A lower value is better for all metrics.

A realistic application of the model could be to find the cheapest resource and task allocation possible that still meets an end-to-end latency constraint. In the case of the surveillance camera system used as a case study, the latency between image capture and a final decision may be a critical factor. Figure 3.11 shows the cost of the solution generated by the optimization to achieve different latency constraints, compared to the cost of pushing all tasks to the leaf nodes. The optimisation approach offers a set of solutions for each latency constraint that improves on either of the distributed approaches in terms of cost, with savings of up to 33.8% in this particular case, depending on the performance required. Centralised computation in this case would not meet the latency constraint for all plotted values. Finally fully-distributed execution on microcontrollers does not meet the tighter latency constraints, demonstrating the importance of this model including hardware heterogeneity.
3.9.3 Evaluation with Synthetic Networks
For a more thorough evaluation, the model can be applied to a number of pseudorandomly generated network topologies. For this study, the number of network nodes is limited to between 20 and 40, with each node potentially having a data source input as well as connections to other nodes. Generation parameters control the breadth and depth of the network topologies in order to ensure generation of a

84

