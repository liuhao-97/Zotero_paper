292

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices
Yu-Hsin Chen , Student Member, IEEE, Tien-Ju Yang , Student Member, IEEE, Joel S. Emer, Fellow, IEEE, and Vivienne Sze , Senior Member, IEEE

Abstract— A recent trend in deep neural network (DNN) development is to extend the reach of deep learning applications to platforms that are more resource and energy-constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efﬁciency and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes and often require specialized hardware to exploit sparsity for performance improvement. Therefore, many DNN accelerators designed for large DNNs do not perform well on these models. In this paper, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly ﬂexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations and therefore is able to improve both processing speed and energy efﬁciency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65-nm CMOS process achieves a throughput of 1470.6 inferences/s and 2560.3 inferences/J at a batch size of 1, which is 12.6× faster and 2.5× more energyefﬁcient than the original Eyeriss running MobileNet.
Index Terms— Deep neural network accelerators, deep learning, energy-efﬁcient accelerators, dataﬂow processing, spatial architecture.
I. INTRODUCTION
The development of deep neural networks (DNNs) has shown tremendous progress in improving accuracy over the past few years [1]. In addition, there has been an increasing effort to reduce the computational complexity of DNNs, particularly for those targeted at mobile devices [2]. Various different techniques have been widely explored in the design of DNN models including reduced precision of weights and activations [3]–[8], compact network architectures [9]–[11] (i.e., compact DNNs), and increasing sparsity in the ﬁlter
Manuscript received December 7, 2018; revised February 26, 2019; accepted March 19, 2019. Date of publication April 11, 2019; date of current version June 11, 2019. This work was supported by the DARPA Young Faculty Award, MIT’s Center for Integrated Circuits and Systems, and gifts from Google, Intel, and Nvidia. This paper was recommended by Guest Editor B. Murmann. (Corresponding author: Yu-Hsin Chen.)
Y.-H. Chen, T.-J. Yang, and V. Sze are with the Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail: yhchen@mit.edu).
J. S. Emer is with the Massachusetts Institute of Technology, Cambridge, MA 02139 USA, and also with Nvidia, Westford, MA 01886 USA.
Color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/JETCAS.2019.2910232

weights [12]–[14] (i.e., sparse DNNs). While these approaches provide theoretical reductions in the size and number of operations and storage cost, specialized hardware is often necessary to translate these theoretical beneﬁts into measurable improvements in energy efﬁciency and processing speed.
Support for reduced precision has been demonstrated in recent hardware implementations, including Envision [15], Thinker [16], UNPU [17], Loom [18], and Stripes [19]. These works have shown various methods that efﬁciently translate reduced bitwidth from 16-bits down to 1-bit into both energy savings and increase in processing speed. Specialized hardware for binary networks have also been widely explored [20]–[24]. In this work, we focus on complementary approaches that have been less explored, speciﬁcally the support for diverse ﬁlter shapes for compact DNNs, as well as support for processing in the compressed domain for sparse DNNs. While compact and sparse DNNs have fewer operations and weights, they also introduce new challenges in hardware design for DNN acceleration.
A. Challenges for Compact DNNs
The trend for compact networks is evident in how the iconic DNNs have evolved over time. Early models, such as AlexNet [25] and VGG [26], are now considered large and over-parameterized. Techniques such as using deeper but narrower network structures and bottleneck layers were proposed to pursue higher accuracy while restricting the size of the DNN (e.g., GoogLeNet [9] and ResNet [27]). This quest further continued with a focus on drastically reducing the amount of computation, speciﬁcally the number of multiplyand-accumulates (MACs), and the storage cost, speciﬁcally the number of weights. Techniques such as ﬁlter decomposition as shown in Fig. 1 have since become popular for building compact DNNs targeted at mobile devices (e.g., SqueezeNet [11] and MobileNet [10]). This evolution has resulted in a more diverse set of DNNs with widely varying shapes and sizes.
One effect of compact DNNs is that any data dimension in a DNN layer can diminish. In addition, due to latency constraints, it is increasingly desirable to run DNNs at smaller batch sizes (i.e., smaller N). Table I summarizes the data dimensions that are used to describe a DNN layer and the common reasons for each dimension to diminish. This suggests that less assumptions can be made on the dimensions of a DNN layer.
For hardware designers, widely varying DNN layer shapes, especially diminishing dimensions, is challenging as they

2156-3357 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

293

Fig. 1. Various ﬁlter decomposition approaches [10], [26], [28].
TABLE I REASONS FOR DIMINISHING DATA DIMENSIONS IN A DNN LAYER

Fig. 2. Data reuse of the three data types in each layer of the three DNNs. Each data point represents a layer, and the red point indicates the median
amount of data reuse among all the layers in a DNN. For example, incp3ared5 × 5 means the reduction layer with 5 × 5 ﬁlters in Inception module 3a in GoogLeNet. (a) Input activations (iacts). (b) Weights (batch size = 1). (c) Partial sums (psums).

Fig. 3. Two common DNN accelerator designs: (a) Spatial accumulation array [29]–[32]: iacts are reused vertically and psums are accumulated horizontally. (b) Temporal accumulation array [15]–[17]: iacts are reused vertically and weights are reused horizontally.

result in changes in a key property of DNNs: data reuse, which is the number of MACs that use the same piece of data, i.e., MACs/data. Most DNN accelerators rely on data reuse as a means to improve efﬁciency. The amount of data reuse for each of the three data types in a DNN layer, i.e., input activations (iacts), weights and partial sums (psums), is a function of the layer shape and size. For example, the amount of iact reuse is proportional to the number of output channels as well as the ﬁlter size in a layer. Therefore, diminished data dimensions suggest that it is more difﬁcult to exploit data reuse from any speciﬁc dimension.
Fig. 2 shows that the variation in data reuse increases in all data types in more recent DNNs, and the amount of reuse also decreases in iacts and psums. This variation and overall reduction in data reuse makes the design of DNN accelerators more challenging in two ways.
1) Array Utilization: Many existing DNN accelerators [15]–[17], [29]–[32] rely on a set of pre-selected data dimensions to exploit both high parallelism across an array of processing elements (PEs) for high performance and data

reuse for high energy efﬁciency. For instance, Fig. 3 shows two designs that are commonly used. A spatial accumulation array architecture (Fig. 3a), which is often used for a weightstationary dataﬂow, relies on both output and input channels to map the operations spatially onto the PE array to exploit parallelism. At the same time, each iact can be reused across the PE array vertically with weights from different output channels, while psums from the PEs in the same row can be further accumulated spatially together before written back to the global buffer. Similarly, a temporal accumulation array architecture (Fig. 3b), which is often used for a outputstationary dataﬂow, relies on another set of data dimensions to achieve high compute parallelism. In this case, each iact is still reused vertically across different PEs in the same column, while each weight is reused horizontally across PEs in the same row.
When the set of pre-selected data dimensions diminish due to a change in DNN shapes and sizes, e.g., the number of output channels in a layer (M) is less than the height of the PE array, efﬁciency decreases. Speciﬁcally, these spatial

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

294

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Fig. 4. Array utilization of different architectures for depth-wise (DW) layers in MobileNet. The colored blocks are the utilized part of the PE array. For Eyeriss [33], the different colors denote the parts that run different channel groups (G). Please refer to Table I for the meaning of the variables.
mapping constraints result in both reduced array utilization (i.e., fewer PEs are used) as well as lower energy efﬁciency. Furthermore, these inefﬁciencies are magniﬁed as the size of the PE array is scaled up, because the diminished dimension is even more likely to be unable to ﬁll the array. For example, as shown in Fig. 4, the aforementioned spatial and temporal accumulation arrays will ﬁnd it difﬁcult to fully utilize the array due to the lack of input and output channels in the depthwise (DW) layers in MobileNet. In contrast, Eyeriss [33] can still achieve high array utilization under such circumstances by mapping the independent channel groups onto different part of the PE array due to the ﬂexibility of its Row-Stationary (RS) dataﬂow.
2) PE Utilization: A lower data reuse also implies that a higher data bandwidth is required to keep the PEs busy. If the on-chip network (NoC) for data delivery to the PEs is designed for high spatial reuse scenarios, e.g., a broadcast network, the insufﬁcient bandwidth can lead to reduced utilization of the PEs (i.e., increased stall cycles), which further reduces accelerator performance. For instance, even though Eyeriss can better utilize the array as shown in Fig. 4, its broadcast NoC (which supports multicast) is not going to provide adequate bandwidth to support high throughput processing at high parallelism, thus the performance will still suffer. However, if the NoC is optimized for high bandwidth scenarios, e.g., many unicast networks, it may not be able to take advantage of data reuse when available.
An additional challenge lies in the fact that all DNNs that the hardware needs to run will not be known at design time [34]; as a result, the hardware has to be ﬂexible enough to efﬁciently support a wide range of DNNs. To build a truly ﬂexible DNN accelerator, the new challenge is to design an architecture that can accommodate a wide range of shapes and sizes of DNN layers. In other words, the data has to be ﬂexibly mapped spatially according to the speciﬁc shape and size of the layer, instead of with a set of pre-selected dimensions, in order to maximize the utilization of the PE array. Also, the data delivery NoC has to be able to provide high bandwidth when data reuse is low while still being able to exploit data reuse with high parallelism when the opportunity presents itself.
B. Challenges for Sparse DNNs
Sparse activations naturally occur in DNNs for several reasons. One is that many DNNs use the rectiﬁed linear unit (ReLU) as the activation function, which sets negative

values to zero; this sparsity tends to increase in deeper layers and can go above 90%. Another increasingly important reason is that many popular DNNs are in the form of autoencoders [35]–[37] or generative adversarial networks (GAN) [38], which contain decoder layers that use zero insertion to up-sample the input feature maps, resulting in over 75% zeros.
There has also been a signiﬁcant amount of work to make the weights in a DNN sparse. Various metrics are used to decide which weights to prune (i.e., set to zero), including saliency [12], magnitude [13], and energy consumption [14]. These pruned networks have weight sparsity of up to 90%.
Sparsity in weights and activations can be translated into improved energy efﬁciency and processing speed in two ways: (1) The MAC computation can be either gated or skipped; the former reduces energy while the latter reduces both energy and cycles. (2) The weights and activations can be compressed to reduce the amount of storage and data movement; the former reduces energy while the latter reduces both energy and cycles. However, it is quite challenging to design DNN accelerators that can actually harness these beneﬁts from sparsity due to the following reasons:
1) Irregular Accesses Patterns: Computation gating can effectively translate sparsity in both weights and activations into energy savings, and its implementation can be realized at a low cost by recognizing if either the weight or activation is zero and gating the datapath switching and memory accesses accordingly. For example, Eyeriss has demonstrated gating for sparse activations.
To improve throughput in addition to saving energy consumption, it is desirable to skip the cycles of processing MACs that have zero weights or iacts. However, this requires more complex read logic as it must ﬁnd the next non-zero value to read without wasting cycles reading zeros. A natural way to address this issue is to keep the weights and iacts in a compressed format that can indicate the location of the next non-zero relative to the current one. However, compressed formats tend to be of variable length and thus must be accessed sequentially. This makes it difﬁcult to divide up the compressed data for parallel processing across PEs without compromising compression efﬁciency. Furthermore, this presents a challenge if sparsity in both weights and activation must be simultaneously recognized, as it is difﬁcult to ‘jump ahead’ (e.g., skip non-zero weights when the corresponding iact is zero) for many of the most efﬁcient compression formats; the irregularity introduced by jumping ahead also prevents the use of pre-fetching as a means of improving throughput. Thus, the control logic to process the compressed data can be quite complex and adds overhead to the PEs.
Accordingly, there has been limited hardware in this space. Cnvlutin [39] only supports skipping cycles for activations and does not compress the weights, while Cambricon-X [40] does not keep activations in compressed form. Due to the complexity of the logic to skip cycles for both weights and activations, existing hardware for sparse processing is typically limited to a speciﬁc layer type. For instance, EIE targets fully-connected (FC) layers [41], while SCNN targets convolutional (CONV) layers [42].

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

295

2) Workload Imbalance and PE Utilization: With computation skipping for sparse data, the amount of work to be performed at each PE now depends on sparsity. Since the number of non-zero values varies across different layers, data types, or even regions within the same ﬁlter or feature map, it creates an imbalanced workload across different PEs and the throughput of the entire DNN accelerator will be bounded by the PE that has the most non-zero MACs. This leads to a decrease in PE utilization.
C. Contributions of This Work
To address these challenges, we present Eyeriss v2, a ﬂexible architecture for DNN processing that can adapt to a wide range of ﬁlter shapes and sizes used in compact DNNs such as MobileNet. This is achieved through the design of a highly ﬂexible on-chip network (NoC), which is currently the bottleneck for dealing with a more diverse set of DNNs. In addition, Eyeriss v2 also supports sparse DNNs by exploiting the sparsity in the weights and activations across a variety of DNN layers and translates them into improvements in both energy efﬁciency and processing speed. Finally, similar to the original Eyeriss, Eyeriss v2 does not make any assumption about whether the total storage capacity required by a DNN layer can ﬁt on-chip or not; instead, it optimizes the way to tile data of different types to achieve high on-chip reuse and energy efﬁciency. In summary, the contributions of this paper include:
• A novel NoC, called hierarchical mesh, that is designed to adapt to a wide range of bandwidth requirements. When data reuse is low, it can provide high bandwidth (via unicast) from the memory hierarchy to keep the PEs busy; when data reuse is high, it can still exploit spatial data reuse (via multicast or broadcast) to achieve high energy efﬁciency. For a compact DNN such as MobileNet, the hierarchical mesh increases the throughput by 5.6× and energy efﬁciency by 1.8×. (Section III)
• A PE that exploits the sparsity in weights and activations to achieve improved throughput and energy efﬁciency across a variety of DNN layers. Data is kept in compressed sparse column (CSC) format for both on-chip processing and off-chip access to reduce storage and data movement costs. Mapping of the weights to a PE is performed by taking the sparsity into account to increase reuse within PE, and can therefore reduce the impact of workload imbalance. Overall, exploiting sparsity results in an additional 1.2× and 1.3× improvement in throughput and energy efﬁciency, respectively, for MobileNet. (Section IV)
• A ﬂexible accelerator, Eyeriss v2, that combines the above contributions to efﬁciently support both compact and sparse DNNs. Eyeriss v2 running sparse MobileNet is 12.6× faster and 2.5× more energy efﬁcient than the original Eyeriss (scaled to the same number of PEs and storage capacity as Eyeriss v2), i.e., Eyeriss v1, running MobileNet (49.2M MACs). Eyeriss v2 is also 42.5× faster and 11.3× more energy efﬁcient with sparse AlexNet compared to Eyeriss v1 running AlexNet

Fig. 5. Comparison of the architecture of original Eyeriss and Eyeriss v2. (a) Original Eyeriss. (b) Eyeriss v2.
Fig. 6. Eyeriss v2 top-level architecture.
(724.4M MACs). Finally, Eyeriss v2 running sparse MobileNet is 225.1× faster and 42.0 × more energy efﬁcient than Eyeriss v1 running AlexNet. It is evident that supporting sparse and compact DNNs have a signiﬁcant impact on speed and energy consumption. (Section V)
II. ARCHITECTURE OVERVIEW Fig. 5 shows a comparison between the original Eyeriss [33] and the Eyeriss v2 architecture. Similar to the original Eyeriss architecture, Eyeriss v2 is composed of an array of processing elements (PE), where each PE contains logic to compute multiply-and-accumulate (MAC) and local scratch pad (SPad) memory to exploit data reuse, and global buffers (GLB), which serve an additional level of memory hierarchy between the PEs and the off-chip DRAM. Therefore, both the original Eyeriss and Eyeriss v2 have a two-level memory hierarchy. The main difference is that Eyeriss v2 uses a hierarchical structure, where the PEs and GLBs are grouped into clusters in order to support a ﬂexible on-chip network (NoC) that connects the GLBs to the PEs at low cost; in contrast, the original Eyeriss used a ﬂat multicast NoC between the GLB and PEs. As with the original Eyeriss, Eyeriss v2 uses separate NoCs to transfer each of the three data types, i.e., input activation (iact), weight, and partial sums (psums), between the GLBs and PEs, with each NoC tailored for the corresponding dataﬂow of that data type. Details of the NoC are described in Section III. Fig. 6 shows the top-level architecture of Eyeriss v2 and Table II summarizes the components in the architecture. It consists of 16 PE clusters and 16 GLB clusters arranged in an 8×2 array. Each PE cluster contains 12 PEs arranged in a 3 × 4 array. Each GLB cluster has a capacity of 12 KB and consists

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

296

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

TABLE II EYERISS V2 ARCHITECTURE HIERARCHY

Fig. 7. Common NoC Designs.

of SRAMs that are banked for different data types: iacts have three banks, each of which is 1.5 kB, and psums have four banks, each of which is 1.875 kB.
A hierarchical NoC is used to connect the PEs and GLBs: the PE and GLB clusters are connected through 2D mesh on-chip networks that consist of router clusters. Within each router cluster, there are 3, 3, and 4 routers for iact, weight and psum, respectively. Between the PE cluster and the router cluster, an all-to-all NoC is used to connect all the PEs to the routers for each data type. Between the GLB cluster and the router cluster, each router is paired with a speciﬁc port of the GLB cluster, which can read from and write to one SRAM bank or off-chip I/O. Therefore, data from either off-chip or a GLB cluster ﬁrst goes into the router cluster, and then can be unicast to the local PE cluster, multicast to PE clusters on the same row or column in the mesh network, or broadcast to all PE clusters. The decision is based on the shape and size of the DNN layer and the processing dataﬂow. The design motivation and implementation details of this hierarchical mesh network and the dataﬂow are described in Section III.
The data movement through the two-level memory hierarchy on Eyeriss v2 is as follows:
• iacts are read from off-chip into the GLB cluster, where they can be stored into the GLB memory or get passed directly to the router cluster depending on the conﬁguration.
• psums are always stored in the GLB memory once they get out of the PE cluster. The ﬁnal output activations skip the GLB cluster and go directly off-chip.
• weights are not stored in GLB and get passed to the router clusters and eventually stored in the SPads in each PE directly.
Eyeriss v2 adopts the Row-Stationary (RS) dataﬂow [43] used in the original Eyeriss, and further explores tiling the MAC operations spatially across PEs through any layer dimension, including the channel group dimension (G in Table I). This is especially important for layers such as the depth-wise (DW) CONV layers in MobileNet, which lacks the input and output channels that are commonly used for spatial tiling and therefore greatly improves the array utilization.
Each PE contains multiply-and-accumulate (MAC) datapaths designed to process 8-bit ﬁxed-point iacts and weights, which is the commonly accepted bitwidth for inference. Since many layers receive iacts after ReLU, the iacts can be set to either signed or unsigned, which further extends the scale of

iact representation. Psums are accumulated at 20-bit precision, which has shown no accuracy impact in our experiments. When the accumulation is done, the 20-bit psums are converted back to 8-bit output activations and sent off-chip. The PE contains separate SPads for iact, psum and weights. Details of the PE architecture are described in Section IV.
Finally, Eyeriss v2 has a two-level control logic similar to the original Eyeriss. The system-level control coordinates the off-chip data accesses and data trafﬁc between the GLB and PEs, and the lower-level control is within each PE and controls the progress of processing of each PE independently. The chip can be reconﬁgured to run the dataﬂow that maximizes the energy efﬁciency and throughput for the processing of each DNN layer. This includes setting up the speciﬁc data trafﬁc pattern of the NoCs, data accesses to the GLB and SPads, and workload distribution for each PE. For each layer, a 2134-bit command that describes the optimized conﬁguration is sent to the chip and accessed statically throughout the processing of this layer. Only one layer is processed at a time. When the processing for a layer is done, the chip is reconﬁgured for the processing of the next layer.
III. FLEXIBLE HIERARCHICAL MESH ON-CHIP NETWORK
One of the key features required to support compact DNNs is a ﬂexible and efﬁcient on-chip network (NoC). This section will provide details on the implementation of the NoC in Eyeriss v2 as well as describe how the NoC is conﬁgured for various use cases.
A. Motivation
The NoC is an indispensable part of modern DNN accelerators, and its design has to take the following factors into consideration: (1) support processing with high parallelism by efﬁciently delivering data between storage and datapaths, (2) exploit data reuse to reduce the bandwidth requirement and improve energy efﬁciency, and (3) can be scaled at a reasonable implementation cost.
Fig. 7 shows several NoC designs commonly used in DNN accelerators. Due to the property of DNN that data reuse for all data types cannot be maximally exploited simultaneously, a mixture of these NoCs is usually adopted for different data types. For example, a DNN accelerator can use a 1D horizontal multicast network to reuse the same weight across PEs in the same row and a 1D vertical multicast network to reuse the same iact across PEs in the same column. This setup will then require an unicast network that gathers the unique output activations from each PE. This combination, however,

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

297

Fig. 8. (a) High-level structure of the hierarchical mesh network (HM-NoC), and its different operating modes: (b) High bandwidth mode, (c) High reuse mode, (d) grouped-multicast mode, and (e) interleaved-multicast mode. In each mode, the colored arrows show the routing path; different colors denote the path for unique data.

implies that each weight needs to have the amount of reuse with different iacts at least equal to the width of the PE array, and the number of iact reuse with different weights at least equal to the height of the PE array. If these conditions are not fulﬁlled, the PE array will not be fully utilized, which will impact both throughput and energy efﬁciency.
While it was easy to satisfy such conditions with large DNNs, the rise of compact DNN models has made this design approach less effective. The key reason is that it is much more difﬁcult to assume the amount of data reuse or required data bandwidth for each data type, as it will vary across layers and DNN models. For example, the lack of input or output channels in the depth-wise layers of MobileNet or in the bottleneck layers of ResNet and GoogLeNet has made it very difﬁcult to efﬁciently utilize the aforementioned example well due to its rigid NoC design. In layers such as fully-connected layers, commonly used in RNNs and CNNs, it will also require a large batch size to improve the amount of reuse for weights, which can be challenging in real-time applications that are sensitive to the processing latency.
The varying amount of data reuse for each DNN data type across different layers or models pose a great challenge to the NoC design. The broadcast network can exploit the most data reuse, but its low source bandwidth can limit the throughput when data reuse is low. The unicast network can provide the most source bandwidth but misses out on the data reuse opportunity when available. Taking the best from both worlds, an all-to-all network that connects any data sources to any destinations can adapt to the varying amount of data reuse and bandwidth requirements. However, the cost of its design increases quadratically with the number of nodes, e.g., PEs, and therefore is difﬁcult to scale up to the amount of parallelism required for DNN accelerators.
B. High-Level Concept and Use Cases
To deal with this problem, we propose the hierarchical mesh network (HM-NoC) in Eyeriss v2 as shown in Fig. 8a. HM-NoC takes advantage of the all-to-all network, but solves the scaling problem by creating a two-level hierarchy. The all-to-all network is limited within the scope of a cluster at the lower level. In Eyeriss v2, there are only 12 PEs in each cluster, which effectively reduce the cost of the all-to-all network. At the top level, the clusters are further connected

with a mesh network. While this example shows a 2 ×1 mesh, Eyeriss v2 uses a 8×2 mesh. Scaling up the architecture at the cluster level with the mesh network is much easier than with the all-to-all network since the implementation cost increases linearly instead of quadratically.
Fig 8b to 8e shows how the HM-NoC can be conﬁgured into four different modes depending on the data reuse opportunity and bandwidth requirements.
• In the high bandwidth mode (Fig. 8b), each GLB bank or off-chip data I/O can deliver data independently to the PEs in the cluster, which achieves unicast.
• In the high reuse mode (Fig. 8c), data from the same source can be routed to all PEs in different clusters, which achieves broadcast.
• For situations where the data reuse cannot fully utilize the entire PE array with broadcast, different multicast modes, speciﬁcally grouped-multicast (Fig. 8d) and interleavedmulticast (Fig. 8e), can be adopted according to the desired multicast patterns.
Fig. 9 shows several example use cases of how HM-NoC adapts different modes for different types of layers. For simplicity, we are only showing a simpliﬁed case with 2 PE clusters with 2 PEs in each cluster, and it omits the NoC for psums. However, the same principles apply to NoC for all data types and at larger scales.
• Conventional CONV layers (Fig. 9a): In normal CONV layers, there is plenty of data reuse for both iacts and weights. To keep all 4 PEs busy at the lowest bandwidth requirement, we need 2 iacts and 2 weights from the data source (ignoring the reuse from SPad). In this case, either the HM-NoC for iact or weight has to be conﬁgured into the grouped-multicast mode, while the other one conﬁgured into the interleaved-multicast mode.
• Depth-wise (DP) CONV layers (Fig. 9b): For DP CONV layers, there can be nearly no reuse for iacts due to the lack of output channels. Therefore, we can only exploit the reuse of weights by broadcasting the weights to all PEs while fetching unique iacts for each PE.
• Fully-connected (FC) layers (Fig. 9c): Contrary to the DP CONV layers, FC layers usually see little reuse for weights, especially when the batch size is limited. In this case, the modes of iact and weight NoCs are swapped from the previous one: the weights are now unicast to the PEs while the iacts are broadcast to all PEs.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

298

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Fig. 9. Examples of weight and iact hierarchical mesh networks conﬁgured in different modes for different types of DNN layers: (a) CONV layers; (b) depthwise (DW) CONV layers; (c) fully-connected (FC) layers. Green arrows and blue arrows show the routing paths in the weight and iact NoC, respectively.

C. Implementation Details
To support the various uses cases described in Section III-B, each of the HM-NoC employs circuit-switched routing, which mainly consists of muxes and is statically conﬁgured by the conﬁguration bits as described in Section II. Therefore, the implementation cost of each router is very low. A separate HM-NoC is implemented for each data type (iact, psum, and weights) that is tailored for their given dataﬂow. The speciﬁcations of the routers for each data type are summarized in Table II. For iacts and weights, each port has a bitwidth of 24 bits such that it can send and receive three 8b uncompressed iact values or two 12b compressed iact run-data pairs per cycle. Section IV describes the compression format in more detail. For psum, each port has a bitwidth of 40-bit to send and receive two psums per cycle. We will now describe how the routers, GLB and PEs are connected in the HM-NoC for each data type.
1) HM-NoC for Input Activations: The HM-NoC implementation for iacts is shown in Fig. 10. There are three iact routers per router cluster, one for each iact SRAM bank in the GLB cluster. Each router for iact has four source ports (to receive data) and four destination ports (to transmit data). Three of the source and destination ports are used to receive and transmit data from the other clusters in the mesh, which are highlighted with bold arrows in Fig. 10; while a mesh network typically requires four pairs of source and destination ports, we only require three pairs since we only have 8 × 2 clusters and thus either the east or west port can be omitted. The fourth source port connects to the GLB cluster to receive data either from the memory bank or off-chip, and the fourth destination port connects to all the V PEs in the cluster. Thanks to the all-to-all network in the PE cluster, data from any router can go to any PE in the same cluster.
Fig. 11 shows the implementation details of the mesh network router for iacts. It has four source (src) and four destination (dst) ports. In addition to data (d), each port also has two additional signals, ready (r ) and enable (e), for hand-shaking. Each source port generates four enable signals (e.g., e00-e03), each for one destination port, based

Fig. 10. Hierarchical mesh network for input activations. This only shows the top 2 × 2 of the entire 8 × 2 cluster array.
Fig. 11. Implementation details of the mesh network router for input activations. Routers for the other data types use similar logic but with different numbers of ports. d, e, r and m are data, enable, ready and routing mode signals, respectively.
on its own enable signal and the statically conﬁgured routing mode (m). The routing mode can be one of the following: unicast, horizontal multicast, vertical multicast, or broadcast. It determines which ports can be enabled for passing data. For example, in the horizontal multicast mode, ports that connect to other routers in the vertical direction of the mesh network will not be enabled. At each destination port, the destinationspeciﬁc enable signals from all source ports (e.g., e00-e30 for destination port 0) go through an OR gate to generate the ﬁnal enable output. The ready signal from the destination ports to the source ports are generated in a similar fashion with the difference that the source-speciﬁc ready signals (e.g., r00r03 for source port 0) go through an AND gate to generated the ﬁnal ready output at each source port. The output data from all source ports (d0-d3) is MUXed at each destination port, and is chosen based on the enable signals from the

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

299

Fig. 12. Hierarchical mesh network for weights. This only shows the top 2 × 2 of the entire 8 × 2 cluster array.
source ports, i.e., the data from the enabled source port will be passed through.
2) HM-NoC for Weights: The HM-NoC implementation for weights is shown Fig. 12. There are three weight routers per router cluster, one for each row of PEs within a cluster. Since Eyeriss v2 uses the RS dataﬂow, a signiﬁcant amount of weight reuse can be exploited using the SPad within the PE, and only spatial reuse across horizontal PEs needs to be further exploited. Therefore, the implementation of the NoC for the weights can be simpliﬁed at both levels of the HM-NoC to reduce cost but still satisfy the ﬂexibility requirements. Speciﬁcally, the vertical connections of the 2D mesh between the clusters can be removed. Furthermore, within each cluster, each router only needs to connect to one row of PEs. Accordingly, each weight router has two source ports and two destination ports. A source port and a destination port are used to receive and transmit weights coming from neighboring cluster; again, we only need one pair of ports here since we only have 8 × 2 clusters and thus either the east or west port can be omitted. The second source port connects to the GLB cluster to receive data from off-chip, while the second destination port connects to one row of PEs within the cluster. The implementation of the mesh network router for weights is similar to that in Fig. 11.
3) HM-NoC for Partial Sums: The HM-NoC implementation for psums is shown in Fig. 13. There are four psum routers per router cluster, one for each psum SRAM bank in the GLB cluster or, equivalently, one for each column of PEs within a cluster. Similar to the weight NoC, the psum NoC is simpliﬁed for its given dataﬂow; speciﬁcally, the psums are only allowed to be accumulated across PEs in the vertical direction. This is due to the fact that, in the row-stationary dataﬂow, weights are reused across PEs horizontally, which makes it impossible to accumulate psums across PEs horizontally. Thus, the horizontal connections of the 2D mesh between the clusters can be removed since psums won’t be passed horizontally. Within each cluster, the PEs are vertically connected and each router in the cluster only needs to transmit the psum from the psum bank in the GLB cluster to the bottom of each PE

Fig. 13. Hierarchical mesh network for psums. This only shows the a 2 × 2 portion of the entire 8 × 2 cluster array.
column, and receive the updated psum from the top of the same PE column. Accordingly, each psum router has three source ports and three destination ports. One of the source ports is used to receive data from the neighboring router cluster to the north, while one of the destination ports is used to transmit data to the neighboring router cluster to the south. The second pair of source and destination ports are assigned to the psum bank in the GLB, while the third destination port is assigned to the bottom PE in a column of the PE cluster and the third source port is assigned to the top PE in a column of the PE cluster.
D. Scalability
A key design focus of the HM-NoC is to enable strong scaling for Eyeriss v2. In other words, as the architecture scales with more PEs, the performance, i.e., throughput, should scale accordingly for the same problem size. Performance, however, is a function of many factors, including the dataﬂow, NoC design, available on-chip and off-chip data delivery bandwidth, etc. To examine the impact of the HM-NoC, we will assume no limitation on the off-chip bandwidth and no workload imbalance (i.e., no sparsity) in the following scalability experiments.
We proﬁle the performance of Eyeriss v2 at three different scales: 256 PEs, 1024 PEs, and 16384 PEs, where each PE is capable of processing at 1 MAC/cycle. The PE cluster for all scales has a ﬁxed array size of 4 × 4 PEs, and the number of PE clusters scales at 4 × 4, 8 × 8, and 32 × 32. For comparison, we also examine the scalability of the original Eyeriss, i.e., Eyeriss v1, at the same set of scales. For Eyeriss v1, the PEs are arranged in square arrays, i.e., 16×16, 32×32, and 128×128. Both versions of Eyeriss use the row-stationary dataﬂow. For rapid evaluation of architectures at large scales, we have built an analytical model that can search for the operation mappings with the best performance at different scales considering the data distribution and bandwidth limitations of different NoC designs in the two versions of Eyeriss.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

300

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Fig. 14. Normalized performance of (a) Eyeriss v2 and (b) Eyeriss v1 running AlexNet, GoogLeNet, and MobileNet with a batch size of 1 at three different scales. Note that the MobileNet model has a width multiplier of 1.0 and an input size of 224 × 224, which is different from the MobileNet benchmarked in Section V.

Fig. 15. Processing in the PE.

Fig. 14a and 14b show the normalized performance of Eyeriss v2 and Eyeriss v1, respectively, running three DNNs: AlexNet, GoogLeNet, and MobileNet (with width multiplier of 1.0 and input size of 224 × 224)1 at the three different scales with a batch size of 1. For all three DNNs, the performance of Eyeriss v2 scales linearly from 256 to 1024 PEs, and achieves more than 85% of the linearly scaled performance at 16384 PEs. In contrast, the performance of Eyeriss v1 hardly improves when scaled up. This is due to the insufﬁcient bandwidth provided by the broadcast NoC in Eyeriss v1 as discussed in Section III-A. For example, the performance of the FC layers in AlexNet and depth-wise layers in MobileNet do not see any improvement going from 256 PEs to 16384 PEs in Eyeriss v1 due to the insufﬁcient NoC bandwidth for delivering weights and input activation, respectively, to the PEs. The HM-NoC in Eyeriss v2, however, is capable of adapting to the bandwidth requirements, therefore achieving higher performance at large scales. The HM-NoC is doing so while still being able to exploit available data reuse to achieve high energy efﬁciency, which will be demonstrated in Section V-A and V-B. Also note that, at large scales, the external data bandwidth will eventually become the performance bottleneck, and it will require more efforts to integrate the accelerator into the system to harness its full potential.
The implementation of the HM-NoC described in Section III-C targets the size of 8 × 2 PE clusters, and will require modiﬁcations when scaled up. Speciﬁcally, the mesh routers for input activations and weights need an extra pair of source and destination ports in order to handle data delivery for more than two columns of PE clusters. As the area and energy cost of the router grows with the number of ports, the overall cost will increase. However, the same routers can then be used for any architectural scales. Also, as will be shown in Section V, the entire NoC only accounts for less than 3% of the area and 6%-10% of the total energy consumption. The additional complexity in the routers is unlikely to add signiﬁcant cost. In addition, the proportion of cost of different components will stay roughly constant as the system scales thanks to the design of the hierarchical mesh network.
1The large MobileNet model used here is not used for performance and energy efﬁciency benchmarking in Section V since the post-place-and-route simulation turn-around time is not practical; the smaller MobileNet model also has the same accuracy as AlexNet, which makes it a better comparison.

IV. SPARSE PROCESSING WITH SIMD SUPPORT
In the original Eyeriss, sparsity of input activations (iacts), i.e., zeros, is exploited to improve energy efﬁciency by gating the switching of logic and data accesses. In Eyeriss v2, we want to exploit sparsity further in both weights and iacts and improve not only energy efﬁciency but also throughput. Whereas the original Eyeriss only used compression between the GLB and off-chip DRAM, in Eyeriss v2, we keep the data in compressed form all the way to the PE. Processing in the compressed domain provides beneﬁts in terms of reducing on-chip bandwidth requirements as well as on-chip storage, which can result in energy savings and throughput improvements. However, as compressed data often has variable length, this presents challenges in terms of how to manipulate the data (e.g., distributing data across PEs, and sliding window processing within the PE). In this section, we will introduce a new PE architecture that can process sparse data in the compressed domain for higher throughput. We will also introduce support for SIMD in the PE such that each PE can process two MACs per cycle.
A. Sparse PE Architecture
Fig. 15 illustrates how the PE processes uncompressed weights and iacts in the original Eyeriss, where M0 and C0 are the output and input channels processed within the PE, S is the ﬁlter width, and U is the stride. Recall that for the row-stationary dataﬂow, multiple 1-D rows of weights and iact are mapped to a given PE and processed in a sliding window fashion; here, the C0 × M0 rows of weights with width S are assigned to the PE, and the weights belong to M0 output channels and C0 input channels. For each iact, the PE runs through M0 MAC operations sequentially in consecutive cycles with the corresponding column of M0 weights in the weight matrix, and accumulates to M0 partial sums (psums). By going through a window of C0 × S iacts in the stream, the processing goes through all M0 × C0 × S weights in the matrix and accumulates to the same M0 psums. It then slides to the next window in the iact stream by replacing C0 × U iacts at the front of the window with new ones, and repeats the processing with the same weight matrix but accumulates to another set of psums. Note that the access pattern of weights goes through the entire weight matrix once sequentially in a column-major fashion for each window of iacts.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

301

To speed up the processing when the iacts and/or weights are sparse, the goal is to read only the non-zero data in the iact stream and the weight matrix for processing. In addition, we only want to perform the read when both iact and weights are non-zero. The challenge, however, is to correctly and efﬁciently address data for all three data types. For example, when jumping between non-zero iacts in a window, the access pattern of weights does not go through the weight matrix sequentially anymore. Instead, additional logic is required to fetch the corresponding column of weights for the non-zero iact, which is not deterministic. Similarly, when jumping between non-zero weights in a weight column, it also has to calculate the address of the corresponding psum instead of just incrementing the address by one. Since the access order is also not deterministic, prefetching from the weight SPad is very challenging.
In order to achieve the processing of sparse data as described above, we take advantage of the compressed sparse column (CSC) compression format similar to what is described in [41], [44]. For each non-zero value in the data, the CSC format records a count value that indicates the number of leading zeros from the previous non-zero value in the uncompressed data stream; this is similar to the run length in run length coding (RLC). The count value can then be used to calculate the address change between the non-zero data. The added advantage of CSC over RLC is that it has an additional address value that allows the data to be broken into segments (e.g., columns) for easy handling, which we will discuss next; this, of course, also adds overhead in the compression.
Both the iact and weights are encoded in the CSC format. For iacts, the data stream is divided into non-overlapping C0× U segments, and each segment is CSC encoded separately. Doing so enables sliding window processing, which replaces a segment of data with a new one from the stream when the window slides. Since the data length of each segment will be different after CSC coding, additional information is needed to address each encoded segment. Therefore, for each encoded segment, an address value is also recorded in the CSC format that indicates the start address of the encoded segment in the entire encoded stream. The ﬁlter weights are also encoded with CSC compression by dividing each column of M0 weights as a segment and encoding each segment separately. This helps enable fast access of each column of non-zero weights.
Fig. 16 shows an example of CSC compressed weights. The characters in the weight matrix indicate the locations of non-zero values. To read the non-zero weights from a speciﬁc column, e.g., column 1 (assuming indexing starts from 0), the PE ﬁrst reads address[1] and address[2] from the address vector in the CSC compressed weights, which gives the inclusive lower bound and non-inclusive upper bound of the addresses, i.e., 2 and 5, respectively, for reading the data and count vector. The ﬁrst address (in this example, address[1]) is the location of the ﬁrst non-zero weight in each column, highlighted in bold in Fig. 16, within the data vector; it then goes through the three non-zero weights in the column, i.e., c, d and e, to perform the computation. If there is no non-zero weight in a column, the location of the next ﬁrst non-zero value is repeated (e.g., since there are no non-zero

Fig. 16. Example of compressing sparse weights with compressed sparse column (CSC) coding. The ﬁrst non-zero weight in each column is highlighted in bold; the address vector points to the location of these weights within the data vector. If there are no non-zero weights in a column next location is repeated (e.g., repeated 6 in address vector reﬂects the all zero column between f and g).
values in column 3, the value 6 which is the location of g, is repeated such that the difference in consecutive address values is zero, which reﬂects the all zero column.). At the same time, the corresponding addresses of the psums to update can be calculated by accumulating the counts from the count vector.
In the CSC format, the count vector is an overhead in addition to the non-zero data. If the bitwidth of the count is low, it may affect the compression efﬁciency when sparsity is high since the number of consecutive zeros can exceed the maximum count. If the count bitwidth is high, however, the overhead of the count vector becomes more signiﬁcant. From our experiments, setting each count at 4b yields the best compression rate for the 8b iact and weights. Therefore, each count-data pair is 12b and is stored in a 12b word of the data SPad for both iact and weight. This is similar to setting the run-length in the RLC, where 5b was allocated to the run-length in [33].
In summary, both the weights and iacts can be processed directly in the CSC format. The processing can skip the zeros entirely without spending extra cycles, thus improving the processing throughput as well as energy efﬁciency.
Fig. 17 shows the block diagram of the sparse PE that can perform the processing of CSC encoded iacts and weights directly as described above. Processing only non-zero data in the compressed format introduces read dependencies. For the compressed format, the address must be read before the data-count pair. To ensure only non-zero values are read, iact is read before the weight such that a non-zero weight is only read when the corresponding iact is non-zero. To handle these dependencies while still maintaining throughput, the PE is implemented using seven pipeline stages and ﬁve SPads. The ﬁrst two pipeline stages are responsible for fetching non-zero iacts from the SPads. The iact address SPad stores the address vector of the CSC compressed iacts, which is used to address the reads from the iact data SPad that holds the non-zero data vector as well as the count vector. After a non-zero iact is fetched, the next three pipeline stages read the corresponding weights. Similarly, there is a weight address SPad to address the reads from the weight data SPad for the correct column of weights. The ﬁnal two stages in the pipeline perform the MAC computation on the fetched non-zero iact and weight,

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

302

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

TABLE III
DISTRIBUTION OF WEIGHTS IN SPARSE ALEXNET TO THE SPAD IN EACH PE OF EYERISS V2

Fig. 17. Eyeriss v2 PE Architecture. The address SPad for both iact and weight are used to store addr vector in the CSC compressed data, while the data SPad stores the data and count vectors. The text in red denote changes for SIMD (Section IV-B).
and then send the updated psum either back to the psum SPad or out of the PE.
Since either iact and weight can be zero or non-zero, there are three possible scenarios:
• If the iact is zero, the CSC format will ensure that it is not read from the spad and therefore no cycles are wasted.
• If the iact is not zero, its value will be fetched from the iact data SPad and passed to the next pipeline stage.
– If there are non-zero weights corresponding to the non-zero iacts, they will be passed down the pipeline for computation. The zero weights will be skipped since the weights are also encoded with the CSC format.
– If there are no non-zero weights corresponding to the non-zero iacts, the non-zero iacts will not be further passed down in the pipeline. This may not necessarily introduce bubbles in the pipeline since the later stages, i.e., after the weight data Spad stage, can still be working on the computation for the previous nonzero iact if it has multiple corresponding weights.
In the Eyeriss v2 PE, the sizes of the iact address and data SPads are 9 × 4b and 16 × 12b, respectively, which allow for a maximum iact window size of 16. The sizes of the weight address and data SPads are 16 × 7b and 96 × 24b, respectively. This allows for a maximum weight matrix size of 96 × (24b/12b) = 192. The size of the psum SPad is 32 × 20b, and allows for a maximum weight matrix height of 32 (i.e., maximum number of output channels M0). If we fully utilize the iact SPads and psum SPad, it will require a weight matrix size of 32 × 16 = 512, which is larger than the limit of 192; however, the sparse PE design takes advantage of the fact that the sparse pattern of weights is known at compile time; therefore, it is possible to guarantee that the compressed weights will ﬁt in a smaller SPad. Table III shows how many weights are stored in the SPad of each PE for sparse AlexNet. While in most layers the nominal number of weights is higher than 192, the number of non-zero weights after compression in the worst case is smaller and ﬁts in the SPad for processing. By mapping more non-zero weights into each PE instead of mapping based on the nominal number of weights, more operations are performed in a PE, which statistically helps to reduce the amount of workload imbalance caused by the sparsity.

Since the degree of sparsity varies across different DNNs and data types, the PE is also designed to adapt to the scenarios when sparsity is low. In such cases, the PE can directly take in uncompressed iacts and weights instead of the CSC compressed versions to reduce the overhead in data trafﬁc. Both iact and weight address SPads are not used and therefore clock-gated to save energy consumption, and the count in the CSC format is ﬁxed to zero to address the data SPads correctly for processing.
B. SIMD Support in PE
Proﬁling results of the PE implementation shows that the area and energy consumption of the MAC unit is insigniﬁcant compared to other components in a PE. In Eyeriss, for example, the MAC unit takes less than 5% of the PE area, and only consumes 2%–9% of the PE power. This motivates the exploration of SIMD processing in a PE in order to achieve speedup of at most two times.
SIMD is applied to the PE architecture as shown in Fig. 17 by fetching two weights instead of one for computing two MAC operations per cycle with the same iact, i.e., a SIMD width of two. The changes are noted in the red text in the ﬁgure. SIMD processing not only improves the throughput but also further reduces the number of iact reads from the SPad. In terms of architectural changes, SIMD requires the word width of the weight data SPad to be two-word wide, which is why the size of the weight data SPad is 96 × 24b instead of 192 × 12b. The psum SPad also has to have two read and two write ports for updating two psums per cycle. In the case where only an odd number of non-zero weights exist in the column of M0 weights, the second 12b of the last 24b word in a column of non-zero weights is ﬁlled with zero. When the PE logic encounters the all-zero count-data pair, it clockgates the second MAC datapaths as well as the read and write of the second ports in the psum SPad to avoid unnecessary switching, which reduces power consumption.
V. IMPLEMENTATION RESULTS
Eyeriss v2 was implemented in a 65nm CMOS process and the speciﬁcations of the design are summarized in Table IV. The design was placed-and-routed and the results reported in this section are from post-layout cycle-accurate gate-level simulations with (1) technology library from the worst PVT corner, (2) switching activities proﬁled from running the

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

303

TABLE IV EYERISS V2 SPECIFICATIONS

TABLE V
KEY DIFFERENCES BETWEEN THE THREE EYERISS VARIANTS. THE AREA IS LOGIC ONLY

Fig. 18. Eyeriss v2 area breakdown. (a) Overall Area Breakdown. (b) PE Area Breakdown.
actual weights of the DNNs and data from the ImageNet dataset [1], and (3) a batch size of one, which represents a more challenging setup for energy efﬁciency and throughput, but captures the low latency use case.
The overall gate count of Eyeriss v2, excluding SRAMs, is approximately 2695k NAND-2 gates. The area breakdown (Fig. 18) shows that the 192 PEs dominates the area cost, while the area of the hierarchical mesh networks of all data types combined only account for 2.6% of the area. This result proves that it is possible to build in high ﬂexibility at a low cost. Within each PE, all of the SPads combined account for around 72% of the area, while the two MAC units only account for 5%.
A. Performance Analysis
To demonstrate the throughput and energy efﬁciency improvements brought on by the hierarchical mesh network and sparse PE architecture, we have implemented three different variants of Eyeriss: v1, v1.5, and v2. Table V lists the key differences between these Eyeriss variants. For the PE architecture, Dense means the PE can only clock-gate the cycles with zero data but not skip it, while Sparse means the PE can further skip the processing cycles with zero data. Eyeriss v1 is the same design as the original Eyeriss [33], but with the storage capacity, number of PEs and data precision scaled to the same level as v1.5 and v2 for a fair comparison. In short, the comparison between v1 and v1.5 shows the impact of the hierarchical mesh network, while the comparison between v1.5 and v2 shows the impact of the sparse PE architecture along with the support for SIMD processing. These architectures are placed-and-routed and benchmarked with four DNNs that have the same accuracy on the ImageNet

dataset: AlexNet [25], MobileNet (with a width multiplier of 0.5 and input size of 128×128) [10], and the sparse version of them as pruned by the method introduced in [14]. In this section, unless otherwise speciﬁed, AlexNet and MobileNet are referring to the dense model.
The implementation shows that Eyeriss v2 has an area increase of around two times compared to the other versions. The increase in mostly in the PE, which is 73% larger than the original one. The main reason is due to the need to support sparse processing, which requires deeper pipelining in the control logic and additional SPads to store the CSC compressed data. This contributes to a nearly 50% increase in area. Supporting SIMD also contributes to an additional 15% area increase due to the two sets of read and write ports for the psum SPad and the wider bus-width of the PE I/O in addition to the doubling of MAC units.
1) AlexNet: Fig. 19a shows the throughput improvements of different versions of Eyeriss on AlexNet over Eyeriss v1. Results on sparse AlexNet are also included (yellow bars) along with a breakdown of the processing latency across the different layers shown in Fig. 20. For AlexNet, the result shows that Eyeriss v1.5 signiﬁcantly speeds up FC layers. This is because the throughput of FC layers is bandwidthlimited in Eyeriss v1, which is addressed by the hierarchical mesh network in Eyeriss v1.5. Eyeriss v2, on the contrary, signiﬁcantly speeds up the CONV layers over Eyeriss v1.5 due to the increased number of multipliers and sparsity in the activations. However, the throughput of the FC layers only shows a marginal improvement because the FC layers are still bandwidth-limited even with the hierarchical mesh network. Therefore, speeding up the processing with sparsity and SIMD does not improve the throughput of FC layers as signiﬁcantly as in CONV layers.
The full potential of Eyeriss v2, however, is fully revealed when coupled with sparse AlexNet. The bandwidth requirement of weights is lower in sparse AlexNet since it is very sparse, and the CSC compression can effectively reduce the data trafﬁc. As a result, exploiting sparsity becomes more effective. Overall, Eyeriss v2 achieves 42.5× speedup with sparse AlexNet over Eyeriss v1 with AlexNet.
Fig. 19b shows the improvement on energy efﬁciency. It largely correlates to the speedup in Fig. 19a since the higher overall utilization of the PEs reduces the proportion of the static power consumption, e.g., clock network. Overall, Eyeriss v2 with sparse AlexNet is 11.3× more energy efﬁcient than Eyeriss v1 with AlexNet.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

304

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Fig. 19. (a) Speedup and (b) energy efﬁciency improvement of different versions of Eyeriss over Eyeriss v1 benchmarked with AlexNet.

Fig. 20. Breakdown of processing latency across the different layers of sparse AlexNet running on Eyeriss v2.
2) MobileNet (Width Multiplier of 0.5, Input Size of 128 × 128): Fig. 21a and 21b show the improvement on throughput and energy efﬁciency, respectively, of different versions of Eyeriss on selected layers of MobileNet over Eyeriss v1. Results on sparse MobileNet are also included (yellow bars). The lack of data reuse in MobileNet results in low throughput on Eyeriss v1 due to the low-bandwidth NoC, which is why Eyeriss v1.5 can achieve a signiﬁcant speedup over v1. However, the speedup of Eyeriss v2 over v1.5 is a mixed bag. While layers such as CONV1 and the point-wise (PW) layers can still take advantage of the sparsity in input activations to improve the throughput, the throughput of the Depthwise (DW) CONV layers becomes worse. This is because the CSC compression does not create skippable cycles when the number of input and output channels are both one. Therefore, the sparse PE in Eyeriss v2 does not bring any advantage over the dense PE in Eyeriss v1.5. Furthermore, the deeper pipeline of the sparse PE actually makes the throughput slightly worse in the DW CONV layers.
Sparse MobileNet brings additional beneﬁts on throughput and energy efﬁciency on Eyeriss v2; however, the improvement is not as signiﬁcant as with the sparse AlexNet, since the CSC compression is less effective on sparse MobileNet than on sparse AlexNet due to its small layer sizes. Overall, Eyeriss v2 with sparse MobileNet is 12.6× faster and 2.5× more energy efﬁcient than Eyeriss v1 with MobileNet.
B. Benchmark Results
Table VI summarizes the throughput and energy efﬁciency of Eyeriss v2 benchmarked with four DNNs that have comparable accuracy at a batch size of one. Although Eyeriss v2 achieves the highest GOPS/W2 with sparse AlexNet, it consumes the least amount of time and energy per inference
2In this paper, we calculate GOPS based on the nominal number of operations in the DNN, i.e., including operations with data values of zero.

with the sparse MobileNet. This result echoes the trend of DNN development going toward compact models that are more lightweight but also have less reuse for the hardware to explore, which makes it harder to reduce GOPS/W but can still improve inference/J. Also, Eyeriss v2 achieves 12.6× higher inference/sec for MobileNet than AlexNet, which correlates well to the 14.7× reduction in the nominal number of MACs. This proves that the design has high ﬂexibility to perform well for compact DNN models.
Fig. 22 shows the normalized power breakdown of Eyeriss v2 running a variety of DNN layers. We pick a representative set of layers to show how the different characteristics of the DNN layers impact the hardware. Note that these layers have different energy consumption and efﬁciency. The results are summarized as follows:
• CONV1 of AlexNet (148.1 GOPS/W) shows the case of no sparsity in both activations and weights. Compared to other layers, the high utilization of the PEs makes the proportion of the clock network power consumption low. It also has the highest proportion of MAC power consumption.
• CONV3 of sparse AlexNet (1423.2 GOPS/W) has the highest amount of sparsity in all layers we have tested. Compared to CONV3 of AlexNet (392.0 GOPS/W), the proportions of the clock network, HM-NoC and GLB power consumption are higher. This is mainly due to the workload imbalance induced by sparsity, which lowers the utilization of the active PEs. However, judging from the large proportion of the SPad and MAC power consumption compared to other components such as PE control logic, the PE is still kept fairly busy and data reuse is effectively exploited by the SPads.
• CONV13 DW layer of MobileNet (77.7 GOPS/W) has the lowest GOPS/W among all the layers we have tested. As expected, most of the energy is spent on the clock network. Inside the PE, the lack of reuse and not being able to utilize SIMD also hurt the energy efﬁciency, which is evident by the fact that most of the energy is spent in the control logic instead of the SPads or MACs.
• FC8 of sparse AlexNet (465.1 GOPS/W) shows the case of high sparsity and low data reuse. This combination makes the architecture more bandwidth-limited, and therefore the utilization of active PEs becomes low. That is why this layer has the highest proportion of power consumed by the clock network. The lack of reuse also makes the proportion of the SPad power consumption low

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

305

Fig. 21. (a) Speedup and (b) energy efﬁciency improvement of different versions of Eyeriss over Eyeriss v1 benchmarked with MobileNet. Due to the large number of layers, only a few representative layers are presented.
TABLE VI THROUGHPUT AND ENERGY EFFICIENCY OF EYERISS V2 BENCHMARKED WITH FOUR DNNS THAT HAVE COMPARABLE ACCURACY AND A BATCH
SIZE OF 1. NOTE THAT THE MOBILENET USED FOR BENCHMARK HAS A WIDTH MULTIPLIER OF 0.5 AND AN INPUT SIZE OF 128 × 128

will put more pressure on the external data bandwidth, which should be addressed in the design of future DNN models.

Fig. 22. Eyeriss v2 power breakdown running different DNN layers.
and the NoC power consumption high. However, thanks to sparsity, the overall energy efﬁciency of this layer is still better than CONV1 of AlexNet.
In terms of external DRAM accesses, AlexNet requires much more data than MobileNet as shown in Table VI, which is mainly due to the large amount of weights in the fullyconnected layers. For CONV layers only, the required DRAM accesses are 7.1 MB and 4.6 MB for AlexNet and sparse AlexNet, respectively. Note that Eyeriss v2 does not perform the pooling layers on-chip, and the required DRAM accesses will further decrease if pooling layers are processed on-chip. We have also proﬁled the impact of a limited peak external bandwidth on the performance. With an aggregated external read and write bandwidth of 25600 MB/s, which is at the level of DDR4-3200, the throughput of Eyeriss v2 running sparse AlexNet and sparse MobileNet will decrease by 16% and 24%, respectively, due to the bursty external data access patterns. However, we believe that additional on-chip buffering can alleviate the performance degradation, which we will leave for future endeavors. This result also conﬁrms that, with efﬁcient hardware that can maximize utilization even when data reuse is low, DNNs that do not have enough data reuse to exploit

C. Comparison With Prior Art
Table VII shows the comparison between Eyeriss v2 and the state-of-the-art prior art. Eyeriss v2 is the ﬁrst one to report benchmark results on both large DNNs, e.g., AlexNet, and compact DNNs, e.g., MobileNet. For AlexNet, Eyeriss v2 still achieves comparable throughput and slightly less energy efﬁciency compared to other works that are tailored for the large models. This result is achieved with a batch size of one (while other results use larger batch sizes), and the overhead associated with its additional ﬂexibility to handle the drastically different layer shapes in the compact models. We report results for Eyeriss v2 on a sparse network which is a widely used approach for large DNN models, particularly on mobile devices; unfortunately, the available results for the other works are only on AlexNet. We would expect the sparse AlexNet to potentially provide additional energy efﬁciency improvements on those works, but not throughput improvements.
For MobileNet, Eyeriss v2 achieves 5.3× throughput improvement and 3.9× energy improvement over AlexNet, with the same accuracy. Although the other designs do not report results for MobileNet, our understanding of those designs leads us to believe that they would not achieve comparable improvements, similar to the original Eyeriss, due to the NoC limitations as well as additional mapping inefﬁciencies of the dataﬂow. However, we conjecture that the NoC limitations can be addressed by the proposed HM-NoC.
D. Discussion
Eyeriss v2 focuses its design on improving the throughput and energy efﬁciency for compact and sparse DNN models,

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

306

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

TABLE VII
COMPARISON WITH STATE-OF-THE-ART DESIGNS. FOR EYERISS V2, THE THROUGHPUT AND ENERGY EFFICIENCY ARE BENCHMARKED ON THE SPARSE VERSION OF ALEXNET AND MOBILENET

which is very different from the direction taken in many of the state-of-the-art previous works. With a similar amount of resources, i.e., area, Eyeriss v2 has much fewer number of MACs. However, with the ﬂexibility of the on-chip network and the sparse processing logic that effectively improve throughput based on the sparsity of the data, Eyeriss v2 still achieves comparable throughput and energy efﬁciency for large DNNs against the state-of-the-art that optimizes directly for them. Furthermore, Eyeriss v2 shows a signiﬁcant throughput and energy efﬁciency improvement on sparse MobileNet against Eyeriss v1 as shown in Section V-A.
Supporting sparse processing is a challenging task from an architecture design point of view. First of all, the PE design complexity and cost becomes much higher due to the additional required logic and storage. This has resulted in a signiﬁcant increase in area as shown in Table V. In addition, it makes the support for high SIMD width processing difﬁcult because of the workload imbalance and the high cost in the SPad due to the non-deterministic access patterns. Eyeriss v2, however, still demonstrates a design that can effectively translate the sparsity to signiﬁcant throughput and energy efﬁciency improvement as compared to Eyeriss v1.
It is worth noting that the ﬂexibility provided by the hierarchical mesh network and the throughput boost from the sparse processing logic can be applied separately. Therefore, if sparse networks are not the target workload, the ﬂexible NoC can still be used in conjunction with other techniques such as lower precision and higher parallelism to achieve higher throughput and energy efﬁciency.
VI. CONCLUSION
DNNs are rapidly evolving due to the signiﬁcant amount of research in the ﬁeld; however, the current direction of DNN development also brings new challenges to the design of DNN accelerators due to the widely varying layer shapes in compact DNNs and the varying data sparsity in sparse DNNs. In this work, we propose a new DNN accelerator architecture, called Eyeriss v2, that addresses these challenges. First, the varying layer shapes makes the on-chip network (NoC) the performance bottleneck since conventional NoC design

poses strong assumptions on the amount of data reuse and required data bandwidth for each data type, which is too rigid to adapt. We solve this problem by introducing the hierarchical mesh network (HM-NoC). HM-NoC can be conﬁgured into different modes that can deliver from high bandwidth to high data reuse. More importantly, its implementation cost is also minimized through the hierarchical design that limit the costly all-to-all communication within local clusters as well as the circuit-switched routing. This helps to bring over an order of magnitude speedup for processing MobileNet compared to the original Eyeriss, i.e., Eyeriss v1, scaled to the same number of multipliers and storage capacity as Eyeriss v2. Furthermore, Eyeriss v2 incorporates a new PE architecture that support processing sparse weights and input activations directly in compressed domain to improve not only energy efﬁciency but also throughput. It also adds SIMD support so that each PE can process 2 MACs per cycles. Overall, Eyeriss v2 achieves 42.5× and 11.3× improvement in throughput and energy efﬁciency, respectively, with sparse AlexNet compared to Eyeriss v1 running AlexNet; it also achieves 12.6× and 2.5× improvement in throughput and energy efﬁciency, respectively, with sparse MobileNet compared to Eyeriss v1 running MobileNet.
REFERENCES
[1] O. Russakovsky et al., “ImageNet large scale visual recognition challenge,” Int. J. Comput. Vis., vol. 115, no. 3, pp. 211–252, Dec. 2015.
[2] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, “Efﬁcient processing of deep neural networks: A tutorial and survey,” Proc. IEEE, vol. 105, no. 12, pp. 2295–2329, Dec. 2017.
[3] M. Courbariaux, Y. Bengio, and J.-P. David, “BinaryConnect: Training deep neural networks with binary weights during propagations,” in Proc. NIPS, 2015, pp. 3123–3131.
[4] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “XNOR-Net: ImageNet classiﬁcation using binary convolutional neural networks,” in Proc. ECCV, 2016, pp. 525–542.
[5] E. H. Lee, D. Miyashita, E. Chai, B. Murmann, and S. S. Wong, “LogNet: Energy-efﬁcient neural networks using logarithmic computation,” in Proc. ICASSP, Mar. 2017, pp. 5900–5904.
[6] F. Li and B. Liu, “Ternary weight networks,” in Proc. NIPS Workshop Efﬁcient Methods Deep Neural Netw., 2016.
[7] B. Moons, B. De Brabandere, L. Van Gool, and M. Verhelst, “Energyefﬁcient ConvNets through approximate computing,” in Proc. WACV, Mar. 2016, pp. 1–8.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

CHEN et al.: EYERISS V2: A FLEXIBLE ACCELERATOR FOR EMERGING DNNS ON MOBILE DEVICES

307

[8] P. Judd, J. Albericio, T. Hetherington, T. M. Aamodt, N. E. Jerger, and A. Moshovos, “Proteus: Exploiting numerical precision variability in deep neural networks,” in Proc. Int. Conf. Supercomput., 2016, Art. no. 23.
[9] C. Szegedy et al., “Going deeper with convolutions,” in Proc. IEEE CVPR, Jun. 2015, pp. 1–9.
[10] A. G. Howard et al., “MobileNets: Efﬁcient convolutional neural networks for mobile vision applications,” CoRR, Apr. 2017.
[11] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer. (2016). “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5 MB model size.” [Online]. Available: https:// arxiv.org/abs/1602.07360
[12] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in Proc. NIPS, 1990, pp. 598–605.
[13] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and connections for efﬁcient neural network,” in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 1135–1143.
[14] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convolutional neural networks using energy-aware pruning,” in Proc. CVPR, Jul. 2017, pp. 6071–6079.
[15] B. Moons, R. Uytterhoeven, W. Dehaene, and M. Verhelst, “14.5 envision: A 0.26-to-10 TOPS/W subword-parallel dynamic-voltageaccuracy-frequency-scalable convolutional neural network processor in 28 nm FDSOI,” in ISSCC Dig. Tech. Papers, San Francisco, CA, USA, Feb. 2017, pp. 246–247.
[16] S. Yin et al., “A 1.06-to-5.09 TOPS/W reconﬁgurable hybrid-neuralnetwork processor for deep learning applications,” in Proc. Symp. VLSI Circuits, Jun. 2017, pp. C26–C27.
[17] J. Lee, C. Kim, S. Kang, D. Shin, S. Kim, and H.-J. Yoo, “UNPU: A 50.6 TOPS/W uniﬁed deep neural network accelerator with 1b-to16b fully-variable weight bit-precision,” in ISSCC Dig. Tech. Papers, San Francisco, CA, USA, Feb. 2018, pp. 218–220.
[18] S. Sharify, A. D. Lascorz, K. Siu, P. Judd, and A. Moshovos, “Loom: Exploiting weight and activation precisions to accelerate convolutional neural networks,” in Proc. 55th Annu. Design Automat. Conf., 2018, pp. 20:1–20:6.
[19] P. Judd, J. Albericio, T. Hetherington, T. M. Aamodt, and A. Moshovos, “Stripes: Bit-serial deep neural network computing,” in Proc. 49th Annu. IEEE/ACM Int. Symp. Microarchitecture, Oct. 2016, pp. 19:1–19:12.
[20] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, “YodaNN: An ultralow power convolutional neural network accelerator based on binary weights,” in Proc. ISVLSI, Jul. 2016, pp. 236–241.
[21] K. Ando et al., “BRein memory: A 13-layer 4.2 K neuron/0.8 M synapse binary/ternary reconﬁgurable in-memory deep neural network accelerator in 65 nm CMOS,” in Proc. Symp. VLSI Circuits, Jun. 2017, pp. C24–C25.
[22] Z. Jiang, S. Yin, M. Seok, and J.-S. Seo, “XNOR-SRAM: In-memory computing SRAM macro for binary/ternary deep neural networks,” in Proc. IEEE Symp. VLSI Technol., Jun. 2018, pp. 173–174.
[23] D. Bankman, L. Yang, B. Moons, M. Verhelst, and B. Murmann, “An always-on 3.8 µJ/86% CIFAR-10 mixed-signal binary CNN processor with all memory on chip in 28-nm CMOS,” in ISSCC Dig. Tech. Papers, 2018, pp. 222–224.
[24] H. Valavi, P. J. Ramadge, E. Nestler, and N. Verma, “A mixed-signal binarized convolutional-neural-network accelerator integrating dense weight storage and multiplication for reduced data movement,” in Proc. IEEE Symp. VLSI Circuits, Jun. 2018, pp. 141–142.
[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation with deep convolutional neural networks,” in Proc. NIPS, 2012, pp. 1097–1105.
[26] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” CoRR, Sep. 2014.
[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE CVPR, Jun. 2016, pp. 770–778.
[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proc. CVPR, Jun. 2016, pp. 2818–2826.
[29] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning with limited numerical precision,” CoRR, Jun. 2015.
[30] Z. Du et al., “ShiDianNao: Shifting vision processing closer to the sensor,” in Proc. ISCA, 2015, pp. 92–104.
[31] Nvidia. (2017). NVDLA Open Source Project. [Online]. Available: http://nvdla.org/
[32] N. P. Jouppi et al., “In-datacenter performance analysis of a tensor processing unit,” in Proc. ACM/IEEE 44th Annu. Int. Symp. Comput. Archit., Jun. 2017, pp. 1–12.

[33] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, “Eyeriss: An energyefﬁcient reconﬁgurable accelerator for deep convolutional neural networks,” IEEE J. Solid-State Circuits, vol. 52, no. 1, pp. 127–138, Jan. 2017.
[34] Y.-H. Chen, T.-J. Yang, J. Emer, and V. Sze, “Understanding the limitations of existing energy-efﬁcient design approaches for deep neural networks,” in Proc. SysML, 2018.
[35] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for semantic segmentation,” in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2015, pp. 1520–1528.
[36] A. Dosovitskiy et al., “FlowNet: Learning optical ﬂow with convolutional networks,” in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2015, pp. 2758–2766.
[37] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab, “Deeper depth prediction with fully convolutional residual networks,” in Proc. 4th Int. Conf. 3D Vis. (3DV), Oct. 2016, pp. 239–248.
[38] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672–2680.
[39] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network computing,” in Proc. ISCA, 2016, pp. 1–13.
[40] S. Zhang et al., “Cambricon-x: An accelerator for sparse neural networks,” in Proc. ISCA, 2016, Art. no. 20.
[41] S. Han et al., “EIE: Efﬁcient inference engine on compressed deep neural network,” in Proc. ISCA, Jun. 2016, pp. 243–254.
[42] A. Parashar et al., “SCNN: An accelerator for compressed-sparse convolutional neural networks,” in Proc. ACM/IEEE 44th Annu. Int. Symp. Comput. Archit. (ISCA), Jun. 2017, pp. 27–40.
[43] Y.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial architecture for energy-efﬁcient dataﬂow for convolutional neural networks,” in Proc. 43rd Int. Symp. Comput. Archit. (ISCA), 2016, pp. 367–379.
[44] R. Dorrance, F. Ren, and D. Markovic´, “A scalable sparse matrixvector multiplication kernel for energy-efﬁcient sparse-blas on FPGAs,” in Proc. ISFPGA, 2014, pp. 161–170.
Yu-Hsin Chen (S’11) received the B.S. degree in electrical engineering from National Taiwan University, Taipei, Taiwan, in 2009, and the M. S. and Ph.D. degrees in electrical engineering and computer science (EECS) from the Massachusetts Institute of Technology (MIT), Cambridge, MA, USA, in 2013 and 2018, respectively.
Since 2018, he has been a Research Scientist with the Nvidia’s Architecture Research Group, Santa Clara, CA, USA. His current research focuses on the design of computer architectures for machine learning, deep learning, and domain-speciﬁc processors. In 2018, he received the Jin-Au Kong Outstanding Doctoral Thesis Prize in electrical engineering from MIT. He was a recipient of the 2015 Nvidia Graduate Fellowship, the 2015 ADI Outstanding Student Designer Award, and the 2017 IEEE SSCS Predoctoral Achievement Award. His work on the dataﬂows for CNN accelerators was selected as one of the Top Picks in computer architecture in 2016. He also co-taught a tutorial on Hardware Architectures for Deep Neural Networks at MICRO-49, ISCA2017, and MICRO-50.
Tien-Ju Yang (S’11) received the B.S. degree in electrical engineering and the M.S. degree in electronics engineering from National Taiwan University (NTU), Taipei, Taiwan, in 2010 and 2012, respectively. He is currently pursuing the Ph.D. degree in electrical engineering and computer science with the Massachusetts Institute of Technology, Cambridge, MA, USA, with a focus on energy-efﬁcient deep neural network design. From 2012 to 2015, he was with the Intelligent Vision Processing Group, MediaTek Inc., Hsinchu, Taiwan, as an Engineer. His research interest spans the area of computer vision, machine learning, image/video processing, and VLSI system design. He won the First Place in the 2011 National Taiwan University Innovation Contest.

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

308

IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS, VOL. 9, NO. 2, JUNE 2019

Joel S. Emer (M’73–SM’03–F’04) received the B.S. (Hons.) and M.S. degrees in electrical engineering from Purdue University, West Lafayette, IN, USA, in 1974 and 1975, respectively, and the Ph.D. degree in electrical engineering from the University of Illinois at Urbana–Champaign, Champaign, IL, USA, in 1979.
He was with Intel, where he was an Intel Fellow and the Director of Microarchitecture Research. At Intel, he led the VSSAD Group, where he was a member of Compaq and Digital Equipment Corporation. He is currently a Senior Distinguished Research Scientist with the Nvidia’s Architecture Research Group, Westford, MA, USA, where he is responsible for the exploration of future architectures and modeling and analysis methodologies. He is also a Professor of the Practice with the Massachusetts Institute of Technology, Cambridge, MA, USA, where he teaches computer architecture and supervises graduate students. He has held various research and advanced development positions investigating processor microarchitecture and developing performance modeling and evaluation techniques. He has made architectural contributions to a number of VAX, Alpha, and X86 processors and is recognized as one of the developers of the widely employed quantitative approach to processor performance evaluation. He has been recognized for his contributions in the advancement of simultaneous multithreading technology, processor reliability analysis, cache organization, and spatial architectures for deep learning. Dr. Emer is a fellow of the ACM. He has been a recipient of numerous public recognitions. In 2009, he received the Eckert-Mauchly Award for lifetime contributions in computer architecture, the Purdue University Outstanding Electrical and Computer Engineer Alumni Award, and the University of Illinois Electrical and Computer Engineering Distinguished Alumni Award in 2010 and 2011, respectively. His 1996 paper on simultaneous multithreading received the ACM/SIGARCH-IEEE-CS/TCCA: Most Inﬂuential Paper Award in 2011. He was named to the ISCA and Micro Halls of Fame in 2005 and 2015, respectively. He has had ﬁve papers selected for the IEEE Micro’s Top Picks in Computer Architecture, in 2003, 2004, 2007, 2013, 2015, and 2016. He was the Program Chair of ISCA in 2000 and Micro in 2017.

Vivienne Sze (S’04–M’10–SM’16) received the B.A.Sc. degree (Hons.) in electrical engineering from the University of Toronto, Toronto, ON, Canada, in 2004, and the S.M. and Ph.D. degree in electrical engineering from the Massachusetts Institute of Technology (MIT), Cambridge, MA, USA, in 2006 and 2010, respectively.
She was a member of the Technical Staff, Systems and Applications Research and Development Center, Texas Instruments (TI), Dallas, TX, USA, where she designed low-power algorithms and architectures for video coding. She also represented TI in the JCT-VC Committee of ITU-T and ISO/IEC standards body during the development of High Efﬁciency Video Coding (HEVC), which received a Primetime Engineering Emmy Award. Within the committee, she was the primary coordinator of the core experiment on coefﬁcient scanning and coding, and has chaired/vice-chaired several ad hoc groups on entropy coding. She is currently an Associate Professor with the Electrical Engineering and Computer Science Department, MIT. She has co-edited the book High Efﬁciency Video Coding (HEVC): Algorithms and Architectures (Springer, 2014). Her research interests include energy-aware signal processing algorithms, and low-power circuit and system design for portable multimedia applications, including computer vision, deep learning, autonomous navigation, image processing, and video coding. Dr. Sze received the Jin-Au Kong Outstanding Doctoral Thesis Prize in electrical engineering from MIT in 2011. She was a recipient of the 2019 Edgerton Faculty Achievement Award from MIT, the 2018 Facebook Faculty Award, the 2017 and 2018 Qualcomm Faculty Award, the 2016 and 2018 Google Faculty Research Award, the 2016 AFOSR Young Investigator Research Program (YIP) Award, the 2016 3M Non-Tenured Faculty Award, the 2014 DARPA Young Faculty Award, and the 2007 DAC/ISSCC Student Design Contest Award. She was a co-recipient of the 2018 VLSI Best Student Paper Award, the 2017 CICC Outstanding Invited Paper Award, the 2016 IEEE Micro Top Picks Award, and the 2008 A-SSCC Outstanding Design Award. She will be the Systems Program Chair of SysML in 2020. She is a Distinguished Lecturer of the IEEE Solid-State Circuits Society (SSCS) and currently serves on the Technical Program Committee for the International Solid-State Circuits Conference (ISSCC) and the SSCS Advisory Committee (AdCom). She has also served on the technical program committees for VLSI Circuits Symposium, Micro and the Conference on Systems and Machine Learning (SysML), and as a Guest Editor for the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY (TCSVT).

Authorized licensed use limited to: KAUST. Downloaded on February 05,2023 at 14:42:38 UTC from IEEE Xplore. Restrictions apply.

