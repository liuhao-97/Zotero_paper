2922

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

Dynamic Ofﬂoading for Multiuser Muti-CAP MEC Networks: A Deep Reinforcement Learning Approach

Dong Li

Chao Li , Junjuan Xia , Fagui Liu , , Senior Member, IEEE, Lisheng Fan , Member, IEEE, George K. Karagiannidis , Fellow, IEEE, and Arumugam Nallanathan , Fellow, IEEE

Abstract—In this paper, we study a multiuser mobile edge computing (MEC) network, where tasks from users can be partially ofﬂoaded to multiple computational access points (CAPs). We consider practical cases where task characteristics and computational capability at the CAPs may be time-varying, thus, creating a dynamic ofﬂoading problem. To deal with this problem, we ﬁrst formulate it as a Markov decision process (MDP), and then introduce the state and action spaces. We further design a novel ofﬂoading strategy based on the deep Q network (DQN), where the users can dynamically ﬁne-tune the ofﬂoading proportion in order to ensure the system performance measured by the latency and energy consumption. Simulation results are ﬁnally presented to verify the advantages of the proposed DQN-based ofﬂoading strategy over conventional ones.
Index Terms—DQN, dynamic optimization problem, MEC.
I. INTRODUCTION
In recent years, the research in wireless networks have gradually evolved from the pure communication to communication and computation [1]. Some practical examples include intelligent monitoring, intelligent transport, vehicular networking, etc. To support
Manuscript received September 7, 2020; revised January 5, 2021; accepted February 7, 2021. Date of publication February 12, 2021; date of current version April 2, 2021. This work was supported in part by the NSFC under Grant 61871139/61801132, in part by the International Science and Technology Cooperation Projects of Guangdong Province under Grant 2020A0505100060, in part by the Natural Science Foundation of Guangdong Province under Grants 2017A030308006, 2018A030310338, and 2020A1515010484, in part by the Science and Technology Program of Guangzhou under Grant 201807010103, and in part by the research program of Guangzhou University under Grant YK2020008. The work of Dong Li was supported in part by the Science and Technology Development Fund, Macau SAR under Grant 0003/2019/A1 and in part by the Joint Research Funding Project launched by the Ministry of Science and Technology of the People’s Republic of China and The Science and Technology Development Fund, Macau SAR under Grant 0018/2019/AMJ. The work of Fagui Liu was supported in part by the Major Program of Guangdong Basic and Applied Research under Grant 2019B030302002, in part by the Science and Technology Major Project of Guangzhou under Grant 202007030006, and in part by the Non-Recurring Engineering of Huawei Technology Company OAA[20121100507097B]. The review of this article was coordinated by Prof. P. Lorenz. (Corresponding author: Junjuan Xia.)
Chao Li, Junjuan Xia, and Lisheng Fan are with the School of Computer Science, Guangzhou University, Guangzhou 510006, China (e-mail: lichaoacademic@e.gzhu.edu.cn; xiajunjuan@gzhu.edu.cn; lsfan@ gzhu.edu.cn).
Fagui Liu is with the School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China (e-mail: fgliu@scut.edu.cn).
Dong Li is with the Faculty of Information Technology, Macau University of Science and Technology, Taipa 999078, Macau, China (e-mail: dli@must.edu.mo).
George K. Karagiannidis is with the Aristotle University of Thessaloniki, Thessaloniki 54636, Greece (e-mail: geokarag@auth.gr).
Arumugam Nallanathan is with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London E14FS, U.K (e-mail: a.nallanathan@qmul.ac.uk).
Digital Object Identiﬁer 10.1109/TVT.2021.3058995

Fig. 1. A multiuser MEC network with M users and N CAPs.
these computation-intensive services, cloud computing can be applied to compute the tasks on the cloud, at the cost of transmission and information leakage. To resolve this problem, mobile edge computing (MEC) has been proposed to assist computing the tasks by the near-by computational access points (CAPs) in the networks, which can signiﬁcantly reduce the latency and energy consumption of both communication and computation [2].
A key in the design of MEC networks is the ofﬂoading strategy, which determines how many parts of the tasks will be computed by the CAPs. In this direction, the authors in [3] and [4] investigated time-invariant system environments, and adopted some numerical methods to acquire a static ofﬂoading strategy for multiuser or multi-CAP MEC networks. In practice, the system environments may be time-varying, which will impose a signiﬁcant impact on the ofﬂoading strategy of MEC networks. For the time-varying wireless channels or varying arrival rate of computational tasks, some dynamic ofﬂoading strategies were proposed based on the game theory [5], [6]. As the above binary optimization problem [3]–[6] in MEC networks may be NP-hard, the authors in [7] further employed a deep Q network (DQN) to efﬁciently ﬁnd a binary ofﬂoading strategy for MEC networks. The ofﬂoading performance of MEC networks can be further enhanced with the integration of intelligent reﬂecting surfaces (IRS) technique [8], [9], where the fundamental performance has been analyzed and a novel angle-domain design framework has been proposed, which provides critical guidance for the research of IRS systems.
In this paper, we investigate a multiuser MEC network, where the tasks from users can be partially ofﬂoaded to multiple CAPs. We consider the practical environments where task characteristics and computational capability at the CAPs may be time-varying, thus, creating a dynamic ofﬂoading problem. To solve this problem, we ﬁrst formulate it as a Markov decision process (MDP), and then introduce the state space and action space. We further design a novel ofﬂoading strategy based on the DQN, where the users can dynamically ﬁne-tune the ofﬂoading proportion in order to ensure the system performance measured by the latency and energy consumption. Simulation results are ﬁnally presented to verify the advantages of the proposed DQN-based ofﬂoading strategy over conventional ones.
II. PRELIMINARIES
A. System Model
Fig. 1 shows the system model of a multiuser MEC network, where there are M mobile users and N CAPs. The users have some computational tasks to be implemented, but they have limited computational

0018-9545 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

2923

capabilities. To facilitate the computation, these tasks can be partially
ofﬂoaded to the N CAPs, through the wireless links. Speciﬁcally, the sets of users and CAPs in the network are denoted by {um|1 ≤ m ≤ M } and {CAPn|1 ≤ n ≤ N }, respectively. At each time slot, user um has a computational task Xm, which has dm number of bits and requires cm CPU cycles to process. The computational capability at CAPn is denoted by fn, measured in CPU cycles per second. In practice, the task characteristics and computational capability at the CAPs may be
time-varying, due to the change of environments.
In this paper, without loss of generality, we use a typical form of uniform distribution U (•) to characterize the variation in the time domain.1 Speciﬁcally, cm ∼ U (cmin, cmax), where cmin and cmax are the minimum and maximum cycles, respectively; dm ∼ U (dmin, dmax), where dmin and dmax are the minimum and maximum numbers of bits in the task Xm, respectively; and fn ∼ U (fmin, fmax), with fmin and fmax being the minimum and maximum computational capabilities, respectively. To compute the task Xm, user um can ofﬂoad ρm,n portion of the task to CAPn through the wireless um–CAPn link, where 0 ≤ ρm,n ≤ 1. After the computation is ﬁnished, all the CAPs return the computational results to the users through some dedicated feedback
links. In summary, each time slot can be divided into three stages, i.e.,
task ofﬂoading, task computing and result feedback. The above three
stages can be described as follows:
1) Task ofﬂoading: In this stage, some portions of the tasks at users are ofﬂoaded to the CAPs. Let ρm = [ρm,0, ρm,1, . . ., ρm,N ] denote the 1 × (N + 1) ofﬂoading vector for the task Xm of the user um, where ρm,0 represents the proportion of Xm to be computed locally while ρm,n denotes the portion to be computed by CAPn. Based on ρm, user um ﬂexibly divides its task Xm into N + 1 subtasks, and sends the associated N subtasks to the N CAPs in a sequential way.
2) Task computing: After collecting the subtasks in the ﬁrst stage,
the CAPs can compute the received subtasks in parallel, which
can help reduce the latency in the computation.
3) Result feedback: After the task computation is ﬁnished, the
CAPs can feedback the associated results to the users through
some dedicated feedback channels. Once this stage is ﬁnished,
one time slot has been used up for the communication and
computation.

B. Latency and Energy Consumption

In this paper, we investigate the latency and energy consumption in the process of communication and computation in order to measure the system cost at each time slot.2 At the ﬁrst stage, the data rate of the wireless link from user um to CAPn is

rm,n = B log2

1

+

Pm|hm,n|2 σ2

,

(1)

where B is the wireless bandwidth, Pm is the transmit power at user um, hm,n ∼ CN (0, β) is the instantaneous channel parameter of the

1When other kinds of distribution are used to characterize the time-varying characteristics, the proposed DQN-based optimization framework in this paper can be still applied to optimize the system ofﬂoading.
2As pointed out by many existing works in the literature such as [1-4], latency and energy consumption are two most signiﬁcant performance metrics in the MEC networks. Speciﬁcally, latency is particularly important in the cases of video transmission, navigation, and control-orientated systems, while energy consumption attracts broad interests since the MEC nodes are energy-aware, especially when they have limited energy. Due to these reasons, we adopt the widely-used latency and energy consumption model in this work. Some other metrics such as pricing on the computation will be incorporated to measure the performance of MEC networks in future works.

um − CAPn link, σ2 is the variance of the additive white Gaussian noise (AWGN) at CAPn. From (1), we write the transmission latency and energy consumption as

lm,n = ρm,ndm/rm,n, em,n = Pmρm,ndm/rm,n.

(2)

Then, the latency of task ofﬂoading is given by

N

N

lm = lm,n = ρm,ndm/rm,n.

(3)

n=1

n=1

The largest lm among M ones is used as the system ofﬂoading latency at the ﬁrst stage,

L1 = max {l1, . . . , lM }.

(4)

Similarly, the energy consumption of task ofﬂoading at the ﬁrst stage is given by

E1

=

MN
em,n
m=1 n=1

=

M m=1

N n=1

ρm,n dm rm,n

Pm.

(5)

Now we turn to compute the latency and energy consumption for the computation in the second stage. The local computational latency and energy consumption at user um are

lm,0 = ρm,0cm/f0, em,0 = ζuρm,0cmf02,

(6)

where f0 is the local computational capability and ζu is the energy consumption coefﬁcient of the CPU chip at the users. The computational latency and energy consumption at CAPn are

M

M

ln =

ρm,ncm/fn, en =

ζcρm,ncmfn2 ,

(7)

m=1

m=1

where ζc is the energy consumption coefﬁcient of the CPU chip at the CAPs. From (6)-(7), we can write the latency and energy consumption at the second stage as,

L2 = max{max{l1,0, . . ., lM,0}, max{l1, . . ., lN }},

(8)

M

N

E2 =

em,0 + en

m=1

n=1

M

MN

=

ζuρm,0cmf02 +

ζcρm,ncmfn2 ,

(9)

m=1

m=1 n=1

where both the transmission latency and computational latency are considered in this paper. In some practical application scenarios, the number of bits in the feedback process is much smaller than that in the task. Hence, we can ignore the cost in the third stage. Accordingly, the total system latency and energy consumption at each time slot are summarized as

Ltotal = L1 + L2, Etotal = E1 + E2.

(10)

From the equations above, we can ﬁnd that the energy consumption is affected by both the latency and the associated power. However, the relationship between the latency and energy consumption is quite complicated, since the total energy consumption E1 and E2 are the sum of the individual one while the total latency L1 and L2 are the maximum of the individual one.

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

2924

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

Besides investigating the individual latency and energy consumption, a linear combination of Ltotal and Etotal can be used to measure the system performance,

λLtotal + (1 − λ)Etotal,

(11)

where λ ∈ [0, 1] is a weight factor between the system latency and energy consumption. Note that it is reasonable to use the linear combination of latency and energy consumption as the system cost, in some MEC scenarios, due to the following reasons. Firstly, minimizing the linear combination can help reduce the latency and energy consumption. In particular, when the weight factor λ is 0 or 1, the linear combination degenerates into latency or energy consumption only. In this case, minimizing the linear combination directly leads to minimizing the latency and energy consumption. More importantly, the linear combination provides a ﬂexible form of the system cost for the MEC networks, through adaptively adjusting the linear weight factor. Speciﬁcally, if the latency plays a more important role in the system cost, we can increase the value of λ, while we can reduce λ if the energy consumption becomes more important. Due to these reasons, the linear combination form of latency and energy consumption has been widely used to measure the system cost of MEC networks, in the existing works of the literature such as [10]–[11].

III. DQN-BASED OFFLOADING STRATEGY
As the ofﬂoading strategy determines how many portions of the tasks will be computed by the CAPs, it will affect the system latency and energy consumption signiﬁcantly. Let π = [ρT1 , . . . , ρTM ] be the ofﬂoading matrix. In this paper, we propose two criteria to optimize the ofﬂoading strategy. Speciﬁcally, criterion I optimizes the ofﬂoading strategy by minimizing the linear combination form of the system cost, at each time slot as,

min π

ΦI(π)

=

λLtotal

+

(1

−

λ)Etotal

N
s.t. ρm,0 + ρm,n = 1, ∀m ∈ {1, 2, . . ., M },
n=1

0 ≤ ρm,0 ≤ 1, 0 ≤ ρm,n ≤ 1.

(12a) (12b) (12c)

In contrast, criterion II optimizes the ofﬂoading strategy by minimizing the energy consumption while meeting the requirement of the latency, at each time slot as,

min π

ΦII(π)

=

Etotal

N
s.t. ρm,0 + ρm,n = 1, ∀m ∈ {1, 2, . . ., M },
n=1

Ltotal < Lth,

0 ≤ ρm,0 ≤ 1, 0 ≤ ρm,n ≤ 1,

(13a)
(13b) (13c) (13d)

where Lth is the latency threshold. For the decentralized implementation, criterion I should turn to minimize the locally linear combination form of cost while criterion II turns to minimize the local energy consumption with a given latency constraint, in order to obtain the local ofﬂoading strategy for each user.
Although the above two criteria in (13) and (14) can optimize the ofﬂoading strategy, it is however difﬁcult to employ the conventional optimization method to solve an optimal π for each time slot. This is because that the optimization involves some complicated operations including the max operation and the associated derivative with respect

to π is very complicated to solve. More importantly, the conventional optimization method performs the optimization at each individual time slot, and it cannot perform the optimization for the current time slot by exploiting the optimization result of the previous time slot, which however can act as an important reference for the current time slot. In time-varying environments, where the task characteristics and computational capability are varying, a learning based scheme should be developed to adaptively optimize the ofﬂoading strategy according to the dynamic environments.
In this paper, we adopt the DRL based algorithm to optimize the ofﬂoading strategy for the considered system. As one of the powerful decision-making algorithms in artiﬁcial intelligence ﬁeld, the DRL performs the dynamic programming to achieve an excellent performance and effectiveness in tackling the optimization under dynamic environments. In the following, we will introduce the Markov decision process (MDP) and the implementation of deep Q-network (DQN), which are two important parts in the DRL based optimization framework.

A. Markov Decision Process (MDP)

The MDP is used to characterize the time-varying environments,

which involve the state space and action space. As the time slot t =

1, 2, . . ., ∞, we use S = {st|st = [Dt, Ct, Ft, πt]} to denote the state space, where Dt = [d1(t), . . . , dM (t)] and Ct = [c1(t), . . . , cM (t)] are two 1 × M task characteristic vectors at time slot t; Ft = [f1, . . . , fN ] is the 1 × N computational capability vector at time slot t; and πt is the M × N ofﬂoading matrix at time slot t. In addition, we use A = {am,n ∈ {1, −1, 0}|1 ≤ m ≤ M, 1 ≤ n ≤ N } to denote

the action space. For a given action am,n, we have

⎧

⎨ ρm,n = ρm,n + δ, ρm,0 = ρm,0 − δ If am,n = 1,

⎩

ρm,n ρm,n

= =

ρm,n − ρm,n ,

δ, ρm,0 = ρm,0 ρm,0 = ρm,0

+

δ If

If am,n = −1, am,n = 0,

(14)

where δ ∈ [0, 1) is an iterative gradient to ﬁne-tune the ofﬂoading
matrix. At the current time slot t, the environment state is denoted as st ∈ S and then according to st, the users execute an action noted by at ∈ A. Considering the fairness among users, we can impose a constraint on the ofﬂoading ratio for the execution at, given by

|ρm1,0 − ρm2,0| < κ, ∀m1, m2 ∈ {1, . . . , M },

(15)

where κ ∈ [0, 1] is the fairness factor used to adjust the fairness among
users. A smaller κ indicates a more strict constraint on the fairness. If
(15) cannot hold, the agent will not execute the selection action at, and turn to choose another action to update the ofﬂoading matrix πt. When an element in the matrix πt is updated, the ofﬂoading matrix transits from πt to πt+1. Moreover, Dt, Ct and Ft accordingly transit to Dt+1, Ct+1 and Ft+1, respectively. Therefore, the environment state transits from st to st+1 with a conditional probability P, and meanwhile the users acquire the instant reward. We now discuss the reward function
for the two criteria. For criterion I, we can design the reward function ΨI,t as

ΨI,t = λLtotal,t + (1 − λ)Etotal,t.

(16)

For criteria II, we can design the reward function ΨII,t as

⎧ ⎨−μ1 if Ltotal,t ≥ Lth,

ΨII,t=⎩μ−2μ2ifif

Ltotal,t < Lth and Etotal,t −Etotal,t−1 < 0, Ltotal,t < Lth and Etotal,t −Etotal,t−1 ≥ 0,

(17)

where μ1 and μ2 are two positive values with μ1 > μ2. Speciﬁcally,

if the latency at the current time slot exceeds Lth, then the instant

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

2925

Fig. 2. The framework of the DQN-based ofﬂoading strategy.
reward is −μ1. Otherwise, we need to observe the change in the energy consumption. The instant reward is μ2 if the energy consumption decreases, while equal to −μ2 otherwise. According to the two instant reward functions, we can formulate the long-term expected average rewards for the network optimization under the strategy πt as

Algorithm 1: : DQN-Based Ofﬂoading Strategy.

1: Clear up the memory of ERU

2: Randomly initialize the parameter of evaluation network θ and the parameter of target network θ−, let θ = θ−

3: Loop for each episode:

4: Initialize s0 ∈ S 5: Loop for each time slot t:

6:

Choose at from st using policy derived from

-greedy

7:

Carry out action at, and observe the system cost Ψi,t and

st+1

8:

Store the transition sample (st, at, Ψi,t, st+1) in ERU

9: Catch a minibatch of transitions from ERU

10:

Yt = Ψi,t + ξ minat+1 Qi(st+1, at+1|θ−)

11:

Execute RMSPropOptimizer to (Yt − Qi(st, at|θ))2

respect to θ

12:

Every tcopy time slots reset θ− = θ

13:

Let st = st+1

14: end for

15: end for

T

Vi(st

=

[Dt,

Ct,

Ft,

πt])

=

lim
T →∞

E

ξtΨi,t , (18)

t=1

where i ∈ {I, II} and ξ ∈ (0, 1] is a discount factor to control the effect of historical data. We can try to ﬁnd the optimal ofﬂoading strategy π∗
by minimizing V (st) as

π∗

=

arg

min π

Vi(st),

∀st ∈ S.

(19)

However, it is difﬁcult for the users to know the conditional probability P for the state transition. Hence, we employ the following DQN-based approach to solve the ofﬂoading strategy for the considered MEC networks.

B. DQN-Based Solution
To show the effect of action on the strategy, we rewrite the state value function shown in (19) in a recursive form by using the state-action value function as

Qi(st, at) = Ψi,t + ξ min Qi(st+1, at+1),

(20)

at+1

which is known as the Q function. In the conventional Q-learning algorithm, it is assumed that the number of states is limited, so that we
can use a lookup table to record the state-action value pair. However,
in this paper, due to the large number of environment states, we have to employ a deep neural network (DNN) to approximate the Q function. As shown in Fig. 2, at the current time slot t, we collect the current environment state st as the input data of the evaluation network, and the evaluation network outputs the value Qi(st, a), for a ∈ A. Then, we apply the -greedy policy to select an action at. Next, the users execute the action at, and then the state transits from st to another state st+1 with the instant cost Ψi,t. Based on the cost Ψi,t, we update the parameters of the evaluation network. After many trials, the evaluation is trained to output an optimal value Qi(st, at). Similar to the other deep learning networks, we use the mean square error based loss function to
evaluate the training,

network. Nevertheless, if we use the same DNN to obtain the target value, the optimization object will be changed with the parameter θ at each iteration. Therefore, we apply the target network which has the same structure with the evaluation network, except that the parameter update of the target network θ− is tcopy time slots later than that of the evaluation network. For the two criteria in (13) and (14), we can calculate the target value Yt as

Yt = Ψi,t + ξ min Qi(st+1, at+1|θ−).

(22)

at+1

In addition, the input data is independent in the supervised learning, while the observation data of the network is sequential. Motivated by this, we set an experience relay unit (ERU) in the framework of DQN. For the two criteria, we can collect the transition sample (st, at, Ψi,t, st+1) generated by the interaction between the environment and agent into the memory of ERU. During the training process, we randomly catch a mini-batch of transitions of the ERU memory at each iteration to break the dependence of data set. The proposed DQN-based ofﬂoading strategy is summarized in Algorithm 1.

C. Complexity Analysis

In this subsection, we provide some computational complexity anal-

ysis of the DQN used in this paper. As the complexity of -greedy policy

based Q-learning algorithm is O(T ) [12] and the DQN framework

is a combination of Q-learning and two DNNs with the identical

structure, the computational complexity of the DQN comes from the

matrix operation of DNNs. Since the DNNs of this paper employ

the full-connection networks, the computational complexity of each

training step is O(

J j=1

Kj−1Kj

),

where

Kj

represents

the

neural

size of the j-th layer (1 ≤ j ≤ J) among J layers. As the target value

network only operates the forward propagation at each training step,

the total computational complexity of the DQN algorithm in this paper

is O(3T

J j=1

Kj−1Kj

).

Losst = E[(Yt − Qi(st, at|θ))2],

(21)

IV. SIMULATION RESULTS

where θ is the parameter of the evaluation network, and Yt is the In this part, we demonstrate some simulation results to verify the eftarget value that represents the optimization object of the evaluation fectiveness of the proposed DQN-based dynamic ofﬂoading strategy for

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

2926

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

Fig. 3. Convergence of the proposed DQN approach. (a) Criterion I. (b) Criterion II.

Fig. 4. System cost versus the number of users with N = 15. (a) Criterion I. (b) Criterion II.

the considered MEC networks. To simulate the time-varying environments, we set cm ∼ U (2 × 109, 3 × 109), dm ∼ U (2 × 108, 3 × 108), and fn ∼ U (5 × 109, 7 × 109). The local computational capability f0 is set to U (1.5 × 109, 2 × 109). Moreover, the wireless bandwidth B is equal to 40 MHz, whereas the average channel gain is set to 4,
and the transmit SNR is set to 10 dB. In further, the DQN network
is implemented by using the well-known Tensorﬂow library on the
Python platform, and there are three hidden layers in the network, with
16, 32 and 64 nodes in the layers in order. The Rectiﬁed Linear Unit
(ReLU) is used as the activation function, and the ‘RMSPropOptimizer’
is employed as the optimizer to minimize the loss function in (21). Furthermore, the iterative gradient δ is 0.01, the factor in ε − greedy policy is 0.8, and the sizes of the ERU and minibatch are set to 1000 samples and 200 samples, respectively. We reset θ− = θ every 200 time slots. In each simulation, we initialize ρm,0 = 1 and ρm,n = 0 for 1 ≤ m ≤ M , and repeat the experiment 100 times to calculate the average cost. If not speciﬁed, we set λ to 0.5 for criterion I and Lth = 1 s for criterion II, and set κ = 1 for both criteria.
Fig. 3 shows the training process of the proposed DQN approach
for both criteria, where there are 15 000 time slots and Fig. 3(a) uses
the linear combination to evaluate criterion I while Fig. 3(b) employs
energy consumption to measure criterion II. In particular, three groups of parameter setting are plotted with (M, N ) = (10, 10), (M, N ) = (10, 15), and (M, N ) = (15, 10), respectively. As observed from this ﬁgure, we can ﬁnd that for different numbers of users and CAPs,
the curves of the proposed approach drop sharply with the increasing
number of time slots, and the system cost becomes convergent after enough number of time slots. In particular, when (M, N ) = (10, 10) and (M, N ) = (10, 15), the system cost becomes convergent after

about 5000 time slots, while for (M, N ) = (15, 10), the system cost requires about 10 000 time slots to be convergent. These results verify that after enough number of trials, the proposed DQN approach can ﬁnd a suitable ofﬂoading strategy for the considered MEC network.
Fig. 4 illustrates the system cost of the proposed DQN-based ofﬂoading strategy versus the number of users for both criteria, where Fig. 4(a) uses the linear combination to evaluate criterion I while Fig. 4(b) employs energy consumption to measure criterion II. For comparison, we also plot the system cost of random ofﬂoading, local computing, fully ofﬂoading, and decentralized ofﬂoading in Fig. 4, where the decentralized ofﬂoading adopts the distributed DQN method without a centralized entity and each user can adaptively adjust its own ofﬂoading strategy via observing its local task characteristic and ofﬂoading vector by itself [14]. We can observe from this ﬁgure that for criterion I, the proposed DQN-based ofﬂoading strategy outperforms the other four strategies for various values of M , indicating that the proposed strategy can effectively exploit the communication and computation resources. In contrast, the proposed strategy of criterion II outperforms the random, local computing and decentralized strategies, and it is assumed worse than the full ofﬂoading which however has a large transmission latency. The reason why the proposed strategy outperforms the decentralized one lies in that the latter cannot learn the global features by training separately without interacting with each other. In further, the system cost of the ﬁve strategies increases with a larger M , as more users cause an increasing number of tasks to the MEC network.
Fig. 5 demonstrates the system cost of criterion I with several ofﬂoading strategies versus the weight factor λ, where M = 10, N = 10

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 70, NO. 3, MARCH 2021

2927

Fig. 5. Effect of the weight factor λ on the system cost of criterion I.
and λ varies from 0.1 to 0.9. We use two values of fairness factor with κ = 0.4 and 1, to see the impact of fairness on the system cost. As observed from Fig. 5, we can ﬁnd that for various values of λ, the proposed strategy is superior to the other three strategies, which further veriﬁes the effectiveness of the proposed strategy. Moreover, the system cost of the proposed strategy increases with a smaller κ, due to the sacriﬁce to protect the fairness among users. In further, the system cost of the ﬁve strategies decreases with a larger λ, as the energy consumption plays a more important role than latency in the linearly weighted cost, under the given parameter setting.
V. CONCLUSION
In this paper, we have investigated a multiuser muti-CAP MEC network, where task characteristics and computational capability at the CAPs are time-varying. To propose a dynamic ofﬂoading strategy, we have ﬁrst formulated the dynamic ofﬂoading as MDP, and then introduced the state space and action space. We further designed a novel ofﬂoading strategy based on the DQN, where the users could dynamically ﬁne-tune the ofﬂoading proportion in order to ensure the system performance measured by the latency and energy consumption. Simulation results were ﬁnally presented to verify the advantages of the proposed DQN-based ofﬂoading strategy over the conventional ones.

REFERENCES
[1] J. Tang et al., “Energy minimization in D2D-assisted cache-enabled Internet of Things: A deep reinforcement learning approach,” IEEE Trans. Ind. Informat., vol. 16, no. 8, pp. 5412–5423, Aug. 2020.
[2] Z. Liang, Y. Liu, T. Lok, and K. Huang, “Multiuser computation ofﬂoading and downloading for edge computing with virtualization,” IEEE Trans. Wireless Commun., vol. 18, no. 9, pp. 4298–4311, Sep. 2019.
[3] X. Chen, L. Jiao, W. Li, and X. Fu, “Efﬁcient multi-user computation ofﬂoading for mobile-edge cloud computing,” IEEE/ACM Trans. Netw., vol. 24, no. 5, pp. 2795–2808, Oct. 2016.
[4] J. Yan, S. Bi, Y. J. Zhang, and M. Tao, “Optimal task ofﬂoading and resource allocation in mobile-edge computing with inter-user task dependency,” IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 235–250, Jan. 2020.
[5] H. Cao and J. Cai, “Distributed multiuser computation ofﬂoading for cloudlet-based mobile cloud computing: A game-theoretic machine learning approach,” IEEE Trans. Veh. Technol., vol. 67, no. 1, pp. 752–764, Jan. 2018.
[6] L. Xiao, Y. Li, X. Huang, and X. Du, “Cloud-based malware detection game for mobile devices with ofﬂoading,” IEEE Trans. Mobile Comput., vol. 16, no. 10, pp. 2742–2750, Oct. 2017.
[7] S. Bi and Y. J. A. Zhang, “Computation rate maximization for wireless powered mobile-edge computing with binary computation ofﬂoading,” IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177–4190, Jun. 2018.
[8] X. Hu, C. Zhong, Y. Zhu, X. Chen, and Z. Zhang, “Programmable metasurface-based multicast systems: Design and analysis,” IEEE J. Sel. Areas Commun., vol. 38, no. 8, pp. 1763–1776, Aug. 2020.
[9] X. Hu, C. Zhong, Y. Zhang, X. Chen, and Z. Zhang, “Location information aided multiple intelligent reﬂecting surface systems,” IEEE Trans. Commun., vol. 68, no. 12, pp. 7948–7962, Dec. 2020.
[10] Y. Pan, M. Chen, Z. Yang, N. Huang, and M. Shikh-Bahaei, “Energyefﬁcient NOMA-based mobile edge computing ofﬂoading,” IEEE Commun. Lett., vol. 23, no. 2, pp. 310–313, Feb. 2019.
[11] Z. Yang, C. Pan, J. Hou, and M. Shikh-Bahaei, “Efﬁcient resource allocation for mobile-edge computing networks with NOMA: Completion time and energy minimization,” IEEE Trans. Commun., vol. 67, no. 11, pp. 7771–7784, Nov. 2019.
[12] A. Ndikumana et al., “Joint communication, computation, caching, and control in big data multi-access edge computing,” IEEE Trans. Mob. Comput., vol. 19, no. 6, pp. 1359–1374, Jun. 2020.
[13] J. Chi, A.-Z. Zeyuan, B. Sebastien, and J. M. I., “Is Q-learning provably efﬁcient?,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2018, pp. 4868– 4878.
[14] B. Peng, G. Seco-Granados, E. Steinmetz, M. Frohle, and H. Wymeersch, “Decentralized scheduling for cooperative localization with deep reinforcement learning,” IEEE Trans. Veh. Technol., vol. 68, no. 5, pp. 4295–4305, May 2019.

Authorized licensed use limited to: KAUST. Downloaded on June 06,2022 at 13:41:10 UTC from IEEE Xplore. Restrictions apply.

