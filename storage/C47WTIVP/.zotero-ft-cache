arXiv:1908.09791v5 [cs.LG] 29 Apr 2020

Published as a conference paper at ICLR 2020
ONCE-FOR-ALL: TRAIN ONE NETWORK AND SPECIALIZE IT FOR EFFICIENT DEPLOYMENT
Han Cai1, Chuang Gan2, Tianzhe Wang1, Zhekai Zhang1, Song Han1 1Massachusetts Institute of Technology, 2MIT-IBM Watson AI Lab {hancai, chuangg, songhan}@mit.edu
ABSTRACT
We address the challenging problem of efﬁcient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to ﬁnd a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars’ lifetime Strubell et al. (2019)) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efﬁciently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of subnetworks (> 1019) that can ﬁt different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5× faster than MobileNetV3, 2.6× faster than EfﬁcientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classiﬁcation track and the 4th LPCVC, both classiﬁcation track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.
1 INTRODUCTION
Deep Neural Networks (DNNs) deliver state-of-the-art accuracy in many machine learning applications. However, the explosive growth in model size and computation cost gives rise to new challenges on how to efﬁciently deploy these deep learning models on diverse hardware platforms, since they have to meet different hardware efﬁciency constraints (e.g., latency, energy). For instance, one mobile application on App Store has to support a diverse range of hardware devices, from a high-end Samsung Note10 with a dedicated neural network accelerator to a 5-year-old Samsung S6 with a much slower processor. With different hardware resources (e.g., on-chip memory size, #arithmetic units), the optimal neural network architecture varies signiﬁcantly. Even running on the same hardware, under different battery conditions or workloads, the best model architecture also differs a lot.
Given different hardware platforms and efﬁciency constraints (deﬁned as deployment scenarios), researchers either design compact models specialized for mobile (Howard et al., 2017; Sandler et al., 2018; Zhang et al., 2018) or accelerate the existing models by compression (Han et al., 2016; He et al., 2018) for efﬁcient deployment. However, designing specialized DNNs for every scenario is engineer-expensive and computationally expensive, either with human-based methods or NAS. Since such methods need to repeat the network design process and retrain the designed network from scratch for each case. Their total cost grows linearly as the number of deployment scenarios increases, which will result in excessive energy consumption and CO2 emission (Strubell et al., 2019). It makes them unable to handle the vast amount of hardware devices (23.14 billion IoT devices till
1

Published as a conference paper at ICLR 2020

train a once-for-all network

Previous: O(N) design cost Ours: O(1) design cost

specialized sub-nets

16x~1300x reduction

0 20 40 60 80

direct deploy (no retrain)

Number of Deployment Scenarios

F

P

Cloud AI

Mobile AI

cpu

MCU Tiny AI

G

A

(AIoT) Diﬀerent Hardware / Constraint

Design Cost Top-1 ImageNet Acc (%)

OFA

MobileNetV3

77

75

TraGineOt Mncaen,y 

76.1 75.2

73

73.3

71 70.0 69

70.4

Train FGoeutr FToimures,

67.4

67

6 9 12 15 18 21 24

Samsung Note10 Latency (ms)

Figure 1: Left: a single once-for-all network is trained to support versatile architectural conﬁgurations

including depth, width, kernel size, and resolution. Given a deployment scenario, a specialized sub-

network is directly selected from the once-for-all network without training. Middle: this approach

reduces the cost of specialized deep learning deployment from O(N) to O(1). Right: once-for-all

network followed by model selection can derive many accuracy-latency trade-offs by training only

once,

train a once-for-all network
compared to conventional methods

that

rePqreuviiorues:rOe(pNe) daetseigdntcroasit ning.

Ours: O(1) design cost

Design Cost

20181) and highly dynamic deployment environments (different battery conditions, different latency

requirements, estcpe.)c.ialized sub-nets

16x~1300x

This paper introduces a new solution to tackle this challenregdeuc–tiodnesigning a once-for-all network that

can be directly deployed under diverse archite0ctur2a0l con40ﬁgur6a0tion8s0, amortizing the training cost. The inference is pedrirfeocrt mdepeldoyby(nsoerleetrcatinin) g only part Noufmtbheer oof nDcepel-ofyomre-nat lSlcenneatrwiosork. It ﬂexibly supports different depths, widths, kernel sizes, and resolutions without retrF ainPing. A simple example of Once-for-All
cpu
(OFA) is illustrated in Figure 1 (leMCfUt).TinSypAeI ciﬁcally, we GdecAouple the model training stage and the neural architecCtlouurdeAIsearchMosbtilaegAeI . In th(eAImoT)odeDl itﬀreareinntiHnagrdwsatareg/eC,onwsteraifnot cus on improving the accuracy

of all sub-networks that are derived by selecting different parts of the once-for-all network. In the

model specialization stage, we sample a subset of sub-networks to train an accuracy predictor and

latency predictors. Given the target hardware and constraint, a predictor-guided architecture search

(Liu et al., 2018) is conducted to get a specialized sub-network, and the cost is negligible. As such,

we reduce the total cost of specialized neural network design from O(N) to O(1) (Figure 1 middle).

However, training the once-for-all network is a non-trivial task, since it requires joint optimization of the weights to maintain the accuracy of a large number of sub-networks (more than 1019 in our experiments). It is computationally prohibitive to enumerate all sub-networks to get the exact gradient in each update step, while randomly sampling a few sub-networks in each step will lead to signiﬁcant accuracy drops. The challenge is that different sub-networks are interfering with each other, making the training process of the whole once-for-all network inefﬁcient. To address this challenge, we propose a progressive shrinking algorithm for training the once-for-all network. Instead of directly optimizing the once-for-all network from scratch, we propose to ﬁrst train the largest neural network with maximum depth, width, and kernel size, then progressively ﬁne-tune the once-for-all network to support smaller sub-networks that share weights with the larger ones. As such, it provides better initialization by selecting the most important weights of larger sub-networks, and the opportunity to distill smaller sub-networks, which greatly improves the training efﬁciency. From this perspective, progressive shrinking can be viewed as a generalized network pruning method that shrinks multiple dimensions (depth, width, kernel size, and resolution) of the full network rather than only the width dimension. Besides, it targets on maintaining the accuracy of all sub-networks rather than a single pruned network.

We extensively evaluated the effectiveness of OFA on ImageNet with many hardware platforms (CPU, GPU, mCPU, mGPU, FPGA accelerator) and efﬁciency constraints. Under all deployment scenarios, OFA consistently improves the ImageN1et accuracy by a signiﬁcant margin compared to SOTA hardware-aware NAS methods while saving the GPU hours, dollars, and CO2 emission by orders of magnitude. On the ImageNet mobile setting (less than 600M MACs), OFA achieves a new SOTA 80.0% top1 accuracy with 595M MACs (Figure 2). To the best of our knowledge, this is the ﬁrst time that the SOTA ImageNet top1 accuracy reaches 80% under the mobile setting.

1https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/

2

Published as a conference paper at ICLR 2020

ImageNet Top-1 accuracy (%)
→

81
595M MACs
 80.0% Top-1

Once-for-All (ours)

14x reduction
InceptionV3

Xception

79

EﬃcientNet

MBNetV3

77

ProxylessNAS

ResNetXt-50 NASNet-A
DenseNet-169

DPN-92 DenseNet-264

ResNetXt-101

AmoebaNet DenseNet-121

75

MBNetV2

ResNet-101

PNASNet ShuﬄeNet

InceptionV2

ResNet-50

73

DARTS

2M

4M

8M

16M

32M

64M

IGCV3-D

Model Size

The higher the better

71 MobileNetV1 (MBNetV1)

Handcrafted AutoML

The lower the better

69

0

1

2

3

4

5

6

7

8

9

MACs (Billion)

→

Figure 2: Comparison between OFA and state-of-the-art CNN models on ImageNet. OFA provides 80.0% ImageNet top1 accuracy under the mobile setting (< 600M MACs).

2 RELATED WORK

Efﬁcient Deep Learning. Many efﬁcient neural network architectures are proposed to improve the

hardware efﬁciency, such as SqueezeNet (Iandola et al., 2016), MobileNets (Howard et al., 2017;

Sandler et al., 2018), ShufﬂeNets (Ma et al., 2018; Zhang et al., 2018), etc. Orthogonal to architecting

efﬁcient neural networks, for efﬁcient deep learning,

minocdluedlicnogmn1pertwesosrikonpr(uHnainngetthaalt.,r2e0m1o6v)eissraendoutnhdearnvteurnyitesff(eHcatniveettaelc.h, n2i0q1u5e)

or redundant channels (He et al., 2018; Liu et al., 2017), and quantization that reduces the bit width

for the weights and activations (Han et al., 2016; Courbariaux et al., 2015; Zhu et al., 2017).

Neural Architecture Search. Neural architecture search (NAS) focuses on automating the architecture design process (Zoph & Le, 2017; Zoph et al., 2018; Real et al., 2019; Cai et al., 2018a; Liu et al., 2019). Early NAS methods (Zoph et al., 2018; Real et al., 2019; Cai et al., 2018b) search for highaccuracy architectures without taking hardware efﬁciency into consideration. Therefore, the produced architectures (e.g., NASNet, AmoebaNet) are not efﬁcient for inference. Recent hardware-aware NAS methods (Cai et al., 2019; Tan et al., 2019; Wu et al., 2019) directly incorporate the hardware feedback into architecture search. Hardware-DNN co-design techniques (Jiang et al., 2019b;a; Hao et al., 2019) jointly optimize neural network architectures and hardware architectures. As a result, they can improve inference efﬁciency. However, given new inference hardware platforms, these methods need to repeat the architecture search process and retrain the model, leading to prohibitive GPU hours, dollars, and CO2 emission. They are not scalable to a large number of deployment scenarios. The individually trained models do not share any weight, leading to large total model size and high downloading bandwidth.

Dynamic Neural Networks. To improve the efﬁciency of a given neural network, some work explored skipping part of the model based on the input image. For example, Wu et al. (2018); Liu & Deng (2018); Wang et al. (2018) learn a controller or gating modules to adaptively drop layers; Huang et al. (2018) introduce early-exit branches in the computation graph; Lin et al. (2017) adaptively prune channels based on the input feature map; Kuen et al. (2018) introduce stochastic downsampling point to reduce the feature map size adaptively. Recently, Slimmable Nets (Yu et al., 2019; Yu & Huang, 2019b) propose to train a model to support multiple width multipliers (e.g., 4 different global width multipliers), building upon existing human-designed neural networks (e.g., MobileNetV2 0.35, 0.5, 0.75, 1.0). Such methods can adaptively ﬁt different efﬁciency constraints at runtime, however, still inherit a pre-designed neural network (e.g., MobileNet-v2), which limits the degree of ﬂexibility (e.g., only width multiplier can adapt) and the ability in handling new deployment scenarios where the pre-designed neural network is not optimal. In this work, in contrast, we enable a much more diverse architecture space (depth, width, kernel size, and resolution) and a signiﬁcantly larger number of architectural settings (1019 v.s. 4 (Yu et al., 2019)). Thanks to the diversity and the large design

3

Progressive Shrinking
Published as a conference paper at ICLR 2020

Full Elastic

Full Elastic

Full

Resolution

Kernel Size

Partial

Partial

Partial

Elastic Depth

Full Partial

Elastic Width

Figure 3: Illustration of the progressive shrinking process to support different depth D, width W , kernel size K and resolution R. It leads to a large space comprising diverse sub-networks (> 1019).

space, we can derive new specialized neural networkimscphfooarntrannmecl eany different deploymentimscpchoearnntananerclieos rather

than working on top of an existing neural network that0l.i0m2 its the optimization headroom0..82However, it

is more challenging to train the progressive shrinking algorithm

ntoetwtaockrkletothaischccsihhoearavntilnnelgeeltnhgi00se..18.ﬂ55 erxeiobrgi.lity,

which

motivcshaoatrnteinnsgelus0.t1o1
0.46

dreeosrigg. n

the

O3

O2

0.63

O2

3 METHOD

O1 train with full width

O1 progressively shrink the width

O1 progressively shrink the width

Gradually shrink the width 3.1 PROBLEM FORMALIZATION Keep the most important channels when shrinking via channel sorting

Assuming the weights of the once-for-all network as Wo and the architectural conﬁgurations as

39

{archi}, we then can formalize the problem as

min

Lval C(Wo, archi) ,

(1)

Wo

archi

where C(Wo, archi) denotes a selection scheme that selects part of the model from the once-for-all network Wo to form a sub-network with architectural conﬁguration archi. The overall training objective is to optimize Wo to make each supported sub-network maintain the same level of accuracy as independently training a network with the same architectural conﬁguration.

3.2 ARCHITECTURE SPACE
Our once-for-all network provides one model but supports many sub-networks of different sizes, covering four important dimensions of the convolutional neural networks (CNNs) architectures, i.e., depth, width, kernel size, and resolution. Following the common practice of many CNN models (He et al., 2016; Sandler et al., 2018; Huang et al., 2017), we divide a CNN model into a sequence of units with gradually reduced feature map size and increased channel numbers. Each unit consists of a sequence of layers where only the ﬁrst layer has stride 2 if the feature map size decreases (Sandler et al., 2018). All the other layers in the units have stride 1.
We allow each unit to use arbitrary numbers of layers (denoted as elastic depth); For each layer, we allow to use arbitrary numbers of channels (denoted as elastic width) and arbitrary kernel sizes (denoted as elastic kernel size). In addition, we also allow the CNN model to take arbitrary input image sizes (denoted as elastic resolution). For example, in our experiments, the input image size ranges from 128 to 224 with a stride 4; the depth of each unit is chosen from {2, 3, 4}; the width expansion ratio in each layer is chosen from {3, 4, 6}; the kernel size is chosen from {3, 5, 7}. Therefore, with 5 units, we have roughly ((3 × 3)2 + (3 × 3)3 + (3 × 3)4)5 ≈ 2 × 1019 different neural network architectures and each of them can be used under 25 different input resolutions. Since all of these sub-networks share the same weights (i.e., Wo) (Cheung et al., 2019), we only require 7.7M parameters to store all of them. Without sharing, the total model size will be prohibitive.

3.3 TRAINING THE ONCE-FOR-ALL NETWORK
Na¨ıve Approach. Training the once-for-all network can be cast as a multi-objective problem, where each objective corresponds to one sub-network. From this perspective, a na¨ıve training approach is to directly optimize the once-for-all network from scratch using the exact gradient of the overall objective, which is derived by enumerating all sub-networks in each update step, as shown in Eq. (1). The cost of this approach is linear to the number of sub-networks. Therefore, it is only applicable to scenarios where a limited number of sub-networks are supported (Yu et al., 2019), while in our case, it is computationally prohibitive to adopt this approach.
Another na¨ıve training approach is to sample a few sub-networks in each update step rather than enumerate all of them, which does not have the issue of prohibitive cost. However, with such a large number of sub-networks that share weights, thus interfere with each other, we ﬁnd it suffers from

4

Published as a conference paper at ICLR 2020
Connection to Network Pruning

Train the full model

Network Pruning

Shrink the model (only width)

Fine-tune the small net

single pruned network

Train the full model

Progressive Shrinking

Shrink the model (4 dimensions)

Fine-tune both large and small sub-nets

once-for-all network

•FiPguroregr4e: sPsriovgeresshsrivineksinhrginckainngbceanvbieewveiedwaesd aas gaegneenrearalilziezeddnneetwork pprruunnininggtewchitnhiqmuue cwhith mhucighhheigr hfleerxﬂibexiliitbyiliatyc.roCsosm4padreimd etonnseiotwnsor.k pruning, it shrinks more dimensions (not only width) and provides a much more powerful once-for-all network that can ﬁt different deployment scenarios
rather than a single pruned network.

7x7

5x5

3x3

unit i

unit i

O1

unit i

O1

15

O2

Transform
 Matrix 

25x25

Transform 
 Matrix 

9x9

train with full depth

O2 shrink the depth

O3 shrink the depth

Figure 5: Left: Kernel transformation matrix for elastic kernel size. Right: Progressive shrinking for

elastic depth. Instead of skipping each layer independently, we keep the ﬁrst D layers and skip the

last

(4 c−hanDne)l
layers.
importance

The

weights

cohfanthneeiml
cephaoarrntlanynecll
eayers

are

shared.

0.02 reorg.

sorting

0.82 reorg.

.

sics.ihoegar.tnn,innipgeﬁrl
coag00n..r18te55sascicvuerashcyrindkroinpg. .In
0.63 progressively shrink

0.11
the folOlo2 wing0.4s6ection,
O1
the width

we

introduce

a

csshooarltnuinntgeilo
 n
O3

to

address

this

challenge,
p1

train with full width PrOo1gressive Shrinking.

O2

p2 +

The once-for-all network comprises manyOs1ub-networks of different sizp3es+

+

where small sub-networks are nested in large sub-networks. To prevent interference between the

sub-networks, we propose to enforce a training oprrdogerresfsriovemly slahrrignkethseubw-idnthetworks to small sub-networks

in a progressive manner. We name this training scheme as progressive shrinking (PS). An example of

the training process with PS is provided in Figure 3 and Figure 4, where we start with training the

largest neural network with the maximum kernel size (e.g., 7), depth (e.g., 4), and width (e.g., 6).

Next, we progressively ﬁne-tune the network to support smaller sub-networks by gradually adding

them into the sampling space (larger sub-networks may also be sampled). Speciﬁcally, after training

the largest network, we ﬁrst support elastic kernel size which can choose from {3, 5, 7} at each layer,

while the depth and width remain the maximum values. Then, we support elastic depth and elastic

width sequentially, as is shown in Figure 3. The resolution is elastic throughout the whole training

process, which is implemented by sampling different image sizes for each batch of training data. We

also use the knowledge distillation technique after training the largest neural network (Hinton et al.,

2015; Ashok et al., 2018; Yu & Huang, 2019b). It combines two loss terms using both the soft labels

given by the largest neural network and the real labels.

Compared to the na¨ıve approach, PS prevents small sub-networks from interfering large sub-networks, since large sub-networks are already well-trained when the once-for-all network is ﬁne-tuned to support small sub-networks. Regarding the small sub-networks, they share the weights with the large ones. Therefore, PS allows initializing small sub-networks with the most important weights of well-trained large sub-networks, which expedites the training process. Compared to network pruning (Figure 4), PS also starts with training the full model, but it shrinks not only the width dimension but also the depth, kernel size, and resolution dimensions of the full model. Additionally, PS ﬁne-tunes both large and small sub-networks rather than a single pruned network. As a result, PS provides a much more powerful once-for-all network that can ﬁt diverse hardware platforms and efﬁciency constraints compared to network pruning. We describe the details of the PS training ﬂow as follows:

5

Transformation 
 Matrix: 25x25
Published as a conference paper at ICLR 2020

Transformation 
 Matrix: 9x9

channel
 importance

channel
 importance

0.02
channel
 0.15 reorg. sorting 0.85
0.63

0.82

channel
 sorting

0.11

reorg.

0.46

O2

channel
 sorting
O3
O2

O1
train with full width

O1
progressively shrink the width

O1
progressively shrink the width

Figure 6: Progressive shrinking for elastic width. In this example, we progressively support 4, 3, and

2 channel settings. We perform channel sorting and pick the most important channels (with large L1

norm) to initialize the smaller channel settings. The important channels’ weights are shared.

stage i

stage i

O1 stage i

O1

• Elastic Kernel Size (Figure 5 left). We let the center of a 7x7 convolution kernel also serve as a 5x5 kernel, the center of which can also be a 3x3 kernel. Therefore, the kernel size bOe2comes

elastic. The challenge is that the centering sub-kernels O(e2.g., 3x3 and 5x5) are shared aOn3d need

to ptlraiyn wmituhlftuipll ldeeprtohles (indepenprdoegnretsksievernlyeslharinkdthpeardtepotfha large pkreorgnreesls)i.veTlyhsehrwinkeitghehtdsepotfhcentered

sub-kernels may need to have different distribution or magnitude as different roles. Forcing them

to be the same degrades the performance of some sub-networks. Therefore, we introduce kernel

transformation matrices when sharing the kernel weights. We use separate kernel transformation

matrices for different layers. Within each layer, the kernel transformation matrices are shared

among different channels. As such, we only need 25 × 25 + 9 × 9 = 706 extra parameters to store

the kernel transformation matrices in each layer, which is negligible.

• Elastic Depth (Figure 5 right). To derive a sub-network that has D layers in a unit that originally has N layers, we keep the ﬁrst D layers and skip the last N − D layers, rather than keeping any D layers as done in current NAS methods (Cai et al., 2019; Wu et al., 2019). As such, one depth setting only corresponds to one combination of layers. In the end, the weights of the ﬁrst D layers are shared between large and small models.

• Elastic Width (Figure 6). Width means the number of channels. We give each layer the ﬂexibility to choose different channel expansion ratios. Following the progressive shrinking scheme, we ﬁrst train a full-width model. Then we introduce a channel sorting operation to support partial widths. It reorganizes the channels according to their importance, which is calculated based on the L1 norm of a channel’s weight. Larger L1 norm means more important. For example, when shrinking from a 4-channel-layer to a 3-channel-layer, we select the largest 3 channels, whose weights are shared with the 4-channel-layer (Figure 6 left and middle). Thereby, smaller sub-networks are initialized with the most important channels on the once-for-all network which is already well trained. This channel sorting operation preserves the accuracy of larger sub-networks.

3.4 SPECIALIZED MODEL DEPLOYMENT WITH ONCE-FOR-ALL NETWORK
Having trained a once-for-all network, the next stage is to derive the specialized sub-network for a given deployment scenario. The goal is to search for a neural network that satisﬁes the efﬁciency (e.g., latency, energy) constraints on the target hardware while optimizing the accuracy. Since OFA decouples model training from neural architecture search, we do not need any training cost in this stage. Furthermore, we build neural-network-twins to predict the latency and accuracy given a neural network architecture, providing a quick feedback for model quality. It eliminates the repeated search cost by substituting the measured accuracy/latency with predicted accuracy/latency (twins).
Speciﬁcally, we randomly sample 16K sub-networks with different architectures and input image sizes, then measure their accuracy on 10K validation images sampled from the original training set. These [architecture, accuracy] pairs are used to train an accuracy predictor to predict the accuracy of a model given its architecture and input image size2. We also build a latency lookup table (Cai et al., 2019) on each target hardware platform to predict the latency. Given the target hardware and latency constraint, we conduct an evolutionary search (Real et al., 2019) based on the neural-network-twins to get a specialized sub-network. Since the cost of searching with neural-network-twins is negligible,
2Details of the accuracy predictor is provided in Appendix A.
1
6

.
p p p

Published as aPceonrffeorenrcme paapnercateIsCLoR f20S20ub-networks on ImageNet

w/o PS

w/ PS

78

ImageNet Top-1 Acc (%)

75 73 70 2.5%

2.8%

3.5%

3.4%

3.3%

3.4%

3.7%

3.5%

67 D=2 W=3 K=3

D=2

D=2

D=2

D=4

D=4

D=4

W=3

W=6

W=6

W=3

W=3

W=6

K=7

K=3

K=7

K=3

K=7

K=3

Sub-networks under various architecture configurations D: depth, W: width, K: kernel size

D=4 W=6 K=7

Figure• 7P: IromgargeesNsivetetsohpr1inakcincgurcaocyns(i%ste) nptelyrfiomrmpraonvceessaocfcsuurba-cnyeotwf sourkbs-nuentwdeorrkressoonluItmioang2e2N4e×t. 224. “(D = d, W = w, K = k)” denotes a sub-network with d layers in each unit, and each layer has an width expansion ratio w and kernel size k.
40

we only need 40 GPU hours to collect the data pairs, and the cost stays constant regardless of #deployment scenarios.

4 EXPERIMENTS
In this section, we ﬁrst apply the progressive shrinking algorithm to train the once-for-all network on ImageNet (Deng et al., 2009). Then we demonstrate the effectiveness of our trained once-for-all network on various hardware platforms (Samsung S7 Edge, Note8, Note10, Google Pixel1, Pixel2, LG G8, NVIDIA 1080Ti, V100 GPUs, Jetson TX2, Intel Xeon CPU, Xilinx ZU9EG, and ZU3EG FPGAs) with different latency constraints.
4.1 TRAINING THE ONCE-FOR-ALL NETWORK ON IMAGENET
Training Details. We use the same architecture space as MobileNetV3 (Howard et al., 2019). For training the full network, we use the standard SGD optimizer with Nesterov momentum 0.9 and weight decay 3e−5. The initial learning rate is 2.6, and we use the cosine schedule (Loshchilov & Hutter, 2016) for learning rate decay. The full network is trained for 180 epochs with batch size 2048 on 32 GPUs. Then we follow the schedule described in Figure 3 to further ﬁne-tune the full network3. The whole training process takes around 1,200 GPU hours on V100 GPUs. This is a one-time training cost that can be amortized by many deployment scenarios.
Results. Figure 7 reports the top1 accuracy of sub-networks derived from the once-for-all networks that are trained with our progressive shrinking (PS) algorithm and without PS respectively. Due to space limits, we take 8 sub-networks for comparison, and each of them is denoted as “(D = d, W = w, K = k)”. It represents a sub-network that has d layers for all units, while the expansion ratio and kernel size are set to w and k for all layers. PS can improve the ImageNet accuracy of sub-networks by a signiﬁcant margin under all architectural settings. Speciﬁcally, without architecture optimization, PS can achieve 74.8% top1 accuracy using 226M MACs under the architecture setting (D=4, W=3, K=3), which is on par with MobileNetV3-Large. In contrast, without PS, it only achieves 71.5%, which is 3.3% lower.
4.2 SPECIALIZED SUB-NETWORKS FOR DIFFERENT HARDWARE AND CONSTRAINTS
We apply our trained once-for-all network to get different specialized sub-networks for diverse hardware platforms: from the cloud to the edge. On cloud devices, the latency for GPU is measured with batch size 64 on NVIDIA 1080Ti and V100 with Pytorch 1.0+cuDNN. The CPU latency is measured with batch size 1 on Intel Xeon E5-2690 v4+MKL-DNN. On edge devices, including mobile phones, we use Samsung, Google and LG phones with TF-Lite, batch size 1; for mobile GPU,
3Implementation details can be found in Appendix B.

7

Design

Scratch

High (Low) Workload

Published as a conference paper at ICLR 2020

Repeated architecture 
 design and model training

Model

ImageNet Top1 (%)

MobileNetV2 [31]

72.0

MobileNetV2 #1200

73.5

NASNet-A [44]

74.0

DARTS [25]

73.1

MnasNet [33]

74.0

FBNet-C [36] ProxylessNAS [4]

Proxyl7e4ss.9NAS 74.6

SinglePathNAS [8]

74.7

AutoSlim [38]

74.2

MobileNetV3-Large [15] OFA w/o PS

75F.B2Net 72.4

OFA w/ PS

76.0

OFA w/ PS #25

76.4

OFA w/ PS #75 OFALarge w/ PS #75

M7n6a.9sNet 80.0

MACs
300M 300M 564M 595M 317M 375M 320M 328M 305M 219M 235M 230M 230M 230M 595M

Mobile latency 66ms 66ms
70ms 14.-3k 71ms 63ms 5186m.3sk 59ms 58ms 58ms 58ms -

Search cost Training cost

Total cost (N = 40)

(GPU hours) (GPU hours) GPU hours CO2e (lbs) AWS cost

0

150N

6k

1.7k

$18.4k

0

1200N

48k

13.6k

$146.9k

N48=,04000N 96N

-

N = 110,9020k

250N

14k

544.5k 4.0k

$5875.2k $42.8k

40,000N

-

1,600k

453.8k $4896.0k

216N

360N

23k

6.5k

$70.4k

200N

300N

20k

5.7k

$61.2k

288 + 24N

384N

17k

4.8k

$52.0k

180

300N

12k

3.4k

$36.7k

-

180N

7.2k

1.8k

$22.2k

40

1200

1.2k

0.34k

$3.7k

40

1200

1.2k

0.34k

$3.7k

40

1200 + 25N

2.2k

0.62k

$6.7k

40 453.8k1200 + 75N

4.2k

1.2k1134.5k $13.0k

40

1200 + 75N

4.2k

1.2k

$13.0k

0

30000

60000

90000

120000

Table 1: Comparison with SOTA hardware-aware NAS methods on Pixel1 phone. OFA decouples

model training from neural architecture search. The search cost and training cost both stay constant

as the number of deployment scenarios grows. “#25” denotes the specialized sub-networks are ﬁne-tuned for 25 epochs after grabbing weights from the once-for-all network. “CO2e” denotes CO2 emission which is calculated based on Strubell et al. (2019). AWS cost is calculated based on the

price of on-demand P3.16xlarge instances.

ProxylessNAS FBNet MnasNet
CO

Total cost (lbs of CO2 emission), N = 40

ProxylessNAS

5.7k

FBNet
MnasNet OFA 0.34k
0

6.5k
12500

1300x
25000

454k

37500

50000

ProxylessNAS

5

FBNet

6

MnasNet

45

Figure 8: OFA saves orders of magnitude design cost compared to NAS methods.

OFA

we use Jetson TX2 with Pytorch 1.0+cuDNN, batch size of 16; for embedded FPGA, we use Xilinx ZU9EG and ZU3EG FPGAs with Vitis AI4, batch size 1.

Comparison with NAS on Mobile Devices. Table 1 reports the comparison between OFA and

state-of-the-art hardware-aware NAS methods on the mobile phone (Pixel1). OFA is much more

efﬁcient than NAS when handling multiple deployment scenarios since the cost of OFA is constant

while others emissions of

are lineaArrctohitheectnuurme ber OFA is 16D×efseiwgner than

oPfrodxeyplleoTsyrSsamNcinerAanfSttrc,osh1cme9n×arfieowse(rNt)h.aWn FitEFBhudNNlgleebt=,,a
at4tn0ed,ryt1h,e30t0o×talfeCwOOe2rnce-for-all

than MnasNet (Figure 8). Without retraining, OFA achieves 76.0% top1 accuracy on ImageNetn, etwork

which is 0.8% higher than MobileNetV3-Large while maintaining similar mobile latency. We can

further improve the top1 accuracy to 76.4% by ﬁne-tuning the specialized sub-network for 25 epochs

and to 76.9% by ﬁne-tuning for 75 epochs. Besides, we also observe that OFA with PS can achieve

3.6% better accuracyAthrcanhiwteicthtuoruet PS.

Train from

Edge,


Design

Scratch

Low battery

OFA under Different Computational Resource Constraints. Figure 9 summarizes the results

of OFA under different MACs and Pixel1 latency constraints. OFA achieves 79.1% ImageNet top1

accuracy with 389M MACs, being 2.8% more accurate than EfﬁcientNet-B0 that has similar MACs.

With 595M MACs, OAFrcAhirteeaccthuerse a new SOTATra8i0n.0f%romImageNet top1 accuracy under the mobile

setting (<600M MACs)D, wehsiicghnis 0.2% higher More importantly, OFA runs much faster than

tEhSfaﬁcncraEietfncﬁthcNieentstNoent-hBa2rdwwhairlee.uSsipCneglcoi1uﬁ.d6ca8l×ly,fewwitehr

MACs. 143ms

Pixel1 latency, OFA achieves 80.1% ImageNet top1 accuracy, being 0.3% more accurate and 2.6×

faster than EfﬁcientNet-B2. We also ﬁnd that training the searched neural architectures from scratch

cannot reach the same level of accuracy as OFA, suggesting that not only neural architectures but

also pre-trained weights contribute to the superior performances of OFA.

…

oFRfiegmluarateerkn1ac0bylryce,opOnosrFttArsadcineattnasiplberydodtcruoacmienpitanhrgeiseoonnnRtlseiyrpbeoeenttaPrwcateeredeed(eng-vaorOeirfocefFnhcuAuitcseruavcnrevtdsue)rMwe. iot
Ihbt iimlseNaimneytpVop3sosiionbntlsesoifxvoermrpoarbewviliieoduedservNaincAgeSse.
methods (Tan et al., 2019; Cai et al.,d2e0s1i9g)nduaendtomthoedperol htriabiintiivnegtraining cost.

specialized sub-nets
Train many

4https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html

8

Untitled 4

54.0

75.0

56.0

75.2

58.0

75.3

60.0

75.4

Published

as

a

conference6625..00paper

at ICLR 75.6 75.7

2020

Top-1 ImageNet Acc (%)

OFA

OFA - LTGrGa8 in from scratch

EfﬁcientNet

81
Untitled 1
80 Untitled 2 Untitled 3

OFA
79.6

#5 Acc Loss

OFA #25

#5 Acc Loss

8.3
80.08.9

1.68x MA69.C4 s reducti7o0.n0

10.0

70.9

8.3 11.3
79.816.0

81 MobileNetV3-
Small

71.1

73.0 74.7

80

#5 Acc Loss

MobileNetV3-

#5 Acc Loss

Large

80.1

8.22.6x laten6c7y.4 11.0 reduction70.4

79.8

17.0

73.3

Untitled 4
79.1

11.3

72.1

24.0

76.4

13.3

73.0

24.0

75.2 79.8

79

14.0 78.8 73.5

2.8% higher 15.3

73.9

79

78.7

78.2

78.8

78

accuracy

16.0
178.07.9

74.3 75.0

78

78.0

77

19.0

75.2

77.6

20.0

75.3

77.0

21.0

75.4

3.8% higher

77

accuracy

76 200

22.0

76.3

23.0

24.0

400 600

75.6 75.8 76.0
800 1,000

1,200

76 0

76.4 76.3
50 100 150 200 250 300 350 400

MACs (M)

Google Pixel1 Latency (ms)

Top-1 ImageNet Acc (%) Top-1 ImageNet Acc (%)

OFA OFA - Train from scratch 77

75
73
71.4

74.9
73.3 1.5x speedup 73.3

71
70.4

69 4% higher
accuracy
67.4
67 18 24 30 36 42 48

Google Pixel1 Latency (

Figure 9: OFA achieves 80.0% top1 accuracy with 595M MACs and 80.1% top1 accuracy with 143ms Pixel1 latency, setting a new SOTA ImageNet top1 accuracy on the mobile setting.

Top-1 ImageNet Acc (%)

77

75

74.7

73.1

73

73.3

70.5 71
70.4

69

76.3 75.2

67 67.4 25 40 55 70 85 100

Samsung S7 Edge Latency (ms)

77

76.4

74.9 75
75.2

73.3

73

73.3

71.4

71 70.4
69

Top-1 ITomp-a1gIemaNgeetNeAtcAcc(c (%%))

Top-1 ImageNet Acc (%)

Top-1 ImageNet Acc (%)

OFA #25
7781
75
80
73
7970.4
71
6978

OFOAFA

EMfﬁocbielenNteNtVe3t

79.6 72.8 79.1

76.1

8704..90

1.68x MACs reduc7ti5o.2n

73.3

77
75 79.8 73
71.4

78.8

71

70.4

2.8% higher

accuracy

69

77
75.5
73.6 75
73.3
73
70.4
71 69.8

OFA
76.6 75.2
72.6
70.4

MobileNetV3
76.3 75.2
75.2
73.3

6777 67.4
20 26 32 38 44 50 56 62 68

67

67.4
69

7 9 11 13 15 17 19 21 23

Samsung7N6o.3te8 Latency (ms)
7776 200 400 600 8705.08

77
1,000

Samsung
1,200

Note10
67 40

Lat6e7n.c4y (ms)
80 76.4

120

160

75

M7A4.7Cs (M) 75.2

75

74.7

75.M2 ACs (M)

73.4

73

73.3

73.0

73

73.3

71.5 71.1

71 70.4

71 70.4

200

240

69

69

Top-1 ImageNet Acc (%)

Top-1 ImageNet Acc (%)

67

67.4

18 24 30 36 42 48 54 60

Google Pixel1 Latency (ms)

67

67.4

23 28 33 38 43 48 53 58 63 68

Google Pixel2 Latency (ms)

67

67.4

7 10 13 16 19 22 25

LG G8 Latency (ms)

Figure 10: OFA consistently outperforms MobileNetV3 on mobile platforms.

Top-1 ImageNet Acc (%)

OFA for Diverse Hardware Platforms. Besides the mobile platforms, we extensively studied the effectiveness of OFA on six additional hardware platforms (Figure 11) using the ProxylessNAS architecture space (Cai et al., 2019). OFA consistently improves the trade-off between accuracy and latency by a signiﬁcant margin, especially on GPUs which have more parallelism. With similar latency as MobileNetV2 0.35, “OFA #25” improves the ImageNet top1 accuracy from MobileNetV2’s 60.3% to 72.6% (+12.3% improvement) on the 1080Ti GPU. Detailed architectures of our specialized models are shown in Figure 14. It reveals the insight that using the same model for different deployment
1 scenarios with only the width multiplier modiﬁed has a limited impact on efﬁciency improvement:
the accuracy drops quickly as the latency constraint gets tighter.
OFA for Specialized Hardware Accelerators. There has been plenty of work on NAS for generalpurpose hardware, but little work has been focused on specialized hardware accelerators. We quantitatively analyzed the performance of OFA on two FPGAs accelerators (ZU3EG and ZU9EG) using Xilinx Vitis AI with 8-bit quantization, and discuss two design principles.
Principle 1: memory access is expensive, computation is cheap. An efﬁcient CNN should do as much as computation with a small amount of memory footprint. The ratio is deﬁned as the arithmetic intensity (OPs/Byte). The higher OPs/Byte, the less memory bounded, the easier to parallelize. Thanks to OFA’s diverse choices of sub-network architectures (1019) (Section 3.3), and the OFA
9

MobileNetV2

MnasNet

OFA

5

7

9

11MobileNet1V23

M1n5asNet

OFA

0.5

35.3

37.1

49.4

0.35

77

67.6

126.8

2080Ti Lat0e.3n5 cy (ms) 36

31.8

61.2

0.75

51.6

51.9

54.4

0.5

102.6

94.4

155.3

0.5

48.1

44.0

75.5

1.0

61

61.2

63.9

0.75

150.6

135.4

164.6

0.75

67.8

81.3

Published as a conference paper at ICLR 2020 1.0

185

167.3

186.3

1.0

79

83.7

OFA #25 OFA MobMilenNaseNt-evt2 MobileMNnetaVs2Net SlimmabOleFNAe(tsOurs)

ZU3EG FPGA (GOPS/s)
Out of BRAM Out of BRAM

ZU9EG FPGA (GOPS/s)

Arithmetic Intensity (OPS/Byte)
Top-1 ImageNet Acc (%)

70

77

73.8

53

73 72.6

69
35

75.3

76.4 77 200

72.0 69.8

73 15701.6 73.0
69
100

66

65.4

66

65.4

75.3

76.1

72.0 69.8

77

73

72.0

71.1

69

66

65.4

90

75.7

74.6

68

72.0

69.8
45

18
62
60.3

50
62
60.3

23
62
60.3

5.0 6.0

0
7.0 8.0

58

58 0

58

0

0.35x10 01.45x 18 0.752x2 216.0x 30 4

06.35x 8 0.5x 10 0.7152x 91.0x 11 13 15 0.351x7 109.5x

0.75x

(under diffNeVreIDnItAla10te8n0TciyLcaotennsctyra(minst)s)

NVI(DuInAdVe1r0d0iLffaetreenncyt l(amtse)ncy constraintsI)ntel Xeon CPU Laten(cuyn(dmesr) different latency constraints)

Batch Size = 64

Batch Size = 64

Batch Size = 1

77

73

72.9

70.3

69

75.4 75.8

77

72.0 69.8

73
69.4
69 67.0

73.6 72.8
71.5 69.0

77
73
69.6
69
67.0

73.7 72.8
71.5 69.0

1.0x

Top-1 ImageNet Acc (%)

5.0 6.0 7.0 8.0

66

65.4

62
58 30

on Xilinx ZU9EG FPGA 60.3
45 60 75 90 Jetson TX2 Latency (ms)
Batch Size = 16

66
63.3
62 on Xilinx ZU3EG FPGA

66
63.3
62

59.1
58

59.1
58

105 1.5 2.0 2.5 3.0 3.5 4.0 3.0 4.0 5.0 6.0 7.0 8.0

Xilinx ZU9EG FPGA Latency (ms) Batch Size = 1 (Quantized)

Xilinx ZU3EG FPGA Latency (ms) Batch Size = 1 (Quantized)

Fig7u7re 11: Specialized OFA model7s7 consistently achieve signiﬁcan7t7ly higher ImageNet accuracy

wit7h3similar latency than non-specia7l3ized neural networks on CPU, G73PU, mGPU, and FPGA. More

remarkably, specializing for a new hardware platform does not add training cost using OFA.

69

69

69

66
50.0

Mobi6le6Net-v2 160.0

MnasNet

OFA66(Ours) 80.0

62
37.5

62
120.0

62
60.0

58

58

58

3.205.0 4.0 5.0 6.0 7.0 8.0 3.0 804.0.0 5.0 6.0 7.0 8.0 3.0 44.00.0 5.0 6.0 7.0 8.0

ZU3EG FPGA (GOPS/s)

ZU9EG FPGA (GOPS/s)

Arithmetic Intensity (OPS/Byte)

12.5

40.0

20.0

0.0

0.0

0.0

Figure 12: OFA models improve the arithmetic intensity (OPS/Byte) and utilization (GOPS/s) compared with the MobileNetV2 and MnasNet (measured results on Xilinx ZU9EG and ZU3EG FPGA).

model twin that can quickly give the accuracy/latency feedback (Section 3.4), the evolutionary search can automatically ﬁnd a CNN architecture that has higher arithmetic intensity. As shown in Figure 12, OFA’s arithmetic intensity is 48%/43% higher than MobileNetV2 and MnasNet (MobileNetV3 is not supported by Xilinx Vitis AI). Removing the memory bottleneck results in higher utilization and GOPS/s by 70%-90%, pushing the operation point to the upper-right in the rooﬂine model (Williams et al., 2009), as shown in Figure 13. (70%-90% looks small in the log scale but that is signiﬁcant).
Principle 2: the CNN architecture should be co-designed with the hardware accelerator’s cost model. The FPGA accelerator has a specialized depth-wise engine that is pipelined with the point-wise engine. The pipeline throughput is perfectly matched for 3x3 kernels. As a result, OFA’s searched model only has 3x3 kernel (Figure 14, a) on FPGA, despite 5x5 and 7x7 kernels are also in the search space. Additionally, large kernels sometimes cause “out of BRAM” error on FPGA, giving high cost. On Intel Xeon CPU, however, more than 50% operations are large kernels. Both FPGA and GPU models are wider than CPU, due to the large parallelism of the computation array.
5 CONCLUSION
We proposed Once-for-All (OFA), a new methodology that decouples model training from architecture
1 search for efﬁcient deep learning deployment under a large number of hardware platforms. Unlike
10
1

Published as a conference paper at ICLR 2020

(a) on Xilinx ZU9EG FPGA

(b) on Xilinx ZU3EG FPGA

Figure 13: Quantative study of OFA’s rooﬂine model on Xilinx ZU9EG and ZU3EG FPGAs (log scale). OFA model increased the arithmetic intensity by 33%/43% and GOPS/s by 72%/92% on these two FPGAs compared with MnasNet.

CCoCononvnvv33x3x3x33 MMBMB1B1133x3x3x33 MMBB4433xx33 MB4 3x3 MMBB4433xx33 MB4 3x3 MMBB4433xx33 MB4 3x3 MMBB5533xx33 MB5 3x3 MMBB5533xx33 MB5 3x3 MMBB5533xx33 MB5 3x3 MMBB5533xx33 MB5 3x3 MMBB4433xx33 MB4 3x3 MMBB5533xx33 MB5 3x3 MMBB6633xx33 MB6 3x3 MMBB5533xx33 MMBMB5B6633x3x3x33 PMPooBolo6ilin3ngxg3FFCC Pooling FC

111666444xxx111666444

(a) 4.1ms latency on Xilinx ZU3EG (batch size = 1).

ZZUU33EEGG44.1.1mmZsUs(R3(RE=G=11466.414)m) s (R = 164)
(3(x33x_3M_MBBCConovn1v_1(R_3REx3LE_ULMU6 6BConv1_RELU6
(3(x33x_3M_MBBCConovn4v_4(R_3REx3LE_ULMU6 6BConv4_RELU6
(3(x33x_3M_MBBCConovn4v_4(R_3REx3LE_ULMU6 6BConv4_RELU6
(3x3_MBConv4_RELU6 (3x3_MBConv4_(R3Ex3L_UM6 BConv4_RELU6
(3x3_MBConv5_RELU6 (3x3_MBConv5_(R3Ex3L_UM6 BConv5_RELU6
(3x3_MBConv5_RELU6 (3x3_MBConv5_(R3Ex3L_UM6 BConv5_RELU6
(3x3_MBConv5_RELU6 (3x3_MBConv5_(R3Ex3L_UM6 BConv5_RELU6
(3x3_MBConv5_RELU6 (3x3_MBConv5_(R3Ex3L_UM6 BConv5_RELU6
(3x3_MBConv4_RELU6 (3x3_MBConv4_(R3Ex3L_UM6 BConv4_RELU6 (3(x33x_3M_MBBCConovn5v_5R_RELEULU6 6 (3(x33x_3M_MBBCConovn6v_6(R_3REx3LE_ULMU6 6BConv5_RELU6
(3x3_MBConv5(_3Rx3E_LMU6BConv6_RELU6 (3x3_MBConv5_RELU6 (3(x33x_3M_MBBCConovn6v_6(R_3REx3LE_ULMU6 6BConv5_RELU6
(3x3_MBConv6_RELU6

ConvCCo3onxnv3v33xx33 MB1M3MBxB13133xx33
MMBB4433xx33 MB4 3x3
MMBB4433xx33 MB4 3x3
MMBB6633xx33 MB6 3x3
MMBB6633xx33 MB6 3x3
MMBB4433xx33 MB4 3x3
MMBB3333xx33 MB3 3x3
MMBB3355xx55 MB3 5x5
MMBB3333xx33 MB3 3x3
MMBB4455xx55 MB4 5x5
MMBB3333xx33 MB3M3MBxB63633xx33 MB6M3MxBB63633xx33 MB6M3MBxB63677xx77 MB6M7MxBB47477xx77 MB4M7MBxB67633xx33 MB6M3MBxB33355xx55 MB3PPoo5oxolili5nnggFFCC Pooling FC

ConCCvoon3nvxv333xx33 MBM1MB3B1x1333xx33
MMBB4455xx55 MB4 5x5
MMBB3355xx55 MB3 5x5
MMBB3333xx33 MB3 3x3
MMBB3377xx77 MB3 7x7
MMBB4455xx55 MB4 5x5
MMBB4433xx33 MB4 3x3
MMBB3377xx77 MB3 7x7
MMBB4455xx55 MB4 5x5
MMBB4477xx77 MB4 7x7
MMBB6633xx33 MBM6MB3B4x4333xx33 MBM4MB3B4x4355xx55 MPBPo4ooloi5lixnn5ggFFCC Pooling FC

114444xx114444 144x144
114444xx114444 144x144

(b) 10.9ms latency on Intel Xeon CPU (batch size = 1).

CCPPUU1100.9.9mmss(R(R==114444) )

3x33x_3C_Coonvn_vO_O4040

CPU 10.9ms (R = 144

(3(x33x_3M_MBBCConovn1v_31R_xR3E_ELCULUo6_n6Ov_O_2O42440 (5(x55x_5M_MBBCConovn4v_(43R_xRE3EL_ULMU6B_6OC_O3o23n2v1_RELU6_O24

(5(x55x_5M_MBBCConovn3v_(35R_xRE5EL_ULMU6B_6OC_O3o23n2v4_RELU6_O32 (3(x33x_3M_MBBCConovn3v_(35R_xRE5EL_ULMU6B_6OC_O5o65n6v3_RELU6_O32 (7(x77x_7M_MBBCConovn3v_(33R_xRE3EL_ULMU6B_6OC_O5o65n6v3_RELU6_O56 (5(x55x_5M_MBBCConovn4v_(47R_xRE7EL_ULMU6B_6OC_O1o01n40v43_RELU6_O56 (3(x33x_3M_MBBCConovn4v_(45R_xRE5EL_ULMU6B_6OC_O1o01n40v44_RELU6_O104 ((75((xx7575xx__75MM__MMBBBBCCCCoonnoovvnn34vv__(343RR__xRREE3EELL_UULLMUU66B__66OOC__OO11o0211n4802v484_RELU6_O104 (7(x77x_7M_MBBCConovn4v_(47R_xRE7EL_ULMU6B_6OC_O1o21n82v83_RELU6_O104 (3(x33x_3M_MBBCConovn6v_(65R_xRE5EL_ULMU6B_6OC_O2o42n84v84_RELU6_O128 (3(x33x_3M_MBBCConovn4v_(47R_xRE7EL_ULMU6B_6OC_O2o42n84v84_RELU6_O128 (5(x55x_5M_MBBCConovn4v_(43R_xRE3EL_ULMU6B_6OC_O4o14n61v66_RELU6_O248
1x11x_1C_Coonvn_vO_O16166(346x43_MBConv4_RELU6_O248

1616646x41x010000_0L_iLnien(5aexra5r _MBConv4_RELU6_O416

1x1_Conv_O1664

1664x1000_Linear

(c) 14.9ms latency on NVIDIA 1080Ti (batch size = 64).

Figure 14: OFA can design specialized models for different hardware and different latency constraint. “MB4 3x3” means “mobile block with expansion ratio 4, kernel size 3x3”. FPGA and GPU models are wider than CPU model due to larger parallelism. Different hardware has different cost model, leading to different optimal CNN architectures. OFA provides a uniﬁed and efﬁcient design methodology.

previous approaches that design and train a neural network for each deployment scenario, we designed a once-for-all network that supports different architectural conﬁgurations, including elastic depth, width, kernel size, and resolution. It reduces the training cost (GPU hours, energy consumption, and CO2 emission) by orders of magnitude compared to conventional methods. To prevent sub-networks of different sizes from interference, we proposed a progressive shrinking algorithm that enables a large number of sub-network to achieve the same level of accuracy compared to training them independently. Experiments on a diverse range of hardware platforms and efﬁciency constraints demonstrated the effectiveness of our approach. OFA provides an automated ecosystem to efﬁciently design efﬁcient neural networks with the hardware cost model in the loop.
ACKNOWLEDGMENTS
We thank NSF Career Award #1943349, MIT-IBM Watson AI Lab, Google-Daydream Research Award, Samsung, Intel, Xilinx, SONY, AWS Machine Learning Research Award for supporting this
11

Published as a conference paper at ICLR 2020
research. We thank Samsung, Google and LG for donating mobile phones. We thank Shuang Wu and Lei Deng for drawing the Figure 2.
REFERENCES
Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and Kris M Kitani. N2n learning: Network to network compression via policy gradient reinforcement learning. In ICLR, 2018.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efﬁcient architecture search by network transformation. In AAAI, 2018a.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for efﬁcient architecture search. In ICML, 2018b.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In ICLR, 2019. URL https://arxiv.org/pdf/1812.00332.pdf.
Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one. In NeurIPS, 2019.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In NeurIPS, 2015.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. arXiv preprint arXiv:1904.00420, 2019.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural network. In NeurIPS, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.
Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow, Wen-mei Hwu, and Deming Chen. Fpga/dnn co-design: An efﬁcient design methodology for 1ot intelligence on the edge. In 2019 56th ACM/IEEE Design Automation Conference (DAC), pp. 1–6. IEEE, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In ECCV, 2018.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV 2019, 2019.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Weinberger. Multi-scale dense networks for resource efﬁcient image classiﬁcation. In ICLR, 2018.
12

Published as a conference paper at ICLR 2020
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.
Weiwen Jiang, Lei Yang, Edwin Sha, Qingfeng Zhuge, Shouzhen Gu, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration of neural architectures. arXiv preprint arXiv:1907.04650, 2019a.
Weiwen Jiang, Xinyi Zhang, Edwin H-M Sha, Lei Yang, Qingfeng Zhuge, Yiyu Shi, and Jingtong Hu. Accuracy vs. efﬁciency: Achieving both through fpga-implementation aware neural architecture search. In Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1–6, 2019b.
Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon See, and Yap-Peng Tan. Stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks. In CVPR, 2018.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In NeurIPS, 2017.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In ECCV, 2018.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019.
Lanlan Liu and Jia Deng. Dynamic deep neural networks: Optimizing accuracy-efﬁciency trade-offs by selective execution. In AAAI, 2018.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efﬁcient convolutional networks through network slimming. In ICCV, 2017.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for efﬁcient cnn architecture design. In ECCV, 2018.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiﬁer architecture search. In AAAI, 2019.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In ACL, 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In ECCV, 2018.
Samuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: An insightful visual performance model for ﬂoating-point programs and multicore architectures. Technical report, Lawrence Berkeley National Lab.(LBNL), Berkeley, CA (United States), 2009.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural architecture search. In CVPR, 2019.
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, and Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In CVPR, 2018.
13

Published as a conference paper at ICLR 2020

Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728, 2019a.
Jiahui Yu and Thomas Huang. Universally slimmable networks and improved training techniques. In ICCV, 2019b.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. In ICLR, 2019.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional neural network for mobile devices. In CVPR, 2018.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. In ICLR, 2017.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.

A DETAILS OF THE ACCURACY PREDICTOR

We use a three-layer feedforward neural network that has 400 hidden units inOnceefoar Acllh#25layer as the

accuracy predictor. Given a model, we encodUentietleadc1h layer in the neural network into a on1e6.3-hot vecto7r2.4

based on its kernel size and expand ratio, and we Untitled 2 assign zero vectors to layers that a8.r7e skipped72..2

Besides, we have an additional one-hot vectoUnrtittlehda3 t represents the input image size. We c4.o5 ncatenate72.7

these vectors into a large vector that represents the whole neural network architecture and 2i.n3 put image72.8

1.0

74.1

size, which is then fed to the three-layer feedforward neural network to get the predicted accuracy. In

0.5

74.7

our experiments, this simple accuracy prediction model can provide very accurate pred0.i2ctions. At75.1

convergence, the root-mean-square error (RMSE) between predicted accuracy and estimated accuracy

on the test set is only 0.21%. Figure 15 shows the relationship between the RMSE of the accuracy

prediction model and the ﬁnal results (i.e., the accuracy of selected sub-networks). We can ﬁnd that

lower RMSE typically leads to better ﬁnal results.

75.5

Acc of Selected Sub-net (%)

74.6

73.8

72.9

72.0 0

5

10 15 20

RMSE of Acc Prediction Model (%)

Figure 15: Performances of selected sub-networks using different accuracy prediction model.

B IMPLEMENTATION DETAILS OF PROGRESSIVE SHRINKING
After training the full network, we ﬁrst have one stage of ﬁne-tuning to incorporate elastic kernel size. In this stage (i.e., K ∈ [7, 5, 3]), we sample one sub-network in each update step. The network is ﬁne-tuned for 125 epochs with an initial learning rate of 0.96. All other training settings are the same as training the full network.
14

Published as a conference paper at ICLR 2020 Next, we have two stages of ﬁne-tuning to incorporate elastic depth. We sample two sub-networks and aggregate their gradients in each update step. The ﬁrst stage (i.e., D ∈ [4, 3]) takes 25 epochs with an initial learning rate of 0.08 while the second stage (i.e., D ∈ [4, 3, 2]) takes 125 epochs with an initial learning rate of 0.24. Finally, we have two stages of ﬁne-tuning to incorporate elastic width. We sample four sub-networks and aggregate their gradients in each update step. The ﬁrst stage (i.e., W ∈ [6, 4]) takes 25 epochs with an initial learning rate of 0.08 while the second stage (i.e., W ∈ [6, 4, 3]) takes 125 epochs with an initial learning rate of 0.24.
15

