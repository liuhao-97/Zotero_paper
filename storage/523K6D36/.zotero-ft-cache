IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

369

Inter-Cluster Thread-to-Core Mapping and DVFS on Heterogeneous Multi-Cores

Basireddy Karunakar Reddy , Amit Kumar Singh, Member, IEEE, Dwaipayan Biswas , Geoff V. Merrett , Member, IEEE, and Bashir M. Al-Hashimi, Fellow, IEEE

Abstract—Heterogeneous multi-core platforms that contain different types of cores, organized as clusters, are emerging, e.g., ARM’s big.LITTLE architecture. These platforms often need to deal with multiple applications, having different performance requirements, executing concurrently. This leads to the generation of varying and mixed workloads (e.g., compute and memory intensive) due to resource sharing. Run-time management is required for adapting to such performance requirements and workload variabilities and to achieve energy efﬁciency. Moreover, the management becomes challenging when the applications are multi-threaded and the heterogeneity needs to be exploited. The existing run-time management approaches do not efﬁciently exploit cores situated in different clusters simultaneously (referred to as inter-cluster exploitation) and DVFS potential of cores, which is the aim of this paper. Such exploitation might help to satisfy the performance requirement while achieving energy savings at the same time. Therefore, in this paper, we propose a run-time management approach that ﬁrst selects thread-to-core mapping based on the performance requirements and resource availability. Then, it applies online adaptation by adjusting the voltage-frequency (V-f) levels to achieve energy optimization, without trading-off application performance. For thread-to-core mapping, ofﬂine proﬁled results are used, which contain performance and energy characteristics of applications when executed on the heterogeneous platform by using different types of cores in various possible combinations. For an application, thread-to-core mapping process deﬁnes the number of used cores and their type, which are situated in different clusters. The online adaptation process classiﬁes the inherent workload characteristics of concurrently executing applications, incurring a lower overhead than existing learning-based approaches as demonstrated in this paper. The classiﬁcation of workload is performed using the metric Memory Reads Per Instruction (MRPI). The adaptation process pro-actively selects an appropriate V-f pair for a predicted workload. Subsequently, it monitors the workload prediction error and performance loss, quantiﬁed by instructions per second (IPS), and adjusts the chosen V-f to compensate. We validate the proposed run-time management approach on a hardware platform, the Odroid-XU3, with various combinations of multi-threaded applications from PARSEC and SPLASH benchmarks. Results show an average improvement in energy efﬁciency up to 33 percent compared to existing approaches while meeting the performance requirements.
Index Terms—Heterogeneous multi-cores, multi-threaded applications, run-time management, performance, energy consumption
Ç

1 INTRODUCTION AND MOTIVATION
HETEROGENEOUS multi-core architectures are computing alternatives for several application domains such as embedded [1] and cloud [2]. These architectures integrate several types of processing cores within a single chip. For example, ARM’s big.LITTLE architecture contains two types of cores; big and LITTLE, where big cores are grouped into one cluster and LITTLE cores into another [3]. The big cluster has both higher cache capacity and computational power than the LITTLE one. In such architectures, distinct features of different types of cores can be exploited to meet end user
 B. K. Reddy, D. Biswas, G. V. Merrett, and B. M. Al-Hashimi are with the Department of Electronics and Computer Science, University of Southampton, Southampton, Hampshire SO17 1BJ, United Kingdom. E-mail: {krb1g15, db9g10, gvm, bmah}@ecs.soton.ac.uk.
 A. K. Singh is with the School of Computer Science and Electronic Engineering, University of Essex, Colchester, Essex CO4 3SQ, United Kingdom. E-mail: a.k.singh@essex.ac.uk.
Manuscript received 16 Dec. 2016; revised 9 Aug. 2017; accepted 6 Sept. 2017. Date of publication 26 Sept. 2017; date of current version 14 Sept. 2018. (Corresponding author: Karunakar Reddy Basireddy.) Recommended for acceptance by S. Le Beux, P.V. Gratz, and I. O’Connor. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identiﬁer below. Digital Object Identiﬁer no. 10.1109/TMSCS.2017.2755619

requirements. These architectures are also equipped with dynamic voltage and frequency scaling (DVFS) capabilities that enable on-the-ﬂy linear reduction of frequency (f) and voltage (V), yielding a cubic reduction in dynamic power consumption (/ V 2f). This facilitates to save energy if the power consumption is reduced enough to cover the extra time it takes to run the workload at a lower voltage-frequency (V-f).
Modern systems equipped with heterogeneous multicore chips need to deal with multiple applications running concurrently (at the same time) while achieving the desired levels of performance for each of them. Moreover, modern applications are multi-threaded (to exploit multi-core chips), which can be mapped onto different cores for parallel execution, and thus reducing the overall completion time.
Efﬁcient run-time management of multi-threaded applications on heterogeneous multi-cores is of paramount importance to achieve energy efﬁciency and high performance requirements, that have been a key research focus for mobile and embedded systems [4], [5], [6]. In general, for each application, the management process ﬁrst ﬁnds a thread-to-core mapping deﬁning the number of used cores and their type, and then operating voltage-frequency levels of cores by looking the workload while satisfying the

2332-7766 ß 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tp://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

370

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

Fig. 1. Execution time (seconds) and energy consumption (J) values by executing the Blackscholes application (from PARSEC benchmark [9]) with various core combinations, including inter-cluster, on ARM’s big. LITTLE architecture containing four big (B) and four LITTLE (L) cores.
performance requirement. As part of these, the following experimental observations have been made, which form the motivation behind the proposed approach.
Observation 1: Fig. 1 shows the motivation to map an application on a heterogeneous multi-core architecture containing two types of cores, big (B) and LITTLE (L), where 4B and 4L cores are present. The horizontal axis shows various possible resource combinations. The vertical primary (left-hand side) and secondary (right-hand side) axes show execution time and energy consumption, respectively, when executing at the various resource combinations. The application execution time scales well with number of cores and further it beneﬁts from mapping onto big and LITTLE clusters at the same time (referred to as inter-cluster thread-to-core mapping). It can be seen that executing on 4L and 4B cores is beneﬁcial in terms of execution time and energy consumption, and thus thread-to-core mapping should utilize the 4B and 4L cores.
Observation 2: Fig. 2 demonstrates the variations in workload when multiple applications are run in two different conﬁgurations: individually (left) and concurrently (right) on the A15 cluster of Odroid-XU3 platform [7]. Here, we consider three applications having different workloads from SPLASH: fmm (fm), radix (rd) and raytrace (ra), and their respective combinations fm-rd, fm-ra, rd-ra and fm-rd-ra. The metric considered to classify the workload is Memory Reads Per Instruction (MRPI ¼ Li2nstcraucchteiornesadretrierfeidlls) as opposed to the more commonly used CPU cycles [8] for classifying the application workloads. Selection of MRPI is inﬂuenced by its relatively low overhead (two performance counters only) and high correlation with the memory intensiveness of an application. Furthermore, we experimentally veriﬁed its frequency agnostic behaviour as compared to other metrics, such as Memory Reads Per Cycle (MRPC), having maximal variations with respect to frequency. MRPI classiﬁes the workload based on the degree of memory intensiveness. It can be observed from Fig. 2 that the different workload classes of the applications fm, rd and ra can clearly be classiﬁed as compute intensive, memory intensive, and mixed (compute and memory intensive), respectively, when run individually. However, it is completely different in the case of concurrent execution having greater workload variability due to applications’ interference. Such classiﬁcation can help to apply appropriate voltage-frequency levels to optimize energy while satisfying performance constraint.
There are existing approaches for run-time management of concurrently executing applications [6], [10], [11], [12], [13], [14]. However, the approaches of [10], [11] consider homogeneous multi-core architectures and thus cannot be

Fig. 2. Variation in MRPI for individual (left) and concurrent (right) execution of multiple applications.
applied to heterogeneous ones. Approaches proposed in [6], [13] do not exploit the inter-cluster thread-to-core mapping (observation 1) and run-time workload classiﬁcation (observation 2) for performance-constrained applications, missing the opportunity for energy savings. In [12], application threads are mapped to more than one type of cores, but the approach heavily depends on off-line regression analysis of performance and energy for all possible threadto-core mappings and V-f levels, which is non-scalable. Moreover, the V-f level is not adjusted during execution, i.e., observation 2 is not exploited, which is beneﬁcial for adapting to workload variations. In contrast, our approach exploits observation 1 for thread-to-core mapping and observation 2 for DVFS to achieve energy savings.
The key steps in runtime management of concurrent execution of multiple applications on a heterogeneous multi-core system are summarized in Fig. 3. Considering the above two observations made on inter-cluster exploitation and workload classiﬁcation, following four main challenges are associated with the run-time management (described subsequently):
(i) Efﬁcient inter-cluster thread-to-core mapping for multiple applications (left-most part of Fig. 3).
(ii) Workload classiﬁcation for concurrently executing applications based on the identiﬁed thread-to-core mapping (middle part of Fig. 3).
(iii) Identiﬁcation of appropriate V-f level of cores for the associated workloads (right most part of Fig. 3).
(iv) Analysing concurrent applications’ interference and taking appropriate measures to meet performance requirements.
(i) Inter-cluster thread-to-core mapping step needs to identify the number of used cores and their type for each application while meeting the performance requirement. In case of multiple applications, the mapping process has the challenge of allocating the right number and type of cores to each application from the available cores (4 LITTLE and 4 big cores for ARM’s big.LITTLE architecture presented in the OdroidXU3 [7]), such that their performance requirements are
Fig. 3. Key steps in runtime management of concurrent execution of multi-threaded applications on a heterogeneous multi-core architecture.

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

371

satisﬁed and energy consumption is minimized. The leftmost part of Fig. 3 shows an example thread-to-core mapping for each application, where threads are allocated onto different types of cores (highlighted in various grey-scales).
(ii) Workload classiﬁcation step needs to classify the workload within each cluster for the concurrently executing applications (shown under Execution in the left-most part of Fig. 3) by taking the workload of each core into account. The classiﬁcation could be into various classes such as compute intensive, memory intensive and mixed [15], which results from the varying ways in which they exercise the hardware. From a performance perspective, it is desirable to run a compute intensive application at a higher clock frequency as compared to memory intensive one such that high performance can be achieved. However, an appropriate metric needs to be identiﬁed in order to classify the workloads, using MRPI in our case. A high workload on the processing core means a low MRPI and vice versa. Classiﬁcation at a given time interval plays a pivotal role for achieving the desired energy-performance trade-off, which is discussed in detail further in Section 4.
(iii) Appropriate V-f identiﬁcation is required for the associated workloads such that the desired energy-performance trade-off can be achieved. Since multiple applications are executed concurrently, the V-f level needs to be identiﬁed by taking the performance requirements of all of them into account. Further, as different set of V-f levels are available for the cores situated into different clusters, e.g., big and LITTLE clusters in ARM’s big.LITTLE architecture, it becomes challenging to identify the most suitable V-f levels for different clusters while respecting applications’ performance constraints. The right most part of Fig. 3 shows an example demonstration of V-f assignment.
(iv) Applications’ interference due to concurrent execution of applications may degrade their performance. In order to meet the performance requirements, the interference level should be analysed and then it should be used to take appropriate measures. The interference level can be measured as the joint performance degradation of applications when executing concurrently in comparison to individual executions. Clustered heterogeneous architectures such as ARM’s big.LITTLE represent different amounts of interference on big and LITTLE cluster for the same workload due to different amounts of available memory for them and thus they need to be analysed separately. Thereafter, they need to be exploited to meet the performance requirements.
A close observation of the existing run-time management approaches indicates that they cannot address all the aforementioned challenges for executing multi-threaded applications on heterogeneous multi-core platforms (described in the Section 2). In order to overcome the limitations of the existing approaches towards addressing the above mentioned challenges, this paper makes the following contributions:
(1) Ofﬂine analysis of individual applications for performance and energy consumption when mapped to various possible resource combinations on a given heterogeneous multi-core platform to obtain proﬁled data.
(2) For concurrently executing applications, an online mapping strategy facilitated by sorted proﬁle data, to compute the minimum energy consumption point, while satisfying the performance and resource constraints. For each application, the computed point

deﬁnes thread-to-core mapping, and the platform is conﬁgured following the mapping to start the application execution. (3) For the chosen thread-to-core mappings of concurrently executing applications, an online energy optimization technique that ﬁrst classiﬁes their inherent workload characteristics and then pro-actively selects an appropriate voltage-frequency (V-f) pair according to predicted workload in order to minimize the switching transitions and energy. (4) Implementation and validation of both the ofﬂine and online steps on a real hardware platform, specifically Odroid-XU3 platform [7]. The ofﬂine analysis is performed by taking resource/core combinations from the 4 A15 (big) and 4 A7 (LITTLE) cores present on Odroid-XU3 platform [7]. The online mapping strategy chooses thread-to-core mappings such that total number of used cores does not exceed the available cores (4 A15 cores and 4 A7 cores). For online energy optimization, appropriate V-f for various workload classes is determined by performing ofﬂine design space exploration (DSE) that uses a custom program to generate a varying number of memory accesses. Subsequently, it monitors the workload prediction error and performance loss, quantiﬁed by instructions per second at runtime and adjusts the chosen V-f to compensate. The proposed approach shifts heavy computations to ofﬂine and thus helps in reducing the runtime overheads compared to learning-based approaches [8], [16]. The proposed approach is validated on the Odroid-XU3 platform with the various combinations of applications from PARSEC and SPLASH benchmarks. To the best of our knowledge, this is the ﬁrst study on runtime management of concurrent multi-threaded applications on heterogeneous multi-core architecture where more types of cores are used by an application at the same time and V-f is adjusted at various time intervals during execution through workload selection, classiﬁcation and prediction. The rest of the paper is organized as follows. Section 2 presents related works. Section 3 introduces the system model describing the application, architecture and problem deﬁnition in more details. Section 4 describes various stages of the proposed methodology. Section 5 presents the experimental results and their analysis with chosen benchmark applications and hardware platform. Finally, Section 6 concludes the paper.
2 RELATED WORK
There have been several works on ofﬂine optimization to achieve performance-energy trade-off points for multi-core systems by employing DVFS and/or task mapping [17], [18], [19], [20], [21]. However, these works have several drawbacks, such as they consider a single application at a time and thus cannot handle concurrent applications [17], [18], [19], [20], cannot be applied for online optimization as they perform heavy time consuming computations, and most of them are not evaluated on real hardware platform [18], [20], [21]. Online optimization has also been considered to cater for dynamic workload scenarios in order to optimize energy consumption while respecting the timing constraint. For online optimization, either all the processing is performed at run-time or else the optimization is supported by ofﬂine analysed results.

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

372

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

For performing all the processing at run-time, several works have been reported [8], [15], [16], [22], [23], [24]. In [22], the online algorithm utilizes hardware performance monitoring counters (PMCs) to achieve energy savings without recompiling the applications. The authors of [23] present an accurate run-time prediction of execution time and a corresponding DVFS technique based on memory resource utilization. A similar approach, which is a hardware-speciﬁc implementation of the stall-based model, is proposed in [15]. In [24], an adaptive DVFS approach for FPGA-based video motion compensation engines using run-time measurements of the underlying hardware is introduced. In [8], online reinforcement learning based adaptive DVFS is performed to achieve energy savings. These approaches perform well for unknown applications to be executed at run-time, but lead to inefﬁcient results as optimization decisions need to be taken quickly and ofﬂine analysis results are not used. Further, they are agnostic of concurrent workload variations and thus fail to adapt for concurrently executing multiple applications. Recently, there has been focus on online optimization facilitated by ofﬂine analysis results [6], [10], [11], [12], [13], [25], [26]. Such approaches lead to better performance results than only online optimizations as they take advantage from both ofﬂine and online computations. In [10], thread-to-core mapping and DVFS is performed based on power constraint. Similarly, in [11], ﬁrst thread-to-core mapping is obtained based on utilization and then DVFS is applied depending upon the surplus power. However, the approaches of [10], [11] target homogeneous multi-core architectures and thus cannot be applied to heterogeneous ones.
For heterogeneous multi-cores, recently some works have been reported that consider multi-threaded applications [6], [12], [13], [25], [26]. However, most of these approaches map application threads completely on one type of core situated within a cluster [6], [13], [25], [26]. This reduces the threadto-core mapping complexity, but misses to beneﬁt from the distribution of an application threads to multiple types of cores at a given moment of time. In [25], performance impact estimation (PIE) is used as a mechanism to predict which thread-to-core mapping is likely to provide the best performance in order to map the threads on the most appropriate core type. In a similar direction, some proposals have used workload memory intensity as an indicator to guide applications’ thread-to-core mapping [27], [28], [29], [30], [31]. For a given platform containing two types of cores as big and LITTLE, such proposals map memory-intensive workloads on a LITTLE core and compute-intensive workloads on a big core. Similarly, in [13], at a given moment of time, all the threads of an application are mapped on one type of cores. The threads are moved from one core type to another when it beneﬁcial by checking at a regular interval. However, DVFS is not exploited in [25] and [13], which can help to achieve further energy savings. In contrast, the approaches of [6], [12], [26] exploit DVFS, but they have several drawbacks. For example, in [26], design space is explored for a single application, which increases exponentially if concurrent applications have to be considered. In [6], each application is executed as single threaded and use only one type of core for it at a time. In [12], application threads are mapped to more than one type of cores, but the approach heavily depends on off-line regression analysis of performance and energy for all possible thread-to-core mappings and V-f settings, which is non-scalable. Additionally, V-f

setting is not adjusted during execution, which is beneﬁcial for adapting to workload variations.
In contrast to existing approaches, our approach considers concurrent execution of multiple applications, distributes threads on more types of cores at the same time (performs inter-cluster thread-to-core mapping), applies adaptive DVFS to save energy consumption and has been implemented in hardware.
3 SYSTEM AND PROBLEM FORMULATION
This section describes the system architecture and applications considered in this work along with a detailed problem deﬁnition.
3.1 System Architecture
The modern heterogeneous architectures contain different types of cores in varying number. Further, cores of the same type are grouped into clusters. One such architecture is considered for our work. We have taken a 28 nm Samsung Exynos 5422 chip hosted on the Odroid XU3 board [7], which is based on the ARM’s big.LITTLE heterogeneous architecture and contains two clusters named big and LITTLE [32]. In addition, the chip contains a Mali-T628 GPU and 2 GB DRAM LPDDR3. The big and LITTLE clusters contain high performance Cortex-A15 quad core processor and low power Cortex-A7 quad core processor, respectively. The board also contains four real time current/voltage sensors that facilitate measurement of power consumption (static and dynamic) on the four separate power domains: big (A15) cores, LITTLE (A7) cores, GPU and DRAM. The Odroid-XU3 board can run different ﬂavors of Linux. It also supports core disabling and DVFS, helping in optimizing system operation in terms of performance and energy consumption. DVFS can be used to change V-f levels at a percluster granularity. For each power domain available for a cluster, the supply voltage and clock frequency can be adjusted to pre-set pairs of values. The Cortex-A15 quad core cluster has a range of frequencies between 200 MHz and 2000 MHz with a 100 MHz step, whereas the Cortex-A7 quad core cluster can adjust its frequencies between 200 MHz and 1,400 MHz with a step of 100 MHz. The device ﬁrmware automatically adjusts the voltage for a selected frequency, therefore, adjusting V-f and frequency has interchangeably been used throughout the paper.
3.2 Applications
For multi-core systems, multi-threaded applications represent current and emerging workloads as they can used to evaluate concurrency and parallel processing. Examples of such applications are available in several benchmarks such as PARSEC [9] and SPLASH [33]. Applications from PARSEC and SPLASH benchmarks exhibit different memory behavior, data partitions and data sharing patterns from other benchmarks in common use. The memory behavior of some applications is presented in Fig. 2, which shows whether they are compute intensive, memory intensive or both compute and memory intensive while executing in various time intervals. Such a classiﬁcation helps to take appropriate actions to perform required optimizations.
We have used applications from PARSEC and SPLASH benchmarks on the multi-core architecture of the OdroidXU3 platform. For each considered application, the user can specify a performance requirement in terms of completion

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

373

time of the application. Such performance requirements can be translated to throughput requirements for frame based applications like audio/video applications, where throughput is expressed as a frame rate to guarantee a good user experience. In a similar manner, the completion time requirement can also be translated to an instructions per second requirement, as the total number of instructions in an application is known.

3.3 Problem Deﬁnition

For an application with R threads to be mapped onto a het-

erogeneous multi-core architecture with N clusters, i.e., N

core types (C1, C2, C3; . . . ; CN ), where each cluster contains li (i ¼ 1; . . . ; N) cores, the possible number of thread-to-core
mappings (TCmap) can be represented as,

8

>>>>< X N li þ YN li

X N R ! li & R ! li

TCmap

¼

i¼1
>>>>: R Ã

N

i¼1
þ RN

i¼1
X N R < li & R < li:

(1)

i¼1

For run-time power management, the modern cluster-based architectures support cluster-wide DVFS, where cores of the same type organized as a cluster are set to the same V-f level from a predeﬁned set of V-f pairs [3]. For example, OdroidXU3 [7], and Mediatek X20 [34] platforms employ such architecture. Let li be the number of cores of type Ci in a cluster Ei and nFi be the number of available V-f levels. Then, to incorporate the V-f levels (nFi) into thread-to-core mapping decisions, Equation (1) will be modiﬁed as follows,

8

>>>><

X N

li

Ã

nFi

þ

YN li

Ã

nFi

TCmap

VF

¼

i¼1
>>>>: X N R Ã nFi

i¼1
YN þ R Ã nFi

i¼1

i¼1

X N R ! li & R ! li
i¼1
X N R < li & R < li:
i¼1

(2)

As it can be seen from Equation (2), the initial design space is prohibitively large to explore during the application execution at different time intervals, and thus cannot be applied at runtime. In order to overcome this issue, the exploration of mapping can be ﬁxed to the initial design space, and DVFS exploration can be carried at run-time during different time intervals by ﬁxing the mapping from the initial design space. This helps to solve the thread-to-core mapping and DVFS problems orthogonally. We tackle the problem in the same manner, as deﬁned below:
Given an active application or a set of active applications with performance constraints and a clustered heterogeneous multi-core architecture supporting DVFS
Find an efﬁcient static thread-to-core mapping for each application at runtime and then apply DVFS during the application execution to minimize the energy consumption
subject to meeting performance requirement of each application without violating the resource constraints (number of available cores in a platform)
For a total of n applications, there are 2n possible usecases, where each use-case represent a set of active applications. Finding all the possible mappings for each use-case might not be possible within a limited time in case the number of applications and/or cores in each heterogeneous cluster increases. Therefore, the mappings can be explored for

Fig. 4. Overview of a three-layer run-time management (left) and our contributions (right).
an individual application and used in conjunction for various use-cases at run-time, which also reduces the overhead to store the mappings. We employ the same measures.
4 PROPOSED RUN-TIME MANAGEMENT
A three-layer view of a typical run-time management is presented in Fig. 4 (left), where each layer interacts with the others to execute an application, as indicated by arrows. The top most layer is the application layer, which is composed of multiple applications having various workload classes. The middle layer is the operating system layer (e.g., iOS, Linux, etc.), which coordinates an application’s execution on the hardware (bottom), consisting of multi-core processors. An overview of the proposed run-time management approach employed by the OS has been illustrated in Fig. 4 (right), which has the following stages:
(a) Thread-to-core mapping (b) Workload selection and prediction (c) Workload classiﬁcation and frequency selection (d) Performance observation and compensation
The novel aspects of proposed run-time management of concurrent execution of multiple applications are as follows:
 Run-time identiﬁcation of energy efﬁcient intercluster thread-to-core mapping that satisﬁes performance and resource constraints.
 Online selection and classiﬁcation of concurrent workloads.
 A pro-active online DVFS technique using workload prediction, which takes performance degradation into account and adjusts the chosen V-f setting to compensate.
A detailed discussion of each stage is presented in the following sections.
4.1 Thread-to-Core Mapping In order to meet the performance requirements of the applications to be run concurrently, and to minimize the energy consumption, an effective thread-to-core mapping is important. This involves choosing an appropriate number of cores and their type for each application. Since there are several thread-to-core mapping options for each application,

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

374

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

Fig. 5. Design points representing performance and energy trade-off points for Blackscholes application.
exploring the whole mapping space is time consuming. Therefore, at run-time, thread-to-core mapping is facilitated through an extensive ofﬂine analysis to guide application execution towards an energy efﬁcient point. The ofﬂine analysis evaluates performance, and energy consumption for all the possible thread-to-core mappings at the maximum available frequency that helps to meet high performance requirements. At run-time, one of these mappings that leads to energy-efﬁciency while meeting performance and resource constraints is selected for each application. The following sections present a detailed discussion on offline analysis and run-time mapping selection.
4.1.1 Ofﬂine Analysis
For each available application, the ofﬂine analysis computes all the possible thread-to-core mappings and their performance and energy consumption on a given clusterbased heterogeneous architecture. For the considered applications, the number of threads is greater than the number of cores available on the chosen hardware platform (Odroid-XU3). Therefore, following Equation (1), the total number of thread-to-core mappings for each application is 24 (TCmap ¼ 4 þ 4 þ 4 Ã 4). Fig. 1 presents an example analysis for the Blackscholes application that shows 24 mappings and their respective performance (1/execution time) and energy consumption.
The analysis results for each application are stored as design points that represent performance and energy tradeoff points for all possible thread-to-core mappings at the maximum frequency. Each design point is represented as 4tuple: (Prf; EC; nL; nb), where Prf, EC, nL, and nb denote performance, energy consumption, number of LITTLE cores, and number of big cores, respectively. These design points are sorted in descending order to quickly identify the points meeting a certain level of performance. This helps in minimizing the run-time overhead while choosing the minimum energy point for each performance-constrained application. Fig. 5 shows the design points for the Blackscholes application corresponding to Fig. 1. Similarly, design points are stored for other applications as an outcome of the ofﬂine analysis.
4.1.2 Run-Time Mapping Selection
For a set of active applications, the runtime mapping has to identify appropriate design points for each application such that the overall energy consumption is minimized without violating the performance and resource constraints (number of cores available in the platform). For example, in case of the Odroid-XU3 platform, the total number of used big and LITTLE cores should not exceed four of each.

Algorithm 1. Run-Time Thread-to-Core Mapping Selection
Input: CAApps, AppsPrfr, DP Output: Map for each application 1: for each application Am do 2: Choose points DAm (2 DP ) such that Prf > AmPrfr; 3: end for 4: for each combination point CP (from CAApps) do 5: Compute energy consumption of CP as Energy½CP 
(Equation (3)); 6: Compute total number of used cores of different types
(Equation (4)); 7: Add CP with its Energy½CP  and Ci UsedCores½CP  in set
CPS; 8: end for 9: From CPS, select the point having minimum energy
consumption (minEnergy½CP ) and satisfying resource constraint (i.e., Ci UsedCores½CP  < availableCi Cores); 10: For the minEnergy½CP , return number of used cores and their types for each application as Map;

Algorithm 1 describes the run-time mapping selection algorithm. The algorithm takes concurrently active applications (CAApps), their performance requirements (AppsPrfr) and design points (generated in the previous step, DP ¼ D1; . . . ; Dm) as input and provides a static thread-tocore mapping Map in terms of number of used cores and their types as output for each application. It has been observed in [10] that allocating more number of threads than cores does not actually give any performance beneﬁts. Moreover, by varying number of threads per core (1, 2, .., t) where the value of t can vary depending on the application and resource allocation), the design space becomes prohibitively large. Therefore, to reduce the mapping complexity, the number of threads are chosen the same as the number of cores. For each application, the algorithm ﬁrst chooses performance requirement satisfying points from its design points. Since the points are stored in decreasing order of performance, the points are chosen as the ﬁrst entry in the storage to the last entry meeting the performance requirement. Thus, a quick selection of points take place. Then, for each combination point CP (formed by considering one point from each application), energy consumption and used cores of type Ci are computed as follows.

NrX CApps

Energy½CP  ¼

Energym

(3)

m¼1

NrX CApps

Ci UsedCores½CP  ¼

Ci coresm;

(4)

m¼1

where, Energym and Ci coresm are the energy consumption and used Ci type cores of application m, respectively. For NrCApps active applications, a combination point contains one point from each application.
After above computations for different combination points, the point having minimum energy consumption (based on minimum value selection algorithm) and satisfying the resource constraint is chosen (line 9, Algorithm 1). Then, for this chosen point, the number of used cores and their types (nL and nb) for each application are returned as the thread-to-core mapping Map, which is further

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

375

controlled by sched_setafﬁnity interface in the Linux scheduler. Our approach is generic, but one-time ofﬂine analysis is required when the application or platform changes. In case a new application needs to be executed and its ofﬂine analysis is not done, the best effort or online learning heuristics [12] can be employed to obtain the mapping, but achieved results might not be efﬁcient.

Algorithm 2. Proposed Online DVFS Approach

Input: Application scenario, Ts, f-tab and len

Output: V-f pair for each cluster

1: Initialisation: predicted workload (Pw) ¼ 0, cl ¼ cb ¼ 0;

2: PMUINITIALIZE

3: fcur ¼ cpufreq get frequencyðcore#Þ

4: while (1) do

5: compute new IPS value (IPSn) for each application

6: wait for Ts

/*DVFS interval*/

7: /*Set V-f level of A15-cluster*/

8: if (Ãcb 6¼ len) then

9: actual workloadðAwÞ ¼ Pmc data A15ðÞ

10: prediction error ðPeÞ ¼ Aw À Pw

11: q ¼ 0

12: FIND SET VF Aw; Pw; Pe; cb; core# 13: q ¼ 1

14: else

15: wait for Ts Ã len 16: Ãcb ¼ 0

/*Adaptive sampling*/

17: end if

18: /*Set V-f level of A7-cluster*/

19: if (Ãcl 6¼ len) then 20: actual workload ðAwÞ ¼ Pmc data A7ðÞ 21: prediction error ðPeÞ ¼ Aw À Pw

22: FIND SET VF Aw; Pw; Pe; cl; core#

23: else

24: wait for Ts Ã len

/*Adaptive sampling*/

25: Ãcl ¼ 0

26: end if

27: end while

28: function FIND_SET_VFAw; Pw; Pe; c; core# 29: Pw ¼ EWMAPw; Aw; Pe; &c 30: classify Pw, compute d and get fn from f À tab 31: if q ¼¼ 0 then

32: for each cluster do

33:

Perf loss ¼ ððIPSreq À IPSnÞ=IPSreqÞ

34: end for

35:  ¼ Perf loss Ã 100

36: end if

37: if ( > x%) then

38:

fn ¼ fn þ  Ã fmax

39: end if

40: if (fnew 6¼ fcur) then

41: cpufreq set frequencyðcore#; fnÞ

42:

fcur ¼ fnew

43: *cÀÀ

44: else

45: *cþþ

46: end if

47: end function

48: PMUTERMINATE ()

Simultaneous execution of multiple applications may affect each other due to interference in the shared memory. Thus, their execution time might increase in comparison to

the scenario when executed individually. The stretching in the execution time depends upon the compute and memory intensiveness of the applications, e.g., higher stretching is expected for memory intensive applications due to heavy memory accesses in the shared memory. The degraded performances of the applications are compensated by proper V-f selections during various time intervals during execution, which is described in the following sections.

4.2 Workload Selection and Prediction

V-f setting is a function of workload, at time tj it can be represented as



V -f ¼

f ðwj Þ

Individual workload

fðw1j; w2j; . . . ; wRjÞ Concurrent workloads;

(5)

where, 1 to R represent threads of the application(s). Moreover, in a cluster based architecture, the V-f of an individual cluster will be set by considering the workloads of all the cores within a cluster which can be represented as

V -fEi ¼ fðWEi Þ:

(6)

It is important to note that, for concurrent execution of multi-threaded applications, the V-f setting of each cluster should be chosen in such a way that all the applications meet their performance requirements. Furthermore, these applications generate varying and mixed workloads due to resource sharing (e.g., L2 cache and memory) showing intra and inter workload variations during their execution (see Fig. 2). Therefore, we need to select a representative V-f pair to guide the further stages in achieving energy efﬁciency without performance loss. Pseudo code of the proposed online DVFS is given in Algorithm 2.
Assume that there are R concurrently executing threads of application(s) on cluster Ei, and wrj is the workload of a thread r for time interval tjÀ1 ! tj. There will be R different workloads at every time interval of execution. The workload is quantiﬁed by the MRPI, where a low value represents a high load on the core and vice versa. If there are multiple workloads running within a cluster, choosing a V-f setting based on an average or single workload may lead to violation of performance requirements for some of the applications. For example, setting the V-f according to the high MRPI (memory-intensive) workload may hurt the performance of the compute-intensive workload, as memoryintensive workload can be run at lower frequencies than compute-intensive workload. Therefore, in a cluster-based DVFS supporting architectures, V-f level of the cores within a cluster should be set based on the most compute-intensive (minimum MRPI) workload running on those cores. To meet each application’s performance requirement, the V-f setting for cluster Ei for the time interval tjÀ1 ! tj (considering single V-f domain for whole cluster) is set by the following workload (line 9 and 20 in Algorithm 2)

WEji ¼ min ðw1ji; w2ji; w3ji; w4ji; :::; wRjiÞ:

(7)

Concurrent execution of multiple applications create contention/interference for shared resource, which impacts the performance of an individual application. Furthermore, the memory access latency experienced by each application, calculated at run-time based on the average memoryintensiveness of the running applications, is used to minimize

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

376

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

TABLE 1 Design Time Analysis of Workload Classes (MRPI Range) and Corresponding Frequencies

MRPI Range
>0.036 (0.033,0.036] (0.03,0.033] (0.027,0.03] (0.024,0.027] (0.021,0.024]

A15

Frequency (MHz) MRPI Range

1000 1100 1200 1300 1400 1500

(0.018,0.021] (0.015,0.018] (0.012,0.015] (0.009,0.012]
<0.009

Frequency (MHz)
1600 1700 1800 1900 2000

MRPI Range
>0.055 (0.05,0.055] (0.044,0.05] (0.04,0.044] (0.032,0.04]
<0.032

A7
Frequency (MHz)
900 1000 1100 1200 1300 1400

the wasted cycles/switching activity by scaling down the frequency. We represent this latency as d amount of increase in WEji. Therefore, Equation (7) can be modiﬁed as

WEji ¼ min ðw1ji; w2ji; w3ji; w4ji; :::; wRjiÞ þ dji:

(8)

The value of d is computed from average MRPI of the cores within a cluster. If all the running applications are memoryintensive, then the value of d will be high due to increased memory trafﬁc. We experimentally veriﬁed that, d increases the application execution time from 1.08 to 3.80 percent, when multiple applications are executed concurrently. Based on the above observation, the d value is set to 4.5 percent of average MRPI.
Proactive control of V-f is of utmost importance for online energy minimization [8]. Therefore, the future workload needs to be predicted at tj to set the V-f pair for the interval tj ! tjþ1. An exponential weighted moving average (EWMA) ﬁlter [35] is used to predict workload pjþ1 during the interval tj ! tjþ1 (line 29, Algorithm 2)

pjþ1 ¼ g Ã aj þ ð1 À gÞ Ã pj;

(9)

where g, pj and aj are the smoothing factor, predicted and actual workloads respectively during the interval tjÀ1 ! tj. It is to be noted that WEji , computed from Equation (8) represents the actual workload aj. To minimize miss-predictions caused by dynamic variations in the workload, the predicted workload of the interval tjÀ1 ! tj is compared against the actual workload measured from hardware
PMCs (line 10 and 21 in Algorithm 2). Subsequently, com-
puted prediction error Pe (the difference between actual and predicted workloads) is used to improve the workload prediction for tj ! tjþ1. The effectiveness of proactive V-f control depends on the accuracy of workload prediction,
hence an evaluation is provided in Section 5.

4.3 Workload Classiﬁcation and Frequency Selection
It is essential to classify the predicted workload for identifying an appropriate V-f pair for meeting the performance requirements and optimizing the energy. We use hardware PMCs for periodically getting information regarding architectural parameters during application execution (line 9 and 29, Algorithm 2). The modiﬁed performance monitoring tool perfmon [36] (enabled access to the A15- and A7-clusters) is used for accessing the PMCs, initialized and terminated through PMUINITIALIZE and PMUTERMINATE (line 2 and 48, Algorithm 2).
Classiﬁcation of the workload as compute-intensive or memory-intensive, depends on the instruction mix during the time interval Ts. For example, if there is a large

proportion of load/store instructions causing frequent cache misses, then the workload can be classiﬁed as memory-intensive. Furthermore, when there are frequent branch miss-predictions and lower-level cache misses (e.g., L1 in Odroid-XU3), the number of instructions executed and MRPI could be low. However, the penalty (measured in cycles) will remain intact no matter what the frequency is, because a branch miss-prediction involves only in-core operations [37]. Therefore, the workload will be treated as compute-intensive and the highest frequency is selected to minimize performance loss. On the other hand, if the processing core is idle or running only background processes, the number of instructions and MRPI may be low. However, this will not come under the compute-intensive case and the operating frequency can be set to a minimum value to minimize the power consumption. Here, the unused core is said to be idle, i.e., no application thread is executing on that core. The idle cores can be identiﬁed from the resource combination achieved by the thread-to-core mapping, which decides number of cores and their type allocated to each application (see Table 4). For example, in an octa-core (c0; . . . ; c7) platform, if thread-to-core mapping allocates four cores (c0, .., c3) to an application, the remaining four cores (c4; . . . ; c7) will be idle. The status of the core (used/ idle) can be maintained in a shared location.
Workload classes are predetermined by observing the variation in execution time through a custom program, which generates a varying number of L2-cache misses by performing memory accesses on a large array. Subsequently, the experiment is repeated ten times across all available frequencies (200 MHz–2000 MHz) on an Odroid-XU3 platform [7]. The A15-core is an out of order core which takes advantage of memory level parallelism such that part of an L2 cache miss latency overlaps with other independent L2 cache misses. Furthermore, the A15-cluster has greater L2-cache capacity compared to the A7-cluster. The inﬂuence of these factors is seen during exploration, and taken into account while choosing the MRPI range and its corresponding frequency. The classiﬁed workloads and corresponding optimal V-f settings obtained from f-tab are given in Table 1, where various MRPI ranges are mapped to frequency through DSE, and used at run-time to set the operating frequency to a desired value through the utility cpufreq-set, thereby minimizing runtime overheads (lines 41, Algorithm 2).
The range of MRPI values having little ( < 1 percent) or no effect on execution time for the same frequency are grouped into a single class. Application execution intervals with a large MRPI are actually memory-intensive workloads, so they can be run at lower frequencies to save energy as higher frequencies will simply result in stalls while waiting for data from memory. This motivates us to assign a low

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

377

TABLE 2 Selected Applications from PARSEC [9] and
SPLASH [33] Benchmarks

Fig. 6. Effect of adaptive sampling on energy and performance for various application scenarios.
frequency to large MRPI workloads, and it is decided by the speed of memory ($933 MHz in our case).
4.4 Performance Observation and Compensation
It is important to evaluate performance during execution to ensure all applications meet their performance requirements. Moreover, dynamic resource availability in a multicore platform may impact the run-time performance of the applications. Therefore, we use instructions per second as a metric for quantifying the run-time performance of each application for every elapsed time interval Ts. The performance loss is calculated once (lines 31-35, Algorithm 2) by comparing the achieved IPS (IPSn) on each cluster with their required IPS (IPSreq) at every time interval. If the maximum performance loss % of all the currently running applications during the interval tjÀ1 ! tj is signiﬁcant, the selected V-f (fn) is increased by *fmax (lines 32 - 39, Algorithm 2) for subsequent time interval (tj ! tjþ1) to compensate. Furthermore, the frequency is modiﬁed only when the performance loss () is signiﬁcant to minimize the overheads associated with DVFS [38]. We experimentally veriﬁed and set the value of  to 1 percent by taking the variations in PMC data into account [39]. It is worth noting that setting any core’s V-f to a new value within a cluster is sufﬁcient to change the V-f of remaining cores belonging to the same cluster.
4.5 Adaptive Sampling
A smaller time interval (Ts, difference between tjÀ1 and tj), for which a value of V-f is computed and set, increases the run-time overhead and degrades overall performance. Therefore, the value of Ts is experimentally chosen to be 200 ms so as to minimize the overhead on application performance considering the overheads associated with the PMC data collection, subsequent processing, system reliability and DVFS [38]. Furthermore, it can be observed from Fig. 2 that not every combination of applications exhibits large variation in workload during the execution, for example fm and fm-ra. This negligible variations will have no inﬂuence on the V-f setting. Therefore, to further reduce the run-time overheads, time period is adjusted according to the application workload variations.
To accomplish this we use counters cb and cl for tracking the workload variations on A15- and A7-clusters, respectively. These counters get incremented when the workload at tj is signiﬁcantly different (MRPI range) than that of the workload at tjÀ1 (lines 40-46, Algorithm 2). When cb or cl is equal to a conﬁgurable parameter len, the run-time adaptation on A15or A7-cluster (PMC data collection and subsequent processing) is paused for len*Ts period (lines 15 and 24, Algorithm 2). We evaluated the effect of adaptive sampling for the

Benchmark PARSEC SPLASH

App Name
blackscholes bodytrack swaptions freqmine vips
water-spatial raytrace fmm

Abbreviation
bl bo sw fr vi
wa ra fm

application workload combinations fm, ra, fm-ra and fm-rd (see Fig. 2), where an average improvement of 0.9 and 0.6 percent in energy and performance are observed respectively when adaptive sampling is enabled, as shown in Fig. 6.

5 EXPERIMENTAL VALIDATION
5.1 Experimental Setup
The proposed run-time management approach for energy optimization is extensively validated on an Odroid-XU3 platform running a modiﬁed Ubuntu Linux Kernel 3.10.96 with a number of combinations of applications from PARSEC [9] and SPLASH [33] benchmarks. The details of the Odroid-XU3 platform are already provided in Section 3.1. We selected applications from PARSEC and SPLASH, based on variations in MRPI values. Table 2 lists the considered applications. The applications are taken in various combinations to make sets of simultaneously active applications. To have better predictability and to ensure that each application meets its performance requirement, the system is not overloaded, i.e., no two applications share the cores. This allows scheduler not to delay the application execution. If applications arrive at different times, the later arrived ones can be mapped by taking the available resources and current status of the existing applications, computed as the remaining time to complete them. If existing applications are going to complete soon, the freed resources by them can be considered to decide the mapping of the current application, otherwise it should be decided based on the current available resources. This also avoids the overhead of data transfer for existing applications as their mapping is not disturbed. Energy consumption is calculated as a product of average power consumption (dynamic and static) and execution time. This includes both the core and memory energy consumption of all the software components (proposed algorithms (Algorithm 1 and 2), proﬁled data, OS, applications, etc.). The proposed run-time management approach is compared against various approaches, given in Table 3, to show energy savings while satisfying the performance constraints. As part of these, the state-of-the-art solution for the run-time resource management of the big.LITTLE, Heterogeneous Multi-Processing (HMP) scheduler [40] with various DVFS governors (ondemand, performance, conservative and interactive) is considered. HMP is a patch to the standard scheduler in the Linux kernel which dynamically dispatches threads to big and LITTLE cluster according to their characteristics.
Furthermore, a mapping approach, which allocates the application’s threads onto only one type of core(s) based

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

378

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

TABLE 3 Approaches Considered for Comparison

Reference
[12] [25], [28], [29], [30], [31] [40], [41] [40], [41] [40], [41] [40], [41] proposed

Approach
Exhaustive Search-based Workload Memory Intensity based thread-to-core mapping HMP + Ondemand HMP + Performance HMP + Conservative HMP + Interactive Inter-cluster Thread-to-core Mapping and DVFS

Abbreviation
ES WMI
HMPO HMPP HMPC HMPI ITMD

on memory intensiveness [25], [28], [29], [30], [31] is considered for the comparison. As concurrent execution of multi-threaded applications is not taken into account by the above approaches, following changes are made for a fair comparison.
 In case of single-application scenario, a memory intensive application’s threads are mapped onto LITTLE cluster.
 In multiple-application scenario, applications are sorted based on their memory intensiveness and then one with the high memory intensity is mapped onto little cores and remaining applications are allocated onto big cluster with equal number of cores.
The proposed approach is also compared against a recently published exhaustive search-based (ES) approach [12]. As part of this, we used the thread-to-core mappings produced by our approach and varied the frequencies (247 design points; 200 MHz - 1,400 MHz on LITTLEcluster and 200 MHz - 2,000 MHz on big-cluster) for different application scenarios. Then, selected the conﬁguration (number of cores and their frequencies), having the lowest energy consumption while satisfying the performance requirements. Stamoulis and Marculescu [14] presented a process variation- and workload-aware thread-to-core mapping approach on heterogeneous multi-core systems. However, we could not consider this approach for the direct comparison with the proposed approach for the following reasons. It is proposed for maximizing the throughput under both performance and power constraints, while our approach minimizes the energy consumption under performance constraints. Moreover, the system architecture is different than the one (cluster-based architecture) used in this paper.
To show the effectiveness of the proposed methodology compared to various existing approaches in terms of energy savings and performance, single and multiple-application scenarios are considered for the validation. Moreover, the validation of the workload prediction is also presented.
5.2 Energy Savings and Performance Comparison
5.2.1 Energy Savings
Table 4 presents the resource combinations achieved by the proposed mapping approach at run-time for various application scenarios. The mapping approach takes the individual application performance requirements into account, and chooses the points that minimize total energy consumption from the sorted proﬁled data, without violating

TABLE 4 Resource Combination Achieved by Our Mapping Approach at
Run-Time for Different Application Scenarios

App scenario single double triple

App combination
bl bo sw fr wa ra
bl-bo bl-sw fr-sw wa-bo wa-bo wa-ra
bl-bo-sw bl-bo-fr sw-bo-fr bl-sw-fr wa-ra-fm wa-ra-vi

Resource combination
4L+4B 4L+4B 4L+4B
4L 4L+4B 3L+4B
2L+2B : 2L+2B 4B : 4L 4L : 4B
2L+2B : 2L+2B 4L+3B : 1B 4L+3B : 1B
3B : 1B : 4L 3B : 1B : 4L 4L : 1B : 3B 3B : 1B : 4L 3L+2B : 1B : 1L+1B 2L+2B : 1B : 2L+1B

the resource constraints. As discussed earlier, the selected thread-to-core mapping is not altered during the application execution.
In single-application scenario, there is only one active application. Fig. 7 shows the comparison of the adopted approach with existing techniques in terms of energy consumption. First, an energy efﬁcient thread-to-core mapping is determined to satisfy the given performance requirement and resource availability. The experimental observation shows that, for most applications our thread-to-core mapping approach tends to choose all available cores, except for fr and ra (see single-application scenario in Table 4), as it is the energy efﬁcient point. Afterwards, the proposed online DVFS approach takes control of the frequency scaling to minimize the wasted cycles in case of memory-intensive workloads. It periodically samples the PMCs data and uses a proactive V-f setting strategy using the workload prediction. From Fig. 7, it can be observed that the proposed method ITMD outperforms all existing approaches which used HMP scheduler for thread-to-core mapping with various Linux governors for DVFS and WMI. Except for fr and ra, energy savings are mostly due to DVFS as both HMP and proposed approach have the similar thread-to-core mapping. The higher energy savings in case of fr are because of mapping threads to power efﬁcient A7 (L) cores, which is the same as that of WMI. It also has long execution that beneﬁts from periodic DVFS. On an average, proposed approach achieves, 25, 20, 27, 22, and 33 percent energy savings while meeting performance requirements compared to HMPO, HMPC, HMPP, HMPI and WMI, respectively. Furthermore, as our approach (ITMD) applies online DVFS at regular intervals, it provides better energy savings (17 percent) than the exhaustive search-based approach.
Moreover, Fig. 8 shows the adaptiveness of the proposed online DVFS technique to workload variations for the application fr. A high MRPI leads to scaling down the frequency, thereby ITMD approach minimizes the power consumption, whereas HMPO and HMPC runs at max frequency. This is due to the fact that whilst the application is memory

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

379

Fig. 7. Comparison of proposed approach with reported approaches for Fig. 9. Comparison of proposed approach with reported approaches for

single active application.

two active applications.

Fig. 8. MRPI and frequency at different time intervals of the application fr execution for various approaches.
intensive, it places a high load on the processor cores as far as the load measured by the kernel is concerned. Therefore, these select the highest frequency even if it does not offer improvement in performance.
In case of multiple-application scenario, at a given moment two or more active applications will be contending for resources to meet their requirements. Such scenarios can be observed in a mobile phone where user tries to run more applications at the same time, e.g., mp3-decoder to listen to music and jpeg-decoder to view an image. A set of two applications from Table 2 are considered to stress on effectiveness of the adopted approach in choosing resources and V-f pair for minimizing the energy consumption while meeting each application performance requirement. Due to limited resource availability and contention, the energy savings are comparatively less than the single-application scenario. Fig. 9 gives the energy consumption for various approaches. On an average, proposed approach achieves 13, 14, 10, 20, 15, and 23 percent energy savings while meeting performance requirements compared to ES, HMPO, HMPC, HMPP, HMPI and WMI, respectively. Moreover, chosen resource combinations are presented in row two of Table 4.
To further validate the ability of the proposed approach to adapt to concurrent execution of multiple applications, three-application scenario is also considered. Increase in number of active applications will lead to reduced solution space for choosing an energy efﬁcient thread-to-core mapping. As mentioned before, it is caused by the resource constraints (see Table 4 for resource combination) and increased contention due to concurrent workloads and demand for meeting their requirements. Furthermore, the online DVFS will have a little choice to scale down the frequency as it has to satisfy the performance requirement of different dynamic workloads (e.g., compute and memory). This results in decreased energy savings compared to single and two-application scenarios. Fig. 10 presents the comparison of adopted methodology with various previous techniques. On an average, proposed technique achieves 11,

Fig. 10. Comparison of proposed approach with reported approaches for three active applications.
Fig. 11. Percentage of energy savings achieved by proposed ITM and ITMD, respectively.
12, 9, 16, 14, and 30 percent energy savings while meeting performance requirements compared to EC, HMPO, HMPC, HMPP, HMPI and WMI, respectively.
The four and more applications scenario seems to be not feasible because of high resource contention, leading to not meeting given requirements (some applications were terminated by out of memory (OOM) killer daemon when multiple memory intensive applications are run). It is explained further in the following section. On an average the adopted approach achieves energy savings up to 33 percent compared to existing techniques.
5.2.2 Breakdown of Energy Savings by Our Mapping and DVFS Approaches
Individual contribution of the thread-to-core mapping (ITM) and online DVFS in energy savings is computed by disabling and enabling DVFS respectively. Further, percentage energy savings are calculated by comparing against the HMPP, as shown in Fig. 11 for different application scenarios. On an average ITM achieves energy savings of 9 percent w.r.t HMMP. Further, when proposed online DVFS is applied on top of ITM (ITMD), an extra 11 percent of energy savings is obtained. It can be observed from Fig. 8 that, the workload varies over time, for example from low MRPI to

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

380

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

Fig. 12. Performance improvement/degradation of the adopted approach and HMPP.
high MRPI (at 8th and 10th time intervals). As the online DVFS is applied at regular intervals, the proposed approach exploits these variations to achieve energy efﬁciency even for a compute-intensive application.
5.2.3 Performance
As discussed earlier, performance requirements are deﬁned for each application. The proposed approach always tries to meet the performance requirement of each application, i.e., ﬁnishing the execution within the stipulated time. To validate the adaptability of the proposed approach to the performance requirements, the achieved performance is compared against the given performance requirement, computed as percentage improvement, for all the application scenarios. The average percentage improvement in each scenario is presented in Fig. 12 in comparison with HMPP (performance requirements-unaware), as it maximizes the performance. The ﬁgure shows that ITMD always outperforms HMPP even when there is a high contention due to more active applications (e.g., three-application scenario). Moreover, in some cases, the adopted technique achieves up to 15 percent improvement over given performance requirements, whereas HMPP achieves 10 percent improvement. Additionally, the following observation can be made from Fig. 12. As the number of active applications increases, meeting high performance requirements is not feasible (see three-application scenario in Fig. 12) due to resource constraints and interference. Therefore, choosing a low performance requirement or a platform with more resources may guarantee meeting the requirements while running higher number of active applications.
To further substantiate the need for performance requirements-aware approaches, we recorded the number of violations by disabling the performance requirements-aware property of the proposed approach, resembling the

technique presented in [12]. As a result, the mapping algorithm produces thread-to-core mappings that minimize the total energy consumption, which may not satisfy the performance constraints. For single application scenario, the average percentage of performance requirement-violating mappings are nearly zero. This is because, using all the cores (4L and 4B) leads to minimum energy and better performance for all the applications (except for fr (4L)). In case of multiple applications executing concurrently, performance violations are signiﬁcantly high. The average percentage of performance requirement-violating mappings are 98.2 and 99.6 percent for two- and three-application scenarios, respectively.
5.3 Workload Prediction
The accuracy of the predicted workload as compared to the actual workload of the prior time intervals depends on the smoothing factor g (9). The optimal value of g was experimentally obtained by sweeping it between 0.1 and 1, and observing the corresponding workload misspredictions (under/over) for various application workloads. A value of 0.6 is used for all the experiments as it resulted in relatively accurate workload prediction. Fig. 13 shows the actual and predicted values for three different application scenarios along with the percentage root mean square error (that is up to 2.4 percent). The ﬁgure shows that the prediction slightly goes up with the number of active applications, which is due to increased dynamic workload variations. To improve the accuracy in such cases, we will look into better workload prediction techniques in the future.

5.4 Overheads of the Proposed Approach
5.4.1 Run-Time Overhead
The run-time overhead of the adopted approach includes time for ﬁnding thread-core-mapping (Tmap) and V-f pair (TV -f ), which can be represented as,

To ¼ Tmap þ TV -f

(10)

TV -f

¼

Tex

À

r Ã len Ts

Ã

Ts

Ã

½Vf So

þ

PMCo

þ

Proco;

(11)

where To, Tex, r, VfSo, PMCo, and Proco, represent total overhead, execution time, number of times the adaptation is paused, overheads associated with V-f switching, PMC collection and remaining processing steps (involving len and Ts, shown in Algorithm 2), respectively. Tmap depends on the implementation of the Algorithm 1, in our case it is up to 1.6 ms (averaged over various application scenarios). Moreover, TV -f is about 300 ms, which is 0.15 percent of Ts

Fig. 13. Workload prediction using EWMA for three different application scenarios—one, two, and three active applications (left to right). Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

REDDY ET AL.: INTER-CLUSTER THREAD-TO-CORE MAPPING AND DVFS ON HETEROGENEOUS MULTI-CORES

381

paper can be found at doi:10.5258/SOTON/D0249 (https:// doi.org/10.5258/SOTON/D0249).

Fig. 14. Run-time overhead of the proposed approach.
(200 ms). Fig. 14 illustrates the total run-time overhead, computed as percentage of total execution time, for eight application scenarios. The run-time overhead for application scenario bl-bo-fr, having a long execution time of 1,053 sec is $0.17 percent, which is very minimal. Whereas, commonly used learning-based approaches have signiﬁcant overheads (up to 216 sec for learning and 1 sec for subsequent stages) for a single-application scenario [16]), which gets further aggravated by dynamic workload variations causing frequent re-learning. Therefore, the scalability of such approaches in comparison to the proposed technique is limited for multi-core platforms executing multiple multithreaded applications concurrently.
5.4.2 Ofﬂine Analysis Overhead
As discussed earlier, the proﬁled data of each application contains performance (1/execution time), energy consumption, number of big, and number of LITTLE cores for each design point. The total number of design points for each application is 24, which results in a small storage overhead of 770 bytes. The energy overhead due to storing of proﬁled data is already included in the energy consumption values reported in the previous sections.
6 CONCLUSION
We proposed a run-time management methodology for concurrently executing multi-threaded applications on a heterogeneous multi-core system. It uses the knowledge from design time analysis for efﬁcient thread-to-core mapping and workload classiﬁcation through MRPI to make runtime decisions. Furthermore, it also employs workload selection and prediction techniques for pro-active V-f control and online performance observation and compensation to adapt to the dynamic variations. Validation on OdroidXU3 platform for various application scenarios shows an average improvement up to 33 percent in energy consumption compared to the existing approaches while achieving up to 15 percent performance improvement over given performance requirements. The advances reported in this paper are important contributions towards the development of future energy efﬁcient, feature rich embedded systems with heterogeneous many-cores. In future, we will look into per-core DVFS techniques which allow to control the V-f level of each core separately.
ACKNOWLEDGEMENTS
This work was supported in parts by the EPSRC Grant EP/ L000563/1 and the PRiME Programme Grant EP/K034448/1 (www.prime-project.org). Experimental data used in this

REFERENCES
[1] A. K. Singh, M. Shaﬁque, A. Kumar, and J. Henkel, “Mapping on multi/many-core systems: Survey of current and emerging trends,” in Proc. 50th Annu. Des. Autom. Conf., 2013, Art. no. 1.
[2] B. Khemka, et al., “Utility maximizing dynamic resource management in an oversubscribed energy-constrained heterogeneous computing system,” Sustainable Comput.: Inf. Syst., vol. 5, pp. 14–30, 2015.
[3] P. Greenhalgh, “big.LITTLE processing with ARM cortex-a15 & cortex-a7,” ARM White paper, pp. 1–8, 2011.
[4] D. N. Truong, et al., “A 167-processor computational platform in 65 nm CMOS,” IEEE J. Solid-State Circuits, vol. 44, no. 4, pp. 1130– 1144, Apr. 2009.
[5] A. Prakash, H. Amrouch, M. Shaﬁque, T. Mitra, and J. Henkel, “Improving mobile gaming performance through cooperative CPU-GPU thermal management,” in Proc. 53nd ACM/EDAC/IEEE Des. Autom. Conf., 2016, pp. 1–6.
[6] B. Donyanavard, T. Mu€ck, S. Sarma, and N. Dutt, “SPARTA: Runtime task allocation for energy efﬁcient heterogeneous manycores,” in Proc. 11th IEEE/ACM/IFIP Int. Conf. Hardw./Softw. Codesign Syst. Synthesis, 2016, Art. no. 27.
[7] Odroid-XU3. [Online]. Available: www.hardkernel.com/main/ products
[8] R. A. Shaﬁk, S. Yang, A. Das, L. A. Maeda-Nunez, G. V. Merrett, and B. M. Al-Hashimi, “Learning transfer-based adaptive energy minimization in embedded systems,” IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 35, no. 6, pp. 877–890, Jun. 2016.
[9] C. Bienia and K. Li, “Parsec 2.0: A new benchmark suite for chipmultiprocessors,” in Proc. 5th Annu. Workshop Modeling Benchmarking Simul., 2009, vol. 2011, pp. 1–9.
[10] R. Cochran, C. Hankendi, A. K. Coskun, and S. Reda, “Pack & cap: Adaptive DVFS and thread packing under power caps,” in Proc. 44th Annu. IEEE/ACM Int. Symp. Microarchitecture, 2011, pp. 175–185.
[11] H. Sasaki, S. Imamura, and K. Inoue, “Coordinated powerperformance optimization in manycores,” in Proc. 22nd Int. Conf. Parallel Archit. Compilation Techn., 2013, pp. 51–61.
[12] A. Aalsaud, R. Shaﬁk, A. Raﬁev, F. Xia, S. Yang, and A. Yakovlev, “Power-aware performance adaptation of concurrent applications in heterogeneous many-core systems,” in Proc. Int. Symp. Low Power Electron. Des., 2016, pp. 368–373.
[13] J. Ma, G. Yan, Y. Han, and X. Li, “An analytical framework for estimating scale-out and scale-up power efﬁciency of heterogeneous manycores,” IEEE Trans. Comput., vol. 65, no. 2, pp. 367– 381, Feb. 2016.
[14] D. Stamoulis and D. Marculescu, “Can we guarantee performance requirements under workload and process variations?” in Proc. Int. Symp. Low Power Electron. Des., 2016, pp. 308–313.
[15] V. Spiliopoulos, G. Keramidas, S. Kaxiras, and K. Efstathiou, “Power-performance adaptation in intel core i7,” in Proc. 2nd Workshop Comput. Archit. Operating Syst. Codes., 2011, pp. 1–10.
[16] A. K. Singh, C. Leech, K. R. Basireddy, B. M. Al-Hashimi, and G. V. Merrett, “Learning-based run-time power and energy management of multi/many-core systems: Current and future trends,” J. Low Power Electron., vol. 13, no. 3, pp. 310–325, Sep. 2017.
[17] C.-H. Hsu and U. Kremer, “Compiler-directed dynamic voltage scaling for memory-bound applications,” Dept. of Comput. Sci., Rutgers Univ., Newark, NJ, USA, Tech. Rep. DCS-TR-498, 2002.
[18] J. Luo and N. K. Jha, “Power-efﬁcient scheduling for heterogeneous distributed real-time embedded systems,” IEEE Trans. Comput.Aided Des. Integr. Circuits Syst., vol. 26, no. 6, pp. 1161–1170, Jun. 2007.
[19] M. Qiu and E. H.-M. Sha, “Cost minimization while satisfying hard/ soft timing constraints for heterogeneous embedded systems,” ACM Trans. Des. Autom. Electron. Syst., vol. 14, no. 2, 2009, Art. no. 25.
[20] L. K. Goh, B. Veeravalli, and S. Viswanathan, “Design of fast and efﬁcient energy-aware gradient-based scheduling algorithms heterogeneous embedded multiprocessor systems,” IEEE Trans. Parallel Distrib. Syst., vol. 20, no. 1, pp. 1–12, Jan. 2009.
[21] K. Ma, X. Li, M. Chen, and X. Wang, “Scalable power control for many-core architectures running multi-threaded applications,” in Proc. ACM SIGARCH 38th Annu. Int. Symp. Comput. Archit. News, 2011, vol. 39, no. 3, pp. 449–460.
[22] A. Weissel and F. Bellosa, “Process cruise control: Event-driven clock scaling for dynamic power management,” in Proc. Int. Conf. Compilers Archit. Synthesis Embedded Syst., 2002, pp. 238–246.

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

382

IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS, VOL. 4, NO. 3, JULY-SEPTEMBER 2018

[23] L. C. Singleton, C. Poellabauer, and K. Schwan, “Monitoring of cache miss rates for accurate dynamic voltage and frequency scaling,” in Proc. Conf. 12th, Multimedia Comput. Netw., 2005, pp. 121–125.
[24] A. Nabina and J. L. Nunez-Yanez, “Adaptive voltage scaling in a dynamically reconﬁgurable FPGA-based platform,” ACM Trans. Reconﬁgurable Technol. Syst., vol. 5, no. 4, 2012, Art. no. 20.
[25] K. Van Craeynest, A. Jaleel, L. Eeckhout, P. Narvaez, and J. Emer, “Scheduling heterogeneous multi-cores through performance impact estimation (PIE),” ACM SIGARCH Comput. Archit. News, vol. 40, no. 3, pp. 213–224, 2012.
[26] E. Del Sozzo, G. C. Durelli, E. Trainiti, A. Miele, M. D. Santambrogio, and C. Bolchini, “Workload-aware power optimization strategy for asymmetric multiprocessors,” in Proc. Des. Autom. Test Europe Conf. Exhib., 2016, pp. 531–534.
[27] S. Ghiasi, T. Keller, and F. Rawson, “Scheduling for heterogeneous processors in server systems,” in Proc. 2nd Conf. Comput. Frontiers, 2005, pp. 199–210.
[28] M. Becchi and P. Crowley, “Dynamic thread assignment on heterogeneous multiprocessor architectures,” in Proc. 3rd Conf. Comput. Frontiers., 2006, pp. 29–40.
[29] J. Chen and L. K. John, “Efﬁcient program scheduling for heterogeneous multi-core processors,” in Proc. 46th Annu. Des. Autom. Conf., 2009, pp. 927–930.
[30] D. Koufaty, D. Reddy, and S. Hahn, “Bias scheduling in heterogeneous multi-core architectures,” in Proc. 5th Eur. Conf. Comput. Syst., 2010, pp. 125–138.
[31] D. Shelepov, et al., “HASS: A scheduler for heterogeneous multicore systems,” ACM SIGOPS Operating Syst. Rev., vol. 43, no. 2, pp. 66–75, 2009.
[32] ARM big.LITTLE Technology. [Online]. Available: http://www. arm.com/
[33] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta, “The SPLASH-2 programs: Characterization and methodological considerations,” in Proc. 22nd Annu. Int. Symp. Comput. Archit., 1995, pp. 24–36.
[34] Mediatek X20. [Online]. Available: http://www.96boards.org/ product/mediatek-x20/
[35] S. Sinha, J. Suh, B. Bakkaloglu, and Y. Cao, “Workload-aware neuromorphic design of the power controller,” IEEE J. Emerging Select. Topics Circuits Syst., vol. 1, no. 3, pp. 381–390, Sep. 2011.
[36] S. Eranian, “Perfmon2: A ﬂexible performance monitoring interface for linux,” in Proc. of Ottawa Linux Symposium. New York, NY, USA: Citeseer, 2006, pp. 269–288.
[37] G. Keramidas, V. Spiliopoulos, and S. Kaxiras, “Interval-based models for run-time DVFS orchestration in superscalar processors,” in Proc. 7th ACM Int. Conf. Comput. Frontiers, 2010, pp. 287– 296.
[38] S. Park, et al., “Accurate modeling of the delay and energy overhead of dynamic voltage and frequency scaling in modern microprocessors,” IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol. 32, no. 5, pp. 695–708, May 2013.
[39] A. Das, A. Kumar, B. Veeravalli, R. Shaﬁk, G. Merrett, and B. Al-Hashimi, “Workload uncertainty characterization and adaptive frequency scaling for energy minimization of embedded systems,” in Proc. Des. Autom. Test Europe Conf. Exhib., 2015, pp. 43–48.
[40] K. Yu, D. Han, C. Youn, S. Hwang, and J. Lee, “Power-aware task scheduling for big. little mobile processor,” in Proc. Int. SoC Des. Conf., 2013, pp. 208–212.
[41] Linux-governors. [Online]. Available: https://www.kernel.org/ doc/Documentation/cpu-freq/governors.txt
Basireddy Karunakar Reddy received the MTech degree in microelectronics and VLSI from the Indian Institute of Technology (IIT), Hyderabad, India in 2015. He is working toward the PhD degree in electronic and electrical engineering with the University of Southampton, United Kingdom. His current research interests include design-time and run-time optimization of performance, and energy in multi-core heterogeneous systems.

Amit Kumar Singh (M’09) received the BTech degree in electronics engineering from the Indian Institute of Technology (Indian School of Mines), Dhanbad, India, in 2006, and the PhD degree from the School of Computer Engineering, Nanyang Technological University (NTU), Singapore, in 2013. He was with HCL Technologies, India, for a year and half before starting his PhD at NTU, Singapore, in 2008. He worked as a post-doctoral researcher with the National University of Singapore (NUS) from 2012 to 2014, the University of York, United Kindgom from 2014 to 2016, and the University of Southampton, United Kingdom from 2016 to 2017. He is currently working as a lecturer with the University of Essex, United Kingdom. His current research interests include system level design-time and run-time optimizations of 2D and 3D multi-core systems with a focus on performance, energy, temperature, and reliability. He has published more than 50 papers in the above areas in leading international journals/conferences. He was the recipient of the ISORC 2016 Best Paper Award, PDP 2015 Best Paper Award, HiPEAC Paper Award, and GLSVLSI 2014 Best Paper Candidate. He has served on the TPC of IEEE/ACM conferences like ISED, MES, NoCArc, ESTIMedia, and DATE. He is a member of the IEEE.
Dwaipayan Biswas received the MSc degree in system on chip (SoC) and the PhD degree from the University of Southampton, United Kingdom, in 2011 and 2015, respectively. He is presently a research fellow working on embedded systems and biomedical signal processing with the University of Southampton.
Geoff Merrett (GSM’06-M’09) received the BEng (Hons.) degree in electronic engineering and the PhD degree from the University of Southampton, Southampton, United Kingdom, in 2004 and 2009, respectively. He is currently an associate professor in electronic systems with the University of Southampton. His current research interests include low-power and energy harvesting aspects of embedded & mobile systems. He has published more than 100 articles in journals/conferences in the above areas. He was the general chair of the Energy Neutral Sensing Systems Workshop from 2013 to 2015. He is a fellow of the The Higher Education Academy. He is a member of the IEEE.
Bashir M. Al-Hashimi (M’99-SM’01-F’09) is an ARM professor of Computer Engineering, dean of the Faculty of Physical Sciences and Engineering, and the co-director of the ARM-ECS Research Centre, University of Southampton, Southampton, United Kingdom. He has published more than 380 technical papers. His current research interests include methods, algorithms, and design automation tools for low-power design and test of embedded computing systems. He has authored or co-authored ﬁve books and has graduated 35 PhD students. He is fellow of the IEEE.
" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.

Authorized licensed use limited to: KAUST. Downloaded on April 23,2022 at 09:02:09 UTC from IEEE Xplore. Restrictions apply.

