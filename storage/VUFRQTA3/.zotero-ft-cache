This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118 1
Task-Oriented Communication for Multi-Device Cooperative Edge Inference
Jiawei Shao, Student Member, IEEE, Yuyi Mao, Member, IEEE, and Jun Zhang, Fellow, IEEE

Abstract—This paper investigates task-oriented communication for multi-device cooperative edge inference, where a group of distributed low-end edge devices transmit the extracted features of local samples to a powerful edge server for inference. While cooperative edge inference can overcome the limited sensing capability of a single device, it substantially increases the communication overhead and may incur excessive latency. To enable lowlatency cooperative inference, we propose a learning-based communication scheme that optimizes local feature extraction and distributed feature encoding in a task-oriented manner, i.e., to remove data redundancy and transmit information that is essential for the downstream inference task rather than reconstructing the data samples at the edge server. Specifically, we leverage Tishby’s information bottleneck (IB) principle [1] to extract the taskrelevant feature at each edge device, and adopt the distributed information bottleneck (DIB) framework of Aguerri-Zaidi [2] to formalize a single-letter characterization of the optimal raterelevance tradeoff for distributed feature encoding. To admit flexible control of the communication overhead, we extend the DIB framework to a distributed deterministic information bottleneck (DDIB) objective that explicitly incorporates the representational costs of the encoded features. As the IB-based objectives are computationally prohibitive for high-dimensional data, we adopt variational approximations to make the optimization problems tractable. To compensate for the potential performance loss due to the variational approximations, we also develop a selective retransmission (SR) mechanism to identify the redundancy in the encoded features among multiple edge devices to attain additional communication overhead reduction. Extensive experiments on multi-view image classification and multi-view object recognition tasks evidence that the proposed task-oriented communication scheme achieves a better rate-relevance tradeoff than existing methods.
Index Terms—Task-oriented communication, information bottleneck (IB), distributed information bottleneck (DIB), variational inference.
I. INTRODUCTION
The recent revival of artificial intelligence (AI) has affected a plethora of application domains, including computer vision [3], augmented/virtual reality (AR/VR) [4], and natural language processing (NLP) [5]. Most recently, as edge devices (e.g., mobile phones, sensors) are becoming increasingly widespread and sensory data are easy to access, the potential

of AI technologies has been exemplified at the network edge, which is a new research area named edge inference [6], [7], [8]. Typical edge inference systems convey all the input data from the edge devices to a physically close edge server (e.g., a base station) and leverage deep neural networks (DNNs) to perform inference. However, as the volume of the collected data (e.g., images, high-definition videos, and point clouds) is enormous in typical mobile intelligent applications [9], [10], [11], directly transmitting the raw data to the edge server may lead to a prohibitive communication overhead.
To enable low-latency inference, device-edge co-inference stands out as a promising solution [12], [13], [14], which extracts task-relevant features from the raw input data and forwards them to be processed by an edge server. This exemplifies a recent shift in communication system design for emerging mobile applications, named task-oriented communication, which optimizes communication strategies for the downstream task [15]. While device-edge co-inference, together with taskoriented communication, has enabled low-latency inference for resource-constrained devices, its performance is limited by the sensing capability of a single device. In many applications, e.g., smart drones [10], robotics [11], and security camera systems [16], cooperative inference among multiple devices can significantly enhance the inference performance. Nevertheless, the communication overhead and latency may also increase substantially with the number of devices, forming the bottleneck for multi-device cooperative edge inference. Although most existing works on task-oriented communication have led to efficient communication for single-device edge inference, simply reusing these methods for multi-device cooperative edge inference is infeasible. This is because the correlation among the extracted features of different devices cannot be effectively exploited, which leads to excessive redundancy and latency in transmission. To address the limitation, this paper will develop task-oriented communication strategies for multi-device cooperative edge inference by leveraging Tishby’s information bottleneck (IB) principle [1], as well as the distributed information bottleneck (DIB) framework proposed by Aguerri and Zaidi [2].

J. Shao and J. Zhang are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong (E-mail: jiawei.shao@connect.ust.hk, eejzhang@ust.hk). Y. Mao is with the Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong (E-mail: yuyi-eie.mao@polyu.edu.hk). This work was supported by the General Research Fund (Project No. 15207220) and the Research Impact Fund (R5009-21) from the Hong Kong Research Grants Council. The work of Y. Mao was supported by a start-up fund of the Hong Kong Polytechnic University under Grant P0038174. (The corresponding author is J. Zhang.)

A. Related Works and Motivations
The recent line of research on task-oriented communication [17], [18] has motivated a paradigm shift in communication system design. The conventional data-oriented communication aims to guarantee the correct reception of every single transmitted bit. However, with the increasing number of edge devices, the limited communication resources at the wireless

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
2

network edge cannot cope with the request to transmit the huge volume of data. The task-oriented communication provides an alternative scheme to handle this challenge, which regards a part of the raw input data (e.g., nuisance) as irrelevant and considers to extract and transmit only the task-relevant feature that could influence the inference result. As recovering the original input data with high fidelity at the edge server is unnecessary, task-oriented communication schemes leave abundant room for communication overhead reduction. This insight is consistent with the information bottleneck (IB) principle [1]: A good extracted feature should be sufficient for the inference task while retaining minimal information from the input data. Thus, extracting a minimal sufficient feature in task-oriented communication improves the transmission efficiency.
Recently, there exist several studies taking advantages of deep learning to develop effective feature extraction and encoding methods following the task-oriented design principle [14], [15], [19], [20]. In particular, for the image classification task, an end-to-end architecture was proposed in [19] that leverages a JPEG compressor to encode the internal features extracted by the on-device neural network. In contrast to data-oriented communication that concerns the data recovery quality, the proposed method is directly trained with the crossentropy loss for the targeted classification task. Similar ideas were utilized in the image retrieval task [21] and the point cloud processing application [14]. Besides, the authors of [20] characterized the minimum bit rate of the extracted features required to ensure satisfactory performance of the prediction tasks. These features are learned from the unsupervised neural compressors that bound the encoded rates. Although the aforementioned works can largely reduce the communication overhead in single-device edge inference, simply extending them to the multi-device scenario is not efficient as their coding schemes cannot fully exploit the correlation among the extracted features of different devices, which necessitates the investigation of distributed feature encoding.
Distributed coding is an important problem in information theory and communication, which regards the data encoding of multiple correlated sources from different devices that do not communicate with each other. Remarkably, the SlepianWolf Theorem [22] shows that distributed coding can achieve the joint coding rate for correlated memoryless sources. In other words, the redundancy among the dependent sources can be identified and eliminated before transmission. This classic result has recently received renewed attention and led to several works [23], [24], [25], [26] to develop DNNbased distributed feature encoding schemes in task-oriented communication. The authors of [23] developed a distributed quantization system tailored to the classification task, and proposed a greedy search algorithm as part of the deep learning approach to create rectangular quantization regions. Besides, a significance-aware feature selection mechanism was proposed in [24], which leverages a DNN model to determine the significance value of each feature and communicates only the features that are likely to impact the inference result. These methods have shown effectiveness in saving communication bandwidth compared with the single-device edge inference.

However, there lacks a theoretical and rigorous way to characterize the tradeoff between the communication cost and the inference performance in distributed feature encoding, hindering further performance enhancement.
Recently, the distributed information bottleneck (DIB) framework [2] was proposed to investigate multi-view learning, which establishes a single-letter characterization of the optimal tradeoff between the rate (or the representational cost) and relevance (or task-relevant information) for a class of memoryless sources using mutual information terms. Such an information-theoretical principle fits nicely with the multidevice cooperative edge inference as it can provide an optimal distributed coding scheme under the assumption that the input views are conditionally independent of the inference variable. In this paper, we consider a general scenario where the input views may not be conditionally independent and add a feature extraction step before the coding process to improve the communication efficiency.
B. Contributions
In this paper, we develop task-oriented communication strategies for cooperative edge inference, based on the IB principle for task-relevant feature extraction and the DIB framework for distributed feature encoding. The major contributions are summarized as follows:
• We formulate two design problems for task-oriented communication in multi-device cooperative edge inference systems, namely task-relevant feature extraction and distributed feature encoding. Specifically, we leverage the IB principle to design the local feature extractor, which aims at discarding the task-irrelevant correlated information across input views. Besides, to obtain a practical one-shot coding scheme, we reformulate the DIB objective as the distributed deterministic information bottleneck (DDIB) objective that explicitly incorporates the representational costs of the encoded features (i.e., the number of bits to be transmitted).
• Observing that the mutual information terms in the IBbased objectives are computationally prohibitive for highdimensional data, we resort to the variational approximations to devise tractable upper bounds for the IB and DDIB objectives, resulting in the variational information bottleneck (VIB) objective and the variational distributed deterministic information bottleneck (VDDIB) objective, respectively.
• To compensate for the potential performance loss due to the variational approximations, we introduce a selective retransmission (SR) mechanism in the VDDIB framework to identify the redundancy among the encoded features, which helps to further reduce the communication load. It also provides a flexible way to trade off the communication overhead with the inference performance.
• The effectiveness of the proposed task-oriented communication strategies is validated on multi-view image classification tasks and multi-view object recognition tasks, which demonstrates a substantial reduction in the communication overhead compared with the traditional dataoriented communication strategy. Moreover, extensive

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
3

simulation results demonstrate that our methods outperform existing learning-based feature encoding methods for cooperative edge inference.

C. Organization
The rest of this paper is organized as follows. Section II introduces the system model and formulates the problems of task-relevant feature extraction and distributed feature encoding based on the IB and DIB frameworks, respectively. Section III introduces the VIB objective as a variational upper bound of the IB formulation for tractable optimization. Section IV reformulates the DIB objective to the DDIB objective and derives a tractable VDDIB objective for optimization of the distributed feature encoding. A selective retransmission mechanism is developed in Section V to further reduce the communication overhead. In Section VI, we provide extensive simulation results to evaluate the performance of the proposed methods. Finally, Section VII concludes the paper.

Fig. 1: An example of multi-device cooperative edge inference for multi-view object recognition.

D. Notations
Throughout this paper, upper-case letters (e.g. X, Y ) and lower-case letters (e.g. x, y) stand for random variables and their realizations, respectively. We use X1:K to represent the set of random variables (X1, . . . , XK ). The entropy of Y and the conditional entropy of Y given X are denoted as H(Y ) and H(Y |X), respectively. The mutual information between X and Y is represented as I(X, Y ), and the Kullback-Leibler (KL) divergence between two probability distributions p(x) and q(x) is denoted as DKL (p||q). The statistical expectation of X is denoted as E (X). We further denote the Gaussian distribution with mean vector µ and covariance matrix Σ as N (µ, Σ), and use I to represent the identity matrix. The element-wise product is denoted as ⊙.
II. SYSTEM MODEL AND PROBLEM DESCRIPTION
We consider a multi-device cooperative edge inference system, where a group of low-end edge devices collaborate to perform an inference task with the assistance of an edge server. These devices perceive different views of the same object, and therefore cooperative inference can effectively overcome the limited sensing capability of a single device [27]. Related application scenarios include multi-view object detection [27] in security camera systems and vehicle re-identification [16] in urban surveillance. A sample system for multi-view object recognition is shown in Fig. 1. Compared with the edge devices, the edge server has abundant computational resources so that server-based inference can be executed in real time by collecting the sensing data from multiple edge devices. Nevertheless, edge inference systems typically suffer from limited communication bandwidth, and thus directly transmitting the raw sensory data (e.g., high-definition images and point clouds) from the edge devices to the edge server shall incur significant communication latency. To enable low-latency cooperative edge inference, it is critical to design efficient taskoriented communication strategies that only transmit the taskrelevant information to the edge server for device-edge coinference. Next, we first present a probabilistic modeling of the

Fig. 2: Probabilistic modeling for multi-device cooperative edge inference systems.

multi-device cooperative edge inference systems and introduce two key design problems of task-oriented communication, namely, task-relevant feature extraction and distributed feature encoding.

A. Probabilistic Modeling
The probabilistic modeling for multi-device cooperative edge inference, comprising K edge devices and an edge server, is shown in Fig. 2. The input views of different edge devices, denoted as (x1, . . . , xK ), and the target variable y (e.g., the category of an object), are deemed as different realizations of the random variables (X1, . . . , XK , Y ) with joint distribution p(x1, . . . , xK , y). To perform cooperative inference, each edge device first extracts the task-relevant feature zk from the input xk, and encodes the extracted feature to a compact vector uk for efficient transmission to the edge server. After receiving the encoded features from the edge devices, the edge server performs further processing to derive the final inference result. The extracted features (z1, . . . , zK ) and the encoded features (u1, . . . , uK ) are instantiated by random variables (Z1, . . . , ZK ) and (U1, . . . , UK ), respectively. These random variables constitute the following Markov chain:

Y ↔ Xk ↔ Zk ↔ Uk, ∀k ∈ {1, . . . , K},

(1)

which satisfies p(uk, zk, xk|y) = p(uk|zk)p(zk|xk)p(xk|y), ∀k ∈ {1, . . . , K}. A key design objective is to minimize the communication overhead while providing satisfactory inference performance. The proposed solution involves effective local feature extraction and distributed feature encoding, which are specified in the following two subsections.

B. Task-Relevant Feature Extraction
Given the high dimension of the sensing data, a significant part of the input Xk at the k-th device is superfluous to the

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
4

target variable Y . To achieve efficient task-oriented communication, the extracted feature Zk should capture only the task-relevant information about the target variable Y from the observation Xk. Our design follows the information bottleneck (IB) principle [1], which was shown to be effective for feature extraction and encoding in single-device edge inference systems [15]. The core idea is to maximize the mutual information between the encoded feature Zk and the target variable Y while minimizing the mutual information between the encoded feature Zk and the input view Xk. The objective function is formulated as follows:

LIB,k(γ) := H(Y |Zk) + γI(Xk; Zk),

(2)

where γ ≥ 0 is a weighting factor, and the constant term H(Y ) is eliminated as I(Y ; Zk) = H (Y ) − H (Y |Zk). As minimizing the IB objective function requires high computation cost to obtain the marginal distribution of Zk, we adopt the variational method [2] to reformulate (2) into an amenable form in Section III.
Remark 1. The feature extraction step would benefit the following distributed feature encoding, since the extracted features can be less correlated by discarding the task-irrelevant correlated information across input views. Ideally, if each extracted feature Zk for k ∈ {1, . . . , K} satisfies the minimality and sufficiency conditions [28], [29] as follows:

Sufficiency
I(Xk; Zk) = I(Y ; Xk) = I(Y ; Zk), k ∈ {1, . . . , K}, (3)
Minimality
we prove in Appendix A that the minimal sufficient features meet the conditional independence assumption of the DIB theorem [2]. In such cases, we will be able to obtain the raterelevance tuples in Proposition 1.

C. Distributed Feature Encoding

Once the task-relevant feature is extracted, it will be en-
coded for efficient transmission. Intuitively, there is an implicit
tradeoff between rate and relevance: The inference accuracy
can be improved by transmitting more task-relevant infor-
mation from multiple edge devices at the cost of increased
communication overhead. By analogy with the rate-distortion
tradeoff in source coding problems [30], [31], we resort to the DIB framework [2] to characterize the rate-relevance tradeoff
in edge inference. Consider a feature encoding scheme with blocklength N , which maps the extracted features Zk1:N = (Zk1, . . . , ZkN ) of N input samples to a codeword that belongs to set MNk . The decoder at the edge server processes the codewords from devices to derive the inference result Yˆ 1:N . The relevance ∆N that measures the accuracy of the inference result Yˆ 1:N and the rate RkN accounting for the communication overhead of edge device k are defined respectively as follows:

∆N :=

1 I

Y 1:N ; Yˆ 1:N

,

(4)

N

RkN

≥

1 N

log |MNk |,

k ∈ {1, . . . , K}.

(5)

We endeavor to develop a set of distributed feature encoders

for the edge devices and a joint decoder for the edge server

that maximize the relevance while reducing the sum of the

communication costs, i.e., RsNum :=

K k=1

RkN

.

The

perfor-

mance of the multi-device cooperative edge inference system

can be characterized by the achievable rate-relevance region,

as specified in the following definition.

Definition 1. A tuple (∆, Rsum) is said to be achievable if there exists a blocklength-N feature encoding scheme that satisfies the following inequalities:

∆ ≤ ∆N and Rsum ≥ RsNum.

(6)

The rate-relevance region is given by the closure of all the achievable (∆, Rsum) tuples.

The following proposition establishes a single-letter characterization of the optimal rate-relevance tuples in multi-device cooperative edge inference systems.

Proposition 1. (Distributed Information Bottleneck [2]) Suppose the extracted features Zk for k ∈ {1, . . . , K} are conditionally independent given the target variable Y . Each (∆β, Rβ) with β ≥ 0 is an optimal rate-relevance tuple, i.e., there exists no relevance ∆ ≥ ∆β given the sum rate constraint Rsum = Rβ, where
K
∆β = I(Y ; U1∗:K ), Rβ = ∆β + [I(Zk; Uk∗) − I(Y ; Uk∗)],
k=1
(7) and the encoded features U1∗:K are obtained by minimizing the following distributed information bottleneck (DIB) objective:

K

min LDIB(β) :=H (Y |U1:K ) + β [H (Y |Uk)

{p(uk |zk )}K k=1

k=1

+I (Zk; Uk)] . (8)

Proof. The proof is available in Section 7 of [2].

Remark 2. The optimality of the DIB framework relies on the

conditional independence assumption, which can be satisfied

when the extracted features are minimal and sufficient. Nev-

ertheless, the minimality and sufficiency conditions in (3) may

not be satisfied in general cases for two main reasons. First,

as discussed in [32], there is a maximum value m(Xt, Y ) ≤ 1

for

dataset

(Xt, Y )

such

that

m(Xt; Y )

≥

I(Zt;Y ) I (Zt ;Xt )

for

any

representation Zt, where the Markov chain Zt ↔ Xt ↔ Y

holds and the mutual information I(Zt; Xt) > 0. Thus,

for datasets with m(Xt; Y ) < 1, we cannot obtain the

minimal sufficient feature Zt. Besides, the parameter γ in the

IB objective function (2) controls the rate-relevance tradeoff

and influences the solution. Therefore, even if the minimal

sufficient feature does exist, we cannot guarantee that the

solution obtained by minimizing (2) satisfies the minimality

and sufficiency conditions. But our empirical results in Ap-

pendix D demonstrate that adding an extra feature extraction

step improves the performance of distributed coding. This

can be explained by the effectiveness of feature extraction,

which discards the task-irrelevant information and allows the

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
5

extracted features to be less correlated compared with the raw input views Xk for k ∈ {1, . . . , K}.
Proposition 1 shows a distributed feature encoding scheme by minimizing the DIB objective in (8), where the parameter β balances the communication overhead and the inference performance. However, obtaining the achievable rate-relevance tuples (∆β, Rβ) may need a too large blocklength N according to the definition in (6). In other words, the edge inference system needs to accumulate a significant amount of input samples before performing distributed feature encoding, which shall postpone the inference process. To enable instantaneous edge inference for each input sample, we reformulate the DIB framework to a distributed deterministic information bottleneck (DDIB) objective in Section IV, which replaces mutual information terms I(Zk; Uk) with the representational costs Rbit(Uk) (i.e., the number of bits) of the encoded features. Besides, we adopt variational approximations to derive an upper bound for the conditional entropy terms H(Y |U1:K ) and {H(Y |Uk)}Kk=1 for more tractable optimization.
III. VARIATIONAL APPROXIMATION FOR FEATURE EXTRACTION
In this section, we leverage a variational information bottleneck (VIB) framework to reformulate the IB objective derived in Section II-B to facilitate tractable optimization of feature extraction.

A. Variational Information Bottleneck
Recall the IB objective in (2) with the form of LIB,k(γ) := H(Y |Zk) + γI(Xk; Zk) for device k. Let pθk (zk|xk) denote the conditional distribution parameterized by θk that represents the feature extraction process. Given the joint distribution p(xk, y) and the Markov chain Y ↔ Xk ↔ Zk in (1), the marginal distribution p(zk) and the conditional distribution p(y|z) are fully determined by the conditional distribution pθk (zk|xk) as follows:

p(zk) = p(xk)pθk (zk|xk)dxk,

(9)

p(y|zk) =

p(xk, y)pθk (zk|xk)dxk . p(zk )

(10)

However, the distributions p(zk) and p(y|zk) are generally intractable due to the high dimensionality. Similar to [15], we adopt tools from variational inference to approximate these intractable distributions. The idea of the variational approximation is to posit a family of distributions and find a member of that family which is close to the target distribution [33]. Specifically, we introduce rk(zk) and pφk (y|zk) as the variational distributions to approximate p(zk) and p(y|zk), respectively. The family of variational conditional distributions is parameterized by φk. With the aid of these approximations, the VIB objective is as follows:

LVIB,k(γ; θk, φk) =Ep(xk,y) Epθk (zk|xk) [− log pφk (y|zk)]
+ γDKL(pθk (zk|xk)||rk(zk)) , (11)

for which, the detailed derivation is available in Appendix B-A.

B. DNN Parameterization
To take advantage of deep learning, we formulate the distributions pθk (zk|xk) and pφk (y|zk) in terms of DNNs with parameters θk, φk. A common appoarch to parameterize the condition distribution pθk (zk|xk) is adopting the multivariate Gaussian distribution [28], i.e., pθk (zk|xk) = N (zk|µk, Σk), where the mean vector µk = (µk,1, . . . , µk,d) ∈ Rd is determined by the DNN-based function µk(xk; θk) parameterized by θk, and the covariance matrix k is represented by a diagonal matrix diag{σk2,1, . . . , σk2,d} with σk = (σk,1, . . . , σk,d) ∈ Rd determined by the DNN-based function σk(xk; θk). Besides, we treat the variational marginal distribution rk(zk) as a centered isotropic Gaussian distribution N (zk|0, I). As a result, we simplify the KL-divergence term in (11) as follows:

DKL(pθk (zk|xk)||rk(zk))

d
=

µ2k,i

+

σk2,i 2

−

1

−

log σk,i

.

(12)

i=1

To optimize the negative log-likelihood term in (11), we
adopt the reparameterization trick [34] to sample zk from the learned distribution pθk (zk|xk), where zk = µk + σk ⊙ ϵk, and ϵk is sampled from N (0, I). In this case, the Monte Carlo estimate of the negative log-likelihood term is differ-
entiable with respect to θk. The inference result is obtained based on the extracted feature zk by the function yˆ(zk; φk) with parameters φk. We formulate the variational conditional distribution as pφk (y|zk) ∝ exp (−ℓ(yk, yˆ(zk; φk))), where ℓ(·, ·) denotes the loss function to measure the discrepancy
between y and yˆk. By applying the Monte Carlo sampling method, we obtain an unbiased estimate of the gradient and
hence optimize the objective function in (11). In particular, given a minibatch of data {(xm k , ym)}M m=1 at device k, we have the following empirical estimation of the VIB objective:

1 LVIB,k(γ; θk, φk) ≃ M

M

{− log pφk (ym|zkm)

m=1

+γDKL(pθk (zkm|xm k )||rk(zkm))} . (13)

The training procedures for task-relevant feature extraction are

summarized in Algorithm 1.

IV. VARIATIONAL DISTRIBUTED FEATURE ENCODING
In this section, we develop a distributed feature encoding scheme based on the DIB objective in (8) to achieve efficient task-oriented communication. We first reformulate the DIB objective to a distributed deterministic information bottleneck (DDIB) objective that explicitly takes into account the communication overhead of each encoded feature. Besides, variational approximations are invoked again to make the optimization of the DDIB objective tractable.

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
6

Algorithm 1 Training Procedures for the Task-Relevant Fea-

ture Extraction at Device k

Input: Training dataset D, batch size M , initialized parameters θk,

φk .

Output: The optimized parameters θk and φk

1: repeat

2: Randomly select a minibatch {(xm k , ym)}M m=1 from D. 3: Compute the mean vector {µm k }M m=1 and the standard
deviation vector {σkm}M m=1.

4: while m = 1 to M do

5:

Sample ϵm k ∼ N (0, I).

6:

Compute zkm = µm k + σkm ⊙ ϵm k .

7: end while

8: Compute the loss LVIB,k(γ; θk, φk) based on (13).

9: Update parameters θk, φk through backpropagation.

10: until Convergence of parameters θk and φk

Algorithm 2 Training Procedures for the VDDIB Method Input: Training dataset D, batch size M , optimized parameters
{θk}K k=1, and initialized parameters ϕ, ψ. Output: The optimized parameters ϕ and ψ. 1: repeat 2: Randomly select a minibatch {(xm 1:K , ym)}M m=1 from D. 3: Extract the task-relevant features {z1m:K }M m=1 based on the
learned distributions {pθk (zk|xk)}K k=1. 4: Compute the loss LVDDIB(β; ϕ, ψ) based on (18). 5: Update parameters ϕ, ψ through backpropagation. 6: until Convergence of parameters ϕ and ψ.
a variational distributed deterministic information bottleneck (VDDIB) objective as follows:

A. Distributed Deterministic Information Bottleneck Reformulation
The DIB objective in (8) measures the data rate by the mutual information terms I(Zk, Uk) for k ∈ {1, . . . , K}. Although the mutual information has broad applications in representation learning [2], [35], directly controlling the amount of resources required to represent each Uk (i.e., the number of bits) is more relevant in communication systems. To address the communication cost, we select a deterministic encoding function that maps each task-relevant feature Zk to a bitstream Uk, and adopt the number of bits, denoted as Rbit(Uk), to measure the communication overhead, which results in the following DDIB objective:
K
LDDIB(β) := H (Y |U1:K ) + β [H (Y |Uk) + Rbit (Uk)] .
k=1
(14) In particular, the feature encoder in each edge device first performs the encoding transformation fϕk , followed by the uniform quantizer Q that discretizes each element in fϕk (zk) ∈ Rdk into nk bits. The encoded feature is given by uk = Q(fϕk (zk)), and the gradients in the training process is approximated by the straight-through estimator (STE) [36]. The amount of information needed to be transmitted from the device k to the edge server is Rbit(uk) = nkdk bits1.
B. Variational Distributed Deterministic Information Bottleneck
The conditional distributions {p(y|uk)}Kk=1 and p(y|u1:K ) are fully determined given deterministic mappings from Zk to Uk, k ∈ {1, · · · , K}. However, calculating the conditional entropy terms H(Y |U1:K ) and H(Y |Uk) in (14) is generally intractable due to the high-dimensional integrals. Following Section III, we adopt variational distributions pψ0 (y|u1:K ) and {pψk (y|uk)}Kk=1 to replace the terms p(y|u1:K ) and {p(y|uk)}Kk=1. By denoting ψ := {ψ0, {ψk}Kk=1} as the DNN parameters for the variational distributions, we derive
1Note that the quantization scheme is orthogonal to the proposed DDIB method. We select the uniform quantization in this paper, since it is easier to approximate the gradients and calculate the bit-length.

LVDDIB(β; ϕ, ψ) := Epθ(z1:K ,y) − log pψ0 (y|u1:K )

K

K

+β

− log pψk (y|uk) + Rbit(uk) ,

k=1

k=1

K

with pθ(z1:K , y) = p(x1:K , y) pθk (zk|xk)dx1:K ,

k=1
(15) where the parameters {θk}Kk=1 are obtained by the taskrelevant feature extraction learned from Algorithm 1. The

VDDIB objective is an upper bound of the DDIB objective

as shown in Appendix B-B. The variational distributions are

formulated as follows:

pψ0 (y|u1:K ) ∝ exp(−ℓ(y, yˆ(u1:K ; ψ0))), (16)

pψk (y|uk) ∝ exp(−ℓ(y, yˆ(uk; ψk))), k ∈ {1, . . . , K}, (17)
where the functions yˆ(u1:K ; ψ0) and {yˆ(uk; ψk)}Kk=1 can be interpreted as the predictors that use the received features
to predict the target variable. In the training process, by
leveraging the Monte Carlo sampling method as in (13) and selecting a minibatch {(z1m:K , ym)}M m=1 from the joint distribution pθ(z1:K , y), we have the following expression to
estimate the VDDIB objective function:

1M LVDDIB(β; ϕ, ψ) ≃ M
m=1 K

− log pψ0 (ym|um 1:K ). (18)

+ β {− log pψk (ym|um k ) + Rbit(um k )} .

k=1

The optimization procedures for the VDDIB objective are summarized in Algorithm 2.

Remark 3. Given the random variable U1:K , the VDDIB objective is an upper bound of the DIB objective as shown in the following expression, owing to I(Zk, Uk) ≤ H(Uk) ≤ Rbit(Uk) and the variational approximations:

LDIB(β) ≤ LDDIB(β) ≤ LVDDIB(β; ϕ, ψ). (19)

It is important to note that the distributed feature encoding scheme obtained by minimizing the VDDIB objective does not guarantee to achieve the optimal rate-relevance tuples in

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
7

Proposition 1 due to the approximations in the IB and DIB optimizations. Nevertheless, the empirical results in Section VI-C evidence that the proposed method notably outperforms the existing communication strategies for multi-device cooperative edge inference.
V. DISTRIBUTED FEATURE ENCODING WITH SELECTIVE RETRANSMISSION
As highlighted in Remark 3, minimizing the VDDIB objective may not result in the optimal rate-relevance tuples. To further reduce the communication overhead, we need more effective approaches to identify the redundancy in the encoded features of multiple edge devices. For this purpose, we propose a VDDIB-SR method that introduces a selective retransmission (SR) mechanism to the VDDIB framework, which performs multiple sequential communication rounds [37] between the edge devices and the edge server. Particularly, the edge server selectively activates the edge devices to retransmit their encoded features based on the informativeness of the received features.
A. Selective Retransmission Mechanism
In this framework, each device is allowed to transmit the encoded feature vector uk = [uk,1, . . . , uk,T ] with a maximum number of T attempts (i.e., T − 1 retransmissions). The deterministic mapping from Zk to Uk is defined as uk = Q(fϕk (zk)). The feature encoding transformation fϕk is a DNN-based function parameterized by ϕk. The parameters {ϕk}Kk=1 are defined as ϕ for simplicity. The edge server has T independent predictors, where the t-th predictor outputs the inference result based on the received features after the t-th transmission. Once the received features are sufficient to output a confident result, the remaining retransmission attempts can be saved, which is favorable for latency reduction. To unleash the full benefits of the selective retransmission mechanism, it is critical to design a stopping policy to decide when to terminate the retransmission process. Besides, as the encoded features of multiple edge devices may be redundant for edge inference, it leaves room for further communication overhead reduction by scheduling the edge devices with the most informative features for retransmissions. Thus, an attention module is proposed that activates the edge devices with the most informative features to retransmit. The received features from the activated devices in the t-th transmission are defined as u˜1:K,t, where u˜k,t = uk,t for the activated devices, and u˜k,t = 0 otherwise.
1) Stopping Policy: Inspired by cascade inference [38], [39], we develop a confidence-based stopping policy at the edge server that outputs the confidence scores δτ , τ ∈ {1, . . . , T −1}, of the inference results after each transmission attempt. Its structure is depicted in Fig. 3a, which consists of T − 1 internal predictors, and the confidence scores determine when to terminate the retransmission process. Specifically, the τ -th internal predictor outputs the inference result based on the received features {u˜1:K,t}τt=1 defined as follows:
Internal Predictors: yˆ {u˜1:K,t}τt=1; ψτ , τ ∈ {1, . . . , T }, (20)

where the notation ψτ represents the DNN parameters that form the τ -th internal predictor, and we define ψ := {ψt}Tt=1 for notational simplicity. The corresponding variational distributions are as follows:
pψτ (y|{u˜1:K,t}τt=1) ∝ exp −ℓ(y, yˆ {u˜1:K,t}τt=1; ψτ ,
with τ ∈ {1, . . . , T }. (21)
A threshold value δ0 is selected to decide when to terminate the retransmission process: If a confidence score δτ is above δ0, the intermediate inference result is likely to be correct, and the inference process is terminated with that result; Otherwise, more features from the edge devices are preferred by the edge server to improve the inference performance. In other words, the threshold value δ0 allows us to flexibly control the tradeoff between the inference performance and the communication overhead. Particularly, we use the maximum predicted probability as the confidence score for the classification tasks expressed as follows:

Confidence Scores: δτ = max yˆ {u˜1:K,t}τt=1; ψτ ,

with τ ∈ {1, . . . , T − 1}. (22)

2) Attention Module: The attention module is developed to

determine which devices should be activated for retransmis-

sions. Each attention score ak,τ ∈ {0, 1} for the edge device
k in the τ -th transmission is based on the received features {u˜1:K,t}τt=−11 determined as follows:

Attention Scores: ak,τ = gk,τ−1({u˜1:K,t}τt=−11),

(23)

with k ∈ {1, . . . , K} and τ ∈ {2, . . . , T },

where functions {gk,1, . . . , gk,T −1}Kk=1 are constructed by fully-connected layers. Note that we use a hard binariza-
tion function to discretize the attention scores and select
the straight-through estimator to approximate the gradients. Particularly, we set ak,1 = 1, k ∈ {1, . . . , K}, which means that all the edge devices need to transmit their features in
the first transmission. For notational simplicity, we define the attention vectors as aτ := {ak,τ }Kk=1, τ ∈ {1, . . . , T }. As illustrated in Fig. 3b, if the attention score ak,τ is 1, the feature uk,τ would be transmitted from the edge device k to the edge server; Otherwise, the edge device k would be deactivated in the τ -th transmission. In other words, we have u˜k,τ = ak,τ uk,τ .

B. VDDIB-SR Objective
By adopting the stopping policy and introducing the attention module, we extend the VDDIB objective in (15) to the VDDIB-SR objective as follows:

LVDDIB−SR β, T ; ϕ, ψ, {ψk}Kk=1 ,

:= Epθ(z1:K ,y)

1 T

T

− log pψτ (y|{u˜1:K,t}τt=1)

τ =1

K

KT

+β

− log pψk (y|uk) +

Rbit(u˜k,t)

k=1

k=1 t=1

. (24)

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
8

(a) The stopping policy that determines when to terminate the retransmission process.

(b) The attention module that determines which devices to retransmit. Fig. 3: The selective retransmission mechanism with (a) a stopping policy and (b) an attention module.

In (24), the total loss of the internal predictors is shown in the first term that takes into account all possible stopping points. The second term in the VDDIB-SR objective acts as an auxiliary loss, which maintains the informativeness of each encoded feature and makes the training process robust against dynamic activation caused by the attention module. Besides, each Rbit(u˜k,t) = ak,tnkdk in the third term of the VDDIBSR objective measures the communication overhead at the t-th transmission of the edge device k.
By applying the Monte Carlo sampling method, we obtain an unbiased estimate of the gradient and hence minimize the objective in (24) using backpropagation. Given a minibatch of data {xm 1:K , ym}M m=1, we can extract the task-relevant features {z1m:K }M m=1 by the learned parameters {θk}Kk=1 in the VIB framework. The Monte Carlo estimate of LVDDIB−SR is given as follows:

LVDDIB−SR β, T ; ϕ, ψ, {ψk}Kk=1 ,

1 ≃
M

1 T

T

− log pψτ (ym|{u˜m 1:K,t}τt=1)

τ =1

K

T

+β

− log pψk (ym|um k ) + Rbit(u˜m k,t)

k=1

t=1

(25) .

The training procedures for the VDDIB-SR method are summarized in Algorithm 3.

Remark 4. Denote the parameter space of the VDDIB-SR objective as PT . It is straightforward that P1 ⊆ · · · ⊆ PT , where P1 represents the parameters of the VDDIB objective. Thus, it implies that the minimum of the VDDIB-SR objective is a lower bound of the VDDIB objective in (15), and increasing the maximum transmission attempts T could

Algorithm 3 Training Procedures for the VDDIB-SR Method

Input: Training dataset D, batch size M , optimized parameters

{θk}K k=1, and initialized parameters ϕ, ψ, and {ψk}K k=1. Output: The optimized parameters ϕ, ψ, and {ψk}K k=1.
1: repeat 2: Randomly select a minibatch {(xm 1:K , ym)}M m=1 from D. 3: Extract the task-relevant features {z1m:K }M m=1 based on the
learned distributions {pθk (zk|xk)}K k=1. 4: while τ = 2 to T do

5:

Compute the attention scores ak,τ for k ∈ 1 : K.

6:

Determine the features u˜k,τ := ak,τ uk,τ for k ∈ 1 : K.

7:

Obtain the intermediate inference result based on (20).

8: end while 9: Compute the loss LVDDIB−SR β, T ; ϕ, ψ, {ψk}K k=1 based

on (25).

10: Update parameters ϕ, ψ, and {ψk}K k=1 through backpropagation.

11: until Convergence of parameters ϕ, ψ, and {ψk}K k=1.

achieve a better rate-relevance tradeoff. Particularly, let P = {p(u1:K |z1:K )} denote the distributions of the joint feature encoding scheme. The minimum of LDDIB(β) over P lower bounds LVDDIB−SR β, T ; ϕ, ψ, {ψk}Kk=1 for any integer T . This is consistent with the classic source coding theory [40] that the performance of the joint coding schemes is better than that of the distributed coding methods. The detailed derivation is deferred to Appendix C.
VI. PERFORMANCE EVALUATION
In this section, we evaluate the performance of the proposed task-oriented communication methods for cooperative edge inference on two kinds of cooperative inference tasks, in-

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
9

cluding multi-view image classification and multi-view object recognition.
A. Experimental Setup

TABLE I: The structures of NN1 and NN2 for different tasks.

MNIST

CIFAR-10

Tiny-ImageNet

WRGBD

ModelNet40

NN1

3 FC layers

VGG-19 [47]

PreActResNet18 [48]

2 Conv layers 1 FC layer

VGG-11 [47]

NN2 3 FC layers 2 FC layers

2 Conv layers 1 FC layer

1 FC layer 3 FC layers

1) Datasets: In the multi-view image classification task, we select MNIST [41], CIFAR-10 [42], and Tiny ImageNet [43] for experiments. The MNIST dataset contains handwritten digit images in 10 classes, which has a training set of 60,000 examples and a test set of 10,000 examples. The CIFAR-10 dataset consists of 60,000 color images in 10 classes (6,000 images per class). Particularly, 5,000 images of each class are used for training, and the remaining 1,000 images are used for testing. Tiny ImageNet contains 110,000 images of 200 classes (550 images per class). Each class has 500 training images and 50 validation images. We follow the method in [44] to generate two views for each image by splitting it vertically. Each view has the same size, and we assume that there are two devices with distinct views in the edge inference system.
In the multi-view object recognition task, we use Washington RGB-D (WRGBD) [45] and ModelNet40 [46] to evaluate the proposed methods. The WRGBD dataset includes household objects organized into 51 classes. Each object is captured by three cameras mounted at different heights, at 30◦, 45◦, and 60◦ above the horizon. We select 80% of objects from each class for training and use the remaining objects for testing. Thus, there are 19,390 objects in the training set and 2,729 objects in the test set. ModelNet40 contains 12,311 computeraided design (CAD) objects of 40 categories. Similar to a prior study [27], each object is augmented by rotating 30 degrees along the gravity direction, resulting in twelve views. We choose around 80 objects per class for training and 20 objects per class for testing.
2) Proposed methods: We investigate the performance of the proposed VDDIB and VDDIB-SR methods. The VDDIBSR method with t transmissions is denoted as VDDIB-SR (T=t). Particularly, the VDDIB method can be regarded as a special case of the VDDIB-SR method with T = 1. In the training process, we set γ = 10−4 when minimizing the VIB objective LVIB,k(γ) for feature extraction. To achieve different rate-relevance tuples, the hyperparameter β varies in the range of [10−3, 10−1] when optimizing the VDDIB and VDDIB-SR objectives2.
3) Baselines: There are only a few existing studies that are relevant to feature encoding for cooperative edge inference. We compare the proposed methods against the following five baseline methods:
• Server-based inference: This baseline deploys a powerful DNN on the edge server for inference and adopts the data-oriented communication scheme to transmit the multiple views. The view size is reduced by using PNG compression.
• NN-REG and NN-GBI [23]: Both of them adopt learningbased methods to quantize the input data to bitstreams at the edge device. The DNN-based quantizers are either
2The code is available at https://github.com/shaojiawei07/VDDIB-SR.

trained using the quantization loss regularization (NNREG) or with the greedy boundary insertion (GBI) algorithm (NN-GBI)3. • eSAFS [24]: In this scheme, each edge device extracts a feature vector as well as a small-scale importance vector using DNNs. These importance vectors are transmitted to an edge server for preprocessing, which identifies the important feature vectors that need to be transmitted by the edge devices. • CAFS [25]: This method integrates a context-aware feature selection (CAFS) scheme into the multi-device cooperative edge inference system. Each edge device extracts a task-relevant feature and determines its importance via entropy-based likelihood estimation. The features from different devices are aggregated using a weighted mean of their likelihoods. By regulating the likelihood cutoff level, edge devices with less important features are deactivated.
4) Neural Network Architecture: In the edge inference system, each raw input sample is passed through the first part of the network (denoted as NN1) to extract the viewspecific features, and the multiple features are collected for the processing of the remaining part of the network (denoted as NN2) to output the inference result. Generally, we select a convolutional (Conv) neural network as the backbone of NN1, and NN2 is constructed by several fully-connected (FC) layers. More details about the structures of NN1 and NN2 are summarized in Table I.
The architectures of different methods are presented in Table II, where most of the methods adopt a uniform quantizer to discretize the output of NN1 for efficient transmission. In particular, the NN-GBI method adopts the GBI algorithm [23] to design a non-uniform quantizer, and the server-based inference scheme transmits the raw input views to the edge server via data-oriented communication. After receiving the extracted features, the NN-REG, NN-GBI, and VDDIB methods use vector concatenation for feature aggregation. The CAFS method leverages a weighted pooling layer to fuse the received features, where the weights are determined by the likelihood estimation module [25]. Besides, the eSAFS method performs signification-aware feature selection [6] to identify the most informative feature maps for transmission. Our proposed VDDIB-SR leverages a selective retransmission mechanism to collect the features, where the attention module adopts two fully-connected layers to output the attention scores.
5) Metrics: We investigate the rate-relevance tradeoff of different multi-device edge inference schemes, where the rate is measured by the total number of bits (Rsum) to be transmitted from the edge devices to the edge server, and the relevance
3To reduce the search space and improve the training efficiency, we consider only one possible boundary for each dimension.

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
10

TABLE II: The neural network architectures of different methods.

On-device Network
Server-based Network

Server-based Inference
—
NN1 NN2

NN-REG
NN1
Uniform Quantizer Feature Vector Concatenation
NN2

NN-GBI
NN1
GBI-based Quantizer [23] Feature Vector Concatenation
NN2

eSAFS
NN1
Uniform Quantizer Significant-Aware Feature Selection [6]
NN2

CAFS
NN1 Likelihood Estimation [25] Uniform Quantizer Weighted Pooling Layer
NN2

VDDIB
NN1
Uniform Quantizer Feature Vector Concatenation
NN2

VDDIB-SR
NN1
Uniform Quantizer Selective Retransmission
NN2

TABLE III: The accuracy of the multi-view image classification task under different bit constraints.

MNIST

CIFAR

6 bits

10 bits 14 bits

8 bits

16 bits

NN-REG NN-GBI∗

95.93% 96.62%

97.49% 97.79%

97.78% 98.02%

85.39% 86.37%

89.74% 89.69%

eSAFS

96.97% 97.87% 98.05% 84.50% 88.22%

CAFS

94.14% 97.43% 97.42% 82.83% 89.75%

VDDIB (ours) VDDIB-SR (T=2) (ours)

97.08% 97.13%

97.82% 98.13%

98.06% 98.22%

85.95% 87.61%

90.17% 90.53%

∗ The GBI quantization algorithm is computationally prohibitive when the number of bits is too large.

24 bits 90.14% 90.30% 90.07% 90.34% 90.78% 90.94%

Tiny ImageNet (Top-1/Top-5)

256 bits

512 bits

49.17%/74.65% 50.29%/76.03%

—

—

47.60%/73.25% 49.63%/74.88%

46.24%/71.69% 47.83%/73.12%

48.89%/74.59% 50.37%/75.64%

49.96%/74.88% 51.03%/76.06%

TABLE IV: The accuracy of the multi-view object recognition task under different bit constraints.

NN-REG NN-GBI eSAFS CAFS VDDIB (ours) VDDIB-SR (T=2)

12 bits 93.97% 95.92% 90.45% 86.65% 94.93% 96.78%

WRGBD 15 bits 96.74% 98.62% 95.57% 94.51% 97.47% 99.30%

18 bits 98.50% 98.82% 99.12% 97.33% 98.63% 99.45%

ModelNet40

120 bits 240 bits 360 bits

87.50% 88.25% 89.03%

88.82%

—

—

85.88% 87.87% 89.50%

86.75% 89.56% 90.67%

89.25% 90.03% 90.75%

90.25% 91.31% 91.62%

TABLE V: The accuracy and cost of the server-based inference method.

Accuracy Comm. cost

MNIST 98.6% 0.34 KB

CIFAR-10 91.5% 2.31 KB

Tiny-ImageNet 51.5% (Top-1) 76.9% (Top-5)
9.14 KB

WRGBD 99.8% 4.05 KB

ModelNet40 92.0% 133 KB

is measured by the classification accuracy. In particular, we neglect the feedback signaling cost in the VDDIB-SR and eSAFS [24] methods due to the much lower communication overhead compared with that of the forward transmission, as well as the abundant radio resources for the downlink transmission. In the training process, we use the cross-entropy loss to measure the difference between the predicted results and the ground truths. Besides, the proposed VDDIB-SR method selects the maximum predicted probability in (22) as the confidence score.
B. Multi-View Image Classification
This task aims to classify multiple views of an image to its target label. We compare the inference performance given different constraints on the average received bits Rsum over the test set. Table III shows the classification accuracy under different bit constraints, where the proposed VDDIB method achieves a comparable or better accuracy compared with the baselines. This implies the effectiveness of the IB and DIB frameworks for task-oriented communication in multi-device edge inference systems. Besides, the accuracy achieved by the

TABLE VI: The average accuracy of the single-view inference on different tasks.

Accuracy

MNIST 92.69%

CIFAR-10 83.16%

Tiny-ImageNet 40.84% (Top-1) 65.89% (Top-5)

WRGBD 99.16%

ModelNet40 88.33%

VDDIB-SR method is higher than that of the VDDIB method. This is attributed to the selective retransmission mechanism that eliminates the redundancy among the encoded features. For better comparison, the average accuracy of single-view inference is reported in Table VI. It illustrates that every single view is not sufficient to carry out the inference task with a good accuracy, which demonstrates the advantages of cooperative inference. Finally, compared with the server-based inference scheme shown in Table V, our proposed methods can greatly reduce the communication overhead with little drop in performance.
C. Multi-View Object Recognition
This task aims to recognize the 3D object based on a collection of perceived 2D views. Consider that there are twelve devices with distinct views in the multi-device cooperative edge inference system. We compare the inference performance given different rate constraints. Table IV summarizes the inference accuracy for the multi-view object recognition task. We see that our methods consistently outperform the baselines under all rate constraints. This is because the proposed communication schemes are capable of discarding the taskirrelevant information. Besides, with the selective retransmission mechanism, the VDDIB-SR method secures further performance enhancement than the VDDIB method. Besides, compared with the single-view inference results shown in Table VI, the proposed VDDIB-SR method can effectively leverage the correlation among multiple views for better accuracy. The performance and communication cost of the serverbased inference method is reported in Table V. Although

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
11

Accuracy %) (

98.6

100

98.4

98

98.2

96

Accuracy (%)

98.0

94

97.8

92

97.6 97.4 97.2 97.0
6

Joint Coding VDDIB-SR (T=4) VDDIB-SR (T=3) VDDIB-SR (T=2) VDDIB
8 10 12 14 16 18 20 22 Bits

90

88

Joint Coding VDDIB-SR (T=4)

VDDIB-SR (T=3)

86

VDDIB-SR (T=2)

VDDIB

84

10

12

14

16

18

20

Bits

(a) MNIST dataset.

(b) WRGBD dataset.

Fig. 4: Impact of the maximum transmission attempts T on the rate-relevance tradeoff.

92.0

VDDIB-SR (T=2)

VDDIB-Cascade (T=2)

91.5

Accuracy (%)

91.0

90.5

90.0

89.5

150

200

250

300

350

Bits

Fig. 5: Accuracy as a function of communication cost on the ModelNet40 classification task.

20.0

VDDIB-SR (T=2)

17.5

VDDIB-Cascade (T=2)

15.0

Activation Percentage (%)

12.5

10.0

7.5

5.0

2.5

0.0

2

4

6

8

10

12

Device Index

Fig. 6: The activation percentages of the edge devices in the retransmission.

this method achieves a slight performance gain, it induces overwhelming communication overhead, leading to intolerable latency. Specifically, if the multi-device cooperative inference system is supported by the 3GPP narrowband Internet of Things (NB-IoT) standard with an 18 KB/s uplink rate [49], the data-oriented communication scheme has a communication latency of 560 ms on the ModelNet40 dataset, while the proposed VDDIB-SR method remarkably reduces the latency to 2.5 ms and maintains comparable classification accuracy.
D. Ablation Study
In this subsection, we evaluate the performance of (1) the maximum transmission attempts T and (2) the attention module on the multi-device edge inference systems. An ablation study that validates the effectiveness of the feature extraction step is deferred to Appendix D.
1) Impact of the maximum transmission attempts T : We investigate the performance of the VDDIB-SR method with different values of T on the multi-view image classification task. In particular, we adopt a joint coding scheme as a baseline method for comparison, which assumes that one edge device can access all the extracted view-specific features and perform joint feature encoding before transmission. The relationship between the achieved classification accuracy and the number of transmitted bits is shown in Fig. 4. It can be observed that the performance of the VDDIB-SR method

improves with T , while the joint source coding scheme achieves the best performance and upper bounds the VDDIBSR method. These results are consistent with the analysis in Remark 4 that increasing the maximum transmission attempts T could achieve a better rate-relevance tradeoff4, and the joint coding scheme outperforms the distributed coding methods.
2) Impact of the attention module: To evaluate the performance of the attention module on communication overhead reduction, we select a baseline method denoted as VDDIBCascade for comparison, which activates all the devices to transmit their features in each retransmission round before terminating the retransmission process. We investigate the performance of these two methods on the ModelNet40 dataset. Specifically, the transmission attempt T is set to 2. As shown in Fig. 5, the VDDIB-SR method achieves a better rate-relevance tradeoff compared with the VDDIB-Cascade method, which demonstrates the effectiveness of the attention module. In particular, Fig. 6 depicts the activation percentages of the edge devices in the retransmission, where the VDDIBSR and VDDIB-Cascade methods induce 216 bits and 240 bits overhead, respectively. Both methods achieve an accuracy of around 91.1%, and the communication cost reduction is
4Note that when T is very large, the feedback signaling cost is nonnegligible. Selecting an optimal T that reduces the overall communication costs while maintaining the inference performance requires a dedicated communication protocol to manage the data flow, which is an interesting future research but beyond the scope of this paper.

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
12

attributed to the attention module that can schedule the edge devices with the most informative features to retransmit.

VII. CONCLUSIONS
This work investigated task-oriented communication for multi-device cooperative edge inference, where a group of low-end edge devices transmit the task-relevant features to an edge server for aggregation and processing. Our proposed methods are built upon the IB principle and the DIB framework for feature extraction and distributed feature encoding, respectively. Compared with traditional data-oriented communication, our methods substantially reduce the communication overhead, thus enabling low-latency cooperative inference. Compared with existing methods for feature encoding in cooperative inference, we provide a theoretical framework to characterize the rate-relevance tradeoff, supported by pragmatic approaches for local feature extraction and distributed feature encoding, which consistently outperform baseline methods.
This study demonstrated the effectiveness of task-oriented communication for multi-device cooperative edge inference, which serves for the downstream task rather than for data reconstruction. The shifted objective of the communication system calls for new design tools. The IB-based frameworks, variational approximations, and retransmission mechanisms are promising candidates. Given the above, it is interesting to further investigate task-oriented communication strategies for other edge AI systems and applications, e.g., to design tighter bounds in variational approximations, to develop effective coding schemes for dynamic conditions, and to apply the IBbased methods to decentralized networks [50], etc.

APPENDIX A PROOF OF REMARK 1
We prove that the extracted features (Z1, . . . , ZK ) that satisfy the minimality and sufficiency conditions in (3) are conditionally independent given Y . Define a random variable Q with distribution p(q) that satisfies the following equality5:

K
p(x1:K , y, q) = p(y|q)p(q) p(xk|y, q). (26)
k=1
The random variables Y , Q, Xk, Zk constitute the following Markov chain since the extracted feature Zk is independent of (Y, Q) given the local data Xk:

(Y, Q) ↔ Xk ↔ Zk. k ∈ {1, . . . , K}.

(27)

With the data processing inequality, we have I(Y, Q; Zk) ≤ I(Xk, Zk). Suppose the extracted features satisfy the minimality and sufficiency in (3), which implies I(Xk; Zk) = I(Y ; Zk), k ∈ {1, . . . , K}. By incorporating the chain rule of mutual information, i.e., I(Y ; Zk) ≤ I(Y, Q; Zk), we get I(Y, Q; Zk) = I(Y ; Zk), which shows that the random variables Q, Y, Zk constitute the Markov chain as follows:

Q ↔ Y ↔ Zk, k ∈ {1, . . . , K}.

(28)

5The random variable Q exists for any task by trivially defining Q := X1:K .

As each extracted feature zk only depends on the corresponding observation xk, the conditional distribution of (X1:K , Z1:K ) given (Y, Q) is as follows:

K

p(x1:K , z1:K |y, q) = p(xk|y, q)p(zk|xk),

(29)

k=1

K

= p(xk, zk|y, q).

(30)

k=1

where (29) is due to (26), and (30) is derived from the Markov chains in (27). By integrating out x1:K in (30), the conditional distribution of Z1:K given (Y, Q) is as follows:

K

p(z1:K |y, q) = p(zk|y, q),

(31)

k=1

K

= p(zk|y),

(32)

k=1

where (32) is due to the Markov chains in (28). By integrating

out q in the left-hand side of (32), we have p(z1:K |y) =

K k=1

p(zk

|y),

which

means

that

the

features

Z1:K

are

con-

ditionally independent given the target variable Y .

APPENDIX B DERIVATION OF THE VARIATIONAL UPPER BOUND
A. Variational Information Bottleneck
Recall that the IB objective in (11) has the form LIB,k(γ) = H(Y |Zk) + γI(Xk; Zk) at device k ∈ {1, . . . , K}. Writing it out in full with the conditional distribution pθk (zk|x) and the variational distributions pφk (y|zk), rk(zk), the derivation is shown in (33). LVIB,k(γ; θk, φk) in the formulation is the VIB objective function in (11). As the KL-divergence is nonnegative, the VIB objective function is an upper bound of the IB objective function.

B. Variational Distributed Deterministic Information Bottleneck

Revisit the DDIB objective function LDDIB(β) =

H (Y |U1:K ) + β

K k=1

[H

(Y

|Uk )

+

Rbit

(Uk )]

in

(14).

Writ-

ing it out with the learned distribution pθ(z1:K , y) and the

encoding functions uk = Q (fϕk (zk)), k ∈ {1, . . . , K}, the

derivation is shown in (34). As the KL-divergence is non-

negative, the VDDIB objective function is a variational upper

bound of the DDIB objective function.

APPENDIX C DERIVATION OF REMARK 4

In this part, we detail the derivation that the mini-

mum of LDDIB(β) over P = {p(u1:K |z1:K )} lower

bounds the VDDIB-SR objective. Revisit the DDIB objective

LDDIB(β) = H (Y |U1:K ) + β

K k=1

[H

(Y

|Uk )

+

Rbit (Uk ))]

in (8). It satisfies the inequalities (35) and (36), where (35) is

due to the variational approximations, and (36) holds since the

family of encoding functions parameterized by ϕ is a subset

of P .

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
13

LIB,k(γ) =Ep(xk,y) Epθk (zk|xk)[− log p(y|zk)] + γDKL(p(zk|xk)∥p(zk)) , = Ep(xk,y) Epθk (zk|xk)[− log pφk (y|zk)] + γDKL(p(zk|xk)∥p(zk))
LVIB,k (γ ;θk ,φk )
− Ep(xk,y) {DKL(p(y|zk)∥pφk (y|zk)) + γDKL(p(zk)∥rk(zk))} .

LDDIB(β) =Epθ(z1:K ,y) = Epθ (z1:K ,y)

K
− log p(y|u1:K ) + β [− log p(y|uk) + Rbit(uk)] ,
k=1 K
− log pψ0 (y|u1:K ) + β [− log pψk (y|uk) + Rbit(uk)]
k=1

LVDDIB (β ;ϕ,ψ ) K
− Epθ(z1:K,y) DKL(p(y|u1:K )∥pψ0 (y|u1:K )) + β DKL(p(y∥uk)|pψk (y|uk)) .
k=1

(33) (34)

min LDDIB(β) ≤ min

P

P

Epθ (z1:K ,y)

Ep(u1:K |z1:K )

T
− log p(y|{u1:K,t}τt=1)
τ =1

K

T

+β

− log p(y|uk) + Rbit(uk,t)

,

k=1

t=1

≤ min LVDDIB−SR
PT

β, T ; ϕ, ψ, {ψk}Kk=1

,

(35) (36)

Fig. 7: The corrupted two-view MNIST dataset

TABLE VII: The neural network structure for corrupted two-view MNIST classification.

Feature Extraction
Encoder Decoder

Our method

D-VIB [2]

[Conv+ReLU+MaxPooling] × 2

FC + ReLU + Dropout

—

FC + ReLU

[Conv+ReLU+MaxPooling] × 2

FC + ReLU

FC + ReLU + Dropout

[FC + ReLU] × 2

FC + ReLU

APPENDIX D ABLATION STUDY ON FEATURE EXTRACTION
In this appendix, we evaluate the advantages of the added task-relevant feature extraction step on distributed coding. The D-VIB method in [2] is selected as a baseline, which performs distributed coding on the raw input views without the feature extraction step. For a fair comparison, we reproduce their results following the same experimental setup by Pytorch. A

corrupted two-view MNIST dataset for classification is generated for the two-view classification task, which is visualized in Fig. 7. View 1 is generated by using a 15×15 mask to occlude part of the image. View 2 is generated by adding independent random noise uniformly sampled from [0, 3] to each pixel, and then the pixel values are truncated to [0, 1]. We randomly select 50,000 two-view samples for model training and 20,000 samples for testing. The neural network structures are presented in Table VII, which consists of convolutional (Conv) layers, maxpooling layers, fully-connected (FC) layers, a dropout layer, and the rectified linear unit (ReLU) activation function. Our proposed method utilizes the same neural network structure as D-VIB for a fair comparison. Different from the baseline, we first optimize the layers for feature extraction based on Algorithm 1 and then train the encoder and decoder based on the variational approximation of the DIB loss function [2].
Fig. 8 shows the classification accuracy given different values of β in the DIB loss (8). Besides, we plot the achievable rate-relevance pairs in Fig. 9 according to (7) based on the neural network parameters. Particularly, the conditional entropy of the target variable given the encoded features is estimated by the cross-entropy, and the marginal distributions of the encoded features are approximated by the variational distributions. We observe that our method achieves higher accuracy and a better rate-relevance tradeoff. These can be explained by the advantage of the added feature extraction step, which discards the task-irrelevant information and allows the extracted features to be less correlated compared with the

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
14

Accuracy (%) Rele ance Δ

98

97

96

95

94

93

92

91

Our meth d (w/ feature e%tracti n)

D-VIB (w/o feature extraction)

90

10−5

10−4

10−3

β

Fig. 8: Accuracy as a function of β in DIB for the corrupted two-view MNIST classification task.

2.0

1.5

1.0

0.5

Our method (w/ feature extraction)

D-VIB (w/o feature extraction)

0.0

101

102

Sum rate Rsum

Fig. 9: Rate-relevance tradeoff for the corrupted two-view MNIST classification task.

raw input views.
REFERENCES
[1] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck method,” in Proc. Annu. Allerton Conf. Commun. Control Comput., Monticello, IL, USA, Oct. 2000.
[2] I. E. Aguerri and A. Zaidi, “Distributed variational representation learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 1, pp. 120–138, Jan. 2021.
[3] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proc. IEEE Conf. Comput Vision Pattern Recognit., Las Vegas, NV, USA, Jun. 2016.
[4] X. Hou, S. Dey, J. Zhang, and M. Budagavi, “Predictive view generation to enable mobile 360-degree and VR experiences,” in Proc. Morning Workshop VR AR Netw., Budapest, Hungary, Aug. 2018.
[5] R. Collobert and J. Weston, “A unified architecture for natural language processing: Deep neural networks with multitask learning,” in Proc. Int. Conf. Mach. Learn., Helsinki, Finland, Jul. 2008.
[6] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communicationefficient edge AI: Algorithms and systems,” IEEE Commun. Surv. Tut., vol. 22, no. 4, pp. 2167–2191, Jul. 2020.
[7] J. Shao and J. Zhang, “Communication-computation trade-off in resource-constrained edge inference,” IEEE Commun. Mag., vol. 58, no. 12, pp. 20–26, Dec. 2020.
[8] M. Moldoveanu and A. Zaidi, “On in-network learning: a comparative study with federated and split learning,” in Proc. Int. Workshop Signal Process. Adv. Wireless Commun. (SPAWC), Lucca, Italy, Sep. 2021.
[9] C. Badue, R. Guidolini, R. V. Carneiro, P. Azevedo, V. B. Cardoso, A. Forechi, L. Jesus, R. Berriel, T. M. Paixao, F. Mutz et al., “Selfdriving cars: A survey,” Expert Syst. Appl., p. 113816, Mar. 2020.
[10] E. Unlu, E. Zenou, N. Riviere, and P.-E. Dupouy, “Deep learning-based strategies for the detection and tracking of drones using several cameras,” IPSJ Trans. Comput. Vision Appl., vol. 11, no. 1, pp. 1–13, Jul. 2019.
[11] D. Zou, P. Tan, and W. Yu, “Collaborative visual slam for multiple agents: A brief survey,” Virtual Reality Intell. Hardware, vol. 1, no. 5, pp. 461–482, Oct. 2019.
[12] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence: Paving the last mile of artificial intelligence with edge computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738–1762, Aug. 2019.
[13] J. Shao and J. Zhang, “Bottlenet++: An end-to-end approach for feature compression in device-edge co-inference systems,” in Proc. IEEE Int. Conf. Commun. Workshop, Dublin, Ireland, Jun. 2020.
[14] J. Shao, H. Zhang, Y. Mao, and J. Zhang, “Branchy-gnn: A deviceedge co-inference framework for efficient point cloud processing,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), Toronto, Canada, Jun. 2021.
[15] J. Shao, Y. Mao, and J. Zhang, “Learning task-oriented communication for edge inference: An information bottleneck approach,” IEEE J. Sel. Area Commun., vol. 40, no. 1, pp. 197–211, Nov. 2021.
[16] X. Liu, W. Liu, T. Mei, and H. Ma, “A deep learning-based approach to progressive vehicle re-identification for urban surveillance,” in Proc. Eur. Conf. Comput. Vision (ECCV), Amsterdam, Netherlands, Oct. 2016.
[17] E. C. Strinati and S. Barbarossa, “6G networks: Beyond shannon towards semantic and goal-oriented communications,” Comput. Netw., vol. 190, p. 107930, May 2021.

[18] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, “Toward an intelligent edge: wireless communication meets machine learning,” IEEE Commun. Mag., vol. 58, no. 1, pp. 19–25, Jan. 2020.
[19] A. E. Eshratifar, A. Esmaili, and M. Pedram, “Bottlenet: A deep learning architecture for intelligent mobile cloud computing services,” in Proc. Int. Symp. Low Power Electron. Design (ISLPED), 2019, pp. 1–6.
[20] Y. Dubois, B. Bloem-Reddy, K. Ullrich, and C. J. Maddison, “Lossy compression for lossless prediction,” in Proc. Int. Conf. Learn. Repr. Workshop on Neural Compression, Vienna, Austria, May 2021.
[21] M. Jankowski, D. Gündüz, and K. Mikolajczyk, “Wireless image retrieval at the edge,” IEEE J. Sel. Areas Commun., vol. 39, no. 1, pp. 89–100, May 2021.
[22] D. Slepian and J. Wolf, “Noiseless coding of correlated information sources,” IEEE Trans. Inf. Theory, vol. 19, no. 4, pp. 471–480, Jul 1973.
[23] O. A. Hanna, Y. H. Ezzeldin, T. Sadjadpour, C. Fragouli, and S. Diggavi, “On distributed quantization for classification,” IEEE J. Sel. Areas Inf. Theory, vol. 1, no. 1, pp. 237–249, Apr. 2020.
[24] M. Singhal, V. Raghunathan, and A. Raghunathan, “Communicationefficient view-pooling for distributed multi-view neural networks,” in Proc. Design Automat. Test Eur. Conf. Exhib. (DATE), Grenoble, France, Sep. 2020.
[25] J. Choi, Z. Hakimi, P. W. Shin, J. Sampson, and V. Narayanan, “Context-aware convolutional neural network over distributed system in collaborative computing,” in Proc. ACM/IEEE Design Automat. Conf. (DAC), Las Vegas, NV, USA, Jun. 2019.
[26] Y. Zhou, J. Xiao, Y. Zhou, and G. Loianno, “Multi-robot collaborative perception with graph neural networks,” IEEE Robot. Automat. Lett., vol. 7, no. 2, pp. 2289–2296, Jan. 2022.
[27] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional neural networks for 3d shape recognition,” in Proc. Int. Conf. Comput. Vision, Santiago, Chile, Dec. 2015.
[28] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep variational information bottleneck,” in Proc. Int. Conf. Learn. Repr. (ICLR), Toulon, France, Apr. 2017.
[29] I. Fischer, “The conditional entropy bottleneck,” Entropy, vol. 22, no. 9, Sep. 2020.
[30] T. A. Courtade and T. Weissman, “Multiterminal source coding under logarithmic loss,” IEEE Trans. Inf. Theory, vol. 60, no. 1, pp. 740–761, Nov. 2013.
[31] Y. Ug˘ur, I. E. Aguerri, and A. Zaidi, “Vector gaussian ceo problem under logarithmic loss and applications,” IEEE Trans, Inf. Theory, vol. 66, no. 7, pp. 4183–4202, Feb. 2020.
[32] V. Anantharam, A. Gohari, S. Kamath, and C. Nair, “On hypercontractivity and a data processing inequality,” in IEEE Int. Symp. Inf. Theory, Jun. 2014, pp. 3022–3026.
[33] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference: A review for statisticians,” J. Amer. Statist. Assoc., vol. 112, no. 518, pp. 859–877, Jul. 2017.
[34] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in Proc. Int. Conf. Learn. Repr. (ICLR), Banff, Canada, Apr. 2014.
[35] Z. Goldfeld and Y. Polyanskiy, “The information bottleneck problem and its applications in machine learning,” IEEE J. Sel. Areas Inf. Theory, vol. 1, no. 1, pp. 19–38, Apr. 2020.
[36] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients through stochastic neurons for conditional compu-

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in IEEE Transactions on Wireless Communications. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TWC.2022.3191118
15

tation,” arXiv preprint arXiv:1308.3432, 2013. [Online]. Available: https://arxiv.org/abs/1308.3432. [37] D. B. Kurka and D. Gündüz, “Successive refinement of images with deep joint source-channel coding,” in Proc. IEEE Int. Wkshop Signal Process. Adv. Wireless Commun. (SPAWC), Cannes, France, Jul. 2019. [38] X. Chen, H. Dai, Y. Li, X. Gao, and L. Song, “Learning to stop while learning to predict,” in Int. Conf. Mach. Learn., Vienna, Australia, Apr. 2020. [39] S. Enomoto and T. Eda, “Learning to cascade: Confidence calibration for improving the accuracy and computational cost of cascade inference systems,” in Proc. AAAI Conf. Artif. Intell., Feb. 2021. [Online]. Available: https://www.aaai.org/AAAI21Papers/AAAI-2189.EnomotoS.pdf. [40] A. El Gamal and Y.-H. Kim, Network information theory. Cambridge University Press, 2011. [41] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proc. IEEE, vol. 86, no. 11, pp. 2278– 2324, May 1998. [42] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features from tiny images,” 2009. [Online]. Available: https://www.cs.toronto.edu/∼kriz/learning-features-2009-TR.pdf. [43] P. Chrabaszcz, I. Loshchilov, and F. Hutter, “A downsampled variant of imagenet as an alternative to the cifar datasets,” arXiv preprint arXiv:1707.08819, 2017. [Online]. Available: https://arxiv.org/abs/1707.08819. [44] J. Whang, A. Acharya, H. Kim, and A. G. Dimakis, “Neural distributed source coding,” arXiv preprint arXiv:2106.02797, 2021. [Online]. Available: https://arxiv.org/abs/2106.02797. [45] K. Lai, L. Bo, X. Ren, and D. Fox, “A large-scale hierarchical multiview rgb-d object dataset,” in Proc. IEEE Int. Conf. Robot. Automat., Shanghai, China, May 2011, pp. 1817–1824. [46] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D Shapenets: A deep representation for volumetric shapes,” in Proc. IEEE/CVF Conf Comput. Vision Pattern Recognit. (CVPR), Boston, MA, USA, Jun. 2015. [47] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Int. Conf. Learn. Repr., 2015. [48] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual networks,” in Eur. Conf. Comput. Vision. Springer, 2016, pp. 630–645. [49] A. D. Zayas and P. Merino, “The 3GPP NB-IoT system architecture for the internet of things,” in Proc. IEEE Int. Conf. Commun. Workshop, May 2017, pp. 277–282. [50] M. Moldoveanu and A. Zaidi, “In-network learning for distributed training and inference in networks,” in Proc. IEEE GLOBECOM, Dec. 2021.

Yuyi Mao (Member, IEEE) received the B.Eng. degree in Information and Communication Engineering from Zhejiang University (ZJU), Hangzhou, China, in 2013, and the Ph.D. degree in Electronic and Computer Engineering from the Hong Kong University of Science and Technology (HKUST), Hong Kong, in 2017. He was a Lead Engineer with the Hong Kong Applied Science and Technology Research Institute Co., Ltd. (ASTRI), Hong Kong, and a Senior Researcher with the Theory Lab, 2012 Labs, Huawei Tech. Investment Co., Ltd., Hong Kong. He is currently a Research Assistant Professor with the Department of Electronic and Information Engineering, the Hong Kong Polytechnic University (PolyU), Hong Kong. His research interests include wireless communications and networking, mobile edge computing and learning, and wireless artificial intelligence. He was the recipient of the 2021 IEEE Communications Society Best Survey Paper Award and the 2019 IEEE Communications Society and Information Theory Society Joint Paper Award. He was also recognized as an Exemplary Reviewer of the IEEE Wireless Communications Letters in 2021 and 2019, and the IEEE Transactions on Communications in 2020.
Jun Zhang (Fellow, IEEE) received the B.Eng. degree in Electronic Engineering from the University of Science and Technology of China in 2004, the M.Phil. degree in Information Engineering from the Chinese University of Hong Kong in 2006, and the Ph.D. degree in Electrical and Computer Engineering from the University of Texas at Austin in 2009. He is an Associate Professor in the Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology. His research interests include wireless communications and networking, mobile edge computing and edge AI, and cooperative AI. Dr. Zhang co-authored the book Fundamentals of LTE (Prentice-Hall, 2010). He is a co-recipient of several best paper awards, including the 2021 Best Survey Paper Award of the IEEE Communications Society, the 2019 IEEE Communications Society & Information Theory Society Joint Paper Award, and the 2016 Marconi Prize Paper Award in Wireless Communications. Two papers he co-authored received the Young Author Best Paper Award of the IEEE Signal Processing Society in 2016 and 2018, respectively. He also received the 2016 IEEE ComSoc Asia-Pacific Best Young Researcher Award. He is an Editor of IEEE Transactions on Communications, and was an editor of IEEE Transactions on Wireless Communications (2015-2020). He served as a MAC track co-chair for IEEE Wireless Communications and Networking Conference (WCNC) 2011 and a co-chair for the Wireless Communications Symposium of IEEE International Conference on Communications (ICC) 2021.

Jiawei Shao (Student Member, IEEE) received the B.Eng. degree in telecommunication engineering from Beijing University of Posts and Telecommunications in 2019. He is currently pursuing a Ph.D. degree at Hong Kong University of Science and Technology. His research interests include edge intelligence and federated learning.

© 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: KAUST. Downloaded on October 11,2022 at 08:54:29 UTC from IEEE Xplore. Restrictions apply.

